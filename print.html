<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>PRML-exercise-solution</title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="index.html">TOP</a></li><li class="chapter-item expanded "><a href="ch01.html"><strong aria-hidden="true">1.</strong> 第1章</a></li><li class="chapter-item expanded "><a href="ch02.html"><strong aria-hidden="true">2.</strong> 第2章</a></li><li class="chapter-item expanded "><a href="ch03.html"><strong aria-hidden="true">3.</strong> 第3章</a></li><li class="chapter-item expanded "><a href="ch04.html"><strong aria-hidden="true">4.</strong> 第4章</a></li><li class="chapter-item expanded "><a href="ch05.html"><strong aria-hidden="true">5.</strong> 第5章</a></li><li class="chapter-item expanded "><a href="ch06.html"><strong aria-hidden="true">6.</strong> 第6章</a></li><li class="chapter-item expanded "><a href="ch07.html"><strong aria-hidden="true">7.</strong> 第7章</a></li><li class="chapter-item expanded "><a href="ch08.html"><strong aria-hidden="true">8.</strong> 第8章</a></li><li class="chapter-item expanded "><a href="ch09.html"><strong aria-hidden="true">9.</strong> 第9章</a></li><li class="chapter-item expanded "><a href="ch10.html"><strong aria-hidden="true">10.</strong> 第10章</a></li><li class="chapter-item expanded "><a href="ch11.html"><strong aria-hidden="true">11.</strong> 第11章</a></li><li class="chapter-item expanded "><a href="ch12.html"><strong aria-hidden="true">12.</strong> 第12章</a></li><li class="chapter-item expanded "><a href="ch13.html"><strong aria-hidden="true">13.</strong> 第13章</a></li><li class="chapter-item expanded "><a href="ch14.html"><strong aria-hidden="true">14.</strong> 第14章</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title">PRML-exercise-solution</h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                        

                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="prmlの日本語解答例"><a class="header" href="#prmlの日本語解答例">PRMLの日本語解答例</a></h1>
<p>PRMLの勉強会のときに作成した解答例です。不備や修正すべき点がありましたら
https://github.com/YoshitakaMo/prml-exercise-solution
こちらのissueやPull Requestを使って知らせてください。</p>
<h2 id="上巻ch01-05勉強会"><a class="header" href="#上巻ch01-05勉強会">上巻(ch01-05)勉強会</a></h2>
<p>当研究室の学生で作成しました。</p>
<h2 id="下巻ch06-14勉強会"><a class="header" href="#下巻ch06-14勉強会">下巻(ch06-14)勉強会</a></h2>
<p><a href="https://twitter.com/ag_smith">私</a>, ふたぱらさん, <a href="https://twitter.com/AunderbTommy888">とみーさん</a>, <a href="https://twitter.com/levinthal_prdx">やまを</a>さん, <a href="https://twitter.com/tubuann_only">十勝餡粒々</a>さん, <a href="https://twitter.com/Vincent3711">Vincent37</a>さん, <a href="https://twitter.com/kmd252525">くまもん</a>さん, oguraさん, Matsuiさんで作成しました。</p>
<h2 id="references"><a class="header" href="#references">References</a></h2>
<p>勉強会中では以下のサイトを大いに参考にしました。</p>
<ul>
<li>PRML公式解答例 Tutors' edition</li>
<li><a href="http://sioramen.sub.jp/prml_wiki/doku.php">ゆっくりPRMLしていってね</a></li>
<li><a href="http://prml.yutorihiro.com/">PRML演習問題 全問解答</a></li>
<li><a href="https://github.com/zhengqigao/PRML-Solution-Manual">Solution Manual For PRML</a></li>
<li><a href="https://tips-memo.com/prml">PRML演習問題解答を全力で分かりやすく解説！</a></li>
<li><a href="https://restus.co.jp/prml/solutions-to-exercises.pdf">パターン認識と機械学習演習問題解答 合同会社レスタス</a></li>
<li>他色々……</li>
</ul>
<h2 id="amazonリンク"><a class="header" href="#amazonリンク">Amazonリンク</a></h2>
<ul>
<li><a href="https://www.amazon.co.jp/dp/4621061224">PRML上巻</a></li>
<li><a href="https://www.amazon.co.jp/dp/4621061240">PRML下巻</a></li>
</ul>
<h2 id="contribution"><a class="header" href="#contribution">Contribution</a></h2>
<p>このtutorialは<a href="https://github.com/rust-lang-nursery/mdBook">mdbook</a>で運用しています.</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="prml第1章演習問題解答"><a class="header" href="#prml第1章演習問題解答">PRML第1章演習問題解答</a></h1>
<head>
<style>
  div.panel-primary {
	border: 1px solid #000;
    margin: 10px 5px;
    padding: 16px 10px 0px;
  }
</style>
</head>
<h2 id="演習-11"><a class="header" href="#演習-11">演習 1.1</a></h2>
<div class="panel-primary">
<p>関数$y(x, \mathbf{w})$が多項式$(1.1)$</p>
<p>$$y(x, \mathbf{w})=w_{0}+w_{1} x+w_{2} x^{2}+\cdots+w_{M} x^{M}=\sum_{j=0}^{M} w_{j} x^{j}$$</p>
<p>で与えられたときの$(1.2)$</p>
<p>$$
E(\mathrm{w})=\frac{1}{2} \sum_{n=1}^{N}\left{y\left(x_{n}, \mathbf{w}\right)-t_{n}\right}^{2}
$$</p>
<p>の二乗和誤差関数を考える。この誤差関数を最小にする係数$\mathbf{w}=\left{w_{i}\right}$は以下の線形方程式の解として与えられることを示せ．</p>
<p>$$\sum_{j=0}^{M} A_{i j} w_{j}=T_{i} \tag{1.122}$$</p>
<p>ただし、
$$
A_{i j}=\sum_{n=1}^{N}\left(x_{n}\right)^{i+j},\hspace{1em}T_{i}=\sum_{n=1}^{N}\left(x_{n}\right)^{i} t_{n} \tag{1.123}
$$
ここで，下付き添え字の$i$や$j$は成分を表し，$(x)^j$は$x$の$j$乗を表す．</p>
</div>
<p>$(1.2)$に$(1.1)$式を代入すると
$$
E(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N}\left{\sum_{j=0}^{M} w_{j} {x_{n}}^{j}-t_{n}\right}^{2}
$$
$E(\mathbf{w})$を最小にする$w_i$を求めるための微分を行うと
$$
\begin{aligned} \frac{\partial E}{\partial w_{i}} &amp;=\frac{1}{2} \sum_{n=1}^{N}\left{2\left(\sum_{j=0}^{N} w_{j} x_{n}^{j}-t_{n}\right) x_{n}^{i}\right} \ &amp;=\sum_{n=1}^{N}\left(x_{n}^{i} \sum_{j=0}^{M} w_{j} x_{n}^{j}\right)-\sum_{n=1}^{N} {t_n} {x_{n}}^{i} \end{aligned}
$$
$\frac{\partial E}{\partial w_{i}}=0$となるために,
$$
\sum_{n=1}^{N}\left({x_{n}}^{i} \sum_{j=0}^{M} w_{j} x_{n}^j \right)=\underbrace{\sum_{n=1}^{N} t_{n} {x_{n}}^{i}}<em>{T</em>{i}}
$$</p>
<p>$$
\begin{aligned}(左辺) &amp;=\sum_{n=1}^{N} {x_{n}}^{i}\left(w_{0} {x_{n}}^{0}+w_{1} {x_{n}}^{1}+\cdots+w_{M} {x_{n}}^{M}\right) \ &amp;=\sum_{n=1}^{N} (w_{0} {x_{n}}^{i}+w_{1} {x_{n}}^{i+1}+\cdots+w_{M} {x_n}^{i+M}) \ &amp;=\sum_{n=1}^{N} \sum_{j=0}^{M} {x_{n}}^{i+j} w_{j} \ &amp;=\sum_{j=0}^{M} (\sum_{n=1}^{N} {x_{n}}^{i+j} w_{j}) \ &amp;=\sum_{j=0}^{M} A_{i j} w_{j} \end{aligned}
$$</p>
<p>よって示された。</p>
<h2 id="演習-12"><a class="header" href="#演習-12">演習 1.2</a></h2>
<div class="panel-primary">
<p>正則化された二乗和誤差関数$(1.4)$</p>
<p>$$\widetilde{E}(\mathrm{w})=\frac{1}{2} \sum_{n=1}^{N}\left{y\left(x_{n}, \mathrm{w}\right)-t_{n}\right}^{2}+\frac{\lambda}{2}|\mathrm{w}|^{2}$$</p>
<p>を最小にする係数$w_i$が満たす,$(1.122)$
$$\sum_{j=0}^{M} A_{i j} w_{j}=T_{i}$$
に類似した線形方程式系を書き下せ．</p>
</div>
<p>1.1 と同様に行う。</p>
<p>$$
\tilde{E}(w)=\frac{1}{2} \sum_{n=1}^{N}\left(\sum_{j=0}^{M} w_{i} x_{n}^{j}-t_{n}\right)^{2}+\frac{\lambda}{2}|\mathrm{w}|^{2}
$$</p>
<p>$$
0=\frac{\partial \tilde{E}}{\partial w_{i}}=\sum_{n=1}^{N} x_{n}^{i} \sum_{j=0}^{M} w_{j} x_{n}^{j}+\lambda w_{i}-\sum_{n=1}^{N} x_{n}^{i} t_{n}
$$
整理すると</p>
<p>$$
\sum_{n=1}^{N}\left(x_{n}^{i} \sum_{j=0}^{n} w_{j} x_{n}^{j}\right)=T_{i}-\lambda w_{i} \
$$
$$
\sum_{j=0}^{M} \sum_{n=1}^{N} x_{n}^{i+j} w_{j}=\underbrace{T_{i}-\lambda w_{j}}<em>{T</em>{i}^\prime} \
$$
$$
\sum_{j=0}^{M} A_{i j} w_{j}=T_{i}^\prime
$$</p>
<p>※別例（演習模範回答ーパターン認識と機械学習完全版を参考）</p>
<p>$$
{\tilde{A}<em>{i j}}=A</em>{i j}+\lambda I_{i j}
$$
と置くことにする。</p>
<p>$\frac{\partial \tilde{E}}{\partial w_{i}}=0$となるためには、演習1.1同様に考えると、</p>
<p>$$
\sum_{j=0}^{M} A_{i j} w_{j}+\lambda w_{i}＝\underbrace{\sum_{n=1}^{N} t_{n} {x_{n}}^{i}}<em>{T</em>{i}}
$$
であれば良い。</p>
<p>ここで上式を代入すると、
$$
\sum_{j=0}^{M} \tilde{A}<em>{i j} w</em>{j}＝T_{i}
$$
と書ける。</p>
<h2 id="演習-13"><a class="header" href="#演習-13">演習 1.3</a></h2>
<div class="panel-primary">
<p>3個の色分けされた箱$r$(赤)， $b$(青)， $g$(緑)を考える. 箱$r$には3個のりんご，4個のオレンジ，3個のライムが入っており，箱$b$には1個のりんご， 1個のオレンジ, 0個のライムが入っており， 箱$g$には3個のりんご， 3個のオレンジ, 4個のライムが入っている箱を$p(r)=0.2, p(b)=0.2, p(g)=0.6$という確率でランダムに選び，果物を箱から1個取り出す（箱の中のものは等確率で選ばれるとする）とき，りんごを選び出す確率を求めよ．また，選んだ果物がオレンジであったとき，それが緑の箱から取り出されたものである確率はいくらか？</p>
</div>
<p>りんご, オレンジ, ライムを取り出すという事象をそれぞれ$A$, $O$, $L$とすると,</p>
<p>$$
p(A)=p(r) p(A|r)+p(b) p(A|b)+p(g) p(A|g)=0.34
$$</p>
<p>また, 同様にオレンジを選び出す確率$p(O)$は,</p>
<p>$$
\begin{aligned}
p(O) &amp;=p(O | r) p(r)+p(O | b) p(b)+p(O|g) p(g) \
&amp;=\frac{4}{10} \times 0.2+\frac{1}{2} \times 0.2+\frac{3}{10} \times 0.6=0.36
\end{aligned}
$$</p>
<p>求める値「選んだ果物がオレンジであったとき，それが緑の箱から取り出されたものである確率」はベイズの定理を用いると</p>
<p>$$
p(g|O)=\frac{p(O|g) p(g)}{p(O)}=\frac{3}{10}\times\frac{0.6}{0.36}=0.5
$$</p>
<p>となる。</p>
<h2 id="演習-14"><a class="header" href="#演習-14">演習 1.4</a></h2>
<div class="panel-primary">
<p>連続変数$x$上で定義された確率密度$p_x(x)$を考える. $x=g(y)$により非線形変換を施すと密度は</p>
<p>$$
\begin{aligned}
p_{y}(y) &amp;=p_{x}(x)\left|\frac{\mathrm{d} x}{\mathrm{d} y}\right| \
&amp;=p_{x}(g(y))\left|g^{\prime}(y)\right|
\end{aligned} \tag{1.27}
$$</p>
<p>の変換を受ける．$(1.27)$を微分して$y$に関する密度を最大にする位置$\widehat{y}$と$x$に関する密度を最大にする位置$\widehat{x}$とが，ヤコビ因子の影響により一般には単純な$\widehat{x}=g(\widehat{y})$という関係にないことを示せ．これは確率密度の最大値が，（通常の関数と異なり）変数の選択に依存することを示している．線形変換の場合には最大値の位置が変数自身と同じ変換を受けることを確かめよ．</p>
</div>
定義より
<p>$$p'_x(\hat{x})、p'_y(\hat{y})=0$$</p>
<p>である。</p>
<p>$p_y(y)=p_x(g(y))|g'(y)|$を$y$で微分すると$p'_y(y)=sp'_x(g(y))(g'(y))^2+sp_x(g(y))g''(y)$である。（ただし、$s=-1$ or $+1$である)</p>
<p>$x=g(y)$より、$\hat{x}$のときの$y$を$\grave{y}$とすると$\hat{x}=g(\grave{y})$である。</p>
<p>$$
p'_y(\grave{y})=sp'_x(g(\grave{y}))(g'(\grave{y}))^2+sp_x(g(\grave{y}))g''(\grave{y})
$$</p>
<p>$p'_x(\hat{x})=p'_x(g(\grave{y}))=0$であるから</p>
<p>$$
p'_y(\grave{y})=0+sp_x(g(\grave{y}))g''(\grave{y})
$$</p>
<p>一般に$sp_x(g(\grave{y}))g''(\grave{y}) \ne 0$であるので$p'_y(\grave{y})\ne 0$よって$\grave{y} \ne \hat{y}$となり$\hat{x}=g(\hat{y})$はかならずしも成り立たない。</p>
<p>ただし、$x=g(y)$が線形変換の場合、$g''(y)=0$となるので$p'_y(\grave{y})=0$となり、$\grave{y}=\hat{y}$であるから$\hat{x}=g(\hat{y})$になる。</p>
<h2 id="演習-15"><a class="header" href="#演習-15">演習 1.5</a></h2>
<div class="panel-primary">
<p>$(1.38)$の定義</p>
<p>$$
\operatorname{var}[f]=\mathbb{E}\left[(f(x)-\mathbb{E}[f(x)])^{2}\right] \tag{1.38}
$$</p>
<p>を使って$\mathrm{var}[f(x)]$が</p>
<p>$$
\operatorname{var}[f]=\mathbb{E}\left[f(x)^{2}\right]-\mathbb{E}[f(x)]^{2} \tag{1.39}
$$</p>
<p>を満たすことを示せ．</p>
</div>
<p>$$
\begin{aligned}
\operatorname{var}[f(x)]&amp;=\mathbb{E}\left[(f(x)-\mathbb{E}[f(x)])^{2}\right] \
&amp;= \mathbb{E}\left[ f(x)^2 + \left( \mathbb{E}[f(x)] \right)^2-2f(x)\mathbb{E}[f(x)]\right] \
&amp;= \mathbb{E}[f(x)^2] + \left( \mathbb{E}[f(x)]\right)^2 - 2\mathbb{E}[f(x)]\cdot \mathbb{E}[f(x)] \
&amp;= \mathbb{E}[f(x)^2] - \mathbb{E}[f(x)]^2
\end{aligned}
$$</p>
<p>よって$(1.39)$式が示された．</p>
<h2 id="演習-16"><a class="header" href="#演習-16">演習 1.6</a></h2>
<div class="panel-primary">
<p>2つの変数$x, y$が独立なら，それらの共分散は$0$になることを示せ．</p>
</div>
<p>$f(x), f(y), f(x, y)$を変数$x,y$の確率密度関数と同時確率密度関数としよう,
変数$x,y$は独立なので</p>
<p>$$
f(x, y)=f(x)f(y)
$$</p>
<p>この式を$x,y$の共分散に代入すると</p>
<p>$$
\begin{aligned}
\operatorname{cov}\left(x, y\right) &amp;= \mathbb{E}<em>{x,y}[\left{x-\mathbb{E}\left[x\right]\right}\left{y-\mathbb{E}\left[y\right]\right}]\
&amp;= \mathbb{E}</em>{x,y}\left[x,y\right]-\mathbb{E}\left[x\right]\mathbb{E}\left[y\right]\
&amp;= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} xyf\left(x,y\right)dxdy -\mathbb{E}\left[x\right]\mathbb{E}\left[y\right]\
&amp;= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} xyf\left(x\right)f\left(y\right)dxdy -\mathbb{E}\left[x\right]\mathbb{E}\left[y\right]\
&amp;= \int_{-\infty}^{\infty}xf\left(x\right)dx \cdot \int_{-\infty}^{\infty}yf\left(y\right)dy - \mathbb{E}\left[x\right]\mathbb{E}\left[y\right]\
&amp;= \mathbb{E}\left[x\right]\mathbb{E}\left[y\right] - \mathbb{E}\left[x\right]\mathbb{E}\left[y\right]\
&amp;= 0
\end{aligned}
$$
となる.</p>
<h2 id="演習-17"><a class="header" href="#演習-17">演習 1.7</a></h2>
<div class="panel-primary">
<p>この演習問題では,1変数ガウス分布に関する規格化条件</p>
<p>$$
\int_{-\infty}^{\infty} \mathcal{N}\left(x \mid \mu, \sigma^{2}\right) \mathrm{d} x=1 \tag{1.48}
$$</p>
<p>を証明する. このために, 積分
$$I=\int_{-\infty}^{\infty} \exp \left(-\frac{1}{2 \sigma^{2}} x^{2}\right) \mathrm{d} x　\tag{1.124}$$
を考え，その2乗を
$$I^{2}=\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \exp \left(-\frac{1}{2 \sigma^{2}} x^{2}-\frac{1}{2 \sigma^{2}} y^{2}\right) d x d y　\tag{1.125}$$
の形で書いて評価する. 直交座標系$(x,y)$から極座標$(r,\theta)$に変換し, $u=r^2$を代入する. $\theta$と$u$に関する積分を実行し, 両辺の平方根を取ることにより,
$$I=\left(2 \pi \sigma^{2}\right)^{1 / 2}　\tag{1.126}$$
が得られることを示せ.最後にこの結果からガウス分布$\mathcal{N}\left(x | \mu, \sigma^{2}\right)$が規格化されていることを示せ.</p>
</div>
<p>極座標系$x=r\cos\theta, y=r\sin\theta$とおいて、(1.125)式に代入すると、$f(r,\theta)=f(x,y)\left| \frac{\partial(x,y)}{\partial(r,\theta)} \right|$なので、このヤコビアン$\left| \frac{\partial(x,y)}{\partial(r,\theta)} \right|$は</p>
<p>$$
\frac{\partial(x, y)}{\partial(r, \theta)}=\left|\begin{array}{ll}\frac{\partial x}{\partial r} &amp; \frac{\partial x}{\partial \theta} \
\frac{\partial y}{\partial r} &amp; \frac{\partial y}{\partial \theta}\end{array}\right|=\left|\begin{array}{cc}\cos \theta &amp; -r \sin \theta \
\sin \theta &amp; r \cos \theta\end{array}\right|=r\left(\cos ^{2} \theta+\sin ^{2} \theta\right)=r
$$</p>
<p>なので、</p>
<p>$$
I^2 = \int_{0}^{2\pi}\int_{0}^{\infty}\exp\left(-\frac{1}{2\sigma^2}r^2 \right) r dr d\theta
$$</p>
<p>$u=r^2$と変換すると、$du=2rdr$なので</p>
<p>$$
\begin{aligned}
I^2 &amp;= \int_{0}^{2\pi}\int_{0}^{\infty}\exp\left(-\frac{1}{2\sigma^2}u \right) \cdot \frac{1}{2}du d\theta \
&amp;= \pi \int_{0}^{\infty}\exp{\left( -\frac{1}{2\sigma^2}u\right)}dud\theta \
&amp;= \pi \left[ \exp{\left(-\frac{1}{2\sigma^2}u\right)}\cdot (-2\sigma^2)\right]_{0}^{\infty} \
&amp;= -2\pi\sigma^2(e^{-\infty} - e^0) = 2\pi\sigma^2
\end{aligned}
$$</p>
<p>両辺の平方根をとって式$(1.126)$の$I=\sqrt{2\pi\sigma^2}$を得る。</p>
<p>また、ガウス積分の式は積分範囲が$\mu$だけずれても$-\infty \to \infty$は変わらないので、</p>
<p>$$
\int_{-\infty}^{\infty} \mathcal{N}\left(x | \mu, \sigma^{2}\right) dx=\frac{1}{\left(2 \pi \sigma^{2}\right)^{1 / 2}} \int_{-\infty}^{\infty} \exp \left{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right}  dx= \frac{1}{\sqrt{2\pi\sigma^2}} \cdot \sqrt{2\pi\sigma^2}=1
$$</p>
<p>となり、正規化されていることが示された。</p>
<h2 id="演習-18"><a class="header" href="#演習-18">演習 1.8</a></h2>
<div class="panel-primary">
<p>変数変換を使って1変数ガウス分布</p>
<p>$$
\mathcal{N}\left(x \mid \mu, \sigma^{2}\right)=\frac{1}{\left(2 \pi \sigma^{2}\right)^{1 / 2}} \exp \left{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right} \tag{1.46}
$$</p>
<p>が</p>
<p>$$
\mathbb{E}[x]=\int^{\infty}_{-\infty} \mathcal{N}\left(x \mid \mu, \sigma^{2}\right) x \mathrm{~d} x=\mu \tag{1.49}
$$</p>
<p>を満たすことを確かめよ. 次に, 規格化条件</p>
<p>$$\int_{-\infty}^{\infty} \mathcal{N}\left(x | \mu, \sigma^{2}\right) \mathrm{d} x=1 \tag{1.48}$$</p>
<p>の両辺を$\sigma^2$に関して微分し，ガウス分布が</p>
<p>$$
\mathbb{E}\left[x^{2}\right]=\int_{-\infty}^{\infty} \mathcal{N}\left(x \mid \mu, \sigma^{2}\right) x^{2} \mathrm{~d} x=\mu^{2}+\sigma^{2} \tag{1.50}
$$</p>
<p>を満たすことを確かめよ. 最後に</p>
<p>$$
\operatorname{var}[x]=\mathbb{E}\left[x^{2}\right]-\mathbb{E}[x]^{2}=\sigma^{2} \tag{1.51}
$$</p>
<p>が成り立つことを示せ.</p>
</div>
<p>$(1.46)$より</p>
<p>$$\mathbb{E}[x] = \int_{-\infty}^{\infty}\frac{x}{\sqrt{2\pi\sigma^2}}\exp{(-\frac{1}{2\sigma^2}(x-\mu)^2)}dx$$</p>
<p>ここで、$y=x-\mu$とおくと</p>
<p>$$
\begin{aligned}
\mathbb{E}[x]&amp;=\int_{-\infty}^{\infty}\frac{y+\mu}{\sqrt{2\pi\sigma^2}}\exp{\left( -\frac{y^2}{2\sigma^2} \right)}dy \
&amp;=\int_{-\infty}^{\infty}\frac{y}{\sqrt{2\pi\sigma^2}}\exp{\left( -\frac{y^2}{2\sigma^2} \right)}dy+\mu\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left( -\frac{y^2}{2\sigma^2} \right)}dy \
&amp;=\int_{-\infty}^{\infty}\frac{y}{\sqrt{2\pi\sigma^2}}\exp{\left( -\frac{y^2}{2\sigma^2} \right)}dy+\mu\int_{-\infty}^{\infty}\mathcal{N}(y|0, \sigma^2)dy
\end{aligned}
$$</p>
<p>第1項は奇関数なので$0$であり、$(1.48)$より</p>
<p>$$\begin{aligned}\mathbb{E}[x]&amp;=0+\mu・1\&amp;=\mu\end{aligned}$$</p>
<p>ゆえに$(1.46)$は$(1.49)$を満たす。</p>
<p>次に規格化条件に関して、$(1.46)$と$(1.48)$より</p>
<p>$$\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left(-\frac{1}{2\sigma^2}(x-\mu)^2 \right)}dx=1$$</p>
<p>$$\int_{-\infty}^{\infty}\exp{\left( -\frac{1}{2\sigma^2}(x-\mu)^2 \right)}dx=\sqrt{2\pi\sigma^2}$$</p>
<p>両辺を$\sigma^2$で微分すると</p>
<p>$$\int_{-\infty}^{\infty}\frac{(x-\mu)^2}{2(\sigma^2)^2}\exp{(-\frac{1}{2\sigma^2}(x-\mu)^2)}dx=(2\pi)^\frac{1}{2}\frac{(\sigma^2)^-\frac{1}{2}}{2}$$</p>
<p>整理すると</p>
<p>$$\int_{-\infty}^{\infty}\frac{(x-\mu)^2}{\sqrt{2\pi\sigma^2}}\exp{(-\frac{1}{2\sigma^2}(x-\mu)^2)}dx=\sigma^2$$</p>
<p>ここで、$(左辺)=\mathbb{E}[(x-\mu)^2]$なので</p>
<p>$$\mathbb{E}[(x-\mu)^2]=\sigma^2$$
$$\mathbb{E}[x^2-2\mu x+\mu^2]=\sigma^2$$
$$\mathbb{E}[x^2]-2\mu\mathbb{E}[x]+\mu^2=\sigma^2$$</p>
<p>$(1.49)$より$\mathbb{E}[x]=\mu$なので</p>
<p>$$\mathbb{E}[x^2]-2\mu^2+\mu^2=\sigma^2$$</p>
<p>$$\mathbb{E}[x^2]=\mu^2+\sigma^2$$</p>
<p>ゆえにガウス分布は(1.50)を満たす。</p>
<p>また(1.51)について、(1.49)と(1.50)より</p>
<p>$$
\begin{aligned}
\operatorname{var}[x]&amp;=\mathbb{E}[x^2]-\mathbb{E}[x]^2\&amp;=(\mu^2+\sigma^2)-\mu^2\&amp;=\sigma^2
\end{aligned}
$$</p>
<h2 id="演習-19"><a class="header" href="#演習-19">演習 1.9</a></h2>
<div class="panel-primary">
<p>ガウス分布</p>
<p>$$
\mathcal{N}\left(x \mid \mu, \sigma^{2}\right)=\frac{1}{\left(2 \pi \sigma^{2}\right)^{1 / 2}} \exp \left{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right} \tag{1.46}
$$</p>
<p>のモード（つまり分布が最大となる$x$の値）が, $\mu$で与えられることを示せ. 同様に, 多変量ガウス分布</p>
<p>$$
\mathcal{N}\left(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}\right) = \frac{1}{(2\pi)^\frac{D}{2}}\frac{1}{|\mathbf{\Sigma}|^\frac{1}{2}}\exp\left{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\mathrm{T}\mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right} \tag{1.52}
$$</p>
<p>のモードは$\boldsymbol{\mu}$で与えられることを示せ.</p>
</div>
<p>$$\mathcal{N}\left(x | \mu, \sigma^{2}\right) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left{-\frac{1}{2\sigma^2}(x-\mu)^2\right}$$</p>
<p>の概形から$x$のモードは$\exp$の中身が最大の時、つまり$x=\mu$となるような$x$であるとわかる。厳密には$x$について微分をとって0になるときを計算すればよい。</p>
<p>また、多変量ガウス分布</p>
<p>$$\mathcal{N}\left(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}\right) = \frac{1}{(2\pi)^\frac{D}{2}}\frac{1}{|\mathbf{\Sigma}|^\frac{1}{2}}\exp\left{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\mathrm{T}\mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right}$$</p>
<p>について、これを$\mathbf{x}$で偏微分すると</p>
<p>$\frac{\partial}{\partial \mathbf{x}} \mathbf{x}^\mathrm{T}\mathbf{a}\mathbf{x} = (\mathbf{a}+\mathbf{a}^\mathrm{T})\mathbf{x}$ (1)と$\mathbf{\Sigma}$が対称行列であることを用いて</p>
<p>$$
\begin{aligned}
\frac{\partial}{\partial \mathbf{x}}\mathcal{N}\left(\mathbf{x} | \mu, \mathbf{\Sigma}\right) &amp;= -\frac{1}{2}\mathcal{N}\left(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}\right)\nabla_{\mathbf{x}}{(\mathbf{x}-\boldsymbol{\mu})^\mathrm{T}\mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})}\
&amp;=-\mathcal{N}\left(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}\right)\mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})
\end{aligned}
$$
となり最大値は$\mathbf{x}=\boldsymbol{\mu}$のときであり、このときモードをとる。</p>
<hr />
<p>1,統計のための行列代数上巻 P.355(3.7)式参照</p>
<h2 id="演習-110"><a class="header" href="#演習-110">演習 1.10</a></h2>
<div class="panel-primary">
<p>2変数$x,z$が統計的に独立であるとする. それらの和の平均と分散が</p>
<p>$$
\begin{aligned} \mathbb{E}[x+z] &amp;=\mathbb{E}[x]+\mathbb{E}[z] \ \operatorname{var}[x+z] &amp;=\operatorname{var}[x]+\operatorname{var}[z] \end{aligned}
$$</p>
<p>を満たすことを示せ。</p>
</div>
<p>$x$と$z$は独立より $p(x, z) = p(x)p(z)$ であることに注意すると</p>
<p>$\mathbb{E}[x+z]$ は</p>
<p>$$
\begin{aligned}
\mathbb{E}[x+z] &amp;= \int\int(x+z)p(x, z)\mathrm{d}x\mathrm{d}z \
&amp;= \int\int(x+z)p(x)p(z)\mathrm{d}x\mathrm{d}z \
&amp;= \int\int xp(x)p(z)\mathrm{d}x\mathrm{d}z + \int\int zp(x)p(z)\mathrm{d}x\mathrm{d}z \
&amp;= \int xp(x)\mathrm{d}x \int p(z)\mathrm{d}z + \int zp(z)\mathrm{d}z \int p(x)\mathrm{d}x \
&amp;= \int xp(x)\mathrm{d}x + \int zp(z)\mathrm{d}z \
&amp;= \mathbb{E}[x] + \mathbb{E}[z]
\end{aligned}
$$</p>
<p>となる。</p>
<p>また、上式を利用すると$\mathrm{var}[x+z]$は</p>
<p>$$
\begin{aligned}
\mathrm{var}[x+z] &amp;= \mathbb{E}[{(x+z) - \mathbb{E}[x+z]}^2] \
&amp;= \int\int(x+z - \mathbb{E}[x+z])^2p(x, z)\mathrm{d}x\mathrm{d}z \
&amp;= \int\int(x-\mathbb{E}[x] + z-\mathbb{E}[z])^2p(x)p(z)\mathrm{d}x\mathrm{d}z \
&amp;= \int\int{(x-\mathbb{E}[x])^2 + (z-\mathbb{E}[z])^2 + 2(x-\mathbb{E}[x])(z-\mathbb{E}[z])}p(x)p(z)\mathrm{d}x\mathrm{d}z \
&amp;= \int(x-\mathbb{E}[x])^2p(x)\mathrm{d}x\int p(z)\mathrm{d}z + \int(z-\mathbb{E}[z])^2p(z)\mathrm{d}z\int p(x)\mathrm{d}x \&amp;+ 2\int\int(x-\mathbb{E}[x])(z-\mathbb{E}[z])p(x)p(z)\mathrm{d}x\mathrm{d}z \
\end{aligned}
$$</p>
<p>ここで第3項について</p>
<p>$$
\begin{aligned}
(第3項)
&amp;=2\int\int(x-\mathbb{E}[x])(z-\mathbb{E}[z])p(x)p(z)\mathrm{d}x\mathrm{d}z \
&amp;= 2\int(x-\mathbb{E}[x])p(x)\mathrm{d}x\int(z-\mathbb{E}[z])p(z)\mathrm{d}z \
&amp;= 2(\mathbb{E}[x]-\mathbb{E}[x])(\mathbb{E}[z]-\mathbb{E}[z]) = 0
\end{aligned}
$$</p>
<p>であるため</p>
<p>$$
\begin{aligned}
\mathrm{var}[x+z] &amp;= \int(x-\mathbb{E}[x])^2p(x)\mathrm{d}x\int p(z)\mathrm{d}z + \int(z-\mathbb{E}[z])^2p(z)\mathrm{d}z\int p(x)\mathrm{d}x \
&amp;= \mathrm{var}[x] + \mathrm{var}[z]
\end{aligned}
$$</p>
<p>となる。</p>
<h2 id="演習-111"><a class="header" href="#演習-111">演習 1.11</a></h2>
<div class="panel-primary">
<p>対数尤度関数</p>
<p>$$
\begin{aligned}
\ln p(\bf{x}|\mu,\sigma^2)
&amp;= - \frac{1}{2\sigma^2} \sum_{n=1}^{N} (x_n-\mu)^2 - \frac{N}{2} \ln \sigma^2 - \frac{N}{2} \ln (2\pi)
\end{aligned} \tag{1.54}
$$</p>
<p>の$\mu$と$\sigma^2$に関する微分を$0$とおいて，</p>
<p>$$
\begin{aligned}
\mu_{\mathrm{ML}} &amp;= \frac{1}{N} \sum_{n=1}^{N} x_n
\end{aligned} \tag{1.55}
$$</p>
<p>と</p>
<p>$$
\begin{aligned}
\sigma^2_{\mathrm{ML}} &amp;= \frac{1}{N} \sum_{n=1}^{N} (x_n-\mu)^2
\end{aligned} \tag{1.56}
$$</p>
<p>を確かめよ．</p>
</div>
<p>対数尤度関数$(1.54)$を$f$と置く。
$\mu$で微分すると、第2・第3項は消えて</p>
<p>$$
\begin{aligned}
\frac{\partial f}{\partial \mu}
&amp;=
\frac{1}{\sigma^2} \sum_{n=1}^{N} (x_n-\mu) =
\frac{1}{\sigma^2} \sum_{n=1}^{N} x_n - \frac{1}{\sigma^2} N\mu
\end{aligned}
$$</p>
<p>これを0と置くことで、$\mu$の最尤解$(1.55)$</p>
<p>$$
\begin{aligned}
\mu_{\mathrm{ML}} &amp;= \frac{1}{N} \sum_{n=1}^{N} x_n
\end{aligned}
$$</p>
<p>を得る。</p>
<p>$\sigma^2$で微分すると、第3項は消えて</p>
<p>$$
\begin{aligned}
\frac{\partial f}{\partial \sigma^2}
&amp;=
\frac{1}{2(\sigma^2)^2} \sum_{n=1}^{N} (x_n-\mu)^2 - \frac{N}{2\sigma^2}
\end{aligned}
$$</p>
<p>これを0と置くことで、$\sigma^2$の最尤解$(1.56)$</p>
<p>$$
\begin{aligned}
\sigma^2_{\mathrm{ML}} &amp;= \frac{1}{N} \sum_{n=1}^{N} (x_n-\mu)^2
\end{aligned}
$$</p>
<p>を得る。</p>
<p>$\mu$と$\sigma^2$について同時に最尤推定を行うことで、$\mu_{\mathrm{ML}}$を$\sigma^2_{\mathrm{ML}}$に代入して</p>
<p>$$
\begin{aligned}
\sigma^2_{\mathrm{ML}} &amp;= \frac{1}{N} \sum_{n=1}^{N} (x_n-\mu_{\mathrm{ML}})^2
\end{aligned}
$$
を得る。</p>
<h2 id="演習-112"><a class="header" href="#演習-112">演習 1.12</a></h2>
<div class="panel-primary">
<p>$$
\mathbb{E}[x]=\int_{-\infty}^{\infty} \mathcal{N}\left(x \mid \mu, \sigma^{2}\right) x \mathrm{~d} x=\mu \tag{1.49}
$$</p>
<p>と</p>
<p>$$
\mathbb{E}\left[x^{2}\right]=\int_{-\infty}^{\infty} \mathcal{N}\left(x \mid \mu, \sigma^{2}\right) x^{2} \mathrm{~d} x=\mu^{2}+\sigma^{2} \tag{1.50}
$$</p>
<p>を使って</p>
<p>$$
\mathbb{E}\left[x_{n} x_{m}\right]=\mu^{2}+I_{n m} \sigma^{2} \tag{1.130}
$$
を示せ．ただし，$x_n$と$x_m$は平均$\mu$，分散$\sigma^2$のガウス分布から生成されたデータ点を表し，$I_{nm}$は$n=m$のとき$I_{nm}=1$でそれ以外は$I_{nm}=0$であるとする．これから</p>
<p>$$
\mathbb{E}\left[\mu_{\mathrm{ML}}\right]=\mu \tag{1.57}
$$</p>
<p>と</p>
<p>$$
\mathbb{E}\left[\sigma_{\mathrm{ML}}^{2}\right]=\left(\frac{N-1}{N}\right) \sigma^{2} \tag{1.58}
$$</p>
<p>を証明せよ．</p>
</div>
<p>まず，$\mathbb E[x_nx_m]$について</p>
<p>$n=m$のとき</p>
<p>$$\mathbb E[x_nx_m] = \mathbb E[x_n^2] = \mu^2 + \sigma^2$$</p>
<p>$n \ne m$のとき，$x_n$と$x_m$は独立であるから</p>
<p>$$\mathbb E[x_nx_m] = \mathbb E[x_n]\mathbb E[x_m] = \mu^2$$</p>
<p>まとめると</p>
<p>$$
\mathbb E[x_nx_m] = \mu^2 + I_{nm}\sigma^2
$$</p>
<p>となる．次に，$\mathbb E[\mu_{\mathrm{ML}}]$について</p>
<p>$$
\begin{aligned}
\mathbb E[\mu_{\mathrm{ML}}] &amp;= \mathbb E\left[\frac{1}{N}\sum_{n=1}^N x_n\right] \
&amp;= \frac{1}{N}\sum_{n=1}^N \mathbb E\left[x_n\right] \
&amp;= \frac{1}{N}N \mu \
&amp;= \mu \
\end{aligned}
$$</p>
<p>となる．また，$\mathbb E[\sigma_{\mathrm{ML}}^2]$について</p>
<p>$$
\begin{aligned}
\mathbb E[\sigma_{\mathrm{ML}}^2] &amp;= \mathbb E\left[\frac{1}{N}\sum_{n=1}^N (x_n - \mu_{\mathrm{ML}})^2\right] \
&amp;= \mathbb E \left[\frac{1}{N}\sum_{n=1}^N \left(x_n - \frac{1}{N}\sum_{m=1}^N x_m\right)^2\right] \
&amp;= \frac{1}{N} \sum_{n=1}^N \mathbb E \left[x_n^2 - \frac{2}{N}x_n\sum_{m=1}^N x_m + \frac{1}{N^2}\left(\sum_{m=1}^N x_m\right)^2\right] \
&amp;= \frac{1}{N} \sum_{n=1}^N \left{\mathbb E [x_n^2] - \frac{2}{N} \mathbb E \left[x_n\sum_{m=1}^N x_m \right] + \frac{1}{N^2} \mathbb E \left[\left(\sum_{m=1}^N x_m\right)^2\right]\right} \
&amp;= \frac{1}{N} \sum_{n=1}^N \left{\mathbb E [x_n^2] - \frac{2}{N} \sum_{m=1}^N \mathbb E \left[x_n x_m \right] + \frac{1}{N^2} \mathbb E \left[\left(\sum_{m=1}^N x_m\right)^2\right]\right} \
\end{aligned}
$$</p>
<p>ここで</p>
<p>$$
\begin{aligned}
\left(\sum_{m=1}^N x_m\right)^2 &amp;= (x_1+x_2+\cdots+x_N)(x_1+x_2+\cdots+x_N) \
&amp;= (x_1^2+x_1x_2+\cdots+x_1x_N) + (x_1x_2+x_2^2+\cdots+x_2x_N) + \cdots + (x_1x_N+x_2x_N+\cdots+x_N^2)\
&amp;= \sum_{k=1}^N \sum_{l=1}^N x_k x_l
\end{aligned}
$$</p>
<p>より</p>
<p>$$
\begin{aligned}
\mathbb E[\sigma_{\mathrm{ML}}^2] &amp;= \frac{1}{N} \sum_{n=1}^N \left{\mathbb E [x_n^2] - \frac{2}{N} \sum_{m=1}^N \mathbb E \left[x_n x_m \right] + \frac{1}{N^2} \mathbb E \left[\sum_{k=1}^N \sum_{l=1}^N x_k x_l\right]\right} \
&amp;= \frac{1}{N} \sum_{n=1}^N \left{\mathbb E [x_n^2] - \frac{2}{N} \sum_{m=1}^N \mathbb E \left[x_n x_m \right] + \frac{1}{N^2} \sum_{k=1}^N \sum_{l=1}^N \mathbb E \left[x_k x_l\right]\right} \
&amp;= \frac{1}{N} \sum_{n=1}^N \left{(\mu^2 + \sigma^2) - \frac{2}{N} \left(\mu^2 + \sigma^2 + (N-1)\mu^2\right) + \frac{1}{N^2} \left(N(\mu^2 + \sigma^2) + N(N-1)\mu^2 \right)\right} \
&amp;= \frac{1}{N} \sum_{n=1}^N \left(\mu^2 + \sigma^2 - 2\mu^2 - \frac{2}{N} \sigma^2 + \mu^2 + \frac{1}{N} \sigma^2\right) \
&amp;= \frac{1}{N} \sum_{n=1}^N \left(\frac{N-1}{N} \sigma^2 \right) \
&amp;= \frac{1}{N} N \left(\frac{N-1}{N} \sigma^2 \right) \
&amp;= \frac{N-1}{N} \sigma^2 \
\end{aligned}
$$</p>
<p>となる．</p>
<h2 id="演習-113"><a class="header" href="#演習-113">演習 1.13</a></h2>
<div class="panel-primary">
<p>ガウス分布の分散の推定値</p>
<p>$$
\sigma_{\mathrm{ML}}^{2}=\frac{1}{N} \sum_{n=1}^{N}\left(x_{n}-\mu_{\mathrm{ML}}\right)^{2} \tag{1.56}
$$</p>
<p>において，最尤推定値$\mu_{\mathrm{ML}}$を真の平均の値$\mu$で置き換えよう．この推定量は期待値が真の分散$\sigma^2$となる性質を持つことを示せ．</p>
</div>
<p>$\mu_{\mathrm {ML}}$を真の平均$\mu$で置き換えたものを$\hat\sigma_{\mathrm{ML}}^2$とし，その期待値をとると</p>
<p>$$
\begin{aligned}
\mathbb E[\hat\sigma_{\mathrm{ML}}^2] &amp;= \mathbb E\left[\frac{1}{N}\sum_{n=1}^N (x_n - \mu)^2\right] \
&amp;= \frac{1}{N} \sum_{n=1}^N \mathbb E \left[x_n^2 - 2x_n\mu  + \mu^2 \right] \
&amp;= \frac{1}{N} \sum_{n=1}^N \left{\mathbb E [x_n^2] - 2\mu \mathbb E \left[x_n \right] + \mu^2 \right} \
&amp;= \frac{1}{N} \sum_{n=1}^N \left(\mu^2+\sigma^2 - 2\mu \mu + \mu^2 \right) \
&amp;= \sigma^2
\end{aligned}
$$</p>
<p>となる．</p>
<h2 id="演習-114"><a class="header" href="#演習-114">演習 1.14</a></h2>
<div class="panel-primary">
<p>$w_{ij}$を成分とする任意の正方行列は$w_{ij}=w_{ij}^{\mathrm S}$+$w_{ij}^{\mathrm A}$という形に書けることを示せ．ただし,$w_{ij}^{\mathrm S}$と$w_{ij}^{\mathrm A}$はそれぞれ対称行列と反対称行列の成分であり$w_{ij}^{\mathrm S}=w_{ji}^{\mathrm S}$および$w_{ij}^{\mathrm A}=-w_{ji}^{\mathrm A}$がすべての$i, j$について成り立つ．さてここで，$D$次元における高次の多項式の2次の項</p>
<p>$$
\sum_{i=1}^{D} \sum_{j=1}^{D} w_{i j} x_{i} x_{j} \tag{1.131}
$$</p>
<p>を考えると，</p>
<p>$$
\sum_{i=1}^{D} \sum_{j=1}^{D} w_{i j} x_{i} x_{j}=\sum_{i=1}^{D} \sum_{j=1}^{D} w_{i j}^{\mathrm S} x_{i} x_{j} \tag{1.132}
$$</p>
<p>となり，反対称行列の寄与が消えることを示せ．このことから，一般性を失うことなく，係数$w_{ij}$は対称に選んでよく，すべての$D^2$の成分の選び方が独立ではないことがわかる．これを使って，行列$w_{ij}^{\mathrm S}$の独立パラメータの数が$D(D+1)/2$で与えられることを示せ．</p>
</div>
<p>$w_{ij}=w_{ij}^{\mathrm S}+w_{ij}^{\mathrm A}$であることを示す。
これは任意の正方行列の$i$行$j$列めの成分を$w_{ij}$としたとき、これを使って$w_{ij}^{\mathrm S}=\frac{1}{2}(w_{ij}+w_{ji})$とおくと、$w_{ij}^{\mathrm S}$は常に対称行列となる。同様に、$w_{ij}^{\mathrm A}=\frac{1}{2}(w_{ij}-w_{ji})$とおくと、この$w_{ij}^{\mathrm A}$は常に反対称行列となる。これらを用いると</p>
<p>$$
w_{ij}=w_{ij}^{\mathrm S}+w_{ij}^{\mathrm A}
$$</p>
<p>となるので、<strong>任意の正方行列は、$w_{ij}^{\mathrm S}$を成分とする対称行列と、$w_{ij}^{\mathrm A}$を成分とする反対称行列の和で必ず表現できる</strong>ことが示された。</p>
<p>ここで$w_{ij}^{\mathrm A}$について、</p>
<p>$$
\sum_{i=1}^{D} \sum_{j=1}^{D} w_{ij}^{\mathrm A} x_{i} x_{j} = \sum_{i=1}^{D} \sum_{j=1}^{D} \frac{1}{2}(w_{ij}-w_{ji}) x_{i} x_{j} = 0
$$</p>
<p>となるため、</p>
<p>$$
\sum_{i=1}^{D} \sum_{j=1}^{D} w_{i j} x_{i} x_{j}=\sum_{i=1}^{D} \sum_{j=1}^{D} w_{i j}^{\mathrm S} x_{i} x_{j}
$$</p>
<p>が常に成り立つことが示される。また、この結果から<strong>二次形式の係数行列は対称行列とおいても一般性が失われない</strong>という重要な帰結を得ることができる。</p>
<p>$w_{i j}^{\mathrm S}$については、対角成分を挟んで成分が対称になっていなければならないので、この行列の独立なパラメータは$\displaystyle \sum_{n=1}^{D}n=\frac{D(D+1)}{2}$個である。</p>
<h2 id="演習-115難"><a class="header" href="#演習-115難">演習 1.15（難）</a></h2>
<div class="panel-primary">
<p>この演習問題と次の演習問題では，多項式の独立パラメータの数が多項式の次数$M$や入力空間の次元$D$に対してどのように増えるかを考える．まず，$D$次元の多項式の$M$次の項を書き下すと，</p>
<p>$$
\sum_{i_1=1}^{D} \sum_{i_2=1}^{D} \cdots \sum_{i_{M=1}}^{D} w_{i_{1} i_{2} \cdots i_{M}} x_{i_{1}} x_{i_{2}} \cdots x_{i_{M}} \tag{1.133}
$$</p>
<p>となる．係数$w_{i_{1}i_{2} \cdots i_{M}}$は$D^M$個あるが,そのうち独立なパラメータの数は$x_{i_1}x_{i_2} \cdots x_{i_M}$の多くの置換対称性からそれよりずっと少なくなる．始めに，$M$次の項を</p>
<p>$$
\sum_{i_{1}=1}^{D} \sum_{i_{2}=1}^{i_{1}} \cdots \sum_{i_{M}=1}^{i_{M-1}} \tilde{w}<em>{i</em>{1} i_{2} \cdots i_{M}} x_{i_{1}} x_{i_{2}} \cdots x_{i_{M}} \tag{1.134}
$$</p>
<p>と書き直すことによって係数の冗長性を取り除けることを示せ．ただし，$\tilde{w}$と$w$の厳密な関係は陽に表す必要はないことに注意せよ．この結果を使って，$M$次における独立なパラメータの数、$n(D, M)$が</p>
<p>$$
n(D, M)=\sum_{i=1}^{D} n(i, M-1) \tag{1.135}
$$</p>
<p>という再帰的な関係を満たすことを示せ．さらに，数学的帰納法により以下の結果が成り立つことを示せ．</p>
<p>$$
\sum_{i=1}^{D} \frac{(i+M-2) !}{(i-1) !(M-1) !}=\frac{(D+M-1) !}{(D-1) ! M !} \tag{1.136}
$$</p>
<p>これを示すには，まず$D=1$と任意の$M$の場合を$0!=1$を使って証明し，次に
$D$次元で成り立っていると仮定して，$D+1$次元でも成り立つことを確かめればよい．最後に，上の２つの結果から，数学的帰納法により</p>
<p>$$
n(D, M)=\frac{(D+M-1) !}{(D-1) ! M !} \tag{1.137}
$$</p>
<p>を示せ．これを示すには，まず$M=2$と任意の$D\ge 1$について正しいことを，演習問題1.14の結果との比較によって示し，次に，$(1.135)$と$(1.136)$を使って，$M-1$次で成り立てば$M$次でも成り立つことを示せばよい．</p>
</div>
<p>※ 問題文の意味が分かりにくいが、やろうとしていることは実はただの二項展開である。</p>
<p>問題が分かりにくいので、$D=4, M=2$として$x_1, x_2, x_3, x_4$の4次元の多項式について2次の項を書き下すことを考える。$(1.133)$の記法に従うと</p>
<p>$$
\sum_{i_1=1}^{4} \sum_{i_2=1}^{4} w_{i_{1} i_{2}} x_{i_{1}} x_{i_{2}}  \tag{1.133}
$$</p>
<p>と書けることになるが、この記法では例えば$w_{12}x_1 x_2$と$w_{21}x_2 x_1$が別々に現れて和をとる形になる。しかしかけ算の順序が交換可能であることを用いれば$(w_{12}+w_{21})x_2 x_1$とまとめて書くことができる。</p>
<p>そこで、$x$の添字が常に$i_1 \ge i_2 \ge \cdots i_M$となるようにし、その係数$w$の和をまとめて$\tilde{w}$と書き直す（上の例では$\tilde{w}<em>{21} = w</em>{12} + w_{21}$）ことでも一般性は失われない。</p>
<p>以上から、$(1.133)$式は</p>
<p>$$
\sum_{i_{1}=1}^{D} \sum_{i_{2}=1}^{i_{1}} \cdots \sum_{i_{M}=1}^{i_{M-1}} \tilde{w}<em>{i</em>{1} i_{2} \cdots i_{M}} x_{i_{1}} x_{i_{2}} \cdots x_{i_{M}} \tag{1.134}
$$</p>
<p>のように書き直せることがわかる（$\tilde{w}$と$w$の関係性を陽に表す必要はない）。</p>
<p>次に、この結果を用いると入力空間の次元$D$に対して$M$次における独立なパラメータの数というのは、すでに冗長性がなくなっているために</p>
<p>$$
\sum_{i_{1}=1}^{D} \sum_{i_{2}=1}^{i_{1}} \cdots \sum_{i_{M}=1}^{i_{M-1}} \tilde{w}<em>{i</em>{1} i_{2} \cdots i_{M}}
$$</p>
<p>の$\tilde{w}$の項の数と等しいことがわかる（※このへんでパスカルの三角形や二項展開のことが頭に思い浮かぶかもしれない）。すなわち</p>
<p>$$
n(D,M) = \sum_{i_{1}=1}^{D} \sum_{i_{2}=1}^{i_{1}} \cdots \sum_{i_{M}=1}^{i_{M-1}} 1
$$</p>
<p>となるので、あとはこれを求めれば良い。</p>
<p>この式について$D \to i$, $M \to M-1$とすると</p>
<p>$$
n(i, M-1)=\sum_{i_{1}=1}^{i} \sum_{i_{2}=1}^{i_{1}} \ldots \sum_{i_{(M-1)}=1}^{i_{(M-1)-1}} 1
$$</p>
<p>となるので、$(1.135)$式の右辺から計算すると</p>
<p>$$
\begin{aligned}
&amp; \sum_{i=1}^{D} n(i, M-1) \
=&amp; \sum_{i=1}^{D} \sum_{i_{1}=1}^{i} \sum_{i_{2}=1}^{i_{1}} \ldots \sum_{i_{M-1}=1}^{i_{(M-1)-1}} 1 \
=&amp; \sum_{i_{1}=1}^{D} \sum_{i_{2}=1}^{i_{1}} \sum_{i_{3}=1}^{i_{2}} \cdots \sum_{i_{M}=1}^{i_{M-1}} 1 \quad (\because i \rightarrow i_{1}, i_{n} \rightarrow i_{n+1}) \
=&amp;\ n(D, M)
\end{aligned}
$$</p>
<p>となり、$(1.135)$式が成立することが示された。</p>
<p>後の数学的帰納法については……（書きかけ）</p>
<p>別解としては、結局のところ$n(D,M)$の計算部分は多項式の二項展開と同じなので、<strong>異なる$D$個のパラメータから重複を許して$M$個とってくる場合の数と等しい</strong>ことがわかる。すなわち</p>
<p>$$
n(D, M) = { }<em>{D-1} \mathrm{H}</em>{M-1} = { }<em>{D+M-1} \mathrm{C}</em>{M} = \frac{(D+M-1)!}{(D-1)!M!}
$$</p>
<p>であり、$(1.137)$式が得られる。</p>
<h2 id="演習-116難"><a class="header" href="#演習-116難">演習 1.16（難）</a></h2>
<div class="panel-primary">
<p>演習問題1.15で，$M$次の$D$次元多項式の独立なパラメータの数が(1.135)となることを証明した．次に，$M$次までのすべての項における独立パラメータの総数$N(D,M)$を求めよう．まず$N(D,M)$が</p>
<p>$$
N(D, M)=\sum_{m=0}^{M} n(D, m) \tag{1.138}
$$
を満たすことを示せ．ただし，$n(D,M)$は$m$次の項における独立パラメータの数である．(1.137)の結果と数学的帰納法により，</p>
<p>$$
N(D, M)=\frac{(D+M) !}{D ! M !} \tag{1.139}
$$
を示せ．これを示すには，まず，$M=0$と任意の$D \ge 1$について成り立つことを証明してから，$M$次で成り立つなら$M+1$次で成り立つことを示せばよい．最後にスターリングの近似式，つまり$n$が大きいとき</p>
<p>$$
n ! \simeq n^{n} e^{-n} \tag{1.140}
$$</p>
<p>が成り立つことを使って,$D \gg M$のとき$N(D,M)$が$D^M$で大きくなり，$M \gg D$のときは$M^D$で大きくなることを示せ．$D$次元の3次多項式($M=3$)を考え，独立パラメータの総数を(i)$D=10$ (ii)$D=100$のそれぞれの場合について数値的に評価せよ．これらは典型的な小スケールおよび中スケールの機械学習の応用に対応する．</p>
</div>
<p>演習1.15の結果から$D$次元多項式のある$M$次の独立なパラメータが$n(D,M)$個であることが求まったので、$0 \le m \le M$までの合計のパラメータ数は</p>
<p>$$
N(D,M) = \sum_{m=0}^M n(D,m)
$$</p>
<p>と書ける（証明になっていないけれど自明だと思う）。</p>
<p>次に数学的帰納法を用いて</p>
<p>$$
N(D,M) =\frac{(D+M) !}{D ! M !}
$$</p>
<p>すなわち演習1.15の結果を用いて</p>
<p>$$
\sum_{m=0}^{M} \frac{(D+m-1) !}{(D-1) ! m !}=\frac{(D+M) !}{D ! M !} \tag{*}
$$</p>
<p>であることを示す（$D, M$はそれぞれ$1, 0$以上の整数値）。</p>
<p>(i) $M=0$のとき
$(左辺) = 1$, $(右辺) = 1$となるので成立する。</p>
<p>(ii) $M=k$のとき$(*)$が成立すると仮定する。このとき$M=k+1$において</p>
<p>$$
\begin{aligned}
\sum_{m=0}^{k+1} \frac{(D+m-1) !}{(D-1) ! m !} &amp;=\frac{(D+k) !}{(D-1) !(k+1) !}+\sum_{m=0}^{k} \frac{(D+m-1) !}{(D-1) ! m !} \
&amp;=\frac{(D+k) !}{(D-1) !(k+1) !}+\frac{(D+k) !}{D ! k !} \
&amp;=\frac{(D+k) !}{D !(k+1) !}{D+(k+1)} \
&amp;=\frac{(D+(k+1)) !}{D !(k+1) !}
\end{aligned}
$$</p>
<p>となるので、$M=k+1$のときも成立することが示された。</p>
<p>最後に、まず$D \gg M$の条件下において、$\frac{M}{D} \ll 1$であることとスターリングの公式$n! \simeq n^n e^{-n}$を用いると</p>
<p>$$
\begin{aligned}
N(D, M) &amp;=\frac{(D+M) !}{D ! M !} \
&amp; \simeq \frac{(D+M)^{D+M} e^{-D-M}}{D^{D} e^{-D} \cdot M !} \
&amp;=\frac{e^{-M}}{M ! D^{D}}(D+M)^{D+M} \
&amp;=\frac{D^{M} e^{-M}}{M !}\left(1+\frac{M}{D}\right)^{D+M} \
&amp; \simeq \frac{D^{M} e^{-M}}{M !}\left(1+\frac{M}{D}(D+M)\right) \
&amp;= \frac{D^M e^{-M}}{M!}\left{ 1 + M\left( 1+ \frac{M}{D}\right)\right} \
&amp; \simeq \frac{e^{-M}}{M!}(1+M)D^M
\end{aligned}
$$</p>
<p>となるのでこれは$D^M$が支配的になる。反対に$M \gg D$の条件下では文字を入れ替えた$\displaystyle \frac{e^{-D}}{D!}(1+D)M^D$となり、$M^D$が支配的になる。</p>
<p>数値的に評価すると$D=10$のとき$N(10, 3) = 286$, $D=100$のとき、$\displaystyle N(100,3) = \frac{103!}{100!3!} = 176851$となる（ちなみにスターリングの公式を用いると$\displaystyle N(100,3) \simeq \frac{e^{-3}}{3!}(1+3)100^3 = 33201.7$くらいなのでまだこの近似式は使えない。もっと大きな数になってくるとオーダーは揃うっぽい。）</p>
<h2 id="演習-117"><a class="header" href="#演習-117">演習 1.17</a></h2>
<div class="panel-primary">
<p>ガンマ関数は</p>
<p>$$
\Gamma(x) \equiv \int_{0}^{\infty} u^{x-1} e^{-u} \mathrm{d} u \tag{1.141}
$$
で定義される．部分積分を使って関係式$\Gamma(x+1) = x\Gamma(x)$を証明せよ．また$\Gamma(1)=1$を示し，$x$が整数なら$\Gamma(x+1)=x!$となることを示せ．</p>
</div>
<p>$\Gamma(x+1)$について部分積分を行うと、</p>
<p>$$
\begin{aligned}
\Gamma(x+1) &amp;= \int_{0}^{\infty} u^x(-e^{-u})^{\prime} du \
&amp;= \left [ -u^xe^{-u} \right]<em>{0}^{\infty} + x \int</em>{0}^{\infty}u^{x-1}e^{-u} du \
&amp;= (0 - 0) + x \Gamma(x) \
&amp;= x \Gamma(x)
\end{aligned}
$$</p>
<p>となる。（公式 $ \displaystyle \lim_{u \to \infty} u e^{-u} = \displaystyle \lim_{u \to \infty} \frac{u}{e^u} = 0 $ に注意）</p>
<p>また，$x=1$を代入すると</p>
<p>$$
\Gamma(1) = \int_{0}^{\infty}1\cdot e^{-u}du=\left[ -e^{-u} \right]_{0}^{\infty}=1
$$</p>
<p>これを用いると，$x \ge 1$の整数$x$に対して</p>
<p>$$
\begin{aligned}
\Gamma(x+1) &amp;= x \Gamma(x)\
&amp;= x (x-1) (x-2) \cdots 2 \cdot 1 \cdot \Gamma(1) \
&amp;= x!
\end{aligned}
$$
となる．</p>
<p>※ ガンマ関数は<strong>階乗の概念を複素数全体に拡張した（複素階乗ともいう）特殊関数である</strong>。 （https://ja.wikipedia.org/wiki/%E3%82%AC%E3%83%B3%E3%83%9E%E9%96%A2%E6%95%B0）</p>
<h2 id="演習-118"><a class="header" href="#演習-118">演習 1.18</a></h2>
<div class="panel-primary">
<p>$D$次元の単位球の表面積$S_D$，体積$V_D$を導くのに
$$
I=\left(2 \pi \sigma^{2}\right)^{1 / 2} \tag{1.126}
$$
を使うことができる．これにはまず，直交座標から極座標への変換から導かれる</p>
<p>$$
\prod_{i=1}^{D} \int_{-\infty}^{\infty} e^{-x_{i}^{2}} \mathrm{d} x_{i}=S_{D} \int_{0}^{\infty} e^{-r^{2}} r^{D-1} \mathrm{d} r \tag{1.142}
$$</p>
<p>という事実を考える．ガンマ関数の定義$(1.141)$と$(1.126)$から，この式の両辺を評価し，</p>
<p>$$
S_{D}=\frac{2 \pi^{D / 2}}{\Gamma(D / 2)} \tag{1.143}
$$
を示せ．次に半径$0$から$1$まで積分し，$D$次元単位球の体積が</p>
<p>$$
V_D = \frac{S_D}{D} \tag{1.144}
$$
で与えられることを示せ．最後に$\Gamma(1)=1$および$\Gamma(3/2)=\sqrt{\pi} / 2$から，$(1.143)$と$(1.144)$が$D=2$および$D=3$の通常の表現に帰着されることを示せ．</p>
</div>
<p>Wikipediaの超平面や http://www.oit.ac.jp/ge/~nakano/Ex-0002.pdf なども参照。</p>
<p>$(1.126)$式の$\displaystyle \int_{-\infty}^{\infty}\exp\left( -\frac{1}{2\sigma^2}x^2\right)dx = \sqrt{2\pi\sigma^2}$をうまく使う．</p>
<p>$(1.142)$式の左辺の形にするために$\sigma^2 = 1/2$を代入すると，</p>
<p>$$
\prod_{i=1}^{D} \int_{-\infty}^{\infty} e^{-x_{i}^{2}} \mathrm{d} x_{i} = \left( \sqrt{2\pi\cdot (1/2)} \right)^D = \pi^{D/2}
$$</p>
<p>となる。一方で$(1.142)$式の右辺はガンマ関数の形なので$(1.141)$式について$u=r^2$を代入すると、$du=2rdr$なので</p>
<p>$$
\begin{aligned}
\Gamma(D/2) &amp;= \int_{0}^{\infty}r^{D-2}e^{-r^2}\cdot 2rdr \
&amp;= 2\int_{0}^{\infty}r^{D-1}e^{-r^2}dr
\end{aligned}
$$</p>
<p>よって、$\displaystyle \int_{0}^{\infty}r^{D-1}e^{-r^2}dr = \frac{\Gamma (D/2)}{2}$である。</p>
<p>$(1.142)$式を成立させるための$S_D$は</p>
<p>$$
S_D = \frac{2}{\Gamma(D/2)}\cdot \pi^{D/2} = \frac{2\pi^{D/2}}{\Gamma(D/2)}
$$</p>
<p>となり，$(1.143)$式が示された．</p>
<p>ここで，次元の考察から$D$次元球の表面積と体積は</p>
<p>$$
S_D(r) = S_D(1)r^{D-1},\hspace{1em}V_D(r) = V_D(1)r^D
$$</p>
<p>と書けることに注意する．$r$が$0 \to 1$を動いて全表面積を積分すれば$V_D(1)$になることを利用して，</p>
<p>$$
V_D(1) = \int_{0}^{1}S_D(1)r^{D-1}dr = S_D(1)\left[ \frac{r^D}{D} \right]_{0}^{1} = \frac{S_D}{D}
$$</p>
<p>もしくは、$V_D(r) = V_D(1)r^D$ の両辺をrで微分すると、</p>
<p>$$
S_D(r) = V_D(1)Dr^{D-1}
$$</p>
<p>$S_D(r) = S_D(1)r^{D-1}$ より、</p>
<p>$$
S_D(1)r^{D-1} = V_D(1)Dr^{D-1}
$$</p>
<p>$$
V_D(1) = S_D(1)/D
$$</p>
<p>これより，$V_D = S_D/D$を得る．$\Gamma(1)=1$, $\Gamma(3/2) = \pi / 2$を用いると、$S_2 = 2\pi, V_2 = \pi$, $S_3 = 4\pi，V_3 = \frac{4\pi}{3}$となり、$D=2$, $D=3$で見慣れた式になることがわかる．</p>
<p>（こちらの演習では、$S_D = S_D(1)$, $V_D = V_D(1)$と定義していることに注意する。）</p>
<h2 id="演習-119"><a class="header" href="#演習-119">演習 1.19</a></h2>
<div class="panel-primary">
<p>$D$次元の半径$a$の球と，同じ中心を持つ一辺2$a$の超立方体を考える．球面は超立方体の各面の中心で接している．演習問題1.18の結果を使って，球の体積と立方体の体積の比が</p>
<p>$$
\frac{球の体積}{立方体の体積}=\frac{\pi^{D / 2}}{D 2^{D-1} \Gamma(D / 2)} \tag{1.145}
$$</p>
<p>で与えられることを示せ．スターリングの公式</p>
<p>$$
\Gamma(x+1) \simeq(2 \pi)^{1 / 2} e^{-x} x^{x+1 / 2} \tag{1.146}
$$</p>
<p>が$x \gg 1$で成り立つことを使って$D \to \infty$の極限で比の値$(1.145)$が$0$に収束することを示せ．また，超立方体の中心から1つの頂点までの距離を中心から側面までの距離で割った比が$\sqrt{D}$となることを示し，$D \to \infty$のとき$\infty$に発散することを示せ．これらの結果から，高次元空間では立方体の体積のほとんどはたくさんの頂点に集中し，非常に長い「スパイク」になっていることがわかる！</p>
</div>
<p>演習問題1.18の結果から、$D$次元の半径$a$の球の体積は$\displaystyle \frac{2 \pi^{D / 2}}{\Gamma(D / 2)}a^{D}$である。また、$D$次元の一辺$2a$の超立方体の体積は$(2a)^{D}$である。よって、その比率は</p>
<p>$$
\frac{\pi^{D / 2}}{D 2^{D-1} \Gamma(D / 2)}
$$</p>
<p>となる。これを$f(D)$とする。</p>
<p>スターリングの公式を用いると、</p>
<p>$$
\begin{aligned}
f(D) &amp;= \frac{\pi^{D/2}}{2^{D}\Gamma{\left(\frac{D}{2}+1 \right)}} \
&amp;\simeq \frac{\pi^{D/2}}{2^{D}\cdot\sqrt{2\pi} \cdot e^{-D/2}\cdot \left( \frac{D}{2} \right)^{\frac{D+1}{2}}} \
&amp;= \frac{1}{\sqrt{\pi D}}\cdot \left( \frac{\pi e}{2D} \right)^{\frac{D}{2}}
\end{aligned}
$$</p>
<p>となるので、$D \to \infty$で$f(D) \to 0$となる。</p>
<p>超立方体の中心から側面までの距離は$a$で、中心から1つの頂点までの距離は$\sqrt{D}a$である（$D=2$で$\sqrt{2}a$, $D=3$で$\sqrt{3}a$となることと合致する）。よって比は$\sqrt{D}$となる。このとき$D \to \infty$でこの比$\sqrt{D}$は$\infty$に発散することがわかる。</p>
<p>このことは<strong>高次元になればなるほど超立方体の体積が角に集中することになり、その角は非常に長いスパイク状である</strong>という帰結を表している。</p>
<p>参考：https://windfall.hatenablog.com/entry/2015/07/02/084623 球面集中現象とも。</p>
<h2 id="演習-120"><a class="header" href="#演習-120">演習 1.20</a></h2>
<div class="panel-primary">
<p>この演習問題では高次元ガウス分布の振る舞いを扱う．$D$次元ガウス分布</p>
<p>$$
p(\mathbf{x})=\frac{1}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \exp \left(-\frac{|\mathbf{x}|^{2}}{2 \sigma^{2}}\right) \tag{1.147}
$$</p>
<p>を考えよう．極座標系で，角度方向については積分し，半径に関する密度を求めたい．このために，半径$r$にある厚さ$\epsilon$の薄皮に関して$\mathbf{x}$の確率密度の積分をとると，$\epsilon \ll 1$のとき$p(r)\epsilon$となることを示せ（注：確率密度関数の添え字が（簡略化のため）省略されていて少々紛らわしいが，$p(r)$は半径$r$を確率変数と見たときの確率密度，$p(\mathbf{x})$は$\mathbf{x}$を確率変数と見たときの確率密度を表すことに注意されたい）．ただし，</p>
<p>$$
p(r)=\frac{S_{D} r^{D-1}}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \exp \left(-\frac{r^{2}}{2 \sigma^{2}}\right) \tag{1.148}
$$</p>
<p>であり，$S_D$は$D$次元単位球の表面積である関数$p(r)$が１つの停留点を持ち，$D$が大きいとき，$\widehat{r} \simeq \sqrt{D} \sigma$にあることを示せ．$\epsilon \ll \widehat{r}$について$p(\widehat{r}+\epsilon)$を考え，$D$が大きいとき</p>
<p>$$
p(\widehat{r}+\epsilon)=p(\widehat{r}) \exp \left(-\frac{\epsilon^{2}}{\sigma^{2}}\right) \tag{1.149}
$$</p>
<p>となることを示せ．これから$\widehat{r}$が確率密度の最大値を与える半径となり，$p(r)$が$\hat{r}$での最大値から$\sigma$の長さスケールで指数的に減衰していることがわかる．我々はすでに，$D$が大きいとき$\sigma \ll \widehat{r}$であることを見てきたので，ほとんどの確率質量が大きな半径の薄皮に集中していることがわかる．最後に，$\mathbf{x}$の確率密度$p(\mathbf{x})$は，半径$\hat{r}$にある地点よりも，原点での方が$\exp (D/2)$倍大きいことを示せ．このことから，ほとんどの高次元ガウス分布の確率質量は$\mathbf{x}$の確率密度の高いところとは異なる半径のところにあることがわかる．後の章でモデルパラメータのベイズ推論を考える際に，高次元空間の分布のこの性質を使って重要な結論を導くことになる．</p>
</div>
<p>混乱を避けるため，以下では$p(\mathbf{x})$を$p_{\mathbf{x}}(\mathbf{x})$，$p(r)$を$p_r(r)$と表す．</p>
<p>まず，半径$r$，厚さ$\epsilon$の薄皮の体積を求める．</p>
<p>$S_D$は$D$次元単位超球の表面積なので，半径$r$の位置での表面積は$S_D r^{D-1}$となる．$\epsilon \ll 1$の条件下では近似的に表面積が一定であると考えて良いので，薄皮の体積は$S_D r^{D-1}\epsilon$となる．</p>
<p>次に，$p_{\mathbf{x}}(\mathbf{x})$を薄皮$(shell)$に関して積分する．薄皮なので確率密度も一定と見なせるので</p>
<p>$$
\begin{aligned}
\int_{shell} p_{\mathbf{x}}(\mathbf{x}) \mathrm{d} \mathbf{x} \simeq &amp; ~ p_{\mathbf{x}}(\mathbf{x}=r) S_D r^{D-1}\epsilon \
=&amp; \frac{1}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \exp \left(-\frac{r^{2}}{2 \sigma^{2}}\right) S_D r^{D-1}\epsilon \
=&amp; p_r(r)\epsilon \
\end{aligned}
$$</p>
<p>となる．</p>
<p>また，$(1.148)$を$r$で微分すると</p>
<p>$$
\begin{aligned}
\frac{\mathrm{d}}{\mathrm{d} r} p_r(r) =&amp; \frac{\mathrm{d}}{\mathrm{d} r} \left{ \frac{S_{D} r^{D-1}}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \exp \left(-\frac{r^{2}}{2 \sigma^{2}}\right) \right}\
=&amp; \frac{S_{D}}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \left{ (D-1)r^{D-2} \exp \left(-\frac{r^{2}}{2 \sigma^{2}}\right) + r^{D-1} \left(-\frac{r}{\sigma^{2}}\right) \exp \left(-\frac{r^{2}}{2 \sigma^{2}}\right) \right} \
=&amp; \frac{S_{D}}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \exp \left(-\frac{r^{2}}{2 \sigma^{2}}\right) \left{ (D-1)r^{D-2} - \frac{r^D}{\sigma^{2}} \right} \
\end{aligned}
$$</p>
<p>となり，これが$0$となるときの$r$を$\hat{r}$とすると</p>
<p>$$
\begin{aligned}
&amp; (D-1)\hat{r}^{D-2} - \frac{\hat{r}^D}{\sigma^{2}} = 0 \
\Leftrightarrow ~ &amp; \hat{r} = \sigma \sqrt{D-1}
\end{aligned}
$$</p>
<p>ここで$D \gg 1$とすると</p>
<p>$$
\hat{r} \simeq \sigma \sqrt{D}
$$</p>
<p>となる．</p>
<p>次に，$p_r(\hat{r}+\epsilon)$を考える．</p>
<p>$$
\begin{aligned}
p_r(\hat{r}+\epsilon) =&amp; \frac{S_{D} (\hat{r}+\epsilon)^{D-1}}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \exp \left(-\frac{(\hat{r}+\epsilon)^{2}}{2 \sigma^{2}}\right) \
=&amp; \frac{S_{D} \hat{r}^{D-1}}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \exp \left(-\frac{\hat{r}^{2}}{2 \sigma^{2}}\right) \left( 1 + \frac{\epsilon}{\hat{r}} \right)^{D-1} \exp \left(-\frac{1}{2 \sigma^{2}} (2\hat{r}\epsilon + \epsilon^2 )\right) \
=&amp; p_r(\hat{r}) \left( 1 + \frac{\epsilon}{\hat{r}} \right)^{D-1} \exp \left(-\frac{1}{2 \sigma^{2}} (2\hat{r}\epsilon + \epsilon^2 )\right) \
=&amp; p_r(\hat{r}) \exp \left{ \ln \left( 1 + \frac{\epsilon}{\hat{r}} \right)^{D-1} - \frac{1}{2 \sigma^{2}} (2\hat{r}\epsilon + \epsilon^2 )\right} \
=&amp; p_r(\hat{r}) \exp \left{ (D-1) \ln \left( 1 + \frac{\epsilon}{\hat{r}} \right) - \frac{1}{2 \sigma^{2}} (2\hat{r}\epsilon + \epsilon^2 )\right} \
\end{aligned}
$$</p>
<p>ここで$\ln (1+x)$のマクローリン展開より</p>
<p>$$
\begin{aligned}
\ln (1+x) =&amp; x - \frac{1}{2}x^2 + \frac{1}{3}x^3 - \cdots \
\simeq&amp; ~ x - \frac{1}{2}x^2
\end{aligned}
$$</p>
<p>の近似を利用すると</p>
<p>$$
\begin{aligned}
p_r(\hat{r}+\epsilon)
\simeq&amp; ~  p_r(\hat{r}) \exp \left{ (D-1) \left( \frac{\epsilon}{\hat{r}} - \frac{\epsilon^2}{2 \hat{r}^2} \right) - \frac{1}{2 \sigma^{2}} (2\hat{r}\epsilon + \epsilon^2 )\right} \
=&amp; p_r(\hat{r}) \exp \left{ \frac{\hat{r}^2}{\sigma^2} \left( \frac{\epsilon}{\hat{r}} - \frac{\epsilon^2}{2 \hat{r}^2} \right) - \frac{1}{2 \sigma^{2}} (2\hat{r}\epsilon + \epsilon^2 )\right} ~(\because \hat{r} = \sigma \sqrt{D-1}) \
=&amp; p_r(\hat{r}) \exp \left(-\frac{\epsilon^{2}}{\sigma^{2}}\right) \
\end{aligned}
$$</p>
<p>となり，$(1.149)$が得られる．</p>
<p>また</p>
<p>$$
\begin{aligned}
p_{\mathbf{x}}(\mathbf{x} = \mathbf{0}) =&amp; \frac{1}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \
p_{\mathbf{x}}(|\mathbf{x}| = \hat{r}) =&amp; \frac{1}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \exp \left(-\frac{\hat{r}^{2}}{2 \sigma^{2}}\right) \
\simeq &amp; \frac{1}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \exp \left(-\frac{D}{2}\right) ~ (D \gg 1) \
\end{aligned}
$$</p>
<p>であることより，確率密度$p_{\mathbf{x}}(\mathbf{x})$は半径$\hat{r}$にある地点での値$p_{\mathbf{x}}(|\mathbf{x}| = \hat{r})$に比べ，原点での方が$\exp (D/2)$倍大きいことが示された。</p>
<h2 id="演習-121"><a class="header" href="#演習-121">演習 1.21</a></h2>
<div class="panel-primary">
<p>2つの非負の数$a$と$b$があったとき，$a \le b$なら$a \le (ab)^{\frac{1}{2}}$であることを示せ．この結果を使って，2クラスのクラス分類問題の決定領域を誤識別率が最小になるように選ぶと，この確率が</p>
<p>$$
p(\text { 誤り } ) \le \int\left{p\left(\mathbf{x}, \mathcal{C}<em>{1}\right) p\left(\mathbf{x}, \mathcal{C}</em>{2}\right)\right}^{1 / 2} \mathrm{d} \mathbf{x} \tag{1.150}
$$</p>
<p>を満たすことを示せ．</p>
</div>
<p>$a \le b$の両辺の平方根をとると、</p>
<p>$$
a^{\frac{1}{2}} \le b^{\frac{1}{2}}
$$</p>
<p>さらに両辺に$a^{\frac{1}{2}}$を掛けると、</p>
<p>$$
a \le (ab)^{\frac{1}{2}}
$$</p>
<p>が得られる。</p>
<p>また、$p(\text { 誤り } )$を最小にするには、決定領域$\mathcal{R}<em>{1}$において$p\left(\mathbf{x}, \mathcal{C}</em>{2}\right) \le p\left(\mathbf{x}, \mathcal{C}<em>{1}\right)$、$\mathcal{R}</em>{2}$において$p\left(\mathbf{x}, \mathcal{C}<em>{1}\right) \le p\left(\mathbf{x}, \mathcal{C}</em>{2}\right)$でなければならない。
これと$a \le (ab)^{\frac{1}{2}}$より、</p>
<p>$$
\begin{aligned} p(\text { 誤り } )
&amp;=\int_{R_1} p\left(\mathbf{x}, \mathcal{C}<em>{2}\right) \ \mathrm{d} \mathbf{x}  + \int</em>{R_2}\ p\left(\mathbf{x}, \mathcal{C}<em>{1}\right) \mathrm{d} \mathbf{x} \
&amp;\le \int</em>{R_1}\left{p\left(\mathbf{x}, \mathcal{C}<em>{1}\right) p\left(\mathbf{x}, \mathcal{C}</em>{2}\right)\right}^{1 / 2} \mathrm{d} \mathbf{x} + \int_{R_2}\left{p\left(\mathbf{x}, \mathcal{C}<em>{1}\right) p\left(\mathbf{x}, \mathcal{C}</em>{2}\right)\right}^{1 / 2} \mathrm{d} \mathbf{x} \
&amp;=\int\left{p\left(\mathbf{x}, \mathcal{C}<em>{1}\right) p\left(\mathbf{x}, \mathcal{C}</em>{2}\right)\right}^{1 / 2} \mathrm{d} \mathbf{x}
\end{aligned}
$$</p>
<p>となり、式$(1.150)$が得られる。</p>
<h2 id="演習-122"><a class="header" href="#演習-122">演習 1.22</a></h2>
<div class="panel-primary">
<p>$L_{kj}$を要素とする損失行列が与えられたとき，期待リスクが最小になるのは，各$\mathbf{x}$に対し，</p>
<p>$$
\sum_{k} L_{k j} p\left(\mathcal{C}_{k} \mid \mathbf{x}\right) \tag{1.81}
$$</p>
<p>を最小にするクラスを選んだときである．損失行列が$L_{kj}=1-I_{kj}$で与えられたとき，これが最大事後確率のクラスを選ぶ規準に帰着されることを確かめよ．ただし，$I_{kj}$は単位行列の成分を表す．また，この損失行列はどのように解釈できるか？</p>
</div>
<p>$(1.81)$式に$L_{kj}=1-I_{kj}$を代入すると、</p>
<p>$$
\sum_k {(1-I_{kj})p(\mathcal{C}_k | x})
$$</p>
<p>となり、$\sum_k {p(\mathcal{C}<em>k | x)} = 1$であることを用いると、$1-\sum_k{I</em>{kj}p(\mathcal{C}_k|x)}$と表せる。この式の最小化は$p(\mathcal{C}_k|x)$の最大化と同一である。</p>
<p>損失行列$1-I_{kj}$の解釈としては、損失行列の対角成分が$0$、他の成分が$1$の行列であるから、この行列の期待値の最小化は誤分類の最小化と解釈できる。</p>
<h2 id="演習-123"><a class="header" href="#演習-123">演習 1.23</a></h2>
<div class="panel-primary">
<p>一般の場合に，損失行列とクラスに対する事前確率が与えられたときに，期待損失を最小にする規準を導け．</p>
</div>
<p>期待損失は損失行列$L$と事後確率$p\left(\mathcal{C}_{k}|x\right)$で下記のように簡単に表すことができる,</p>
<p>$$
\begin{aligned}
\sum_{k}^{}L_{kj}P\left(\mathcal{C}_{k}|x\right)
\end{aligned}
$$</p>
<p>損失行列$L$とクラスの事前確率$p\left(\mathcal{C}<em>{k}\right)$を代入すると
$$
\begin{aligned}
\sum</em>{k}^{}L_{kj}P\left(\mathcal{C}<em>{k}|x\right)
&amp;= \frac{1}{p\left(x\right)}\sum</em>{k}^{}L_{kj}p\left(\mathcal{C}<em>{k}\right)p\left(x|\mathcal{C}</em>{k}\right)
\end{aligned}
$$
となる.</p>
<p>これによって、期待損失を最小化するには$x$を適切な$\mathcal{C}<em>{k}$に当てはめて、$L</em>{kj}$と$p\left(\mathcal{C}_{k}\right)$のトレードオフ関係を適切に扱うことが期待損失を最小にする基準となっている。</p>
<h2 id="演習-124"><a class="header" href="#演習-124">演習 1.24</a></h2>
<div class="panel-primary">
<p>クラス分類問題を考え，クラス$\mathcal{C}<em>k$からの入力ベクトルをクラス$\mathcal{C}<em>j$と分類したときの損失行列を$L</em>{kj}$とし，棄却オプションを選んだときの損失を$\lambda$とする．このとき期待損失を最小とする決定規準を見つけよ．損失行列が$L</em>{kj}=1-I_{kj}$のときは，決定規準は1.5.3節で議論した棄却規準に帰着されることを確かめよ．また，$\lambda$と棄却しきい値$\theta$にはどんな関係があるか？</p>
</div>
<p>$(1.81)$より、クラス$\mathcal{C}_k$からの入力ベクトルをクラス$\mathcal{C}_j$と分類したとき、棄却を考えない場合は</p>
<p>$$
j = \arg \min_j \sum_{k}^{}L_{kj}p(C_k|\mathbf{x})
$$</p>
<p>この$j$を選んだ場合の損失関数は$\min_j \sum_{k}^{}L_{kj}p(\mathcal{C}_k|\mathbf{x})$となるので、棄却オプションを選んだ時に損失を最小にする決定規準は</p>
<p>$$
\text{choose}\left{\begin{array}{ll}
\text{class} \quad j &amp; (\min_j \sum_{k}^{}L_{kj}p(\mathcal{C}_k|\mathbf{x}) &lt; \lambda) \
\text{棄却} &amp; (\text{上記以外})\end{array}\right.
$$</p>
<p>$L_{kj}=1-I_{kj}$のとき、$\sum_{k}^{}L_{kj}p(\mathcal{C}_k|\mathbf{x})=1-p(\mathcal{C}_j|\mathbf{x})$となる(演習1.22参照)。</p>
<p>上記規準に照らし合わせると$1-p(\mathcal{C}_j|\mathbf{x}) \ge \lambda$、つまり$p(\mathcal{C}_j|\mathbf{x}) \le 1-\lambda$のときに棄却となる。</p>
<p>1.5.3節の議論より、$p(\mathcal{C}_j|\mathbf{x}) \le \theta$の時に棄却となるので、$\theta = 1-\lambda$としたとき決定規準は1.5.3節で議論した棄却規準に帰着する。</p>
<h2 id="演習-125"><a class="header" href="#演習-125">演習 1.25</a></h2>
<div class="panel-primary">
<p>単一の目標変数$t$の</p>
<p>$$
\mathbb{E}[L]=\iint{y(\mathbf{x})-t}^{2} p(\mathbf{x}, t) \mathbf{d} \mathbf{x} \mathrm{d} t \tag{1.87}
$$</p>
<p>の二乗損失関数のベクトル値$\mathbf{t}$で表される多変数の場合への以下の一般化について考える．</p>
<p>$$
\mathbb{E}[L(\mathbf{t}, \mathbf{y}(\mathbf{x}))]=\iint|\mathbf{y}(\mathbf{x})-\mathbf{t}|^{2} p(\mathbf{x}, \mathbf{t}) \mathrm{d} \mathbf{x} \mathrm{d} \mathbf{t} \tag{1.151}
$$</p>
<p>変分法によって，この期待損失を最小化する関数$\mathbf{y}(\mathbf{x})$が$\mathbf{y}(\mathbf{x})=\mathbb{E}_{\mathbf{t}}[\mathbf{t} | \mathbf{x}]$で与えられることを示せ．単一の目標変数$\mathbf{t}$の場合はこの結果が</p>
<p>$$
y(\mathbf{x})=\frac{\int t p(\mathbf{x}, t) \mathrm{d} t}{p(\mathbf{x})}=\int t p(t \mid \mathbf{x}) \mathrm{d} t=\mathbb{E}_{t}[t \mid \mathbf{x}] \tag{1.89}
$$</p>
<p>に帰着されることを確かめよ．</p>
</div>
<p>$(1.151)$は$(1.87)$で$t$をベクトル$\mathbf{t}$で置き換えることで得られる．</p>
<p>$$
\mathbb{E} \left[ L \right]  = \iint|\mathbf{y}(\mathbf{x})-\mathbf{t}|^{2} p(\mathbf{x}, \mathbf{t}) \mathrm{d} \mathbf{x} \mathrm{d} \mathbf{t} = \int G(\mathbf{y}, \mathbf{y}', \mathbf{x}) \mathrm{d} \mathbf{x}
$$</p>
<p>とおき，変分法を用いて期待損失を最小にする$\mathbf{y}$を求める．オイラー方程式より，</p>
<p>$$
\begin{aligned}
\frac{\delta \mathbb{E}[L]}{\delta \mathbf{y}(\mathbf{x})} =&amp; \frac{\partial{G}}{\partial{\mathbf{y}}} - \frac{\mathrm{d}}{\mathrm{d}\mathbf{x}} \left(\frac{\partial{G}}{\partial{\mathbf{y}'}}\right) = \mathbf{0} \
\end{aligned}
$$</p>
<p>ここで$\displaystyle \frac{\partial{G}}{\partial{\mathbf{y}'}} = \mathbf{0}$であることに注意すると</p>
<p>$$
\begin{aligned}
&amp; \frac{\delta \mathbb{E}[L]}{\delta \mathbf{y}(\mathbf{x})} =\frac{\partial{G}}{\partial{\mathbf{y}}} = \mathbf{0}  \
\Leftrightarrow ~ &amp; \frac{\partial}{\partial{\mathbf{y}}} \int|\mathbf{y}(\mathbf{x})-\mathbf{t}|^{2} p(\mathbf{x}, \mathbf{t}) \mathrm{d} \mathbf{t} = \mathbf{0}  \
\Leftrightarrow ~ &amp; \int 2{\mathbf{y}(\mathbf{x})-\mathbf{t}} p(\mathbf{x}, \mathbf{t}) \mathrm{d} \mathbf{t} = \mathbf{0}  \
\Leftrightarrow ~ &amp; \mathbf{y}(\mathbf{x}) = \frac{\int \mathbf{t} p(\mathbf{x}, \mathbf{t}) \mathrm{d} \mathbf{t}}{\int p(\mathbf{x}, \mathbf{t}) \mathrm{d} \mathbf{t}} = \frac{\int \mathbf{t} p(\mathbf{x}, \mathbf{t}) \mathrm{d} \mathbf{t}}{p(\mathbf{x})} = \int \mathbf{t} p(\mathbf{t} | \mathbf{x}) \mathrm{d} \mathbf{t} = \mathbb{E}_t \left[ \mathbf{t} | \mathbf{x} \right]\
\end{aligned}
$$</p>
<p>となる．ここでベクトル$\mathbf{t}$をスカラー$t$で置き換えると$(1.89)$が得られる．</p>
<h2 id="演習-126"><a class="header" href="#演習-126">演習 1.26</a></h2>
<div class="panel-primary">
<p>$$
\mathbb{E}[L(\mathbf{t}, \mathbf{y}(\mathbf{x}))]=\iint|\mathbf{y}(\mathbf{x})-\mathbf{t}|^{2} p(\mathbf{x}, \mathbf{t}) \mathrm{d} \mathbf{x} \mathrm{d} \mathbf{t} \tag{1.151}
$$</p>
<p>の2乗を展開し，</p>
<p>$$
\mathbb{E}[L]=\int{y(\mathbf{x})-\mathbb{E}[t \mid \mathbf{x}]}^{2} p(\mathbf{x}) \mathrm{d} \mathbf{x}+\int \operatorname{var}[t \mid \mathbf{x}] p(\mathbf{x}) \mathrm{d} \mathbf{x} \tag{1.90}
$$</p>
<p>に類似の結果を導き，目標変数ベクトル$\mathbf{t}$の場合に期待二乗損失を最小にする関数$\mathbf{y}(\mathbf{x})$がやはり$\mathbf{t}$の条件付き期待値で与えられることを示せ．</p>
</div>
<p>まず$(1.151)$の$2$乗部分を展開すると</p>
<p>$$
\begin{aligned}
&amp; ~ |\mathbf{y}(\mathbf{x})-\mathbf{t}|^2 \
=&amp; ~ |\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}]+\mathbb{E}[\mathbf{t}|\mathbf{x}]-\mathbf{t}|^2\
=&amp; ~ |\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}]|^2 + (\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}])^\mathrm{T}(\mathbb{E}[\mathbf{t}|\mathbf{x}]-\mathbf{t}) + (\mathbb{E}[\mathbf{t}|\mathbf{x}]-\mathbf{t})^\mathrm{T}(\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}]) + |\mathbb{E}[\mathbf{t}|\mathbf{x}]-\mathbf{t}|^2 \
=&amp; ~ |\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}]|^2 + 2(\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}])^\mathrm{T}(\mathbb{E}[\mathbf{t}|\mathbf{x}]-\mathbf{t}) + |\mathbb{E}[\mathbf{t}|\mathbf{x}]-\mathbf{t}|^2 \
\end{aligned}
$$</p>
<p>となる．ここで第$2$項について，</p>
<p>$$
\begin{aligned}
&amp; \iint(\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}])^\mathrm{T}(\mathbb{E}[\mathbf{t}|\mathbf{x}]-\mathbf{t})p(\mathbf{x},\mathbf{t})\mathrm{d}\mathbf{x}\mathrm{d}\mathbf{t} \
=&amp; \int(\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}])^\mathrm{T}\int(\mathbb{E}[\mathbf{t}|\mathbf{x}]-\mathbf{t})p(\mathbf{x},\mathbf{t})\mathrm{d}\mathbf{t}\mathrm{d}\mathbf{x} \
=&amp; \int(\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}])^\mathrm{T}(\mathbb{E}[\mathbf{t}|\mathbf{x}]-\mathbb{E}[\mathbf{t}|\mathbf{x}])\mathrm{d}\mathbf{x} \
=&amp; \mathbf{0}
\end{aligned}
$$</p>
<p>となる．$2$行目から$3$行目への変形には $\mathbb{E}[\mathbf{t}|\mathbf{x}]$ が定数であることと $\displaystyle \int\mathbf{t}p(\mathbf{x},\mathbf{t})\mathrm{d}\mathbf{t}=\mathbb{E}[\mathbf{t}|\mathbf{x}]$ を用いた．
これらと $\displaystyle \int p(\mathbf{x},\mathbf{t})\mathrm{d}\mathbf{t}=p(\mathbf{x}), \int|\mathbf{t}-\mathbb{E}[\mathbf{t}|\mathbf{x}]|^2p(\mathbf{t}|\mathbf{x})\mathrm{d}\mathbf{t}=\mathrm{var}[\mathbf{t}|\mathbf{x}]$ に注意すると， $\mathbb{E}[L]$ は</p>
<p>$$
\begin{aligned}
\mathbb{E}[L]
&amp;= \iint|\mathbf{y}(\mathbf{x})-\mathbf{t}|^2p(\mathbf{x},\mathbf{t})\mathrm{d}\mathbf{x}\mathrm{d}\mathbf{t} \
&amp;= \iint|\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}]|^2p(\mathbf{x},\mathbf{t})\mathrm{d}\mathbf{x}\mathrm{d}\mathbf{t} + \iint|\mathbb{E}[\mathbf{t}|\mathbf{x}]-\mathbf{t}|^2p(\mathbf{x},\mathbf{t})\mathrm{d}\mathbf{x}\mathrm{d}\mathbf{t}\
&amp;= \int|\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}]|^2\int p(\mathbf{x},\mathbf{t})\mathrm{d}\mathbf{t}\mathrm{d}\mathbf{x} + \iint|\mathbf{t}-\mathbb{E}[\mathbf{t}|\mathbf{x}]|^2p(\mathbf{t}|\mathbf{x})\mathrm{d}\mathbf{t}p(\mathbf{x})\mathrm{d}\mathbf{x}\
&amp;= \int|\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}]|^2p(\mathbf{x})\mathrm{d}\mathbf{x} + \int\mathrm{var}[\mathbf{t}|\mathbf{x}]p(\mathbf{x})\mathrm{d}\mathbf{x}\
\end{aligned}
$$</p>
<p>と変形でき，期待二乗損失を最小にする $\mathbf{y}(\mathbf{x})$ が $\mathbf{t}$ の条件付き期待値で与えられることがわかる．</p>
<h2 id="演習-127"><a class="header" href="#演習-127">演習 1.27</a></h2>
<div class="panel-primary">
<p>回帰の問題で，損失関数$L_q$が</p>
<p>$$
\mathbb{E}\left[L_{q}\right]=\iint|y(\mathbf{x})-t|^{q} p(\mathbf{x}, t) \mathrm{d} \mathbf{x} \mathrm{d} t \tag{1.91}
$$</p>
<p>で与えられるときの期待損失を考える．$y(\mathbf{x})$が$\mathbb{E}[L_q]$を最小化するために満たすべき条件を書き下せ.$q=1$に対しては解が条件付きメディアンになる，つまり，$t \lt y(\mathbf{x})$となる確率質量と$t \ge y(\mathbf{x})$となる確率質量は等しいことを示せ．また，$q \to 0$に対する$L_q$の期待損失を最小にするのは条件付きモード，つまり関数$y(\mathbf{x})$が，各$\mathbf{x}$に対して，$p(t|\mathbf{x})$を最大にする$t$の値に等しくなることを示せ．</p>
</div>
<p>$y(\mathbf{x})$は$\mathbf{x}$と独立なので$\mathbf{x}$に関する積分の中身$\int |y(\mathbf{x})-t|^q p(\mathbf{x},t) \mathrm{d} t$を最小化すれば良い。これを$y(\mathbf{x})$で微分すると、</p>
<p>$$
\begin{aligned}
q \int_{-\infty}^{y(\mathbf{x})} |y(\mathbf{x})-t|^{q-1} p(t|\mathbf{x}) \mathrm{d} t -
q \int_{y(\mathbf{x})}^{\infty} |y(\mathbf{x})-t|^{q-1} p(t|\mathbf{x}) \mathrm{d} t
\end{aligned}
$$</p>
<p>$y(\mathbf{x})$が$\mathbb{E}[L_q]$を最小化するために満たすべき条件は、これが0になることなので</p>
<p>$$
\begin{aligned}
q \int_{-\infty}^{y(\mathbf{x})} |y(\mathbf{x})-t|^{q-1} p(t|\mathbf{x}) \mathrm{d} t
&amp;=
q \int_{y(\mathbf{x})}^{\infty} |y(\mathbf{x})-t|^{q-1} p(t|\mathbf{x}) \mathrm{d} t
\end{aligned}
$$</p>
<p>$q=1$に対しては</p>
<p>$$
\begin{aligned}
q \int_{-\infty}^{y(\mathbf{x})} p(t|\mathbf{x}) \mathrm{d} t
&amp;=
q \int_{y(\mathbf{x})}^{\infty}  p(t|\mathbf{x}) \mathrm{d} t
\end{aligned}
$$</p>
<p>となる。つまり、任意の入力$\mathbf{x}$に対して$t$が$y(\mathbf{x})$の左右にいる確率は等しいのでこれは条件付きmedianである。</p>
<p>$q→0$のとき、再び微分前の式に戻って考える。$|y(\mathbf{x})-t|^q$のグラフを頭の中で頑張って想像して描いてみると、$t$のほとんどの値については$|y(\mathbf{x})-t|^q=1$だが、$y(\mathbf{x})=t$の近傍でだけ（絶対値の中が0に近いので）グラフが急激に落ち込み$|y(\mathbf{x})-t|^q\sim0$となる。</p>
<p>よって$p(\mathbf{x},t)$が最大となる$t$の場所に$y(\mathbf{x})$を合わせると、$\int |y(\mathbf{x})-t|^q p(\mathbf{x},t) \mathrm{d} t$は最小となる。これは条件付きmodeである。</p>
<h2 id="演習-128"><a class="header" href="#演習-128">演習 1.28</a></h2>
<div class="panel-primary">
<p>1.6節でエントロピー$h(x)$のアイディアを確率分布$p(x)$を持つ確率変数$x$の値を観測することによって増える情報量として導入した．また，変数$x,y$が$p(x,y)=p(x)p(y)$となって独立なときは，エントロピーは加法的で$h(x,y)=h(x)+h(y)$となることを見た．この演習問題では，$h$と$p$の間の関数関係$h(p)$を導く．まず$h(p^2)=2h(p)$となることを示し，数学的帰納法により，正の整数$n$に対し$h(p^n)=nh(p)$となることをを示せ．さらに，正の整数$m$に対し，$h(p^{n/m})=(n/m)h(p)$が成り立つことを示せ．このことから$x$が正の有理数のとき，$h(p^x)=xh(p)$が成り立つが，これは連続性により正の実数値の場合も成り立つ．最後にこのことから$h(p)$が$h(p) \propto \ln p$の形を取らなければならないことを示せ．</p>
</div>
<p>まず1.6節の議論から、確率分布$p(x)$に依存し情報量を表す尺度$h(x)$を導入するとあるが、わかりやすくするため、$h(p(x))$と書くことにする。$p(x,y)=p(x)p(y)$と独立なときはエントロピーは加法的$h(x,y)=h(x)+h(y)$となるべきなので、
$$
\begin{aligned}
&amp;h(x,y) = h(p(x,y)) = h(p(x)p(y)) \
&amp;h(x)+h(y) = h(p(x)) + h(p(y))
\end{aligned}
$$
となる。すなわち、$h(p(x))+h(p(y)) = h(p(x)p(y))$である。
ここで$y=x$とすると、
$$
\begin{aligned}
&amp;h(p(x))+h(p(x)) = 2h(p(x)) \
&amp;h(p(x)p(x)) = h(p^2(x))
\end{aligned}
$$</p>
<p>よって$h(p^2)=2h(p)$であることが示された。</p>
<p>$h(p^k)=kh(p)$が成立することを示す。$n=1$のときは自明。$n=2$は上で示した。</p>
<p>$k\ge 3$となる$n=k$のとき$h(p^k)=kh(p)$が成立すると仮定すると、$n=k+1$のとき、
$$
h(p^{k+1})=h(p^kp)=h(p^k)+h(p)=kh(p)+h(p)=(k+1)h(p)
$$
となるので、数学的帰納法から$h(p^k)=kh(p)$であることが示された。</p>
<p>正の整数$m$について、$h(p^m) = mh(p)$となるので
$$
h(p^{n/m}) = nh(p^{1/m}) = \frac{n}{m}mh(p^{1/m}) = \frac{n}{m}h(p)
$$
となる。よって$x$が正の有理数ならば$h(p^x) = xh(p)$となる。またこれは連続性からすべての実数について成立する。</p>
<p>最後に$p=q^k$となるような正の実数$q,k$が存在したとき</p>
<p>$$
\frac{h(p)}{\ln p}=\frac{h(q^k)}{\ln q^k} = \frac{kh(q)}{k\ln q} = \frac{h(q)}{\ln q}
$$</p>
<p>となるので、$h(p)\propto \ln p$である。</p>
<h2 id="演習-129"><a class="header" href="#演習-129">演習 1.29</a></h2>
<div class="panel-primary">
<p>$M$状態の離散確率変数$x$を考え，イェンセンの不等式</p>
<p>$$
f\left(\sum_{i=1}^{M} \lambda_{i} x_{i}\right) \leq \sum_{i=1}^{M} \lambda_{i} f\left(x_{i}\right) \tag{1.115}
$$</p>
<p>を使って，確率分布$p(x)$のエントロピーが$\mathrm{H}[x] \le \ln M$を満たすことを示せ．</p>
</div>
<p>イェンセンの不等式$(1.115)$は以下の通りである。</p>
<p>$f(x)$を実数上の凸関数（いわゆる下に凸）とする。$p_{1},p_{2},\ldots$ を、$p_{1}+p_{2}+\cdots =1$を満たす正の実数の列とする。また、$x_1, , x_2, , \ldots$を実数の列とする。そのとき次式が成り立つ。</p>
<p>$$
\sum_{i=1}^{\infty} p_i f(x_i) \ge f\left( \sum_{i=1}^{\infty} p_i x_i \right)
$$</p>
<p>また関数$f(x)$が凹関数（いわゆる上に凸）ならば上式の不等号は逆になる。</p>
<p>$M$状態存在するとき、確率分布$p(x)$のエントロピーは$(1.98)$式より
$$
\mathrm{H}[x] = -\sum_{i=1}^{M}p(x_i)\ln p(x_i)=\sum_{i=1}^{M}p(x_i)\ln \frac{1}{p(x_i)}
$$
となる。ここで、$p(x_i)\ge 0,\ \sum_{i=1}^{M}p(x_i)=1$である。</p>
<p>$\ln x$は凹関数であることに注意して、イェンセンの不等式の不等号を逆にすると、</p>
<p>$$
\mathrm{H}[x] = \sum_{i=1}^{M}p(x_i)\ln \frac{1}{p(x_i)} \le \ln\left( \sum_{i=1}^{M}p(x_i)\frac{1}{p(x_i)} \right) = \ln \left( \sum_{i=1}^M 1\right) = \ln M
$$</p>
<p>となり、題意は示された。</p>
<h2 id="演習-130"><a class="header" href="#演習-130">演習 1.30</a></h2>
<div class="panel-primary">
<p>2つのガウス分布$p(x)=\mathcal{N}\left(x | \mu, \sigma^{2}\right)$と$q(x)=\mathcal{N}\left(x | m, s^{2}\right)$の間のカルバック–ライブラーダイバージェンス</p>
<p>$$
\begin{aligned}
\mathrm{KL}(p | q) &amp;=-\int p(\mathrm{x}) \ln q(\mathrm{x}) \mathrm{d} \mathrm{x}-\left(-\int p(\mathrm{x}) \ln p(\mathrm{x}) \mathrm{d} \mathrm{x}\right) \
&amp;=-\int p(\mathrm{x}) \ln \left{\frac{q(\mathrm{x})}{p(\mathrm{x})}\right} \mathrm{d} \mathrm{x}
\end{aligned} \tag{1.113}
$$</p>
<p>を計算せよ．</p>
</div>
KLの定義より
<p>$$
KL(p|q)=-\int{p(x)\ln{q(x)}}dx+\int{p(x)\ln{p(x)}}dx
$$</p>
<p>右辺第一項に</p>
<p>$$
q(x)=\frac{1}{(2\pi s^2)^\frac{1}{2}}\exp\left(-\frac{(x-m)^2}{2s^2}\right)
$$</p>
<p>を代入すると</p>
<p>$$
-\int{p(x)\ln{q(x)}}dx=\frac{1}{2}\left(\ln({2\pi s^2})\int{p(x)}dx+\frac{1}{s^2}\left(\int{p(x)x^2}dx-2m\int{p(x)x}dx+m^2\int{p(x)}dx\right)\right)
$$</p>
<p>ここで
$$
\int{p(x)}dx=1
$$</p>
<p>$$
\int{p(x)x}dx=\mu
$$</p>
<p>$$
\int{p(x)x^2}dx=\mu^2+\sigma^2
$$</p>
<p>を代入して
$$
-\int{p(x)\ln{q(x)}}dx=\frac{1}{2}\left(\ln({2\pi s^2}) +\frac{1}{s^2}\left(\mu^2+\sigma^2+m^2-2m\mu \right)\right)
$$
また、KLの式の右辺第二項は微分エントロピーにマイナスをかけたものであるので、
$$
\int{p(x)\ln{p(x)}}dx=-\frac{1}{2}\left(1+\ln{2\pi\sigma^2}\right)
$$
である。</p>
<p>よって
$$
KL(p|q)=\ln\left({\frac{s}{\sigma}}\right)+\frac{1}{2}\left(\frac{\mu^2+\sigma^2+m^2-2m\mu}{s^2}-1\right)
$$</p>
<h2 id="演習-131"><a class="header" href="#演習-131">演習 1.31</a></h2>
<div class="panel-primary">
<p>2つの変数$\mathbf{x}, \mathbf{y}$を考え，同時分布を$p(\mathbf{x},\mathbf{y})$とする．この変数の組の微分エントロピーが</p>
<p>$$
\mathrm{H}[\mathbf{x}, \mathbf{y}] \le \mathrm{H}[\mathbf{x}] + \mathrm{H}[\mathbf{y}] \tag{1.152}
$$</p>
<p>を満たし，等号は$\mathbf{x}$と$\mathbf{y}$が統計的に独立なとき，またそのときに限ることを示せ．</p>
</div>
<p>(i)
変数$\mathbf{x}, \mathbf{y}$及び同時分布$p(\mathbf{x},\mathbf{y})$の微分エントロピーは以下の形式に変換することができる
$$
\begin{aligned}
H[x] &amp;= -\int{p\left(x\right)\ln{p(x)}}dx\
&amp;= -\iint{p\left(x\right)p\left(y\right)\ln{p(x)}}dxdy\
H[y] &amp;= -\int{p\left(y\right)\ln{p(y)}}dy\
&amp;=-\iint{p\left(x\right)p\left(y\right)\ln{p(y)}}dxdy\
H[x,y] &amp;= -\iint{p\left(x, y\right)\ln{p(x,y)}}dxdy
\end{aligned}
$$
すると$\mathrm{H}[\mathbf{x}] + \mathrm{H}[\mathbf{y}]$は</p>
<p>$$
\begin{aligned}
H[x] + H[y] &amp;=-\iint{p\left(x\right)p\left(y\right)\left(\ln{p(x)} + \ln{p(y)}\right)}dxdy\
&amp;=-\iint{p\left(x\right)p\left(y\right)\ln{p(x)p(y)}}dxdy
\end{aligned}
$$
となる。</p>
<p>$p(x,y) \ge p(x)p(y)$によって,$\mathrm{H}[\mathbf{x}, \mathbf{y}] \le \mathrm{H}[\mathbf{x}] + \mathrm{H}[\mathbf{y}]$が満たされ，等号は$p(x,y) = p(x)p(y)$の時、つまり$\mathbf{x}$と$\mathbf{y}$が統計的に独立なとき，またそのときに限ることが示された．
<br><br></p>
<p>(ii)
(1.112)より
$$
H[x,y] =H[x|y]+H[x]
$$
であるので、
$$
H[x|y]+H[x]\le H[x]+H[y]
$$
すなわち
$$
H[y]-H[x|y] \ge 0
$$
を示せば良い。
<br></p>
<p>ここで、(1.121)より(左辺)は
$$
H[y]-H[x|y]=I[x,y]
$$
と書き表せ、これは変数x,yの間の相互情報量を表す。
<br></p>
<p>よって、カルバックーライブラーダイバージェンスの性質から
$$
I[x,y] \ge 0
$$
$$
H[y]-H[x|y] \ge 0
$$
$$
H[x|y]+H[x]\le H[x]+H[y]
$$
$$
H[x,y]\le H[x]+H[y]
$$</p>
<br>
となり、等号成立は $I[x,y]=0$ すなわち$x$と$y$が独立なときに限ることが示された。
<h2 id="演習-132"><a class="header" href="#演習-132">演習 1.32</a></h2>
<div class="panel-primary">
<p>連続変数のベクトル$\mathbf{x}$を考え，それが分布$p(\mathbf{x})$とそれに対応するエントロピー$\mathrm{H}[\mathbf{x}]$を持つとする．$\mathbf{x}$に非特異な線形変換を行い，新たな変数$\mathbf{y} = \mathbf{A}\mathbf{x}$を得たとする．対応するエントロピーが$\mathrm{H}[\mathbf{y}] = \mathrm{H}[\mathbf{x}] + \ln |\det (\mathbf{A})|$で与えられることを示せ．ただし$|\det (\mathbf{A})|$は$\mathbf{A}$の行列式の絶対値である．</p>
</div>
<p>(1.27)より</p>
<p>$$p(x) = p(y)\left| \frac{dy}{dx} \right|$$</p>
<p>ここで$\mathbf{y}=\mathbf{A}\mathbf{x}$より、ヤコビアンは</p>
<p>$$\frac{d\mathbf{y}}{d\mathbf{x}}=\mathbf{A}$$</p>
<p>ゆえに</p>
<p>$$p(\mathbf{x})=p(\mathbf{y})|det(\mathbf{A})|$$</p>
<p>であるから、</p>
<p>$$p(\mathbf{y})=|det(\mathbf{A})|^{-1} p(\mathbf{x})$$</p>
<p>$$d\mathbf{y}=|det(\mathbf{A})| d\mathbf{x} $$</p>
<p>と書き換えることができる。</p>
<p>以上より</p>
<p>$$\begin{aligned}H[\mathbf{y}]&amp;=-\int p(\mathbf{y}) \ln p(\mathbf{y}) d\mathbf{y} \&amp;= -\int {|det(\mathbf{A})|^{-1} p(\mathbf{x})} {\ln (|det(\mathbf{A})|^{-1} p(\mathbf{x})) }|det(\mathbf{A})| d\mathbf{x} \&amp;=-\int {|det(\mathbf{A})|^{-1} p(\mathbf{x})} {\ln (|det(\mathbf{A})|^{-1}}|det(\mathbf{A})| d\mathbf{x}-{|det(\mathbb{A})|^{-1} p(\mathbf{x})} {\ln p(\mathbf{x})}|det(\mathbf{A})| d\mathbf{x}
\&amp;=  |det(\mathbf{A})|\int p(\mathbf{x}) \ln d\mathbf{x} - \int p(\mathbf{x}) \ln p(\mathbf{x}) d\mathbf{x} \&amp;=  \ln |det(\mathbf{A})|+H[\mathbf{x}] \end{aligned} $$</p>
<br>
<p>となり、$\mathrm{H}[\mathbf{y}] = \mathrm{H}[\mathbf{x}] + \ln |\det (\mathbf{A})|$が示された。</p>
<h2 id="演習-133"><a class="header" href="#演習-133">演習 1.33</a></h2>
<div class="panel-primary">
<p>2つの離散確率変数$x,y$の間の条件付きエントロピー$\mathrm{H}[y|x]$が$0$であるとする．すると$p(x) &gt; 0$なるすべての$x$の値に対し，変数$y$は$x$の関数でなければならない，すなわち，各$x$に対して$p(y|x)\ne 0$である$y$が唯一つ存在することを示せ．</p>
</div>
<p>$$ \tag{1.98}
\mathrm{H}[p] = -\sum_{i}p(x_i)\ln p(x_i)
$$</p>
<p>$$ \tag{1.111}
\mathrm{H}[\mathbf{y} \mid \mathbf{x}] = - \iint p(\mathbf{y}, \mathbf{x}) \ln p(\mathbf{y}  \mid  \mathbf{x}) \mathrm{d}\mathbf{y} \mathrm{d}\mathbf{x}
$$</p>
<p>を参考にすると，離散確率変数の条件付きエントロピーは</p>
<p>$$
\begin{aligned}
\mathrm{H}[y \mid x] =&amp; - \sum_{i}\sum_{j} p(y_i, x_j) \ln p(y_i \mid x_j) \
=&amp; - \sum_{i}\sum_{j} p(y_i \mid x_j) p(x_j) \ln p(y_i \mid x_j) \
\end{aligned}
$$</p>
<p>となる．ここで，問題文の$\mathrm{H}[y \mid x]=0$と$p(x) &gt; 0$という条件より，</p>
<p>$$
\begin{aligned}
&amp; \mathrm{H}[y \mid x] = 0 \
\Leftrightarrow &amp; -\sum_{i}\sum_{j} p(y_i \mid x_j) p(x_j) \ln p(y_i \mid x_j) = 0 \
\Leftrightarrow &amp; - \sum_{i}\sum_{j} p(y_i \mid x_j) \ln p(y_i \mid x_j) = 0 \
\end{aligned}
$$</p>
<p>となる必要がある．ここでさらに，$0 \leqslant p(y_i \mid x_j) \leqslant 1$より，$0 &lt; - p(y_i \mid x_j) \ln p(y_i \mid x_j)$であるため結局全ての$i, j$において</p>
<p>$$
p(y_i \mid x_j) \ln p(y_i \mid x_j) = 0
$$</p>
<p>となる必要がある．これが成立するのは，$p(y_i \mid x_j) = 0$または$p(y_i \mid x_j) = 1$のときであるが，$p(y_i \mid x_j)$は確率なので$\displaystyle \sum_i p(y_i \mid x_j) = 1$となる必要がある．つまり，ある$x_j$に対して$p(y_i \mid x_j)$は$1$つだけ$1$で残りが全て$0$でなければならない．これはまさに，各$x$に対して$p(y \mid x)\ne 0$である$y$が唯一つ存在することを示している．</p>
<h2 id="演習-134"><a class="header" href="#演習-134">演習 1.34</a></h2>
<div class="panel-primary">
<p>変分法を使って，</p>
<p>$$
p(x)=\exp \left{-1+\lambda_{1}+\lambda_{2} x+\lambda_{3}(x-\mu)^{2}\right} \tag{1.108}
$$</p>
<p>式の上にある汎関数の停留点が$(1.108)$で与えられることを示せ．また，制約</p>
<p>\begin{align}
\int_{-\infty}^{\infty} p(x) \mathrm{d} x &amp;=1 \tag{1.105} \
\int_{-\infty}^{\infty} x p(x) \mathrm{d} x &amp;=\mu \tag{1.106} \
\int_{-\infty}^{\infty}(x-\mu)^{2} p(x) \mathrm{d} x &amp;=\sigma^{2} \tag{1.107}
\end{align}</p>
<p>を使ってラグランジュ乗数を消去し，最大エントロピー解がガウス分布</p>
<p>$$
p(x)=\frac{1}{\left(2 \pi \sigma^{2}\right)^{1 / 2}} \exp \left{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right} \tag{1.109}
$$</p>
<p>で与えられることを示せ．</p>
</div>
<p>(1.108)式の上にある汎関数を$\mathrm{F}(p(x))$とおくと</p>
<p>$$
\begin{aligned}
\mathrm{F}(p(x)) &amp;= \int^{\infty}<em>{-\infty} { -\ln p(x) + \lambda_1 + \lambda_2x + \lambda_3(x-\mu)^2 }p(x)\mathrm{d}x + (-\lambda_1-\lambda_2\mu-\lambda_3\sigma^2) \
&amp;= \int^{\infty}</em>{-\infty}\mathrm{G}(p(x))\mathrm{d}x + \mathrm{C}
\end{aligned}
$$</p>
<p>と変形できる。
変分法により</p>
<p>$$
\begin{aligned}
\frac{\delta\mathrm{F}(p(x))}{\delta p(x)} &amp;= \frac{\partial\mathrm{G}(p(x))}{\partial p(x)} \
&amp;= -1 -\ln p(x) + \lambda_1 + \lambda_2x + \lambda_3(x-\mu)^2
\end{aligned}
$$</p>
<p>これを$0$とおくと</p>
<p>$$
\begin{aligned}
\ln p(x) &amp;= -1 + \lambda_1 + \lambda_2x + \lambda_3(x-\mu)^2 \
p(x) &amp;= \exp {-1 + \lambda_1 + \lambda_2x + \lambda_3(x-\mu)^2}
\end{aligned}
$$</p>
<p>となり、(1.108)が導出できる。
また、最大エントロピー解がガウス分布であるとすると</p>
<p>$$
\begin{aligned}
p(x) &amp;= \exp {-1 + \lambda_1 + \lambda_2x + \lambda_3(x-\mu)^2} \
&amp;= \exp(-1 + \lambda_1 + \lambda_2x) \exp {\lambda_3(x-\mu)^2} \
&amp;= \frac{1}{(2\pi\sigma)^{1/2}}\exp{-\frac{(x-\mu)^2}{2\sigma^2}}
\end{aligned}
$$</p>
<p>とおける。
上式より</p>
<p>$$
\begin{aligned}
\lambda_1 &amp;= 1-\frac{1}{2}\ln(2\pi\sigma^2) \
\lambda_2 &amp;= 0 \
\lambda_3 &amp;= -\frac{1}{2\sigma^2}
\end{aligned}
$$</p>
<p>とするとこれは(1.105), (1.106), (1.107)を満たし、$p(x)$がガウス分布であることを示す。</p>
<h2 id="演習-135"><a class="header" href="#演習-135">演習 1.35</a></h2>
<div class="panel-primary">
<p>$$
\int_{-\infty}^{\infty} x p(x) \mathrm{d} x =\mu \tag{1.106}
$$</p>
<p>と</p>
<p>$$
\int_{-\infty}^{\infty}(x-\mu)^{2} p(x) \mathrm{d} x =\sigma^{2} \tag{1.107}
$$</p>
<p>を使って，1変数ガウス分布</p>
<p>$$
p(x)=\frac{1}{\left(2 \pi \sigma^{2}\right)^{1 / 2}} \exp \left{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right} \tag{1.109}
$$</p>
<p>のエントロピーが</p>
<p>$$
\mathrm{H}[x]=\frac{1}{2}\left{1+\ln \left(2 \pi \sigma^{2}\right)\right} \tag{1.110}
$$</p>
<p>で与えられることを示せ．</p>
</div>
<p>$(1.104)$の微分エントロピーの定義式の$\ln p(x)$の$p(x)$に$(1.105)$を代入すると</p>
<p>$$
\begin{aligned} H[x]
&amp;=-\int p(x) \ln \left(\frac{1}{\left(2 \pi \sigma^{2}\right)^{\frac{1}{2}}} \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right)\right) dx \
&amp;=-\int p(x)\left(-\frac{1}{2} \ln \left(2 \pi \sigma^{2}\right)-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right)dx \
&amp;=\frac{1}{2} \ln \left(2 \pi \sigma^{2}\right) \int p(x) d x+\frac{1}{2 \sigma^{2}} \int p(x)(x-\mu)^{2}dx \
&amp;=\frac{1}{2}\left{1+\ln \left(2 \pi \sigma^{2}\right)\right}
\end{aligned}
$$</p>
<p>式(1.106)の$\int_{-\infty}^{\infty} x p(x) \mathrm{d} x=\mu$と式(1.107)の$\int_{-\infty}^{\infty}(x-\mu)^{2} p(x) \mathrm{d} x=\sigma^{2}$を利用した。</p>
<h2 id="演習-136"><a class="header" href="#演習-136">演習 1.36</a></h2>
<div class="panel-primary">
<p>真に凸な関数はすべての弦が関数の上にあるものとして定義される．これが関数の2階微分が正であることと等価であることを示せ．</p>
</div>
<p>テイラー展開の2次まで使うと、$x=x_0$の周りで展開したとき</p>
<p>$$
f(x)=f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)+\frac{1}{2} f^{\prime\prime}\left(x^{<em>}\right)\left(x-x_{0}\right)^{2}
$$
となるような$x^{</em>}$が存在する。今、$f^{\prime \prime}\left(x^{*}\right)&gt;0$なので,</p>
<p>$$
f(x)&gt;f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)
$$</p>
<p>が成り立つ。今、$x_0$を$a, b, \lambda$を使って$x_{0}=\lambda a+(1-\lambda) b$とすると、$x=a$の点で成り立つ式は</p>
<p>$$
\begin{aligned}
f(a)&amp;&gt;f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(a-x_{0}\right) \
&amp;= f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)(1-\lambda)(a-b)
\end{aligned}\tag{1}
$$
同様に、$x=b$の点で成り立つ式は</p>
<p>$$
\begin{aligned} f(b) &amp;&gt;f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(b-x_{0}\right) \ &amp;=f\left(x_{0}\right)-f^{\prime}\left(x_{0}\right) \lambda(a-b) \end{aligned}\tag{2}
$$
(1)を$\lambda$倍、(2)を$(1-\lambda)$倍して足し合わせると、</p>
<p>$$
\lambda f(a)+(1-\lambda) f(b)&gt;f\left(x_{0}\right)=f(\lambda a+(1-\lambda) b)
$$
となり、凸性の条件式(1.114)を得る。</p>
<h2 id="演習-137"><a class="header" href="#演習-137">演習 1.37</a></h2>
<div class="panel-primary">
<p>$$
\mathrm{H}[\mathbf{y} \mid \mathbf{x}]=-\iint p(\mathbf{y}, \mathbf{x}) \ln p(\mathbf{y} \mid \mathbf{x}) \mathrm{d} \mathbf{y} \mathrm{d} \mathbf{x} \tag{1.111}
$$</p>
<p>の定義と確率の乗法定理から，</p>
<p>$$
\mathrm{H}[\mathbf{x}, \mathbf{y}]=\mathrm{H}[\mathbf{y} \mid \mathbf{x}]+\mathrm{H}[\mathbf{x}] \tag{1.112}
$$</p>
<p>を証明せよ．</p>
</div>
<p>式$(1.104)$の$H[\mathbf{x}]= - \int p(\mathbf{x}) \ln p(\mathbf{x}) \mathrm{d}\mathbf{x}$ より、$p(\mathbf{x},\mathbf{y})$の微分エントロピーは、</p>
<p>$$
\begin{aligned}
H[\mathbf{x},\mathbf{y}]
&amp;= - \int \int p(\mathbf{x},\mathbf{y}) \ln p(\mathbf{x},\mathbf{y}) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y}
\end{aligned}
$$</p>
<p>同時分布$p(\mathbf{x},\mathbf{y}) = p(\mathbf{y}|\mathbf{x})p(\mathbf{x})$ より、</p>
<p>$$
\begin{aligned}
H[\mathbf{x},\mathbf{y}]
&amp;= - \int \int p(\mathbf{x},\mathbf{y}) \ln (p(\mathbf{y}|\mathbf{x})p(\mathbf{x})) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y} \
&amp;= - \int \int p(\mathbf{x},\mathbf{y}) (\ln p(\mathbf{y}|\mathbf{x}) + \ln p(\mathbf{x})) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y} \
&amp;= - \int \int p(\mathbf{x},\mathbf{y}) \ln p(\mathbf{y}|\mathbf{x}) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y} - \int \int p(\mathbf{x},\mathbf{y}) \ln p(\mathbf{x}) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y}
\end{aligned}
$$</p>
<p>$ \int \int p(\mathbf{x},\mathbf{y}) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y}  = \int p(\mathbf{x}) \mathrm{d}\mathbf{x}$ より、</p>
<p>$$
\begin{aligned}
H[\mathbf{x},\mathbf{y}]
&amp;= - \int \int p(\mathbf{x},\mathbf{y}) \ln p(\mathbf{y}|\mathbf{x}) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y} - \int p(\mathbf{x}) \ln p(\mathbf{x}) \mathrm{d}\mathbf{x} \
&amp;= H[\mathbf{y}| \mathbf{x}] + H[\mathbf{x}]
\end{aligned}
$$</p>
<h2 id="演習-138"><a class="header" href="#演習-138">演習 1.38</a></h2>
<div class="panel-primary">
<p>数学的帰納法により，凸関数に関する不等式</p>
<p>$$
f(\lambda a+(1-\lambda) b) \leq \lambda f(a)+(1-\lambda) f(b) \tag{1.114}
$$</p>
<p>から
$$
f\left(\sum_{i=1}^{M} \lambda_{i} x_{i}\right) \leq \sum_{i=1}^{M} \lambda_{i} f\left(x_{i}\right) \tag{1.115}
$$</p>
<p>が導かれることを示せ．</p>
</div>
<p>$(1.114)$式は、高校数学でやったJensenの不等式</p>
<p><img src="/attachment/60af4328d83a250c8eef7952" alt="Jensenの不等式.png" /></p>
<br>
<p>$(1.114)$式の凸性を表す式$f(\lambda a+(1-\lambda) b) \leq \lambda f(a)+(1-\lambda) f(b)$から</p>
<p>$$
f\left(\sum_{i=1}^{M} \lambda_{i} x_{i}\right) \leq \sum_{i=1}^{M} \lambda_{i} f\left(x_{i}\right) \cdots(*)
$$</p>
<p>が成立することを数学的帰納法で示す。ここで、$\lambda_i\geq0$および$\sum_{i}\lambda_i = 1$である。</p>
<br>
<p>$(ⅰ) M=1$のとき、</p>
<p>$\lambda=1$なので、$(*)$式は$f(x_1) \le f(x_1)$となるので成立する。</p>
<p>$(ⅱ) M=2$のとき、</p>
<p>$\lambda_2=1-\lambda_1$であることに注意すると</p>
<p>$$
f\left(\lambda_{1} x_{1}+\left(1-\lambda_{1}\right) x_{2}\right) \leq \lambda_{1} f\left(x_{1}\right)+\left(1-\lambda_{1}\right) f\left(x_{2}\right)
$$</p>
<p>これは$(1.114)$式と同じなので、成立する。</p>
<p>$(ⅲ) M=k\ (k\ge 2)$のとき、</p>
<p>$$
f\left(\sum_{i=1}^{k} \lambda_{i} x_{i}\right) \leq \sum_{i=1}^{k} \lambda_{i} f\left(x_{i}\right) <br />
$$</p>
<p>成立していると仮定すると、$M=k+1$のとき</p>
<p>$$
\begin{aligned}
f\left(\sum_{i=1}^{k+1} \lambda_{i} x_{i}\right) &amp;=f\left(\sum_{i=1}^{k} \lambda_{i} x_{i}+\lambda_{k+1} x_{k+1}\right) \
&amp;=f\left(\left(1-\lambda_{k+1}\right) \frac{\sum_{i=1}^{k} \lambda_{i} x_{i}}{1-\lambda_{k+1}}+\lambda_{k+1} x_{k+1}\right) \
&amp; \leq\left(1-\lambda_{k+1}\right) f\left(\frac{\sum_{i=1}^{k} \lambda_{i} x_{i}}{1-\lambda_{k+1}}\right)+\lambda_{k+1} f\left(x_{k+1}\right)\hspace{1em}((1.114)式より)
\end{aligned}
$$</p>
<p>ここで、$\displaystyle \frac{\sum_{i=1}^{k} \lambda_{i}}{1-\lambda_{k+1}}=\frac{1-\lambda_{k+1}}{1-\lambda_{k+1}}=1$となることに注意して、</p>
<p>また、$f\left(\sum_{i=1}^{k} \lambda_{i} x_{i}\right) \leq \sum_{i=1}^{k} \lambda_{i} f\left(x_{i}\right) $の仮定が適用できるので、</p>
<p>$$
\begin{aligned}
&amp;\left(1-\lambda_{k+1}\right) f\left(\frac{\sum_{i=1}^{k} \lambda_{i} x_{i}}{1-\lambda_{k+1}}\right)+\lambda_{k+1} f\left(x_{k+1}\right) \
\leq&amp;\left(1-\lambda_{k+1}\right) \sum_{i=1}^{k} \frac{\lambda_{i}}{1-\lambda_{k+1}} f\left(x_{i}\right)+\lambda_{k+1} f\left(x_{k+1}\right)  \
=&amp;\sum_{i=1}^{k} \lambda_{i} f\left(x_{i}\right)+\lambda_{k+1} f\left(x_{k+1}\right) \
=&amp;\sum_{i=1}^{k+1} \lambda_{i} f\left(x_{i}\right)
\end{aligned}
$$</p>
<p>となるので、$M=k+1$でも成立することが示された。</p>
<p>したがって数学的帰納法より、$(1.115)$式が成立することが示された。</p>
<h2 id="演習-139"><a class="header" href="#演習-139">演習 1.39</a></h2>
<div class="panel-primary">
<p>2つの2値変数$x,y$が表1.3の同時分布を持つとする．以下の量を計算せよ．</p>
<p>(a) $\mathrm{H}[x]$, (b) $\mathrm{H}[y]$, (c) $\mathrm{H}[y|x]$, (d) $\mathrm{H}[x|y]$, (e) $\mathrm{H}[x,y]$, (f) $\mathrm{I}[x,y]$</p>
<p>これらのさまざまな量の間の関係を示す図を描け．</p>
</div>
<p>まず表1.3の同時分布$p(x,y)$を元に$p(x),p(y),p(y|x),p(x|y)$の同時分布の表を作成する。$\displaystyle p(y|x) = \frac{p(x,y)}{p(x)}$を用いる。</p>
<p>$$
\begin{array} {rr|rr}
&amp;&amp;&amp;y\
&amp; &amp; 0 &amp; 1 \
\hline x &amp; 0 &amp; 1/3 &amp; 1/3 \
&amp; 1 &amp; 0 &amp; 1/3 \
\end{array} \
p(x,y)
$$
$$
\begin{array} {rr|r}
&amp; &amp; \
\hline x &amp; 0 &amp; 2/3 \
&amp; 1 &amp; 1/3 \
\end{array} \
p(x)
$$
$$
\begin{array} {rr}
&amp; y \
0 &amp; 1 \
\hline 1/3 &amp; 2/3 \
\end{array} \
p(y)
$$
$$
\begin{array} {rr|rr}
&amp;&amp;&amp;y\
&amp; &amp; 0 &amp; 1 \
\hline x &amp; 0 &amp; 1/2 &amp; 1/2 \
&amp; 1 &amp; 0 &amp; 1 \
\end{array} \
p(y|x)
$$
$$
\begin{array} {rr|rr}
&amp;&amp;&amp;y\
&amp; &amp; 0 &amp; 1 \
\hline x &amp; 0 &amp; 1 &amp; 1/2 \
&amp; 1 &amp; 0 &amp; 1/2 \
\end{array} \
p(x|y)
$$</p>
<p><strong>(a)</strong>
$$
\begin{aligned}
\mathrm{H}[x]&amp;= -\sum_{i}p(x)\ln p(x) \
&amp;= -\left{ \frac{2}{3}\ln\frac{2}{3}+\frac{1}{3}\ln\frac{1}{3} \right} \
&amp;= \ln 3 - \frac{2}{3}\ln 2
\end{aligned}
$$</p>
<p><strong>(b)</strong>
$$
\begin{aligned}
\mathrm{H}[y]&amp;= -\left{ \frac{1}{3}\ln\frac{1}{3}+\frac{2}{3}\ln\frac{2}{3} \right} \
&amp;= \ln 3 - \frac{2}{3}\ln 2
\end{aligned}
$$</p>
<p><strong>(c)</strong>
$$
\begin{aligned}
\mathrm{H}[y|x]&amp;= -\sum_{i}\sum_{j}p(y,x)\ln p(y|x)dydx \
&amp;= -\left{ \frac{1}{3}\ln\frac{1}{2}+\frac{1}{3}\ln\frac{1}{2} + \frac{1}{3}\ln 1 \right} \
&amp;= \frac{2}{3}\ln 2
\end{aligned}
$$</p>
<p><strong>(d)</strong>
$$
\begin{aligned}
\mathrm{H}[x|y]&amp;= -\left{ \frac{1}{3}\ln 1 + \frac{1}{3}\ln\frac{1}{2}+\frac{1}{3}\ln\frac{1}{2} \right} \
&amp;= \frac{2}{3}\ln 2
\end{aligned}
$$</p>
<p><strong>(e)</strong>
$$
\begin{aligned}
\mathrm{H}[x,y]&amp;= -\left{ \frac{1}{3}\ln \frac{1}{3} + \frac{1}{3}\ln \frac{1}{3}+\frac{1}{3}\ln\frac{1}{3} \right} \
&amp;= \ln 3
\end{aligned}
$$
または$\mathrm{H}[x,y]=\mathrm{H}[y|x]+\mathrm{H}[x]=\mathrm{H}[x|y]+\mathrm{H}[y]=\ln 3$からも求まる。</p>
<p><strong>(f)</strong>
$$
\begin{aligned}
\mathrm{I}[x,y]&amp;= \mathrm{H}[x] - \mathrm{H}[x|y] = \mathrm{H}[y] - \mathrm{H}[y|x]  \
&amp;= \ln 3 - \frac{4}{3}\ln 2
\end{aligned}
$$</p>
<img src="/attachment/5f9a86d38fcbd9600d3963ed" width="600px">
<h2 id="演習-140"><a class="header" href="#演習-140">演習 1.40</a></h2>
<div class="panel-primary">
<p>イェンセンの不等式</p>
<p>$$
f\left(\sum_{i=1}^{M} \lambda_{i} x_{i}\right) \leq \sum_{i=1}^{M} \lambda_{i} f\left(x_{i}\right) \tag{1.115}
$$</p>
<p>を$f(x)=\ln x$に適用し，実数集合の算術平均が，幾何平均より決して小さくならないことを示せ．</p>
</div>
<p>算術平均$\displaystyle x_a=\frac{1}{N}\sum_{i=1}^N x_i$, 幾何平均$\displaystyle x_g = \left( \prod_{i=1}^N x_i\right)^{\frac{1}{N}}$である。
これについて$x_a \ge x_g$が成立することをイェンセンの不等式を用いて示す。</p>
<p>問題設定で$f(x) = \ln x$に適用し、とあるので$x&gt;0$である。よって$\ln x_a \ge \ln x_g$であることを示すことにする。すなわち</p>
<p>$$
\ln x_a = \ln \left( \sum_{i=1}^N \frac{1}{N}x_i \right)
$$</p>
<p>$$
\ln x_g = \ln \left( \prod_{i=1}^N x_i \right)^{\frac{1}{N}} = \sum_{i=1}^N \frac{1}{N} \ln x_i
$$</p>
<p>凹関数である$f(x)=\ln x$を用いてイェンセンの不等式を適用すると</p>
<p>$$
\ln x_a = \ln \left( \sum_{i=1}^N \frac{1}{N}x_i \right) \ge \sum_{i=1}^N \frac{1}{N} \ln x_i = \ln x_g
$$</p>
<p>となるので、$x_a \ge x_g$となることが示された。</p>
<h2 id="演習-141"><a class="header" href="#演習-141">演習 1.41</a></h2>
<div class="panel-primary">
<p>確率の加法･乗法定理を使って，相互情報量$I(\mathbf{x},\mathbf{y})$が</p>
<p>$$
\mathrm{I}[\mathbf{x}, \mathbf{y}]=\mathrm{H}[\mathbf{x}]-\mathrm{H}[\mathbf{x} \mid \mathbf{y}]=\mathrm{H}[\mathbf{y}]-\mathrm{H}[\mathbf{y} \mid \mathbf{x}] \tag{1.121}
$$</p>
<p>の関係を満たすことを示せ．</p>
</div>
<p>$$
\begin{aligned}
\mathrm{I}[\mathbf{x}, \mathbf{y}] &amp;=\mathrm{KL}(p(\mathbf{x}, \mathbf{y}) | p(\mathbf{x}) p(\mathbf{y})) \
&amp;=-\iint p(\mathbf{x}, \mathbf{y}) \ln \left(\frac{p(\mathbf{x}) p(\mathbf{y})}{p(\mathbf{x}, \mathbf{y})}\right) d\mathbf{x} d\mathbf{y}
\end{aligned}
$$</p>
<p>である。ここで確率の乗法定理$p(\mathbf{x},\mathbf{y})=p(\mathbf{y}|\mathbf{x})p(\mathbf{x})$から</p>
<p>$$
\begin{aligned}
\mathrm{I}[\mathbf{x}, \mathbf{y}]&amp;=-\iint p(\mathbf{x}, \mathbf{y}) \ln \left(\frac{p(\mathbf{x}) p(\mathbf{y})}{p(\mathbf{y} \mid \mathbf{x})p(\mathbf{x})}\right) d \mathbf{x} d \mathbf{y} \
&amp;=-\iint p(\mathbf{x}, \mathbf{y}) \ln \left(\frac{p(\mathbf{y})}{p(\mathbf{y} \mid \mathbf{x})}\right) d \mathbf{x} d \mathbf{y} \
&amp;=-\iint p(\mathbf{x}, \mathbf{y}) \ln p(\mathbf{y}) d \mathbf{x} d \mathbf{y}+\iint p(\mathbf{x}, \mathbf{y}) \ln p(\mathbf{y} \mid \mathbf{x}) d \mathbf{x} d \mathbf{y}\
&amp;=-\int p(\mathbf{y}) \ln p(\mathbf{y}) d \mathbf{y}+\iint p(\mathbf{x}, \mathbf{y}) \ln p(\mathbf{y} \mid \mathbf{x}) d \mathbf{x} d \mathbf{y} \
&amp;=\mathrm{H}[\mathbf{y}]-\mathrm{H}[\mathbf{y} | \mathbf{x}]
\end{aligned}
$$</p>
<p>最後は</p>
<p>$$
\mathrm{H}[\mathbf{y} \mid \mathbf{x}]=-\iint p(\mathbf{y}, \mathbf{x}) \ln p(\mathbf{y} \mid \mathbf{x}) \mathrm{d} \mathbf{y} \mathrm{d} \mathbf{x}  \tag{1.111}
$$</p>
<p>式を用いた。また、$p(\mathbf{x}, \mathbf{y})=p(\mathbf{x}|\mathbf{y})p(\mathbf{y})$を用いれば同様にして</p>
<p>$$
\mathrm{I}[\mathbf{x}, \mathbf{y}] =\mathrm{H}[\mathbf{x}]-\mathrm{H}[\mathbf{x|y}]
$$</p>
<p>が求まる。</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="prml第2章演習問題解答"><a class="header" href="#prml第2章演習問題解答">PRML第2章演習問題解答</a></h1>
<head>
<style>
  div.panel-primary {
	border: 1px solid #000;
    margin: 10px 5px;
    padding: 16px 10px 0px;
  }
</style>
</head>
<h2 id="演習-21"><a class="header" href="#演習-21">演習 2.1</a></h2>
<div class="panel-primary">
<p>ベルヌーイ分布</p>
<p>$$
\textrm{Bern}(x|\mu)=\mu^x(1-\mu)^{1-x} \tag{2.2}
$$</p>
<p>が次の性質を満たすことを確かめよ.</p>
<p>$$
\sum_{x=0}^{1} p(x | \mu) =1 \tag{2.257}
$$</p>
<p>$$
\mathbb{E}[x] = \mu \tag{2.258}
$$
$$
\operatorname{var}[x] = \mu(1-\mu) \tag{2.259}
$$</p>
<p>ベルヌーイ分布に従う二値確率変数$x$のエントロピー$\mathrm{H}[x]$が</p>
<p>$$
\mathrm{H}[x]=-\mu \ln \mu-(1-\mu) \ln (1-\mu) \tag{2.260}
$$
で与えられることを示せ．</p>
</div>
<p>$\textrm{Bern}(x|\mu)=\mu^x(1-\mu)^{1-x}$であるから、</p>
<p>$$
\sum_{x=0}^{1} p(x | \mu)=\mu^{0}(1-\mu)^{1}+\mu^{1}(1-\mu)^{0}=1
$$</p>
<p>$$
\mathbb{E}[x]=\sum_{x=0}^{1}x\mu^x(1-\mu)^{1-x} =\mu
$$</p>
<p>$$
\operatorname{var}[x] = \mathbb{E}[x^2]-(\mathbb{E}[x])^2=\mu-\mu^2=\mu(1-\mu)
$$</p>
<p>また、エントロピー$\textrm{H}[x]$については</p>
<p>$$
\begin{aligned} H[x] &amp;=-\sum_{x=0}^{1} \operatorname{Bern}(x | \mu) \cdot \ln \operatorname{Bern}(x | \mu) \
&amp;=-\sum_{x=0}^{1}\left(\mu^{x}(1-\mu)^{1-x} \ln \mu^{x}(1-\mu)^{1-x}\right) \
&amp;=-((1-\mu) \cdot \ln (1-\mu)+\mu \ln \mu) \ &amp;=-\mu \ln \mu-(1-\mu) \ln (1-\mu) \end{aligned}
$$</p>
<h2 id="演習-22"><a class="header" href="#演習-22">演習 2.2</a></h2>
<div class="panel-primary">
<p>ベルヌーイ分布の</p>
<p>$$
\textrm{Bern}(x|\mu)=\mu^x(1-\mu)^{1-x} \tag{2.2}
$$</p>
<p>の表現では, $x$の2つの値$0$と$1$に関して対称ではない. 場合によっては，対称な$x \in{-1,1}$を用いた等価な表現の方が便利である. このとき分布は</p>
<p>$$
p(x | \mu)=\left(\frac{1-\mu}{2}\right)^{(1-x) / 2}\left(\frac{1+\mu}{2}\right)^{(1+x) / 2} \tag{2.261}
$$</p>
<p>と書くことができる. ただし, $\mu \in[-1,1]$である. この分布$(2.261)$が正規化されていることを示し, その平均, 分散, およびエントロピーを計算せよ.</p>
</div>
<p>$x \in{-1,1}$の二値のときに正規化されていることをまず示す。
$$
\begin{aligned} \sum_{x=-1}^{1} p(x | \mu) d x &amp;=p(x=-1 | \mu)+p(x=1 | \mu) \ &amp;=\left(\frac{1-\mu}{2}\right)+\left(\frac{1+\mu}{2}\right)=1 \end{aligned}
$$
続いて、$\mathbb[x], \operatorname{var}[x], \textrm{H}[x]$について、
$$
\begin{aligned}
\mathbb{E}[x] &amp;=\sum x p(x | \mu)=(-1) \frac{1-\mu}{2}+\frac{1+\mu}{2}=\mu \
\operatorname{var}[x] &amp;=\mathbb{E}\left[x^{2}\right]-{\mathbb{E}[x]}^{2}=\left(\frac{1-\mu}{2}\right)+\left(\frac{1+\mu}{2}\right)-\mu^{2}=1-\mu^{2} \
\textrm{H}[x] &amp;=-\sum p(x | \mu) \ln p(x | \mu) \
&amp;=-\left(\frac{1-\mu}{2} \ln \frac{1-\mu}{2}+\frac{1+\mu}{2} \ln \frac{1+\mu}{2}\right) \end{aligned}
$$</p>
<h2 id="演習-23"><a class="header" href="#演習-23">演習 2.3</a></h2>
<div class="panel-primary">
<p>この演習問題では, 二項分布</p>
<p>$$
\operatorname{Bin}(m \mid N, \mu)=\left(\begin{array}{l}N \ m\end{array}\right) \mu^{m}(1-\mu)^{N-m} \tag{2.9}
$$</p>
<p>が正規化されていることを証明する. まず, 全部で$N$個ある対象から$m$個の同じものを選ぶ組み合わせの数の定義(2.10）を用いて,</p>
<p>$$
\left(\begin{array}{l}N \ m\end{array}\right)+\left(\begin{array}{c}N \ m-1\end{array}\right)=\left(\begin{array}{c}N+1 \ m\end{array}\right) \tag{2.262}
$$</p>
<p>を示せ. この結果を用い, 帰納法で次の結果を証明せよ.</p>
<p>$$
(1+x)^{N}=\sum_{m=0}^{N}\left(\begin{array}{l}N \ m\end{array}\right) x^{m} \tag{2.263}
$$</p>
<p>これは, <b>二項定理 (binomialtheorem)</b>として知られ，すべての実数値$x$について成り立つ．最後に二項分布が次のように正規化されていることを，$(1-\mu)^N$を和の外に出してから，二項定理を使うことで示せ．</p>
<p>$$
\sum_{m=0}^{N}\left(\begin{array}{l}N \ m\end{array}\right) \mu^{m}(1-\mu)^{N-m}=1 \tag{2.264}
$$</p>
</div>
<p>$(2.262)$を示す。</p>
<p>$$
\begin{aligned}\left(\begin{array}{c}N \ m\end{array}\right)+\left(\begin{array}{c}N \
m-1\end{array}\right) &amp;=\frac{N !}{m !(N-m) !}+\frac{N !}{(m-1) !(N-m+1)!} \
&amp;=\frac{N !}{(m-1) !(N-m) !}\left(\frac{1}{m}+\frac{1}{N-m+1}\right) \
&amp;=\frac{N !}{(m-1) !(N-m) !}\left(\frac{N+1}{m(N-m+1)}\right) \
&amp;=\frac{(N+1) !}{m !(N+1-m) !}=\left(\begin{array}{c}N+1 \
m\end{array}\right) \end{aligned}
$$</p>
<p>二項定理$(2.263)$を帰納法で示す。
$N=1$のとき、</p>
<p>$$
(左辺)=(1+x)=\left(\begin{array}{c}1 \ 0\end{array}\right) x^{0}+\left(\begin{array}{c}1 \ 1\end{array}\right) x^{1}=\sum_{n=0}^{1}\left(\begin{array}{c}1 \ m\end{array}\right) x^{m}
$$</p>
<p>であり成立する。次に、$N=k$において式$(2.263)$が成立すると仮定したとき、$N=k+1$のときは</p>
<p>$$
\begin{aligned}(1+x)^{k+1} &amp;=(1+x) \sum_{m=0}^{k}\left(\begin{array}{c}k \ m\end{array}\right) x^{m}=\sum_{m=0}^{k}\left(\begin{array}{c}k \ m\end{array}\right) x^{m}+x \sum_{m=0}^{k}\left(\begin{array}{c}k \ m\end{array}\right) x^{m} \ &amp;=1+\sum_{m=1}^{k}\left{\left(\begin{array}{c}k \ m\end{array}\right)+\left(\begin{array}{c}k \ m-1\end{array}\right)\right} x^{m}+x^{k+1} \ &amp;=1+\sum_{m=1}^{k}\left(\begin{array}{c}k+1 \ m\end{array}\right) x^{m}+x^{k+1} \ &amp;=\sum_{m=0}^{k+1}\left(\begin{array}{c}k+1 \ m\end{array}\right) x^{m} \end{aligned}
$$</p>
<p>よって、$N=k+1$のときでも成立するので、帰納法より式$(2.263)$は示された。</p>
<p>最後に正規化の式$(2.264)$について、二項定理から</p>
<p>$$
\left{(1-\mu)+\mu\right}^N=\sum_{m=0}^{N}\left(\begin{array}{c}N \ m \end{array} \right) \mu^m (1-\mu)^{N-m}
$$
が成立する。ここで、$(左辺)=1^N=1$なので、式(2.264)は成立する。</p>
<h2 id="演習-24"><a class="header" href="#演習-24">演習 2.4</a></h2>
<div class="panel-primary">
<p>二項分布の平均が</p>
<p>$$
\mathbb{E}[m] \equiv \sum_{m=0}^{N} m \operatorname{Bin}(m \mid N, \mu)=N \mu \tag{2.11}
$$</p>
<p>であることを示せ．これには，正規化条件$(2.264)$の両辺を$\mu$で微分し，変形して$n$の平均を求めよ．同様に，$(2.264)$の両辺を$\mu$について2階微分し，二項分布の平均$(2.11)$も用いて，二項分布の分散の結果</p>
<p>$$
\operatorname{var}[m] \equiv \sum_{m=0}^{N}(m-\mathbb{E}[m])^{2} \operatorname{Bin}(m \mid N, \mu)=N \mu(1-\mu) \tag{2.12}
$$</p>
<p>を証明せよ．</p>
</div>
<p>$(2.264)$の両辺を$\mu$で微分すると</p>
<p>$$
\begin{array}{l}
\displaystyle \sum_{m=0}^{N}\left(\begin{array}{l}N \ m\end{array}\right)\left(m \mu^{m-1}(1-\mu)^{N-m}-(N-m) \mu^{m}(1-\mu)^{N-m-1}\right)=0 \
\displaystyle \sum_{m=0}^{N}\left(\begin{array}{l}N \ m\end{array}\right) \mu^{m-1}(1-\mu)^{N-m-1}(m(1-\mu)-(N-m)\mu)=0 \
\displaystyle \sum_{m=0}^{N}\left(\begin{array}{l}N \ m\end{array}\right) \mu^{m-1}(1-\mu)^{N-m-1}(m-N \mu)=0 \
\displaystyle \sum_{m=0}^{N}\left(\begin{array}{l}N \ m\end{array}\right) m \mu^{m-1}(1-\mu)^{N-m-1}=N\mu \displaystyle \sum_{m=0}^{N}\left(\begin{array}{l}N \ m\end{array}\right) \mu^{m-1}(1-\mu)^{N-m-1}
\end{array}
$$</p>
<p>両辺に$\mu(1-\mu)$をかけると</p>
<p>$$
\underbrace{\sum_{m=0}^{N}\left(\begin{array}{l}N \ m\end{array}\right) m \mu^{m}(1-\mu)^{N-m}}<em>{\mathbb E[m]} = N\mu \underbrace{\sum</em>{m=0}^{N}\left(\begin{array}{l}N \ m\end{array}\right) \mu^{m}(1-\mu)^{N-m}}_{1}
$$</p>
<p>よって$\mathbb E[m]=N\mu$が示された。</p>
<p>分散について,</p>
<p>$$
\begin{aligned} \operatorname{var}[m] &amp;=\mathbb{E}\left[m^{2}\right]-(\mathbb{E}[m])^{2} \ &amp;=\mathbb{E}[m(m-1)]+\mathbb{E}[m]-(\mathbb{E}[m])^{2} \end{aligned}
$$</p>
<p>であるから、$\mathbb{E}[m(m-1)]$の値を求める。</p>
<p>$$
\begin{aligned} E[m(m-1)] &amp;=\sum_{m=0}^{N} m(m-1)\left(\begin{array}{l}N \ m\end{array}\right) \mu^{m}(1-\mu)^{N-m} \
&amp;=\sum_{m=2}^{N} m(m-1)\left(\begin{array}{l}N \ m\end{array}\right) \mu^{m}(1-\mu)^{N-m} \ &amp;=\sum_{m=2}^{N} m(m-1) \frac{N(N-1)(N-2) !}{(N-m) ! m(m-1)(m-2) !} \mu^{m}(1-\mu)^{N-m} \
&amp;=N(N-1) \sum_{m=2}^{N}\left(\begin{array}{l}N-2 \ m-2\end{array}\right) \mu^{m}(1-\mu)^{N-m} \
&amp;=N(N-1) \mu^{2} \underbrace{\sum_{\ell=0}^{N-2}\left(\begin{array}{c}N-2 \ \ell \end{array}\right) \mu^{\ell}(1-\mu)^{N-2-\ell}}_1 \
&amp;=N(N-1) \mu^{2} \end{aligned}
$$</p>
<p>よって</p>
<p>$$
\begin{aligned} \operatorname{var}[m] &amp;=N(N-1) \mu^{2}+N \mu-(N \mu)^{2} \ &amp;=N \mu(1-\mu) \end{aligned}
$$</p>
<p>これは$(2.12)$と一致する。</p>
<h2 id="演習-25"><a class="header" href="#演習-25">演習 2.5</a></h2>
<div class="panel-primary">
<p>この演習問題では,</p>
<p>$$
\operatorname{Beta}(\mu \mid a, b)=\frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} \mu^{a-1}(1-\mu)^{b-1} \tag{2.13}
$$</p>
<p>のベータ分布が,</p>
<p>$$
\int_{0}^{1} \operatorname{Beta}(\mu \mid a, b) \mathrm{d} \mu=1 \tag{2.14}
$$</p>
<p>が成立するように正しく正規化されていることを証明する. これは</p>
<p>$$
\int_{0}^{1} \mu^{a-1}(1-\mu)^{b-1} \mathrm{d} \mu=\frac{\Gamma(a) \Gamma(b)}{\Gamma(a+b)} \tag{2.265}
$$</p>
<p>を示すことと等価である．ガンマ関数の定義</p>
<p>$$
\Gamma(x) \equiv \int_{0}^{\infty} u^{x-1} e^{-u} \mathrm{d} u \tag{1.141}
$$</p>
<p>より，</p>
<p>$$
\Gamma(a) \Gamma(b)=\int_{0}^{\infty} \exp (-x) x^{a-1} \mathrm{d} x \int_{0}^{\infty} \exp (-y) y^{b-1} \mathrm{d} y \tag{2.266}
$$</p>
<p>を得る。この式を用いて，次のようにして$(2.265)$を証明せよ．まず$y$についての積分を, $x$についての積分の被積分関数の中に移す．次に$x$を固定して$t=y+x$と置換し，$x$と$t$の積分の順序を交換する．最後に，$t$を固定して$x=t\mu$と置換する．</p>
</div>
<p>式$(2.265)$を証明する。</p>
<p>$$
\begin{aligned} \Gamma(a) \Gamma(b) &amp;=\int_{0}^{\infty} \exp (-x) x^{a-1} dx \int_{0}^{\infty} \exp (-y) y^{b-1} dy \
&amp;=\int_{0}^{\infty} \exp (-x) x^{a-1}\left{ \int_{0}^{\infty} \exp (-y) y^{b-1} dy\right} dx \
&amp;=\int_{0}^{\infty} \int_{0}^{\infty} \exp (-x-y) x^{a-1} y^{b-1} dydx \end{aligned}
$$</p>
<p>$x$を固定してから$t=y+x$とおくと, $0 \le y \le \infty$なので, $x\le t \le \infty$となる。</p>
<p>$$
\Gamma(a) \Gamma(b)=\int_{0}^{\infty} \int_{x}^{\infty} \exp (-t) x^{a-1}(t-x)^{b-1} dt dx
$$</p>
<p>次に上式の$t$に関する積分と$x$に関する積分を入れ替える.</p>
<p>積分範囲について注意すると、$x$は$0\le x\le \infty$, $t$は$x\le t \le \infty$であり, 順番を入れ替えると$0\le t \le \infty$の積分範囲で, $x$は$0\le x \le t$の範囲となる。よって,</p>
<p>$$
\Gamma(a) \Gamma(b)=\int_{0}^{\infty} \int_{0}^{t} \exp (-t) x^{a-1}(t-x)^{b-1} dx dt
$$</p>
<p>さらに$x=t\mu$と置換すると, 積分範囲は$0\le \mu \le 1, dx=td\mu$である.</p>
<p>$$
\Gamma(a) \Gamma(b)=\int_{0}^{\infty} \int_{0}^{1} \mu^{a-1} t^{a-1} \exp (-t) \cdot t^{b-1}(1-\mu)^{b-1} \cdot t d\mu dt
$$</p>
<p>$\mu$と$t$の積分に分離すると</p>
<p>$$
\Gamma(a) \Gamma(b)=\int_{0}^{\infty} t^{a+b-1} \exp (-t) dt \cdot \int_{0}^{1} \mu^{a-1}(1-\mu)^{b-1} d\mu
$$</p>
<p>ここで$\int_{0}^{\infty} \exp (-t) t^{a+b-1} d t=\Gamma(a+b)$なので,</p>
<p>$$
\int_{0}^{1} \mu^{a-1}(1-\mu)^{b-1} \mathrm{d} \mu=\frac{\Gamma(a) \Gamma(b)}{\Gamma(a+b)}
$$</p>
<p>よって, 式$(2.265)$は示された。</p>
<h2 id="演習-26"><a class="header" href="#演習-26">演習 2.6</a></h2>
<div class="panel-primary">
<p>$(2.265)$の結果を用いて，ベータ分布</p>
<p>$$
\operatorname{Beta}(\mu \mid a, b)=\frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} \mu^{a-1}(1-\mu)^{b-1} \tag{2.13}
$$</p>
<p>の平均，分散，およびモードがそれぞれ</p>
<p>$$
\mathbb{E}[\mu] =\frac{a}{a+b} \tag{2.267}
$$
$$
\operatorname{var}[\mu] =\frac{a b}{(a+b)^{2}(a+b+1)} \tag{2.268}
$$
$$
\operatorname{mode}[\mu] =\frac{a-1}{a+b-2} \tag{2.269}
$$</p>
<p>になることを示せ.</p>
</div>
<p>式(2.265)$\int_{0}^{1} \mu^{a-1}(1-\mu)^{b-1} d \mu=\frac{\Gamma(a) \Gamma(b)}{\Gamma(a+b)}$を用いる。</p>
<p>$$
\begin{aligned} \mathbb{E}[\mu] &amp;=\int_{0}^{1} \mu \cdot \operatorname{Beta}(\mu | a, b) d \mu \ &amp;=\int_{0}^{1} \mu \frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} \mu^{a-1}(1-\mu)^{b-1} d \mu \ &amp;=\frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} \int_{0}^{1} \mu^{a}(1-\mu)^{b-1} d \mu=\frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} \cdot \frac{\Gamma(a+1) \Gamma(b)}{\Gamma(a+b+1)} \end{aligned}
$$</p>
<p>ここでガンマ関数の性質$\Gamma(x+1)=x\Gamma(x)$（演習1.17）を利用すると, 平均$\mathbb{E}[\mu]$は</p>
<p>$$
\mathbb{E}[\mu]=\frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} \cdot \frac{a \Gamma(a) \Gamma(b)}{(a+b) \Gamma(a+b)}=\frac{a}{a+b}
$$</p>
<p>次に, 分散$\operatorname{var}[\mu]$の算出のために$\mathbb{E}[\mu^2]$を計算する</p>
<p>$$
\begin{aligned} \mathbb{E}\left[\mu^{2}\right] &amp;=\int_{0}^{1} \mu^{2} \cdot \operatorname{Beta}(\mu | a, b) d \mu \ &amp;=\frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} \int_{0}^{1} \mu^{a+1}(1-\mu)^{b-1} d \mu \ &amp;=\frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} \cdot \frac{\Gamma(a+2) \Gamma(b)}{\Gamma(a+b+2)} \
&amp;= \frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} \cdot \frac{a(a+1)\Gamma(a) \Gamma(b)}{(a+b+1)(a+b)\Gamma(a+b)} = \frac{a(a+1)}{(a+b+1)(a+b)} \end{aligned}
$$</p>
<p>よって, 求める分散は</p>
<p>$$
\begin{aligned} \operatorname{var}[\mu] &amp;=\mathbb{E}\left[\mu^{2}\right]-{\mathbb{E}[\mu]}^{2} \ &amp;=\frac{a(a+1)}{(a+b+1)(a+b)}-\frac{a^{2}}{(a+b)^{2}} \ &amp;=\frac{\left(a^{2}+a\right)(a+b)-a^{2}(a+b+1)}{(a+b)^{2}(a+b+1)} \ &amp;=\frac{a b}{(a+b)^{2}(a+b+1)} \end{aligned}
$$</p>
<p>$\operatorname{mode}[\mu]$は$\mu$の最頻値, 上に凸で$\operatorname{Beta}(\mu|a,b)$の傾きが0になる点。</p>
<p>$$
\begin{aligned} \frac{\partial \operatorname{Beta}}{\partial \mu} &amp;=\frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} \frac{\partial}{\partial \mu}\left[\mu^{a-1}(1-\mu)^{b-1}\right] \ &amp;=\frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)}\left{(a-1) \mu^{a-2}(1-\mu)^{b-1}-(b-1) \mu^{a-1}(1-\mu)^{b-2}\right} \ &amp;=\frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} \mu^{a-2}(1-\mu)^{b-2}{(a-1)(1-\mu)-(b-1) \mu}
\end{aligned}
$$
ここで$\mu&gt;0, 1-\mu&gt;0$であるから
$$
(a-1)(1-\mu)-(b-1) \mu=0
$$
を考えれば良い
これを解いて
$$
\space \mu=\frac{a-1}{a+b-2}
$$
(厳密には$a&gt;1, b&gt;1$でないと成立しない)</p>
<h2 id="演習-27"><a class="header" href="#演習-27">演習 2.7</a></h2>
<div class="panel-primary">
<p>$\mu$の事前分布がベータ分布</p>
<p>$$
\operatorname{Beta}(\mu \mid a, b)=\frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} \mu^{a-1}(1-\mu)^{b-1} \tag{2.13}
$$</p>
<p>である二項分布</p>
<p>$$
\operatorname{Bin}(m \mid N, \mu)=\left(\begin{array}{l}N \ m\end{array}\right) \mu^{m}(1-\mu)^{N-m} \tag{2.9}
$$</p>
<p>に従う確率変数$x$を考える．ここで,$x=1$の事象が$m$回, $x=0$が$l$回生じたとする．このとき，$\mu$の事後平均が，事前平均と$\mu$の最尤推定量の間の値になることを示せ．これには，事前平均の$\lambda$倍と，最尤推定量の$(1-\lambda)$倍の和で事後平均が書けることを示せばよい．ただし，$0 \leq \lambda \leq 1$である．よって，事後分布が，事前分布と最尤推定解との間のものになることが分かる．</p>
</div>
<p>$0 \le \lambda \le 1$として、事後平均が事前平均の$\lambda$倍と最尤推定量の$(1-\lambda)$倍の和であることを示せばよい。
$(2.15)$より、ベータ分布の平均は$\displaystyle \frac{a}{a+b}$で、これが事前分布の平均である。$x=1$を$m$回、$x=0$を$l$回観測すると、事後分布は$(2.18)$で与えられ、平均値は$(2.19)$, $(2.20)$から$\displaystyle \frac{m+a}{m+a+l+b}$となる。</p>
<p>一方、最尤推定量は$(2.7)$, $(2.8)$の通り、$\displaystyle \mu_{\textrm{ML}}=\frac{m}{N}=\frac{m}{m+l}$であるので、題意としては</p>
<p>$$
\lambda \frac{a}{a+b}+(1-\lambda) \frac{m}{m+l}=\frac{m+a}{m+a+l+b}
$$</p>
<p>となるような$\lambda$が$0 \le \lambda \le 1$に存在することを示す。</p>
<p>$$
\frac{\lambda a(m+l)-\lambda m(a+b)}{(a+b)(m+l)}=\frac{m+a}{m+a+l+b}-\frac{m}{m+l}
$$
$$
\lambda \frac{a l-b m}{(a+b)(m+l)}=\frac{(m+a)(m+l)-m(m+a+l+b)}{(m+a+l+b)(m+l)}
$$
$$
\begin{aligned} \lambda &amp;=\frac{a+b}{a l-b m} \cdot \frac{m^{2}+m l+a m+a l-m^{2}-m a-m l-m b}{m+a+l+b} \ &amp;=\frac{a+b}{m+a+l+b} \end{aligned}
$$
$m,l \ge 0$, $a, b \gt 0$ なので、$m+a+l+b \ge a+b \gt 0$</p>
<p>よって$0 \le \lambda \le 1$となる。</p>
<h2 id="演習-28"><a class="header" href="#演習-28">演習 2.8</a></h2>
<div class="panel-primary">
<p>同時確率が$p(x,y)$であるような２つの変数$x$と$y$を考える. これについて，次の2つの結果を証明せよ．</p>
<p>$$
\mathbb{E}[x] =\mathbb{E}<em>{y}\left[\mathbb{E}</em>{x}[x | y]\right] \tag{2.270}
$$</p>
<p>$$
\operatorname{var}[x] =\mathbb{E}<em>{y}\left[\operatorname{var}</em>{x}[x | y]\right]+\operatorname{var}<em>{y}\left[\mathbb{E}</em>{x}[x | y]\right] \tag{2.271}
$$</p>
<p>ただし，$\mathbb{E}_{x}[x | y]$は，条件付き分布$p(x|y)$の下での$x$の期待値である. 条件付き分散についても同様である.</p>
</div>
<p>※ 添字の$x, y$と関数としての$x, y$を分けて考えておく必要がある。PRML P.19の式$(1.36)$にあるように、$\mathbb{E}_x[f(x,y)]$の添字$_x$は関数$f(x,y)$の$x$の分布に関する平均を表している。式$(1.37)$〜$(1.39)$の関数$f$を$f(x)=x$として考える。</p>
<p>はじめに</p>
<p>$$
\mathbb{E}_{x}[f| y ]=\int f p(x | y) d x=\int f \frac{p(x, y)}{p(y)} dx
$$</p>
<p>であるから、</p>
<p>$$
\begin{aligned} \mathbb{E}<em>{y}\left[\mathbb{E}</em>{x}[f | y]\right] &amp;=\int \left[ \int f \frac{p(x,y)}{p(y)}dx \right] p(y) dy \
&amp;=\int\left[\int f p(x, y) d x\right] d y \
&amp;=\int f \int p(x, y) d y d x \
&amp;=\int f p(x) d x \
&amp;=\mathbb{E}<em>{x}[f] = \mathbb{E}</em>{x}[x]
\end{aligned}
$$</p>
<p>ここで、重積分の交換と式(1.31)の<strong>確率の加法定理</strong>である$\displaystyle p(x)=\int p(x,y)dy$を使った。</p>
<p>次に式$(2.271)$の右辺について</p>
<p>$$
\begin{aligned}
&amp; \mathbb{E}<em>{y}\left[\operatorname{var}</em>{x}[x | y]\right]+\operatorname{var}<em>{y}\left[\mathbb{E}</em>{x}[x | y]\right] \
=&amp; \mathbb{E}<em>{y}\left[\mathbb{E}</em>{x}\left[x^{2} | y\right]-\mathbb{E}<em>{x}[x | y]^{2}\right]+\mathbb{E}</em>{y}\left[\mathbb{E}<em>{x}[x | y]^{2}\right]-\left{\mathbb{E}</em>{y}\left[\mathbb{E}<em>{x}[x | y]\right]\right}^{2} \
=&amp; \mathbb{E}</em>{y}\left[\mathbb{E}<em>{x}\left[x^{2} | y\right]\right]-\mathbb{E}</em>{y}\left[\mathbb{E}<em>{x}[x | y]^{2}\right]+\mathbb{E}</em>{y}\left[\mathbb{E}<em>{x}[x | y]^{2}\right]-\left{\mathbb{E}</em>{y}\left[\mathbb{E}<em>{x}[x | y]\right}\right}^{2} \
=&amp; \mathbb{E}</em>{y}\left[\mathbb{E}<em>{x}\left[x^{2} | y\right]\right]-\left{\mathbb{E}</em>{y}\left[\mathbb{E}_{x}[x | y]\right]\right}^{2} \end{aligned}
$$</p>
<p>ここで式$(2.270)$の結果を用いる。今度は$f(x)=x^2$とすると</p>
<p>$$
\begin{aligned}
\mathbb{E}<em>{y}\left[\mathbb{E}</em>{x}\left[x^{2} | y\right]\right]-\left{\mathbb{E}<em>{y}\left[\mathbb{E}</em>{x}[x | y]\right]\right}^{2} &amp;= \mathbb{E}<em>{x}\left[x^{2}\right]-\left{\mathbb{E}</em>{x}[x]\right}^{2} \
&amp;= \operatorname{var}_{x}[x]
\end{aligned}
$$</p>
<p>これは式$(2.271)$の左辺なので、示された。</p>
<h2 id="演習-29"><a class="header" href="#演習-29">演習 2.9</a></h2>
<div class="panel-primary">
<p>この演習問題では，ディリクレ分布が正規化されていることを，帰納法によって証明する．
ディリクレ分布で$M=2$とした特殊な場合であるベータ分布が正規化されていることは，演習問題2.5ですでに示した．
次に，ディリクレ分布が$M-1$変数の場合に正規化されているとの仮定の下，$M$変数でも正規化されていることを証明する.
これにはまず$M$変数のディリクレ分布から, $\sum_{k=1}^{M} \mu_{k}=1$の
制約を使って$\mu_M$を除去して，ディリクレ分布を</p>
<p>$$
p_{M}\left(\mu_{1}, \ldots, \mu_{M-1}\right)=C_{M} \prod_{k=1}^{M-1} \mu_{k}^{\alpha_{k}-1}\left(1-\sum_{j=1}^{M-1} \mu_{j}\right)^{\alpha_{M}-1} \tag{2.272}
$$</p>
<p>と書く．すると，あとは$C_M$を表す式を求めればよい．これには，この式を，範囲に注意しつつ$\mu_{M-1}$について積分し，さらに，この積分の範囲が0から1となるように変数を置換する.
$C_{M-1}$での結果が正しいと仮定して(2.265)を用いて，$C_M$の式を導出せよ．</p>
</div>
<p>※ 誘導にしたがって解くものの、指示の意味がピンと来ないかもしれない。まずは制約$\displaystyle{\sum_{k=1}^{M} \mu_{k}=1}$を使ってまずは制約$\displaystyle{\sum_{k=1}^{M} \mu_{k}=1}$を使って$\mu_M$を除去し、$\mu_{M-1}$以下からなる式に変形していく。次に、$\mu_{M-1}$について積分を行うことで、$M-2$変数の周辺分布の形$p_{M-1}(\mu_1, \mu_2, \ldots, \mu_{M-2})$を計算することができる。これと、$M-1$変数のディリクレ分布の結果が正しいと仮定したときに得られる結果と比較することで、$C_M$についての式が得られる。</p>
<p>$M−1$変数の場合に正規化されているとの仮定の下で、$M$変数の場合に正規化されていることを証明する。$M$変数のディリクレ分布から、$\sum_{k=1}^M μ_k = 1$の制約を用いて$\mu_M$を除去すると、以下の$M−1$変数の確率分布が得られる.</p>
<p>$$
p_M(\mu_1,...,\mu_{M-1}) = C_M \prod_{k=1}^{M-1} \mu_k^{\alpha_k-1} \left( 1 - \sum_{j=1}^{M-1} \mu_j \right )^{\alpha_M-1}
$$</p>
<p>ここで、$\displaystyle C_{M}=\frac{\Gamma\left(\alpha_{1}+\cdots+\alpha_{M}\right)}{\Gamma\left(\alpha_{1}\right) \cdots \Gamma\left(\alpha_{M}\right)}$, 制約として$\displaystyle 0 \leq \mu_{i} \leq 1(i=1, \ldots, M-1) , \sum_{k=1}^{M-1} \mu_{k} \leq 1$が存在する。</p>
<p>確率分布$p_M$を変数$\mu_{M-1}$で積分すると、$M-2$変数の周辺分布が得られる（確率の加法定理）。$\mu_{M-1}$の積分範囲は、上記の制約により、$0$から$1-\sum_{j=1}^{M-2}\mu_j$までとなる。</p>
<p>$$
\begin{aligned}
&amp; p_{M-1}\left(\mu_{1}, \ldots, \mu_{M-2}\right) \=&amp; \int_{0}^{1-\sum_{j=1}^{M-2} \mu_{j}} p_{M}\left(\mu_{1}, \ldots, \mu_{M-1}\right) d \mu_{M-1} \=&amp; C_{M}\left[\prod_{k=1}^{M-2} \mu_{k}^{\alpha_{k}-1}\right] \int_{0}^{1-\sum_{j=1}^{M-2} \mu_{j}} \mu_{M-1}^{\alpha_{M-1}-1}\left(1-\sum_{j=1}^{M-1} \mu_{j}\right)^{\alpha_{M}-1} d \mu_{M-1}
\end{aligned}
$$</p>
<p>ここで積分範囲を$[0,1]$にするために次の変数変換を行う</p>
<p>$$
\begin{aligned}
\mu_{M-1} &amp;= t\left(1-\sum_{j=1}^{M-2} \mu_{j}\right) \ 1-\sum_{j=1}^{M-1} \mu_{j} &amp;=1-\sum_{j=1}^{M-2} \mu_{j}-\mu_{M-1} \ &amp;=\left(1-\sum_{j=1}^{M-2} \mu_{j}\right)-t\left(1-\sum_{j=1}^{M-2} \mu_{j}\right) \ &amp;=(1-t)\left(1-\sum_{j=1}^{M-2} \mu_{j}\right)
\end{aligned}
$$</p>
<p>これによって</p>
<p>$$
\begin{aligned}
\quad p_{M-1}\left(\mu_{1}, \ldots, \mu_{M-2}\right) &amp;= C_{M}\left[\prod_{k=1}^{M-2} \mu_{k}^{\alpha_{z}-1}\right] \int_{0}^{1}\left{t\left(1-\sum_{j=1}^{M-2} \mu_{j}\right)\right}^{\alpha_{w-1}-1}\left{(1-t)\left(1-\sum_{j=1}^{M-2} \mu_{j}\right)\right}^{\alpha_{n-1}}\left(1-\sum_{j=1}^{M-2} \mu_{j}\right) d t \
&amp;= C_{M}\left[\prod_{k=1}^{M-2} \mu_{k}^{\alpha_{k}-1}\right]\left(1-\sum_{j=1}^{M-2} \mu_{j}\right)^{\alpha_{y-1}+\alpha_{n}-1} \int_{0}^{1} t^{\alpha_{n-1}-1}(1-t)^{\alpha_{n-1}-1} d t \
&amp;= C_{M}\left[\prod_{k=1}^{n-2} \mu_{k}^{\alpha_{k}-1}\right]\left(1-\sum_{j=1}^{M-2} \mu_{j}\right)^{\alpha_{n-1}+\alpha_{n}-1} \frac{\Gamma\left(\alpha_{M-1}\right) \Gamma\left(\alpha_{M}\right)}{\Gamma\left(\alpha_{M-1}+\alpha_{M}\right)}
\end{aligned}
$$</p>
<p>こうして得られた周辺分布$p_{M-1}(μ_1,\ldots,μ_{M-2})$は、$\alpha'=(\alpha_1,\ldots,\alpha_{M-2},\alpha_{M-1}+\alpha_M)^T$をパラメータとする$M−1$変数のディリクレ分布から変数を一つ除去した確率分布の形をしている。</p>
<p>一方、同じパラメータ$\alpha'=(\alpha_1,\ldots,\alpha_{M-2},\alpha_{M-1}+\alpha_M)^T$を持つ$M−1$変数のディリクレ分布から、$\sum_{k=1}^{M-1} μ_k = 1$の制約を用いて変数を一つ除去すると、以下の確率分布が得られる。</p>
<p>$$
p_{M-1}\left(\mu_{1}, \ldots, \mu_{M-2}\right)=C_{M-1} \prod_{k=1}^{M-2} \mu_{k}^{\alpha_{k}-1}\left(1-\sum_{j=1}^{M-2} \mu_{j}\right)^{\alpha_{M-1}+\alpha_{M}-1}
$$</p>
<p>ここで$\displaystyle{C_{M-1}=\frac{\Gamma\left(\alpha_{1}+\cdots+\alpha_{M-2}+\left(\alpha_{M-1}+\alpha_{M}\right)\right)}{\Gamma\left(\alpha_{1}\right) \cdots \Gamma\left(\alpha_{M-2}\right) \Gamma\left(\alpha_{M-1}+\alpha_{M}\right)}}$。
帰納法の仮定より、この確率分布は正規化されている。</p>
<p>上記の$p_{M−1}$の定数部分は</p>
<p>$$
\begin{aligned}
&amp; C_{M} \frac{\Gamma\left(\alpha_{M-1}\right) \Gamma\left(\alpha_{M}\right)}{\Gamma\left(\alpha_{M-1}+\alpha_{M}\right)} \=&amp; \frac{\Gamma\left(\alpha_{1}+\cdots+\alpha_{M}\right)}{\Gamma\left(\alpha_{1}\right) \cdots \Gamma\left(\alpha_{M}\right)} \frac{\Gamma\left(\alpha_{M-1}\right) \Gamma\left(\alpha_{M}\right)}{\Gamma\left(\alpha_{M-1}+\alpha_{M}\right)} \=&amp; \frac{\Gamma\left(\alpha_{1}+\cdots+\alpha_{M-2}+\left(\alpha_{M-1}+\alpha_{M}\right)\right)}{\Gamma\left(\alpha_{1}\right) \cdots \Gamma\left(\alpha_{M-2}\right) \Gamma\left(\alpha_{M-1}+\alpha_{M}\right)} \=&amp; C_{M-1}
\end{aligned}
$$</p>
<p>よって、$M$変数のディリクレ分布も正規化されていることが示された。</p>
<h2 id="演習-210"><a class="header" href="#演習-210">演習 2.10</a></h2>
<div class="panel-primary">
<p>ガンマ関数の性質$\Gamma(x+1)=x\Gamma(x)$を用いて，</p>
<p>$$
\operatorname{Dir}(\boldsymbol{\mu} | \boldsymbol{\alpha})=\frac{\Gamma\left(\alpha_{1}+\cdots+\alpha_{K}\right)}{\Gamma\left(\alpha_{1}\right) \cdots \Gamma\left(\alpha_{K}\right)} \prod_{k=1}^{K} \mu_{k}^{\alpha_{k}-1} \tag{2.38}
$$</p>
<p>のディリクレ分布の平均，分散，および共分散の結果を導出せよ.</p>
<p>\begin{aligned}
\mathbb{E}[\mu_j] &amp;= \frac{\alpha_j}{\alpha_0} \tag{2.273} \
\operatorname{var}[\mu_j] &amp;= \frac{\alpha_j(\alpha_0 - \alpha_j)}{\alpha_0^2(\alpha_0 + 1)} \tag{2.274} \
\operatorname{cov}[\mu_j][\mu_l] &amp;= -\frac{\alpha_j\alpha_l}{\alpha_0^2(\alpha_0 + 1)} \hspace{1em} (j \neq l) \tag{2.275}
\end{aligned}</p>
<p>ただし、$\alpha_0$は</p>
<p>$$
\alpha_0 = \sum_{k=1}^K\alpha_k \tag{2.39}
$$</p>
<p>で定義されている。</p>
</div>
<p>ディリクレ分布は正規化されているので、</p>
<p>$$
\int{\prod_{k=1}^{K}} \mu_{k}^{\alpha_{k}-1} d\boldsymbol{\mu}=\frac{\Gamma\left(\alpha_{1}\right) \Gamma\left(\alpha_{2}\right) \cdots \Gamma\left(\alpha_{K}\right)}{\Gamma\left(\alpha_{1}+\cdots+\alpha_{K}\right)}
$$</p>
<p>平均$\mathbb{E}[\mu_j]$について、</p>
<p>$$
\begin{aligned}
\mathbb{E}\left[\mu_{j}\right] &amp;=\int \mu_{i} \operatorname{Dir}(\boldsymbol{\mu} | \boldsymbol{\alpha}) d \boldsymbol{\mu} \
&amp;=\frac{\Gamma\left(\alpha_{1}+\cdots +\alpha_{K}\right)}{\Gamma\left(\alpha_{1}\right) \cdots \Gamma\left(\alpha_{K}\right)} \int \mu_{j} \prod_{k=1}^{K} \mu_{k}^{\alpha_{k}-1} d \boldsymbol{\mu} \
&amp;=\frac{\Gamma\left(\alpha_{1}+\cdots +\alpha_{K}\right)}{\Gamma\left(\alpha_{1}\right) \cdots \Gamma\left(\alpha_{K}\right)} \int \mu_{1}^{\alpha_{1}-1} \mu_{2}^{\alpha_{2}-1} \cdots \mu_{j}^{(\alpha_j +1)-1} \cdot \mu_{K}^{\alpha_{K}-1} d \boldsymbol{\mu}
\end{aligned}
$$</p>
<p>ここで、ディリクレ分布のパラメータ$\boldsymbol{\alpha}$のうち、$\alpha_j \to \alpha_j + 1$となったものだとみなせば、$\displaystyle{\int{\prod_{k=1}^{K}} \mu_{k}^{\alpha_{k}-1} d\boldsymbol{\mu}=\frac{\Gamma\left(\alpha_{1}\right) \Gamma\left(\alpha_{2}\right) \cdots \Gamma\left(\alpha_{K}\right)}{\Gamma\left(\alpha_{1}+\cdots+\alpha_{K}\right)}}$を利用して</p>
<p>$$
\int \mu_{1}^{\alpha_{1}-1} \mu_{2}^{\alpha_{2}-1} \cdots \mu_{j}^{\left(\alpha_{j}+1\right)-1} \cdots \mu_{K}^{\alpha_{K}-1} d \boldsymbol{\mu}=\frac{\Gamma\left(\alpha_{1}\right) \Gamma\left(\alpha_{2}\right) \cdots \Gamma\left(\alpha_{j}+1\right) \cdots \Gamma\left(\alpha_{K}\right)}{\Gamma\left(\alpha_{1}+\cdots+\alpha_{K}+1\right)}
$$</p>
<p>となるので、</p>
<p>$$
\begin{aligned}
\mathbb{E}\left[\mu_{j}\right] &amp;=\frac{\Gamma\left(\alpha_{1}+\cdots +\alpha_{K}\right)}{\Gamma\left(\alpha_{1}\right) \cdots \Gamma\left(\alpha_{K}\right)} \cdot \frac{\Gamma\left(\alpha_{1}\right) \Gamma\left(\alpha_{2}\right) \cdots \Gamma\left(\alpha_{j}+1\right) \cdots \Gamma\left(\alpha_{K}\right)}{\Gamma\left(\alpha_{1}+\cdots+\alpha_{K}+1\right)} \
&amp;=\frac{\Gamma\left(\alpha_{1}+\cdots +\alpha_{K}\right)}{\Gamma\left(\alpha_{1}\right) \cdots \Gamma\left(\alpha_{K}\right)} \cdot \frac{\Gamma\left(\alpha_{1}\right) \cdots \Gamma\left(\alpha_{K}\right)}{\Gamma\left(\alpha_{1}+\cdots+ \alpha_{K}\right)} \cdot \frac{\alpha_{j}}{\sum_{k=1}^{K} \alpha_{k}}=\frac{\alpha_{j}}{\alpha_{0}}
\end{aligned}
$$</p>
<p>同様に、</p>
<p>$$
\begin{aligned}
\mathbb{E}\left[\mu_{j}^{2}\right] &amp;=\int \mu_{j}^{2} \operatorname{Dir}(\boldsymbol{\mu} | \boldsymbol{\alpha}) d \boldsymbol{\mu} \ &amp;=\frac{\Gamma\left(\alpha_{1}+\cdots \alpha_{K}\right)}{\Gamma\left(\alpha_{1}\right) \cdots \Gamma\left(\alpha_{K}\right)} \cdot \frac{\Gamma\left(\alpha_{1}\right) \Gamma\left(\alpha_{2}\right) \cdots \cdot \Gamma\left(\alpha_{j}+2\right) \cdots \Gamma\left(\alpha_{K}\right)}{\Gamma\left(\alpha_{1}+\cdots+\alpha_{K}+2\right)} \
&amp;=\frac{\alpha_{j}\left(\alpha_{j}+1\right)}{\alpha_{0}\left(\alpha_{0}+1\right)} \end{aligned}
$$</p>
<p>と求まるので、分散$\operatorname{var}[\mu_j]$は</p>
<p>$$
\begin{aligned}
\operatorname{var}\left[\mu_{j}\right] &amp;=\mathbb{E}\left[\mu_{j}^{2}\right]-\left{\mathbb{E}\left[\mu_{j}\right]\right}^{2} \
&amp;=\frac{\alpha_{j}\left(\alpha_{j}+1\right)}{\alpha_{0}\left(\alpha_{0}+1\right)}-\frac{\alpha_{j}^{2}}{\alpha_{0}^{2}} \
&amp;=\frac{\alpha_{0} \alpha_{j}\left(\alpha_{j}+1\right)-\alpha_{j}^{2}\left(\alpha_{0}+1\right)}{\alpha_{0}^{2}\left(\alpha_{0}+1\right)} \ &amp;=\frac{\alpha_{j}\left(\alpha_{0}-\alpha_{j}\right)}{\alpha_{0}^{2}\left(\alpha_{0}+1\right)} \end{aligned}
$$</p>
<p>共分散$\operatorname{cov}[\mu_j, \mu_l]$について、$j\neq l$のとき、</p>
<p>$$
\begin{aligned}
\operatorname{cov}\left[\mu_{j}, \mu_{l}\right] &amp;=\mathbb{E}\left[\mu_{j} \mu_{l}\right]-\mathbb{E}\left[\mu_{j}\right] \mathbb{E}\left[\mu_{l}\right] \
&amp;=\frac{\alpha_{j} \alpha_{l}}{\alpha_{0}\left(\alpha_{0}+1\right)}-\frac{\alpha_{j}}{\alpha_{0}} \cdot \frac{\alpha_{l}}{\alpha_{0}} \
&amp;=\frac{\alpha_{0} \alpha_{j} \alpha_{l}-\alpha_{j} \alpha_{l}\left(\alpha_{0}+1\right)}{\alpha_{0}^{2}\left(\alpha_{0}+1\right)} \
&amp;= - \frac{\alpha_{j} \alpha_{l}}{\alpha_{0}^{2}\left(\alpha_{0}+1\right)}
\end{aligned}
$$</p>
<h2 id="演習-211"><a class="header" href="#演習-211">演習 2.11</a></h2>
<div class="panel-primary">
<p>ディリクレ分布の下での$\ln \mu_j$の期待値を，$\alpha_j$についての導関数として表すと，</p>
<p>$$
\begin{aligned}
\mathbb{E}[\ln \mu_j]=\psi(\alpha_j)-\psi(\alpha_0) \tag{2.276}
\end{aligned}
$$</p>
<p>になることを示せ．ただし$\alpha_0$は$(2.39)$で定義され,$\psi(\cdot)$はディガンマ関数 (digamma function)</p>
<p>$$
\begin{aligned}
\psi(a) \equiv \frac{d}{d a} \ln \Gamma(a) \tag{2.277}
\end{aligned}
$$</p>
<p>である．</p>
</div>
<p>ディリクレ分布は$(2.38)$式から</p>
<p>$$
\operatorname{Dir}(\mathbf{\mu} \mid \mathbf{\alpha})=K(\mathbf{\alpha}) \prod_{k=1}^{M} \mu_{k}^{\alpha_{k}-1}
$$</p>
<p>である。ただし、簡単のために$\displaystyle K(\mathbf{\alpha})=\frac{\Gamma\left(\alpha_{0}\right)}{\Gamma\left(\alpha_{1}\right) \cdots \Gamma\left(\alpha_{M}\right)}$と置いた。ただし、$\alpha_0 = \sum_{k=1}^{M}\alpha_k$である（ちなみにPRML本文内の記述と違い、和の上限の文字が$K$から$M$に変更になっている）。</p>
<p>この設問で求める値はディリクレ分布の下での期待値なので、定義から$\displaystyle \mathbb{E}[\ln \mu_j] = \int \ln \mu_j \operatorname{Dir}(\mathbf{\mu} \mid \mathbf{\alpha}) d {\mu}$
である。</p>
<p>この設問の準備として以下の計算を行っておく。</p>
<p>$$
\frac{\partial}{\partial \alpha_j}\prod_{k=1}^{M}\mu_k^{\alpha_k-1}=\frac{\partial}{\partial \alpha_j}\prod_{k=1}^{M} \exp((\alpha_k-1)\ln \mu_k)
$$</p>
<p>これは$\exp\left((\alpha_j-1)\ln \mu_j\right)$のみ$\alpha_j$で微分すれば良いので</p>
<p>$$
\frac{\partial}{\partial \alpha_j}\left{ \exp \left( (\alpha_j - 1) \ln \mu_j \right)\right} = \ln \mu_j \exp\left( (\alpha_j - 1) \ln \mu_j \right)
$$
から、
$$
\frac{\partial}{\partial \alpha_j}\prod_{k=1}^{M}\mu_k^{\alpha_k-1}=\ln \mu_j \prod_{k=1}^M \exp((\alpha_k-1)\ln \mu_k)=\ln \mu_j \prod_{k=1}^{M}\mu_k^{\alpha_k-1}
$$
となる。</p>
<p>よって、</p>
<p>$$
\begin{aligned}
\mathbb{E}\left[\ln \mu_{j}\right] &amp;=K(\mathbf{\alpha}) \int_{0}^{1} \cdots \int_{0}^{1} \ln \mu_{j} \prod_{k=1}^{M} \mu_{k}^{\alpha_{k}-1} \mathrm{d} \mu_{1} \ldots \mathrm{d} \mu_{M} \
&amp;=K(\mathbf{\alpha}) \frac{\partial}{\partial \alpha_{j}} \int_{0}^{1} \ldots \int_{0}^{1} \prod_{k=1}^{M} \mu_{k}^{\alpha_{k}-1} \mathrm{d} \mu_{1} \ldots \mathrm{d} \mu_{M} \
&amp;=K(\mathbf{\alpha}) \frac{\partial}{\partial \alpha_{j}} \frac{1}{K(\mathbf{\alpha})}
\end{aligned}
$$</p>
<p>最後はディリクレ分布が正規化されていること $\displaystyle  \int \operatorname{Dir}(\mathbf{\mu} \mid \mathbf{\alpha}) d \mathbf{\mu} = 1$から$\displaystyle \int_{0}^{1} \ldots \int_{0}^{1} \prod_{k=1}^{M} \mu_{k}^{\alpha_{k}-1} \mathrm{d} \mu_{1} \ldots \mathrm{d} \mu_{M} = \frac{1}{K(\alpha)}$となることを用いた。</p>
<p>この式についてさらに$\displaystyle K(\alpha) = \frac{\Gamma(\alpha_0)}{\prod_{i=1}^M \Gamma(\alpha_k)}$を使って展開すると</p>
<p>$$
\begin{aligned} \mathbb{E}\left[\ln \mu_{j}\right]
&amp;=\frac{\Gamma\left(\alpha_{0}\right)}{\prod_{i=1}^{M} \Gamma\left(\alpha_{k}\right)} \frac{\partial}{\partial \alpha_{j}} \frac{\prod_{i=1}^{M} \Gamma\left(\alpha_{k}\right)}{\Gamma\left(\alpha_{0}\right)} \
&amp;=\frac{\Gamma\left(\alpha_{0}\right)}{\Gamma\left(\alpha_{j}\right)} \frac{\partial}{\partial \alpha_{j}} \frac{\Gamma\left(\alpha_{j}\right)}{\Gamma\left(\alpha_{0}\right)}\hspace{2em}\left(\because 約分 \right) \
&amp;=\frac{\Gamma\left(\alpha_{0}\right)}{\Gamma\left(\alpha_{j}\right)}
\left[
\frac{1}{\left{\Gamma\left(\alpha_{0}\right)\right}^2}
\left{ \left( \frac{\partial}{\partial \alpha_{j}} \Gamma\left(\alpha_{j}\right) \right) \Gamma\left(\alpha_{0}\right)-\Gamma\left(\alpha_{j}\right)\left(\frac{\partial}{\partial \alpha_{j}} \Gamma\left(\alpha_{0}\right)\right)\right}
\right] \
&amp;=\frac{1}{\Gamma\left(\alpha_{j}\right)}
\left(\frac{\partial}{\partial \alpha_{j}} \Gamma\left(\alpha_{j}\right)\right) -\frac{1}{\Gamma\left(\alpha_{0}\right)}\left(\frac{\partial}{\partial \alpha_{j}} \Gamma\left(\alpha_{0}\right)\right)\
&amp;=\frac{\partial}{\partial \alpha_{j}} \ln \Gamma\left(\alpha_{j}\right)-\frac{1}{\Gamma\left(\alpha_{0}\right)}\left(\frac{\partial}{\partial \alpha_{0}} \frac{\partial \alpha_{0}}{\partial \alpha_{j}} \Gamma\left(\alpha_{0}\right)\right) \
&amp;=\frac{\partial}{\partial \alpha_{j}} \ln \Gamma\left(\alpha_{j}\right)-\frac{\partial}{\partial \alpha_{0}} \ln \Gamma\left(\alpha_{0}\right) \hspace{2em}
\left( \because \alpha_0 = \sum_{k=1}^M \alpha_k より, \frac{\partial \alpha_{0}}{\partial \alpha_{j}} = 1\right)
\end{aligned}
$$</p>
<p>最後にディガンマ関数の定義$\displaystyle \psi(a) \equiv \frac{d}{d a} \ln \Gamma(a)$を使うと</p>
<p>$$
\mathbb{E}\left[\ln \mu_{j}\right] = \psi(\alpha_j) - \psi(\alpha_0)
$$</p>
<p>が導かれる。</p>
<p>※ $\alpha_j, \alpha_0$は1変数なので偏微分記号$\partial$、全微分記号$d$どちらで微分しても結果は同じである。</p>
<h2 id="演習-212"><a class="header" href="#演習-212">演習 2.12</a></h2>
<div class="panel-primary">
<p>連続変数$x$の一様分布は</p>
<p>$$
\mathrm{U}(x | a, b)=\frac{1}{b-a}, \quad a \leq x \leq b \tag{2.278}
$$</p>
<p>で定義される．この分布が正規化されていることを確かめ，この分布の平均と分散の式を求めよ．</p>
</div>
<p>まず$\displaystyle \int_a^b \frac{1}{b-a}dx = 1$を証明する。</p>
<p>$$
\int_a^b \frac{1}{b-a}dx = \frac{1}{b-a}\int_a^b1dx = \frac{1}{b-a}(b-a)=1
$$</p>
<p>よって分布が正規化されていることが示された。</p>
<p>分布の平均$\mathbb{E}[x]$は</p>
<p>$$
\mathbb{E}[x] = \int_a^b x \frac{1}{b-a}dx
=\frac{1}{b-a}\left[ \frac{x^2}{2} \right]_{a}^{b}
=\frac{b+a}{2}
$$</p>
<p>分布の分散は</p>
<p>$$
\begin{aligned}
\operatorname{var}[x]
&amp;= \mathbb{E}[x^2]-(\mathbb{E}[x])^2 \
&amp;= \frac{1}{b-a}\left[ \frac{x^3}{3} \right]_{a}^{b}-\left( \frac{b+a}{2} \right)^2 \
&amp;= \frac{b^2+ab+a^2}{3}-\frac{b^2+2ab+a^2}{4} \
&amp;= \frac{(b-a)^2}{12}
\end{aligned}
$$</p>
<p>である。</p>
<h2 id="演習-213"><a class="header" href="#演習-213">演習 2.13</a></h2>
<div class="panel-primary">
<p>2つのガウス分布$p(\mathbf{x})=\mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma})$と$q(\mathbf{x})=\mathcal{N}(\mathbf{x} | \boldsymbol{m}, \mathbf{L})$の間のカルバック-ライブラーダイバージェンス</p>
<p>$$
\begin{aligned}
\mathrm{KL}(p | q) &amp;=-\int p(\mathbf{x}) \ln q(\mathbf{x}) \mathrm{d} \mathbf{x}-\left(-\int p(\mathbf{x}) \ln p(\mathbf{x}) \mathrm{d} \mathbf{x}\right) \
&amp;=-\int p(\mathbf{x}) \ln \left{\frac{q(\mathbf{x})}{p(\mathbf{x})}\right} \mathrm{d} \mathbf{x} \tag{1.113}
\end{aligned}
$$</p>
<p>を求めよ．</p>
</div>
<p>※ 演習問題2.15とほぼ同じ操作を2回やるので、先に2.15をやってきたほうが良い。また、一般にガウス分布の指数部分では二次形式とトレースの関係を利用すると計算が楽になり見通しもよくなることが多い。「二次形式があればトレースを利用するかもしれない」くらいの心持ちを持っていると良いかもしれない。</p>
<p>問題文から
$$
\mathrm{KL}(p | q) =-\int p(\mathbf{x}) \ln q(\mathbf{x}) d\mathbf{x}+\int p(\mathbf{x}) \ln p(\mathbf{x}) d\mathbf{x}
$$
に分解できる。</p>
<p>この第2項は演習問題2.15や付録(B.41)で求められるように（2.15の方を参照）、</p>
<p>$$
\int p(\mathbf{x}) \ln p(\mathbf{x}) d\mathbf{x} = -\frac{1}{2} \ln |\mathbf{\Sigma}| - \frac{D}{2}\left{ 1 + \ln (2 \pi) \right} \tag{1}
$$</p>
<p>である。そこで、第1項を求める。</p>
<p>$$
\begin{aligned}
&amp; -\int p(\mathbf{x}) \ln q(\mathbf{x}) d\mathbf{x} \
&amp;=\int \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Sigma}) \cdot \frac{1}{2}\left{D \ln (2 \pi)+\ln |\mathbf{L}|+(\mathbf{x}-\mathbf{m})^{\mathrm{T}} \mathbf{L}^{-1}(\mathbf{x}-\mathbf{m})\right} d \mathbf{x} \
&amp;=\frac{1}{2}\left{(D \ln (2 \pi)+\ln |\mathbf{L}|) \int \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Sigma}) d \mathbf{x}\right}+\frac{1}{2} \int \mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Sigma}\right) \operatorname{Tr}\left[(\mathbf{x}-\mathbf{m})^{\mathrm{T}} \mathbf{L}^{-1} (\mathbf{x}-\mathbf{m})\right] d\mathbf{x} \
&amp;\hspace{2em} (\because 二次形式はスカラーなので \mathbf{x^{\mathrm{T}}\mathbf{\Sigma}\mathbf{x}} = \operatorname{Tr}[\mathbf{x^{\mathrm{T}}\mathbf{\Sigma}\mathbf{x}}] \
&amp;=\frac{1}{2}{D \ln (2 \pi)+\ln |\mathbf{L}|}+\frac{1}{2} \int \mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Sigma}\right) \operatorname{Tr}\left[(\mathbf{x}-\mathbf{m})(\mathbf{x}-\mathbf{m})^{\mathrm{T}} \mathbf{L}^{-1}\right] d \mathbf{x} \
&amp;\hspace{2em} (\because トレースの循環性\operatorname{Tr}[\mathbf{x^{\mathrm{T}}\mathbf{\Sigma}\mathbf{x}}] = \operatorname{Tr}[\mathbf{xx^{\mathrm{T}}\mathbf{\Sigma}}] )\
&amp;=\frac{1}{2}{D \ln (2 \pi)+\ln |\mathbf{L}|}
+\frac{1}{2}\left{\left(\operatorname{Tr}\left[\int \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \Sigma) \mathbf{xx}^{\mathrm{T}} \mathbf{L}^{-1}d \mathbf{x} \right]
-\operatorname{Tr}\left[\int \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Sigma}) \mathbf{x}\mathbf{m}^{\mathrm{T}} \mathbf{L}^{-1}d \mathbf{x} \right]-\operatorname{Tr}\left[\int \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Sigma}) \mathbf{m} \mathbf{x}^{\mathrm{T}} \mathbf{L}^{-1} d \mathbf{x} \right]
+\operatorname{Tr}\left[\int \mathcal{N}\left(\mathbf{x}|\boldsymbol{\mu}, \mathbf{\Sigma}\right) \mathbf{mm}^{\mathrm{T}} \mathbf{L}^{-1} d \mathbf{x}\right] \right) \right} \
&amp;=\frac{1}{2}{D \ln (2 \pi)+\ln |\mathbf{L}|}
+\frac{1}{2}\left{\operatorname{Tr}\left[\left(\boldsymbol{\mu\mu}^{\mathrm{T}}+\mathbf{\Sigma}\right) \mathbf{L}^{-1}\right]
-\operatorname{Tr} \left[\boldsymbol{\mu} \mathbf{m}^{\mathrm{T}}\mathbf{L}^{-1}\right]
-\operatorname{Tr}\left[\mathbf{m} \boldsymbol{\mu}^{\mathrm{T}} \mathbf{L}^{-1}\right]
+\operatorname{Tr}\left[\mathbf{mm}^{\mathrm{T}} \mathbf{L}^{-1}\right] \right}\
&amp;=\frac{1}{2}{D \ln (2 \pi)+\ln |\mathbf{L}|}+\frac{1}{2}\left{\operatorname{Tr}\left[\left(\boldsymbol{\mu\mu}^{\mathrm{T}}+\mathbf{\Sigma}\right)\mathbf{L}^{-1}\right]
-\mathbf{m}^{\mathrm{T}}\mathbf{L}^{-1}\boldsymbol{\mu}
-\boldsymbol{\mu}^{\mathrm{T}}\mathbf{L}^{-1}\mathbf{m}
+\mathbf{m}^{\mathrm{T}}\mathbf{L}^{-1}\mathbf{m}
\right}\hspace{2em} (\because \operatorname{Tr}[\mathbf{x^{\mathrm{T}}\mathbf{\Sigma}\mathbf{x}}] = \mathbf{xx}^{\mathrm{T}}\mathbf{\Sigma} )
\end{aligned}
$$</p>
<p>これと$(1)$の結果から、</p>
<p>$$
\begin{aligned}
\mathrm{KL}(p | q)
&amp;=-\int p(\mathbf{x}) \ln q(\mathbf{x}) d\mathbf{x}+\int p(\mathbf{x}) \ln p(\mathbf{x}) d\mathbf{x} \
&amp;= \frac{1}{2}\left(\ln \frac{|\mathbf{L}|}{|\Sigma|}+\operatorname{Tr}\left[\left(\boldsymbol{\mu\mu}^{\mathrm{T}}+\mathbf{\Sigma}\right)\mathbf{L}^{-1}\right]
-\mathbf{m}^{\mathrm{T}} \mathbf{L}^{-1} \boldsymbol{\mu}
-\boldsymbol{\mu}^{\mathrm{T}} \mathbf{L}^{-1} \mathbf{m}
+\mathbf{m}^{\mathrm{T}} \mathbf{L}^{-1} \mathbf{m}-D\right)
\end{aligned}
$$</p>
<p>※ちなみに$\operatorname{Tr}(\mathbf{AB}) = \operatorname{Tr}(\mathbf{BA})$が成り立つので$\operatorname{Tr}\left[\left(\boldsymbol{\mu\mu}^{\mathrm{T}}+\mathbf{\Sigma}\right)\mathbf{L}^{-1}\right] = \operatorname{Tr}\left[\mathbf{L}^{-1}\left(\boldsymbol{\mu\mu}^{\mathrm{T}}+\mathbf{\Sigma}\right)\right]$でも良い（公式解答例の記述）。</p>
<p>※精度行列$\mathbf{L}^{-1}$は演習2.17の結果から一般性を失うことなく共分散行列$\mathbf{L}$を対称行列とおいても問題なく、演習2.22の結果から$\mathbf{L}^{-1}$も対称行列となるので$\mathbf{m}^{\mathrm{T}} \mathbf{L}^{-1} \boldsymbol{\mu}=\boldsymbol{\mu}^{\mathrm{T}} \mathbf{L}^{-1} \mathbf{m}$が成立するため、まとめて$2\mathbf{m}^{\mathrm{T}} \mathbf{L}^{-1} \boldsymbol{\mu}$と書いてもよい気がする。</p>
<h2 id="演習-214"><a class="header" href="#演習-214">演習 2.14</a></h2>
<div class="panel-primary">
<p>この演習問題では，共分散が与えられているときに，エントロピーを最大にする多変量分布はガウス分布であることを示す．まず，分布$p(\mathbf{x})$のエントロピーは</p>
<p>$$
\mathrm{H}[\mathrm{x}]=-\int p(\mathrm{x}) \ln p(\mathrm{x}) \mathrm{d} \mathrm{x} \tag{2.279}
$$</p>
<p>である．そして，分布$p(\mathbf{x})$が正規化されていることと，平均と共分散が固定されているということを示す制約式</p>
<p>$$
\int p(\mathbf{x}) \mathrm{d} \mathbf{x}=1 \tag{2.280}
$$</p>
<p>$$
\int p(\mathbf{x}) \mathbf{x} \mathrm{d} \mathbf{x}=\boldsymbol{\mu} \tag{2.281}
$$</p>
<p>$$
\int p(\mathbf{x})(\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}} \mathrm{d} \mathbf{x}=\mathbf{\Sigma} \tag{2.282}
$$</p>
<p>を満たすすべての分布中で，この$\mathrm{H}[\mathbf{x}]$を最大化したい制約$(2.280)$,$(2.281)$，および$(2.282)$を扱うためにラグランジュ乗数を用い，$（2.279)$を変分最大化し，尤度を最大にする分布がガウス分布$(2.43)$であることを示せ．</p>
</div>
<p>ラグランジュ乗数を用いて、$\mathrm{H}[\mathbf{x}]$を最大化する、そのため、ラグランジュ乗数として、定数、$D$次元ベクトルと$D*D$次元の行列を定義して代入すると</p>
<p>$$
\begin{aligned}
\widetilde{H}[p] &amp;= -\int{p(\mathbf{x})\ln{p(\mathbf{x})}}\mathrm{d}\mathbf{x} + \lambda(\int{p(\mathbf{x})}\mathrm{d}\mathbf{x}-1)\
&amp;+ \mathbf{m}^\top (\int{p(\mathbf{x})\mathbf{x}}\mathrm{d}\mathbf{x}-\boldsymbol{\mu})\
&amp;+ \mathbf{tr}{\mathbf{L}\int{p(\mathbf{x})(\mathbf{x} - \boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^\top}\mathrm{d}\mathbf{x} - \mathbf{\Sigma}}\
\end{aligned}
$$</p>
<p>となる．</p>
<p>この式の導関数を求め,さらに導関数を0にした時の$p(\mathbf{x})$を求めよう。</p>
<p>$$
\begin{aligned}
\frac{\mathrm{d}\widetilde{H}[p(\mathbf{x})]}{\mathrm{d}p(\mathbf{x})} &amp;= -1-\ln{p(\mathbf{x})} + \lambda + \mathbf{m}^\top \mathbf{x} + \mathbf{tr}{\mathbf{L}(\mathbf{x} - \boldsymbol{\mu})(\mathbf{x} - \boldsymbol{\mu})^\top}\
\frac{\mathrm{d}\widetilde{H}[p(\mathbf{x})]}{\mathrm{d}p(\mathbf{x})} &amp;= 0\
\ln{p(\mathbf{x})} &amp;=  -1+ \lambda + \mathbf{m}^\top \mathbf{x} + \mathbf{tr}{\mathbf{L}(\mathbf{x} - \boldsymbol{\mu})(\mathbf{x} - \boldsymbol{\mu})^\top}\
p(\mathbf{x}) &amp;= \mathrm{exp}{\lambda -1+ \mathbf{m}^\top \mathbf{x} + \mathbf{tr}{\mathbf{L}(\mathbf{x} - \boldsymbol{\mu})(\mathbf{x} - \boldsymbol{\mu})^\top}}\
&amp;= \mathrm{exp}{\lambda - 1 + \mathbf{y}^\top \mathbf{L}\mathbf{y} + \boldsymbol{\mu} ^\top \mathbf{m} - \frac{1}{4}\mathbf{m}^\top\mathbf{L}^{-1}\mathbf{m}}\
\end{aligned}
$$</p>
<p>となる，ただし$\mathbf{y} = \mathbf{x} - \mu + \frac{1}{2}\mathbf{L}^{-1}\mathbf{m}$.</p>
<p>また$p(\mathbf{x})$は制約$(2.280)$,$(2.281)$，および$(2.282)$を満たし，$p(\mathbf{x})$を$(2.280)$,$(2.281)$に代入すると</p>
<p>$$
\begin{aligned}
\int{\mathrm{exp}{\lambda - 1 + \mathbf{y}^\top \mathbf{L}\mathbf{y} + \boldsymbol{\mu} ^\top \mathbf{m} - \frac{1}{4}\mathbf{m}^\top\mathbf{L}^{-1}\mathbf{m}}}\mathrm{d}\mathbf{y} &amp;= 1\
\int{\mathrm{exp}{\lambda - 1 + \mathbf{y}^\top \mathbf{L}\mathbf{y} + \boldsymbol{\mu} ^\top \mathbf{m} - \frac{1}{4}\mathbf{m}^\top\mathbf{L}^{-1}\mathbf{m}}(\mathbf{y} + \boldsymbol{\mu} - \frac{1}{2}\mathbf{L}^{-1}\mathbf{m})}\mathrm{d}\mathbf{y} &amp;= \boldsymbol{\mu}\
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
(\boldsymbol{\mu} - \frac{1}{2}\mathbf{L}^{-1}\mathbf{m})\int{\mathrm{exp}{\lambda - 1 + \mathbf{y}^\top \mathbf{L}\mathbf{y} + \boldsymbol{\mu} ^\top \mathbf{m} - \frac{1}{4}\mathbf{m}^\top\mathbf{L}^{-1}\mathbf{m}}}\mathrm{d}\mathbf{y} \+ \int{\mathrm{exp}{\lambda - 1 + \mathbf{y}^\top \mathbf{L}\mathbf{y} + \boldsymbol{\mu} ^\top \mathbf{m} - \frac{1}{4}\mathbf{m}^\top\mathbf{L}^{-1}\mathbf{m}}\mathbf{y}}\mathrm{d}\mathbf{y} = \boldsymbol{\mu}\
\boldsymbol{\mu} - \frac{1}{2}\mathbf{L}^{-1}\mathbf{m} = \boldsymbol{\mu}\
\end{aligned}
$$</p>
<p>となって，$\mathbf{m}=\mathbf{0}$であることが分かって，$p(\mathbf{x}) = \mathrm{exp}{\lambda - 1 + (\mathbf{x}-\boldsymbol{\mu})^\top \mathbf{L}(\mathbf{x}-\boldsymbol{\mu})}$となる．</p>
<p>さらに，$(2.282)$に代入すると</p>
<p>$$
\begin{aligned}
\int{\mathrm{exp}{\lambda - 1 + (\mathbf{x} - \boldsymbol{\mu})\mathbf{L}(\mathbf{x} - \boldsymbol{\mu})^\top}(\mathbf{x} - \boldsymbol{\mu})(\mathbf{x} - \boldsymbol{\mu})^\top}\mathrm{d}\mathbf{x} &amp;= \mathbf{\Sigma}\
\end{aligned}
$$</p>
<p>となる．$\mathbf{z} = \mathbf{x} - \boldsymbol{\mu}$で書き換えると</p>
<p>$$
\begin{aligned}
\int{\mathrm{exp}{\lambda - 1 + \mathbf{z}\mathbf{L}\mathbf{z}^\top}\mathbf{z}\mathbf{z}^\top}\mathrm{d}\mathbf{x} &amp;= \mathbf{\Sigma}\
\mathrm{exp}(\lambda - 1)\int{\mathrm{exp}{\mathbf{z}\mathbf{L}\mathbf{z}^\top}\mathbf{z}\mathbf{z}^\top}\mathrm{d}\mathbf{x} &amp;= \mathbf{\Sigma}\
\end{aligned}
$$</p>
<p>となる．$(2.61)$と比較すると，$\mathbf{L} = -\frac{1}{2} \Sigma^{-1}$であることがわかる．</p>
<p>またここで，$\mathrm{exp}(\lambda - 1)$は正規化されていることを保証しているため</p>
<p>$$
\begin{aligned}
\mathrm{exp}(\lambda - 1) &amp;= \frac{1}{(2\pi)^\frac{1}{2}}\frac{1}{|\mathbf{\Sigma}|^\frac{1}{2}}\
\lambda &amp;= \ln\left{\frac{1}{(2\pi)^\frac{1}{2}}\frac{1}{|\mathbf{\Sigma}|^\frac{1}{2}}\right} + 1
\end{aligned}
$$</p>
<p>となる．</p>
<h2 id="演習-215"><a class="header" href="#演習-215">演習 2.15</a></h2>
<div class="panel-primary">
<p>多変量ガウス分布$\mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma})$のエントロピーが</p>
<p>$$
\mathrm{H}[\mathbf{x}]=\frac{1}{2} \ln |\mathbf{\Sigma}|+\frac{D}{2}(1+\ln (2 \pi)) \tag{2.283}
$$</p>
<p>となることを示せ．ただし，$D$は$\mathbf{x}$の次元数である．</p>
</div>
<p>エントロピーの定義の式$(1.104)$に直接ガウス分布の式$(2.43)$を代入して式を変形していく。ガウス分布が正規化されていること$\displaystyle \int \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}) \mathrm{d} \mathbf{x} = 1$を利用する。</p>
<p>$$
\begin{aligned}
\mathrm{H}[\mathbf{x}]
&amp;=-\int p(\mathbf{x}) \ln p(\mathbf{x}) \mathrm{d} \mathbf{x}\
&amp;=-\int \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}) \ln \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}) \mathrm{d} \mathbf{x} \
&amp;= \int \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}) \cdot \frac{1}{2} { D \ln(2 \pi) + \ln |\mathbf{\Sigma}| + (\mathbf{x} - \boldsymbol{\mu})^T \mathbf{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) } \mathrm{d} \mathbf{x} \
&amp;= \frac{1}{2} \left{ D \ln(2 \pi) \int \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}) \mathrm{d} \mathbf{x} + \ln |\mathbf{\Sigma}| \int \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}) \mathrm{d} \mathbf{x} + \int \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}) (\mathbf{x} - \boldsymbol{\mu})^\mathrm{T} \mathbf{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) \mathrm{d} \mathrm{x} \right} \
&amp;= \frac{1}{2} \left{ D \ln(2 \pi) \cdot 1 + \ln |\mathbf{\Sigma}| \cdot 1 + \int \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}) (\mathbf{x} - \boldsymbol{\mu})^\mathrm{T} \mathbf{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) \mathrm{d} \mathbf{x} \right}
\end{aligned}
$$</p>
<p>ここで、第3項$\displaystyle \int \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}) (\mathbf{x} - \boldsymbol{\mu})^\mathrm{T} \mathbf{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) \mathrm{d} \mathbf{x}$について、<strong>二次形式とトレースの関係の式</strong>$\mathbf{x}^{\mathrm{T}}\mathbf{A}\mathbf{x} = \mathrm{Tr}(\mathbf{Axx}^{\mathrm{T}})$を使って変形する。これは任意の$m \times n$行列$\mathbf{M}$と$n \times m$行列$\mathbf{N}$に対して$\mathrm{Tr}(\mathbf{MN}) = \mathrm{Tr}(\mathbf{NM})$が成立する（統計のための行列代数 上巻 第5章 補助定理5.2.1）ことから容易に示せる（下記）。また、トレースの値はスカラーであり、任意のスカラー$k$と任意の正方行列$\mathbf{A}$, $\mathbf{B}$に対して以下の性質（<strong>トレースの線形性</strong>）が成立することを利用する（※統計のための行列代数 上巻 第5章 5.1）。</p>
<p>$$
\begin{aligned}
\mathrm{Tr}(k\mathbf{A}) &amp;= k\mathrm{Tr}(\mathbf{A}) \
\mathrm{Tr}(\mathbf{A}+\mathbf{B}) &amp;= \mathrm{Tr}({\mathbf{A}}) + \mathrm{Tr}({\mathbf{B}})
\end{aligned}
$$</p>
<p>また、$\mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma})$はスカラーであるので、$\displaystyle \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}) \mathrm{Tr}(\mathbf{A}) = \mathrm{Tr}(\mathbf{\mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}) A})$と変形できる。</p>
<p>以上から、
$$
\begin{aligned}
&amp; \int \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}) (\mathbf{x} - \boldsymbol{\mu})^\mathrm{T} \mathbf{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) \mathrm{d} \mathbf{x} \
=&amp; \int \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}) \mathrm{Tr
}\left( \mathbf{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) (\mathbf{x} - \boldsymbol{\mu})^\mathrm{T} \right)\mathrm{d} \mathbf{x} \
=&amp; \int \mathrm{Tr} \left[ \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}) \left( \mathbf{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) (\mathbf{x} - \boldsymbol{\mu})^\mathrm{T} \right)\right]\mathrm{d} \mathbf{x} \
=&amp; \int \mathrm{Tr} \left[ \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}) (\mathbf{x} \mathbf{x}^\mathrm{T} - \mathbf{x} \boldsymbol{\mu}^\mathrm{T} - \boldsymbol{\mu} \mathbf{x}^\mathrm{T} + \boldsymbol{\mu} \boldsymbol{\mu}^\mathrm{T}) \mathbf{\Sigma}^{-1} \right] \mathrm{d} \mathbf{x} \hspace{1em} (\because \mathrm{Tr}(\mathbf{MN}) = \mathrm{Tr}(\mathbf{NM})) \
=&amp; \int \mathrm{Tr} \left[ \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}) (\mathbf{x} \mathbf{x}^\mathrm{T} - 2\mathbf{x} \boldsymbol{\mu}^\mathrm{T} + \boldsymbol{\mu} \boldsymbol{\mu}^\mathrm{T}) \mathbf{\Sigma}^{-1} \right] \mathrm{d} \mathbf{x} \hspace{1em} (\because \mathrm{Tr}(\mathbf{x}\boldsymbol{\mu}^{\mathrm{T}}) = \mathrm{Tr}(\boldsymbol{\mu}\mathbf{x}^{\mathrm{T}}))\
=&amp; \int \mathrm{Tr} [ \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}) \mathbf{xx}^\mathrm{T} \mathbf{\Sigma}^{-1} ] \mathrm{d} \mathbf{x}
-2 \int \mathrm{Tr} [ \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma})\mathbf{x} \boldsymbol{\mu}^\mathrm{T} \mathbf{\Sigma}^{-1} ]\mathrm{d} \mathbf{x}
+ \int \mathrm{Tr} [ \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma})\boldsymbol{\mu} \boldsymbol{\mu}^\mathrm{T} \mathbf{\Sigma}^{-1} ] \mathrm{d} \mathbf{x} \
=&amp; \mathrm{Tr} \left[ \left{ \int \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}) \mathbf{xx}^\mathrm{T} \mathrm{d} \mathbf{x} -2 \int \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma})\mathbf{x} \boldsymbol{\mu}^\mathrm{T} \mathrm{d} \mathbf{x} + \int \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma})\boldsymbol{\mu} \boldsymbol{\mu}^\mathrm{T} \right} \mathbf{\Sigma}^{-1}\right] \
\end{aligned}
$$</p>
<p>ここで、$(2.59)$と$(2.62)$より</p>
<p>$$\int \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}) \mathbf{x} \mathrm{d} \mathbf{x} = \boldsymbol{\mu}$$</p>
<p>$$\int \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}) \mathbf{xx}^\mathrm{T} \mathrm{d} \mathbf{x} = \boldsymbol{\mu\mu}^\mathrm{T} + \mathbf{\Sigma}$$</p>
<p>ゆえに</p>
<p>$$
\begin{aligned}
\mathrm{H}[\mathbf{x}]
&amp;= \frac{1}{2} { D \ln(2 \pi) + \ln |\mathbf{\Sigma}| + \mathrm{Tr}[ { (\boldsymbol{\mu} \boldsymbol{\mu}^\mathrm{T} + \mathbf{\Sigma}) - 2 \boldsymbol{\mu} \boldsymbol{\mu}^T + \boldsymbol{\mu} \boldsymbol{\mu}^\mathrm{T} } \mathbf{\Sigma}^{-1}  ]} \
&amp;= \frac{1}{2} { D \ln(2 \pi) + \ln |\mathbf{\Sigma}| + \mathrm{Tr}[  \mathbf{\Sigma} \mathbf{\Sigma}^{-1}] } \
&amp;= \frac{1}{2} { D \ln(2 \pi) + \ln |\mathbf{\Sigma}| + \mathrm{Tr}[ \mathbf{I} ] } \
&amp;= \frac{1}{2} { D \ln(2 \pi) + \ln |\mathbf{\Sigma}| + D } \
&amp;= \frac{1}{2} \ln |\mathbf{\Sigma}| + \frac{D}{2}\left{ 1 + \ln (2 \pi) \right}
\end{aligned}
$$</p>
<p>したがって題意が示された。</p>
<p>※ 二次形式とトレースの関係$\mathbf{x}^{\mathrm{T}}\mathbf{A}\mathbf{x} = \mathrm{Tr}(\mathbf{Axx}^{\mathrm{T}})$の証明は、任意の$m \times n$行列$\mathbf{M}$と$n \times m$行列$\mathbf{N}$に対して$\mathrm{Tr}(\mathbf{MN}) = \mathrm{Tr}(\mathbf{NM})$が成立する（統計のための行列代数 上巻 第5章 補助定理5.2.1）ことを利用すれば以下のようにして簡単に求められる。</p>
<p>まず二次形式の値はスカラー（$1 \times 1$行列）なので$\mathbf{x}^{\mathrm{T}}\mathbf{A}\mathbf{x} = \mathrm{Tr}(\mathbf{x}^{\mathrm{T}}\mathbf{A}\mathbf{x})$となる。ここで、$\mathrm{Tr}(\mathbf{MN}) = \mathrm{Tr}(\mathbf{NM})$の定理において$\mathbf{M} = \mathbf{x}^{\mathrm{T}}$, $\mathbf{N} = \mathbf{Ax}$とすれば$\mathrm{Tr}(\mathbf{x}^{\mathrm{T}}\mathbf{A}\mathbf{x}) = \mathrm{Tr}(\mathbf{A}\mathbf{x}\mathbf{x}^{\mathrm{T}})$となるので、$\mathbf{x}^{\mathrm{T}}\mathbf{A}\mathbf{x} = \mathrm{Tr}(\mathbf{Axx}^{\mathrm{T}})$となることが示された。</p>
<h2 id="演習-216"><a class="header" href="#演習-216">演習 2.16</a></h2>
<div class="panel-primary">
<p>2つの確率変数$x_1$と$x_2$を考える．これらはそれぞれ平均が$\mu_1$と$\mu_2$で，精度が$\tau_1$と$\tau_2$のガウス分布に従うとする．このとき，変数$x=x_1+x_2$の微分エントロピーの式を導出せよ．これには，まず，次の関係を用いて$x$の分布を求め，指数部分を平方完成する．</p>
<p>$$
p(x)=\int_{-\infty}^{\infty} p\left(x | x_{2}\right) p\left(x_{2}\right) \mathrm{d} x_{2} \tag{2.284}
$$</p>
<p>次に，これが2つのガウス分布のたたみ込みになっており，また，これ自体もガウス分布になっていることに注目する.最後に1変数のガウス分布のエントロピーの結果</p>
<p>$$
\mathrm{H}[x] = \frac{1}{2}{ 1+\ln(2\pi\sigma^2) } \tag{1.110}
$$</p>
<p>を利用する．</p>
</div>
<p>$x_1$、$x_2$について、問題文の条件より</p>
<p>$$p(x_1) = \mathcal{N}(x_1 | \mu_1, \gamma_1^{-1})$$</p>
<p>$$p(x_2) = \mathcal{N}(x_2 | \mu_2, \gamma_2^{-1})$$</p>
<p>変数$x = x_1 + x_2$について、$x_2$が観測され固定された数とみなすと、$x$は$x_1$の線形関数とみなせる。</p>
<p>よって$p(x|x_2)$の分布の平均は$x_1$の平均$\mu_1$に$x_2$を足したものとなり、分散は$x_1$の分散$\gamma_1^{-1}$となる。</p>
<p>ゆえに</p>
<p>$$p(x|x_2) = \mathcal{N}(x | \mu_1 + x_2, \gamma_1^{-1})$$</p>
<p>(2.284)より</p>
<p>$$\begin{aligned}p(x) &amp;= \int_{-\infty}^{\infty} \mathcal{N}(x | \mu_1 + x_2, \gamma_1^{-1}) \cdot \mathcal{N}(x_2 | \mu_2, \gamma_2^{-1}) \mathrm{d} x_2 \&amp;=  \int_{-\infty}^{\infty} \left( \frac{\gamma_1}{2 \pi} \right)^{\frac{1}{2}} \exp \left{- \frac{\gamma_1}{2} (x - \mu_1 - x_2)^2 \right} \cdot \left( \frac{\gamma_2}{2 \pi} \right)^{\frac{1}{2}} \exp \left{- \frac{\gamma_2}{2} (x_2 - \mu_2)^2 \right} \mathrm{d} x_2 \&amp;= \int_{-\infty}^{\infty} \left( \frac{\gamma_1}{2 \pi} \right)^{\frac{1}{2}} \left( \frac{\gamma_2}{2 \pi} \right)^{\frac{1}{2}} \exp \left{- \frac{\gamma_1}{2} (x - \mu_1 - x_2)^2 -   \frac{\gamma_2}{2} (x_2 - \mu_2)^2 \right} \mathrm{d} x_2 \end{aligned}$$</p>
<p>この式の指数部分を$x_2$について平方完成すると</p>
<p>$$\begin{aligned} - \frac{\gamma_1}{2} (x - \mu_1 - x_2)^2 -   \frac{\gamma_2}{2} (x_2 - \mu_2)^2 &amp;= - \frac{1}{2} (\gamma_1 + \gamma_2) x_2^2 + { \gamma_1(x - \mu_1) + \gamma_2 \mu_2 } x_2 - \frac{\gamma_1}{2}(x - \mu_1)^2 - \frac{\gamma_2}{2} \mu_2^2 \&amp;= - \frac{1}{2} (\gamma_1 + \gamma_2) \left{ x_2 - \frac{\gamma_1 (x - \mu_1) + \gamma_2 \mu_2}{\gamma_1 + \gamma_2} \right}^2 + \frac{{\gamma_1 (x - \mu_1) + \gamma_2 \mu_2}^2}{2 (\gamma_1 + \gamma_2)} - \frac{\gamma_1}{2}(x - \mu_1)^2 - \frac{\gamma_2}{2} \mu_2^2  \end{aligned}$$</p>
<p>ここで、</p>
<p>$$m = \frac{\gamma_1 (x - \mu_1) + \gamma_2 \mu_2}{\gamma_1 + \gamma_2}$$</p>
<p>と置くと、指数部分の式は以下のように表せる。</p>
<p>$$- \frac{\gamma_1 + \gamma_2}{2}  \left( x_2 - m \right)^2 + \frac{{\gamma_1 (x - \mu_1) + \gamma_2 \mu_2}^2}{2 (\gamma_1 + \gamma_2)} - \frac{\gamma_1}{2}(x - \mu_1)^2 - \frac{\gamma_2}{2} \mu_2^2$$</p>
<p>この式の$x_2$への依存性を見てみると、ガウス分布の標準的な二次形式部分である第一項に、$x_2$に依存しない項を足したものとなっている。</p>
<p>ここで$x_2$を積分消去すると、指数部分の式は第二項以降となる。</p>
<p>$x$の精度は指数部分の式の$x^2$の係数で直接与えられるため、上記式の第二項以降の項において$x^2$の係数を計算すると</p>
<p>$$\begin{aligned} \frac{{\gamma_1 (x - \mu_1) + \gamma_2 \mu_2}^2}{2 (\gamma_1 + \gamma_2)} - \frac{\gamma_1}{2}(x - \mu_1)^2 - \frac{\gamma_2}{2} \mu_2^2 &amp;= \frac{\gamma_1^2}{2 (\gamma_1 + \gamma_2)} x^2 - \frac{\gamma_1}{2} x^2 + \text{const} \&amp;= - \frac{1}{2} \frac{\gamma_1 \gamma_2}{\gamma_1 +  \gamma_2} x^2 + \text{const} \end{aligned}$$</p>
<p>ただし、$\text{const}$は$x^2$に依存しない数を表す。</p>
<p>ガウス分布の式(2.42)との比較から、$x$の精度を$\gamma$とすると</p>
<p>$$\gamma = \frac{\gamma_1 \gamma_2}{\gamma_1 +  \gamma_2}$$
が得られる。</p>
<p>$(1.110)$の式から微分エントロピーは</p>
<p>$$\begin{aligned}\mathrm{H}[\mathbf{x}] &amp;= \frac{1}{2} \left{ 1 + \ln \left( \frac{2 \pi}{\gamma} \right) \right} \&amp;= \frac{1}{2} \left{ 1 + \ln \left( \frac{2 \pi (\gamma_1 + \gamma_2)}{\gamma_1 \gamma_2} \right) \right}  \end{aligned}$$</p>
<h2 id="演習-217"><a class="header" href="#演習-217">演習 2.17</a></h2>
<div class="panel-primary">
<p>$(2.43)$の多変量ガウス分布を考える精度行列（逆共分散行列）$\mathbf{\Sigma^{-1}}$を対称行列と反対称行列（歪対称行列）の和の形で書くと，反対称行列の項がガウス分布の指数部分には現れなくなるため，一般性を失うことなく精度行列は対称であるとしてよいことを示せ．この結果から，対称行列の逆行列も対称（演習2.22）なので，一般性を失うことなく，共分散行列にも対称なものを選んでよいことになる．</p>
</div>
<p>※ 演習1.14とやろうとしていることはほぼ同じです．行列の二次形式・対称行列・反対称行列についての定理をよく知っていれば瞬殺です．「統計のための行列代数」の第14章の補助定理14.1.1, 補助定理14.6.3あたりに出てきます．</p>
<p>補助定理14.6.3</p>
<p>$n\times n$行列の$\mathbf{A}$は，あらゆる$n$次元ベクトル$\mathbf{x}$に対して，$\mathbf{x'Ax}=0$のときかつそのときに限って，歪対称行列（反対称行列）である.</p>
<p>また，任意の正方行列$\mathbf{M}$は対称行列$\mathbf{A}$と反対称行列$\mathbf{S}$の和でただ1通りに表現できることも必要です．先にそれを示します．</p>
<p>ちなみに対称行列$\mathbf{A}$と反対称行列$\mathbf{S}$の一般形は次の通り．
$$
\mathbf{A}=\left(
\begin{array}{ccccc}a_{11} &amp; a_{12} &amp; a_{13} &amp; \cdots &amp; a_{1 n} \ a_{12} &amp; a_{22} &amp; a_{23} &amp; \cdots &amp; a_{2 n} \ a_{13} &amp; a_{23} &amp; a_{33} &amp; \cdots &amp; a_{3 n} \ \vdots &amp; \vdots &amp; &amp; \ddots &amp; \vdots \ a_{1 n} &amp; a_{2 n} &amp; a_{3 n} &amp; \cdots &amp; a_{n n}\end{array}
\right)
$$</p>
<p>$$
\mathbf{S}=\left(
\begin{array}{ccccc}0 &amp; s_{12} &amp; s_{13} &amp; \cdots &amp; s_{1 n} \ -s_{12} &amp; 0 &amp; s_{23} &amp; \cdots &amp; s_{2 n} \ -s_{13} &amp; -s_{23} &amp; 0 &amp; \cdots &amp; s_{3 n} \ \vdots &amp; \vdots &amp; &amp; \ddots &amp; \vdots \ -s_{1 n} &amp; -s_{2 n} &amp; -s_{3 n} &amp; \cdots &amp; 0\end{array}
\right)
$$</p>
<p>（証明）</p>
<p>任意の正方行列$\mathbf{M}$に対し，$\mathbf{(M+M}^{\mathrm{T}})^{\mathrm{T}}=\mathbf{M}^{\mathrm{T}}+{\mathbf M}$なので，$\mathbf{M}+\mathbf{M}^{\mathrm{T}}$は対称行列である．また，$\mathbf{(M-M}^{\mathrm{T}})^{\mathrm{T}}=-({\mathbf M}-\mathbf{M}^{\mathrm{T}})$なので，$\mathbf{M}-{\mathbf M}^{\mathrm{T}}$は反対称行列である．</p>
<p>よって，$\displaystyle{\mathbf{A}=\frac{\mathbf{M}+\mathbf{M}^{\mathrm{T}}}{2}, \mathbf{S}=\frac{\mathbf{M}-\mathbf{M}^{\mathrm{T}}}{2}}$とすれば，任意の正方行列$\mathbf{M}$は対称行列$\mathbf{A}$と反対称行列$\mathbf{S}$の和で表せることが示される．</p>
<p>また，これが1通りでのみ表せることを示す．そのために$\mathbf{M}=\mathbf{A}<em>{1}+\mathbf{S}</em>{1}=\mathbf{A}<em>{2}+\mathbf{S}</em>{2}$と仮定する．ここで，$\mathbf{A}<em>1,\mathbf{A}<em>2$は対称行列，$\mathbf{S}<em>1,\mathbf{S}<em>2$は反対称行列である．
上式を移行すると$\mathbf{S}</em>{1}-\mathbf{S}</em>{2}=\mathbf{A}</em>{2}-\mathbf{A}</em>{1}$であるが，$\mathbf{S}<em>{1}-\mathbf{S}</em>{2}$は反対称行列，$\mathbf{A}<em>{2}-\mathbf{A}</em>{1}$は対称行列となる．よって，これを満たすのは</p>
<p>$$
\mathbf{S}<em>{1}-\mathbf{S}</em>{2}=\mathbf{A}<em>{2}-\mathbf{A}</em>{1} = \mathbf{O}
$$
のときのみであり，$\mathbf{S}<em>{1}=\mathbf{S}</em>{2}$, $\mathbf{A}<em>{1}=\mathbf{A}</em>{2}$となる．したがって一意性が示された．</p>
<p>これにより，精度行列$\mathbf{\Sigma}^{-1}$も対称行列$\frac{\mathbf{\Sigma}^{-1}+(\mathbf{\Sigma}^{-1})^{\mathrm T}}{2}$, 反対称行列$\frac{\mathbf{\Sigma}^{-1}-(\mathbf{\Sigma}^{-1})^{\mathrm T}}{2}$に分解できる．</p>
<p>ガウス分布の二次形式部分（マハラノビス距離の部分）$\Delta^2 = (\mathbf{x}-\boldsymbol{\mu})^{\mathrm T}\mathbf{\Sigma^{-1}}(\mathbf{x}-\boldsymbol{\mu})$の$\mathbf{\Sigma^{-1}}$の反対称要素は消えることを示す．</p>
<p>$\mathbf{\Sigma^{-1}}=\mathbf{A}+\mathbf{S}$と書く．ここで$\mathbf{A}$は対称行列，$\mathbf{S}$は反対称行列．</p>
<p>ここで，任意の$D$次元ベクトル$\mathbf{y}$と$D\times D$の反対称行列$\mathbf{S}$について，</p>
<p>$$
\mathbf{y}^{\mathrm T}{\mathbf{S}}{\mathbf{y}}=0
$$</p>
<p>が成立することを示す（上述の補助定理14.6.3）．任意の$D\times D$の行列$\mathbf{M}$を使うと</p>
<p>$$
\begin{aligned}
\mathbf{y}^{\mathrm T}{\mathbf{S}}{\mathbf{y}} &amp;=\sum_{j=1}^{D}\left(\sum_{i=1}^{D} y_{i} S_{i j}\right) y_{j} \
&amp;=\sum_{j=1}^{D} \sum_{i=1}^{D} y_{i} \cdot \frac{M_{ij}-M_{ji}}{2}\cdot y_{j} \
&amp;=\frac{1}{2} \left{ \sum_{j=1}^{D} \sum_{i=1}^{D} y_{i} M_{ij} y_{j}-\sum_{j=1}^{D} \sum_{i=1}^{D} y_{j} M_{ji} y_{i}\right} \
&amp;=0
\end{aligned}
$$
よって，</p>
<p>$$
\begin{aligned}
\Delta^{2} &amp;=(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}}(\mathbf{A}+\mathbf{S})(\mathbf{x}-\boldsymbol{\mu}) \
&amp;=(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}} \mathbf{A}(\mathbf{x}-\boldsymbol{\mu})
\end{aligned}
$$</p>
<p>つまり，反対称要素が消えることが示された．</p>
<h2 id="演習-218"><a class="header" href="#演習-218">演習 2.18</a></h2>
<div class="panel-primary">
<p>実対称行列$\mathbf{\Sigma}$を考える．この行列について$(2.45)$</p>
<p>$$
\mathbf{\Sigma}\mathbf{u}_i = \lambda_i\mathbf{u}_i
$$</p>
<p>の固有値の方程式が成立する．この式の複素共役から，もとの式を引いた後，固有ベクトル$\mathbf{u}_i$との内積をとることで，固有値$\lambda_i$が実数となることを示せ．同様に，$\mathbf{\Sigma}$の対称性を用いて，2つの固有ベクトル$\mathbf{u}_i$と$\mathbf{u}_j$が，$\lambda_j \neq \lambda_i$であれば，直交することを示せ．最後に，たとえいくつかの固有値が0であっても，一般性を失うことなく，$(2.46)$式 $\mathbf{u}_i^{\mathrm{T}}\mathbf{u}<em>j = I</em>{ij}$を満たす，正規直交となるように固有ベクトル集合を選ぶことが可能であることを示せ．</p>
</div>
<p>この問題は以下の3つの問題を証明していくことになる。</p>
<p>① <strong>対称行列の固有値は実数となる。</strong>
② <strong>異なる固有値に対応する固有ベクトル同士は直交する。</strong>
③ <strong>いくつかの固有値が0であっても、正規直交となるように固有ベクトル集合を選ぶことができる。</strong></p>
<br>
<p>① <strong>対称行列の固有値は実数となる。</strong></p>
<p>$(2.45)$式</p>
<p>$$
\mathbf{\Sigma}\mathbf{u}_i = \lambda_i\mathbf{u}_i
$$</p>
<p>に左から随伴行列$\mathbf{u}_i^{\dagger}$をかけると（$\lambda_i$はスカラー値であることに注意する）</p>
<p>$$
\mathbf{u}_i^{\dagger}\mathbf{\Sigma}\mathbf{u}_i = \mathbf{u}_i^{\dagger}\lambda_i\mathbf{u}_i
$$</p>
<p>$$
\mathbf{u}_i^{\dagger}\mathbf{\Sigma}\mathbf{u}_i = \lambda_i\mathbf{u}_i^{\dagger}\mathbf{u}_i \tag{1}
$$</p>
<p>また$(2.45)$式の両辺の複素共役をとり、右から$\mathbf{u}_i$をかける。$(\mathbf{\Sigma u_i})^{\dagger}=\mathbf{u}_i^{\dagger}\mathbf{\Sigma}^{\dagger}=\mathbf{u}_i^{\dagger}\mathbf{\Sigma}$、 $\bar{\lambda_i}$を$\lambda_i$の複素共役として</p>
<p>$$
(\mathbf{\Sigma u_i})^{\dagger}\mathbf{u}_i = (\lambda_i\mathbf{u}_i)^{\dagger}\mathbf{u}_i
$$</p>
<p>$$
\mathbf{u}_i^{\dagger}\mathbf{\Sigma}\mathbf{u}_i = \bar{\lambda_i}\mathbf{u}_i^{\dagger}\mathbf{u}_i \tag{2}
$$</p>
<p>よって(1)(2)式の差を取ると</p>
<p>$$
0 = (\lambda_i - \bar{\lambda_i})\mathbf{u_i}^{\dagger}\mathbf{u_i}
$$
となる。$\mathbf{u_i}^{\dagger}\mathbf{u_i}$は$0$ではないため、$\lambda_i = \bar{\lambda_i}$、つまり$\lambda_i$が実数となることが示された。</p>
<br>
<p>② <strong>異なる固有値に対応する固有ベクトル同士は直交する。</strong></p>
<p>次に$(2.45)$式の左側から$\mathbf{u_j}^{\mathrm{T}}$をかける</p>
<p>$$
\mathbf{u}_j^{\mathrm{T}}\mathbf{\Sigma}\mathbf{u}_i = \mathbf{u}_j^{\mathrm{T}}\lambda_i\mathbf{u}_i
$$</p>
<p>$$
\mathbf{u}_j^{\mathrm{T}}\mathbf{\Sigma}\mathbf{u}_i = \lambda_i\mathbf{u}_j^{\mathrm{T}}\mathbf{u}_i
$$</p>
<p>$$
\begin{aligned}
\lambda_i\mathbf{u}_j^{\mathrm{T}}\mathbf{u}_i &amp;= \mathbf{u}_j^{\mathrm{T}}\mathbf{\Sigma}\mathbf{u}_i \
&amp;= (\mathbf{\Sigma}\mathbf{u}_j)^{\mathrm{T}}\mathbf{u}_i \
&amp;= (\lambda_j\mathbf{u}_j)^{\mathrm{T}}\mathbf{u}_i \
&amp;= \lambda_j\mathbf{u}_j^{\mathrm{T}}\mathbf{u}_i
\end{aligned}
$$
ここで$\lambda_i \neq \lambda_j$であれば上式から$\mathbf{u}_j^{\mathrm{T}}\mathbf{u}_i=0$となるので、$\mathbf{u}_i$,$\mathbf{u}_j$が直交していることが示された。この固有ベクトルは定数倍して$\mathbf{u}_i \to \mathbf{u}_i/||\mathbf{u}_i||$というように正規化すれば$(2.46)$式を満足させることができる。</p>
<p>（※統計のための行列代数下巻 P.238 定理21.4.5と同じ）</p>
<p>ちなみに、同じ固有値$\lambda(\neq0)$に対する2個の固有ベクトル$\mathbf{u}_i, \mathbf{u}_j$が存在したとき、この線型結合$a_i\mathbf{u}_i+a_j\mathbf{u}_j$について</p>
<p>$$
\mathbf{\Sigma}(a_i\mathbf{u}_i+a_j\mathbf{u}_j) = \lambda(a_i\mathbf{u}_i+a_j\mathbf{u}_j)
$$</p>
<p>と書くことができ、$a_i\mathbf{u}_i+a_j\mathbf{u}_j$も同じ固有値に対する固有ベクトルとして表現できる。これが得られる場合、例えばグラム・シュミットの直交化法を使うことで$(2.46)$式である$\mathbf{u}_k^{\mathrm{T}}\mathbf{u}<em>l = I</em>{kl}$となるような正規直交された$\mathbf{u}_k$, $\mathbf{u}_l$を$\mathbf{u}_i, \mathbf{u}_j$の組から得ることが可能であることが知られている。以上から、同じ固有値$\lambda_i=\lambda_j(\neq 0)$の場合でも$(2.46)$式を満足させられる固有ベクトルを得ることは可能である。</p>
<br>
<p>③ <strong>いくつかの固有値が0であっても、正規直交となるように固有ベクトル集合を選ぶことができる。</strong></p>
<p>最後に、もしいくつかの$\lambda_i$が$0$だった場合、正則行列についての定理から$\mathbf{\Sigma}$は正則行列ではなく、逆行列を持たない特異行列となる。このとき、$\mathbf{\Sigma}\mathbf{u}_i=\mathbf{0}$となる零ベクトルではない固有ベクトル$\mathbf{u}_i$が存在することになり、これは$\mathbf{\Sigma}$の零空間（核）をなす。さらに、同じ固有値0となる別の固有ベクトル$\mathbf{u}_j$がある場合でも、上記のグラム・シュミット直交化法によって$(2.46)$式を満足させられる固有ベクトルを得ることが可能である。</p>
<h2 id="演習-219"><a class="header" href="#演習-219">演習 2.19</a></h2>
<div class="panel-primary">
<p>固有ベクトルの方程式について$$\mathbf{\Sigma}\mathbf{u}_i=\lambda_i\mathbf{u}_i \tag{2.45}$$が成立する実対称行列$\mathbf{\Sigma}$は，固有値を係数とする固有ベクトルで展開した，</p>
<p>$$
\mathbf{\Sigma}=\sum_{i=1}^{D} \lambda_{i} \mathbf{u}<em>{i} \mathbf{u}</em>{i}^{\mathrm{T}} \tag{2.48}
$$</p>
<p>の形で表せることを示せ．同様に，逆行列$\mathbf{\Sigma}^{-1}$は</p>
<p>$$
\mathbf{\Sigma}^{-1}=\sum_{i=1}^{D} \frac{1}{\lambda_{i}} \mathbf{u}<em>{i} \mathbf{u}</em>{i}^{\mathrm{T}} \tag{2.49}
$$</p>
<p>の形で表現できることを示せ．</p>
</div>
<p>※ $(2.45)$と$(2.48)$を行列形式で表すと変形が簡単になる。このとき、固有値$\lambda_i$を対角行列$\mathbf{\Lambda}$で書くとわかりやすい。</p>
<p>まず$\mathbf{\Lambda}$を$\displaystyle \mathbf{\Lambda} = \left(\begin{array}{cccc}\lambda_{1} &amp; &amp; &amp; 0 \ &amp; \lambda_{2} &amp; &amp; \ &amp; &amp; \ddots &amp; \ 0 &amp; &amp; &amp; \lambda_{D}\end{array}\right)$となるような固有値からなる対角行列であるとする。さらに、$\mathbf{U}$をその列が$D$個の固有ベクトルからなる行列であるとする（つまり$\mathbf{U} = (\mathbf{u}<em>{1}, \mathbf{u}</em>{2}, \cdots, \mathbf{u}_{D})$）。$\mathbf{\Lambda}, \mathbf{U}$はともに$D \times D$の行列である。</p>
<p>これにより$(2.45)$は行列形式で$\mathbf{\Sigma}\mathbf{U} = \mathbf{U}\mathbf{\Lambda}$と表せ（付録C.38などを参照）、$(2.48)$は$\mathbf{\Sigma} = \mathbf{U}\mathbf{\Lambda}\mathbf{U}^{\mathrm{T}}$で表せる（……ということはちょっと計算してみないとわかりにくいかもしれない）。</p>
<p>よって、$\mathbf{\Sigma}\mathbf{U} = \mathbf{U}\mathbf{\Lambda}$の左から$\mathbf{U}^{\mathrm{T}}$を掛けると</p>
<p>$$
\begin{aligned}
\mathbf{U}^{\mathrm{T}}\mathbf{\Sigma}\mathbf{U} &amp;= \mathbf{U}^{\mathrm{T}}\mathbf{U}\mathbf{\Lambda} \
&amp;= \mathbf{\Lambda} \  (\because \mathbf{U}\mathbf{U}^{\mathrm{T}}=\mathbf{U}\mathbf{U}^{\mathrm{T}}=1 )\
\end{aligned}
$$
となる。一方、この式から$\mathbf{\Sigma} = \mathbf{U}\mathbf{\Lambda}\mathbf{U}^{\mathrm{T}}$と表せることも分かり、$(2.48)$の行列形式が得られることが示された。</p>
<p>また、$\mathbf{U}^{\mathrm{T}} = \mathbf{U}^{-1}$であることを利用すれば
$$
\begin{aligned}
\mathbf{\Sigma}^{-1} &amp;= (\mathbf{U}\mathbf{\Lambda}\mathbf{U}^{\mathrm{T}})^{-1} \
&amp;= (\mathbf{U}^{\mathrm{T}})^{-1}\mathbf{\Lambda}^{-1}\mathbf{U}^{-1} \
&amp;= \mathbf{U}\mathbf{\Lambda}^{-1}\mathbf{U}^{\mathrm{T}}
\end{aligned}
$$
となる。ここで$\mathbf{\Lambda}\mathbf{\Lambda}^{-1} = \mathbf{I}$より、$\mathbf{\Lambda}^{-1}$は対角成分が$\frac{1}{\lambda_i}$となる対角成分であることは容易にわかるので、</p>
<p>$$
\mathbf{\Sigma}^{-1} = \sum_{i=1}^{D}\frac{1}{\lambda_i}\mathbf{u}_i\mathbf{u}_i^{\mathrm{T}}
$$</p>
<p>となり、$(2.49)$式が得られることが示された。</p>
<p>※ $\mathbf{\Sigma}\mathbf{u}_i=\lambda_i\mathbf{u}_i$ならば$\mathbf{\Sigma} \mathbf{U}=\mathbf{U\Lambda}$であることは</p>
<p>$$
\begin{aligned}
\mathbf{\Sigma} \mathbf{U} &amp;= \mathbf{\Sigma}\left(\mathbf{u}<em>{1}, \mathbf{u}</em>{2}, \cdots, \mathbf{u}<em>{D}\right)=\left(\mathbf{\Sigma} \mathbf{u}</em>{1}, \mathbf{\Sigma} \mathbf{u}<em>{2}, \cdots \mathbf{\Sigma} \mathbf{u}</em>{D}\right) \
\mathbf{U} \mathbf{\Lambda} &amp;=\left(\mathbf{u}<em>{1}, \mathbf{u}</em>{2}, \cdots, \mathbf{u}<em>{D}\right) \begin{pmatrix}\lambda</em>{1} &amp; &amp; &amp; 0 \ &amp; \lambda_{2} &amp; &amp; \ &amp; &amp; \ddots &amp; \ 0 &amp; &amp; &amp; \lambda_{D}\end{pmatrix}=\left(\lambda \mathbf{u}<em>{1}, \lambda \mathbf{u}</em>{2}, \cdots, \lambda \mathbf{u}_{D}\right)
\end{aligned}
$$</p>
<p>から確かめられる。</p>
<h2 id="演習-220"><a class="header" href="#演習-220">演習 2.20</a></h2>
<div class="panel-primary">
<p>正定値行列$\mathbf{\Sigma}$は次の二次形式が，任意の実ベクトル$\mathbf{a}$について正になるということで定義できる．</p>
<p>$$
\mathbf{a}^{\mathrm{T}}\mathbf{\Sigma}\mathbf{a} \tag{2.285}
$$</p>
<p>$\mathbf{\Sigma}$が正定値になる必要十分条件は，$(2.45)$で定義される$\mathbf{\Sigma}$のすべての固有値$\lambda_i$が正となることであることを示せ．</p>
</div>
<p>※ 行列の定値性と固有値の間の有名な関係を示す問題です。<a href="https://ja.wikipedia.org/wiki/%E8%A1%8C%E5%88%97%E3%81%AE%E5%AE%9A%E5%80%A4%E6%80%A7">Wikipedia</a>にも書かれています。</p>
<p>問題文より、「$\mathbf{\Sigma}$が正定値行列である」ことの定義は、$(2.285)$のように任意の零ベクトルでない実ベクトル$\mathbf{a}$について$\mathbf{a}^{\mathrm{T}}\mathbf{\Sigma}\mathbf{a}&gt;0$が成立することである。</p>
<p>【解法1】 参考：統計のための行列代数下巻 定理21.8.4の証明</p>
<p>$(2.45)$式から$\mathbf{\Sigma}\mathbf{u}_i=\lambda_i\mathbf{u}_i$とする。$\lambda_i$は固有値である。</p>
<p>$(2.45)$式の両辺に左から$\mathbf{u}_i^{\mathrm{T}}$をかけると</p>
<p>$$
\mathbf{u}_i^{\mathrm{T}}\mathbf{\Sigma}\mathbf{u}_i=\lambda_i\mathbf{u}_i^{\mathrm{T}}\mathbf{u}_i
$$
$$
\lambda_i = \frac{\mathbf{u}_i^{\mathrm{T}}\mathbf{\Sigma}\mathbf{u}_i}{\mathbf{u}_i^{\mathrm{T}}\mathbf{u}_i} \tag{1}
$$</p>
<p>ここで、$\mathbf{u}_i^{\mathrm{T}}\mathbf{\Sigma}\mathbf{u}_i,\ \mathbf{u}_i^{\mathrm{T}}\mathbf{u}_i$はともにスカラーである。一方で、$\mathbf{u}_i^{\mathrm{T}}\mathbf{u}_i$は実ベクトルの自身の2乗なので常に$\mathbf{u}_i^{\mathrm{T}}\mathbf{u}_i&gt;0$となる（※特にこの問題設定では$\mathbf{u}_i$は直交行列なので$\mathbf{u}_i^{\mathrm{T}}\mathbf{u}_i=1$となる）。</p>
<p>よって、もし任意の$i$について$\lambda_{i}&gt;0$ならば$(1)$式から$\mathbf{u}_i^{\mathrm{T}}\mathbf{\Sigma}\mathbf{u}_i&gt;0$となる。すなわち、$\mathbf{\Sigma}$が正定値行列となる。反対に、もし$\mathbf{\Sigma}$が正定値行列ならば任意の$i$について$\mathbf{u}_i^{\mathrm{T}}\mathbf{\Sigma}\mathbf{u}_i&gt;0$となるので、すべての固有値$\lambda_i$が正となる。</p>
<p>以上から題意は示された。</p>
<p>【解法2】 PRML公式解答例による方法</p>
<p>$\mathbf{u}<em>{1}, \mathbf{u}</em>{2}, \ldots, \mathbf{u}_{D}$は$\mathbb{R}^D$の基底を張るので任意のベクトル$\mathbf{a}$は係数$a_1, \ldots, a_D$を使って</p>
<p>$$
\mathbf{a}=a_{1} \mathbf{u}<em>{1}+a</em>{2} \mathbf{u}<em>{2}+\ldots+a</em>{D} \mathbf{u}_{D}
$$</p>
<p>と書くことができる。これより、</p>
<p>$$
\begin{aligned}
\mathbf{a}^{\mathrm{T}}\mathbf{\Sigma}\mathbf{a}
&amp;= (a_{1} \mathbf{u}<em>{1}^{\mathrm{T}}+a</em>{2} \mathbf{u}<em>{2}^{\mathrm{T}}+\ldots+a</em>{D} \mathbf{u}<em>{D}^{\mathrm{T}})\mathbf{\Sigma}(a</em>{1} \mathbf{u}<em>{1}+a</em>{2} \mathbf{u}<em>{2}+\ldots+a</em>{D} \mathbf{u}<em>{D}) \
&amp;= \left(a</em>{1} \mathbf{u}<em>{1}^{\mathrm{T}}+\ldots+a</em>{D} \mathbf{u}<em>{D}^{\mathrm{T}}\right)\left(a</em>{1} \lambda_{1} \mathbf{u}<em>{1}+\ldots+a</em>{D} \lambda_{D} \mathbf{u}_{D}\right)\hspace{2em}(\because (2.45))
\end{aligned}
$$</p>
<p>となる。今、$i=j$ならば$\mathbf{u}_i^{\mathrm{T}}\mathbf{u}_j=1$でそれ以外のとき$\mathbf{u}_i^{\mathrm{T}}\mathbf{u}_j=0$であることを利用すれば、</p>
<p>$$
\mathbf{a}^{\mathrm{T}}\mathbf{\Sigma}\mathbf{a} = a_{1}^{2} \lambda_{1}+\ldots+a_{D}^{2} \lambda_{D}
$$</p>
<p>が得られる。よって、もしすべての固有値が正ならば（$\lambda_i&gt;0$）$\mathbf{a}^{\mathrm{T}}\mathbf{\Sigma}\mathbf{a}&gt;0$となるため、$\mathbf{\Sigma}$が正定値行列であることがわかる。</p>
<p>反対に$\mathbf{\Sigma}$が正定値行列であるならばすべての固有値が正となることを示す。このために対偶「ある1つの固有値が0以下でならば、$\mathbf{\Sigma}$は正定置行列ではない」ことを示す。</p>
<p>もしある$\lambda_i$について$\lambda_i \le 0$となるようなものが存在した場合、$\mathbf{a}  = \mathbf{u}_i$とすれば$\mathbf{a}^{\mathrm{T}}\mathbf{\Sigma}\mathbf{a} = \lambda_i\mathbf{u}_i^{\mathrm{T}}\mathbf{u}_i \le 0$となり、$\mathbf{\Sigma}$は正定置行列ではないことが示される。よって対偶を取ると「$\mathbf{\Sigma}$が正定値行列であるならばすべての固有値が正となる」が示される。</p>
<p>以上から必要十分条件が示された。</p>
<h2 id="演習-221"><a class="header" href="#演習-221">演習 2.21</a></h2>
<div class="panel-primary">
<p>大きさが$D\times D$の実対称行列の独立なパラメータは、$D(D+1)/2$個であることを示せ．</p>
</div>
<p>大きさが$D\times D$の実対称行列の全成分の個数は当然$D^2$個である。このうち、対角成分の$D$個を除いて残りのパラメータ（非対角成分）は対角成分に対して対称な値になっていなければならないので、そのパラメータの自由度は$\displaystyle \frac{D^2-D}{2}$個である。これに$D$個を足して</p>
<p>$$
\frac{D^2-D}{2}+D = \frac{D(D+1)}{2}
$$</p>
<p>つまり独立なパラメータは$\displaystyle \frac{D(D+1)}{2}$個である。</p>
<h2 id="演習-222"><a class="header" href="#演習-222">演習 2.22</a></h2>
<div class="panel-primary">
<p>対称行列の逆行列も対称であることを示せ．</p>
</div>
<p>ある任意の対称行列$\mathbf{A}$があり、逆行列が存在する場合それを$\mathbf{A}^{-1}$とすると、</p>
<p>$$
\mathbf{A}\mathbf{A}^{-1} = \mathbf{I}
$$</p>
<p>となる（$\mathbf{I}$は単位行列）。両辺の転置を取り、$\mathbf{A}$は対称行列なので$\mathbf{A} = \mathbf{A}^{\mathrm{T}}$であることに注意すると</p>
<p>$$
(\mathbf{A}\mathbf{A}^{-1})^{\mathrm{T}} = (\mathbf{A}^{-1})^{\mathrm{T}}\mathbf{A}^{\mathrm{T}} = (\mathbf{A}^{-1})^{\mathrm{T}}\mathbf{A} = \mathbf{I}
$$</p>
<p>ここで第3項について、逆行列の定義から</p>
<p>$$
(\mathbf{A}^{-1})^{\mathrm{T}} = \mathbf{A}^{-1}
$$</p>
<p>とならなければならないことがわかる。これは対称行列の逆行列$\mathbf{A}^{-1}$も対称行列となっていることを表している。</p>
<h2 id="演習-223"><a class="header" href="#演習-223">演習 2.23</a></h2>
<div class="panel-primary">
<p>$$
\mathbf{\Sigma}=\sum_{i=1}^{D} \lambda_{i} \mathbf{u}<em>{i} \mathbf{u}</em>{i}^{\mathrm{T}} \tag{2.48}
$$
の固有ベクトル展開を用いて座標系を対角化することで，マハラノビス距離$\Delta$が定数になる超楕円体の内部の体積が，</p>
<p>$$
V_{D}|\mathbf{\Sigma}|^{1 / 2} \Delta^{D} \tag{2.286}
$$</p>
<p>になることを示せ．ただし，$V_{D}$は$D$次元単位球の体積で，マハラノビス距離は $(2.44)$で定義される．</p>
</div>
<p>※ PRML第2章 <strong>2.3 ガウス分布</strong>の議論にある<strong>ガウス分布の幾何的な形状</strong>についての理解を深めるための問題です。</p>
<p>※ そもそも超楕円体って何？って調べてみても意外とGoogleでヒットしないのですが、以下の定義を使います。</p>
<blockquote> 楕円体は，2次曲面の一種です．2次元において，次の方程式：
<p>$$
\frac{x^2}{a^2}+\frac{y^2}{b^2}=1
$$</p>
<p>で表現される図形を楕円と呼びますが，これの$n$次元へ拡張したものと捉えて問題ありません．より厳密な呼び分けとしては，$n=3$のときのみ楕円体と呼び，$n\ge4$のとき<strong>超楕円体</strong>と呼ぶ場合もあるようです．
http://ssr-yuki.hatenablog.com/entry/2020/04/26/230647 </blockquote></p>
<p>マハラノビス距離の2乗$\displaystyle \Delta^2 = (\mathbf{x}-\mathbf{\mu})^{\mathrm{T}}\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})$は、P.78の手続きから固有ベクトル展開を用いて座標系を対角化することで$(2.50)$式$\displaystyle \Delta^2 = \sum_{i=1}^{D}\frac{y_i^2}{\lambda_i}$と書くことができる。例として$D=2$であれば</p>
<p>$$
\frac{y_1^2}{\lambda_1}+\frac{y_2^2}{\lambda_2}=\Delta^2
$$</p>
<p>と書ける。これは平面図形の楕円である（P.79の図2.7のイメージ）。ちなみに$D=3$では楕円体（<a href="https://ja.wikipedia.org/wiki/%E6%A5%95%E5%86%86%E4%BD%93">Wikipediaの楕円体を参照</a>）を表す式になり、$D \ge 4$では超楕円体を表す。</p>
<p>$D$次元の超楕円体の体積$V_e$は以下の式で定義される。</p>
<p>$$
V_e = \int\int\cdots\int dy_1dy_2\cdots dy_D
$$</p>
<p>これは3次元の場合の式$V_3 = \int\int\int dxdydz$の拡張です。この辺についての説明は <a href="http://takun-physics.net/3284">楕円の面積と楕円体の体積の求め方</a>のページも参考にしてみてください。</p>
<p>今、マハラノビス距離$\Delta$は定数ということになっているので、超楕円体は$a_i^2=y_i^2/\lambda_i$の変数変換を行うことで、半径$\Delta$の超球へと変換させることができる。つまりヤコビアン$\mathbf{J}$を使って表現すると</p>
<p>$$
\begin{aligned}
V_e &amp;= \int\int\cdots\int dy_1dy_2\cdots dy_D \
&amp;= \int\int\cdots\int |\mathbf{J}|da_1da_2\cdots da_D
\end{aligned}
$$</p>
<p>となる。ここでヤコビアンは$(2.53)-(2.55)$での議論から</p>
<p>$$
\mathbf{J}=\left(\begin{array}{ccc}\frac{\partial y_{1}}{\partial a_{1}} &amp; \cdots &amp; \frac{\partial y_{1}}{\partial a_{D}} \ \vdots &amp; \ddots &amp; \vdots \ \frac{\partial y_{D}}{\partial a_{1}} &amp; \cdots &amp; \frac{\partial y_{0}}{\partial a_{2}}\end{array}\right)=\left(\begin{array}{ccc}
\sqrt{\lambda_{1}} &amp;  &amp;  &amp; 0 \
&amp; \sqrt{\lambda_{2}} \
&amp; &amp; \ddots &amp; \
0 &amp; &amp; &amp; \sqrt{\lambda_{D}}
\end{array}\right)
$$</p>
<p>なので、$|\mathbf{J}| = \prod_{i=1}^{D}\lambda_i^{1/2} =|\mathbf{\Sigma}|^{1/2}$となる。</p>
<p>一方、$\displaystyle \int\int\cdots\int da_1da_2\cdots da_D = \int \prod_{i=1}^{D}da_i$部分は、半径$\Delta$の$D$次元超球の体積を表しているので（超球は演習問題1.18でも登場。各変数$a_i$の定義域は$-\Delta \le a_i \le \Delta$である）、問題文の通りに$V_D$を$D$次元単位球の体積とすると、</p>
<p>$$
\int\int\cdots\int da_1da_2\cdots da_D = V_D\Delta^D
$$</p>
<p>となる。</p>
<p>以上から求める超楕円体の内部の体積$V_e$は$(2.286)$式の通りに</p>
<p>$$
V_e = V_D|\mathbf{\Sigma}|^{1/2}\Delta^D
$$</p>
<p>となることが示された。</p>
<blockquote>
<p>マハラノビス距離の直感的な理解としては、例えば https://mathwords.net/mahalanobis などのサイトの説明を読んでください。多次元からなるデータ群の中で例えば<strong>外れ値</strong>を検出したい場合、データの各次元への分散まで考慮した<strong>データ群からの距離</strong>を考える必要があります。これを実現するのが<strong>マハラノビス距離</strong>です。マハラノビス距離が大きい → その点での確率密度が小さい → <strong>異常度が高い</strong>と考えることができます。</p>
</blockquote>
<h2 id="演習-224"><a class="header" href="#演習-224">演習 2.24</a></h2>
<div class="panel-primary">
<p>$$
\left(\begin{array}{cc}\mathbf{A} &amp; \mathbf{B} \ \mathbf{C} &amp; \mathbf{D}\end{array}\right)^{-1}=\left(\begin{array}{cc}\mathbf{M} &amp; -\mathbf{M B D}^{-1} \ -\mathbf{D}^{-1} \mathbf{C M} &amp; \mathbf{D}^{-1}+\mathbf{D}^{-1} \mathbf{C M B D}^{-1}\end{array}\right) \tag{2.76}
$$
の両辺に次の行列$\displaystyle \left(\begin{array}{ll}\mathbf{A} &amp; \mathbf{B} \ \mathbf{C} &amp; \mathbf{D}\end{array}\right) \ (2.287)$を掛け，また，</p>
<p>$$
\mathbf{M}=\left(\mathbf{A}-\mathbf{B} \mathbf{D}^{-1} \mathbf{C}\right)^{-1} \tag{2.77}
$$</p>
<p>の定義を用いることで，$(2.76)$の恒等式を証明せよ．</p>
</div>
<p>指示通り$(2.76)$の右辺に左から$\displaystyle \left(\begin{array}{cc}\mathbf{A} &amp; \mathbf{B} \ \mathbf{C} &amp; \mathbf{D} \end{array}\right)$をかけたものを$\mathbf{X}$とおく。これが左辺に左から$\displaystyle \left(\begin{array}{cc}\mathbf{A} &amp; \mathbf{B} \ \mathbf{C} &amp; \mathbf{D} \end{array}\right)$をかけたもの、すなわち単位行列$\mathbf{I}$になっていることを示せば良い。</p>
<p>$$
\begin{aligned} \mathbf{X}
&amp;=\left(\begin{array}{cc}\mathbf{A} &amp; \mathbf{B} \ \mathbf{C} &amp; \mathbf{D} \end{array}\right)
\left(\begin{array}{cc}\mathbf{M} &amp; -\mathbf{MBD}^{-1} \ -\mathbf{D}^{-1} \mathbf{CM} &amp; \mathbf{D}^{-1}+\mathbf{D}^{-1} \mathbf{CMBD}^{-1}\end{array}\right) \
&amp;=\left(\begin{array}{cc}\mathbf{AM}-\mathbf{BD}^{-1}\mathbf{CM} &amp; -\mathbf{AMBD}^{-1}+\mathbf{B}\left(\mathbf{D}^{-1}+\mathbf{D}^{-1} \mathbf{CMBD}^{-1}\right) \ \mathbf{CM}-\mathbf{DD}^{-1} \mathbf{CM} &amp; -\mathbf{CMBD}^{-1}+\mathbf{D}\left(\mathbf{D}^{-1}+\mathbf{D}^{-1} \mathbf{CMBD}^{-1}\right)\end{array}\right)
\end{aligned}
$$</p>
<p>このそれぞれの部分行列成分について計算していくと</p>
<p>$$
\begin{aligned}
\mathbf{X}_{11}
&amp;=\mathbf{AM}-\mathbf{BD}^{-1} \mathbf{CM} \
&amp;=\left(\mathbf{A}-\mathbf{BD}^{-1} \mathbf{C}\right) \mathbf{M} \ &amp;=\left(\mathbf{A}-\mathbf{B D}^{-1} \mathbf{C}\right) \left(\mathbf{A}-\mathbf{BD}^{-1} \mathbf{C}\right)^{-1} \ &amp;=\mathbf{I}
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
\mathbf{X}_{12}
&amp;=-\mathbf{A M B D}^{-1}+\mathbf{B}\left(\mathbf{D}^{-1}+\mathbf{D}^{-1} \mathbf{C M B D}^{-1}\right) \
&amp;=-\mathbf{A M B D}^{-1}+\mathbf{B D}^{-1}+\mathbf{B D}^{-1}\mathbf{C M B D}^{-1} \
&amp;=-\left(\mathbf{A}-\mathbf{B D}^{-1} \mathbf{C}\right) \mathbf{M B D}^{-1}+\mathbf{B D}^{-1} \
&amp;=-\left(\mathbf{A}-\mathbf{B D}^{-1} \mathbf{C}\right)\left(\mathbf{A}-\mathbf{B D}^{-1} \mathbf{C}\right)^{-1} \mathbf{B D}^{-1}+\mathbf{B D}^{-1} \
&amp;=-\mathbf{B D}^{-1}+\mathbf{B D}^{-1} \
&amp;=\mathbf{O}
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
\mathbf{X}_{21}
&amp;=\mathbf{CM}-\mathbf{DD}^{-1}\mathbf{CM} \
&amp;=\mathbf{O}
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
X_{22}
&amp;=-\mathbf{C M B D}^{-1}+\mathbf{D}\left(\mathbf{D}^{-1}+\mathbf{D}^{-1} \mathbf{C M B D}^{-1}\right) \
&amp;=-\mathbf{C M B D}^{-1}+\mathbf{I}+\mathbf{C M B D}^{-1} \ &amp;=\mathbf{I}
\end{aligned}
$$</p>
<p>よって全体として$\mathbf{X} = \mathbf{I}$となっていることが示せたので、$(2.76)$の恒等式は示された。</p>
<h2 id="演習-225"><a class="header" href="#演習-225">演習 2.25</a></h2>
<div class="panel-primary">
<p>2.3.1節や2.3.2節では，多変量ガウス分布の条件付き分布や周辺分布について考察した．より一般的に，$\mathbf{x}$の要素を$\mathbf{x}_a, \mathbf{x}_b$,および$\mathbf{x}_c$の3つに分けることを考える．この分割により，対応する平均ベクトル$\boldsymbol{\mu}$と共分散行列$\mathbf{\Sigma}$は</p>
<p>$$
\boldsymbol{\mu}=\left(\begin{array}{c}\boldsymbol{\mu}<em>{a} \ \boldsymbol{\mu}</em>{b} \ \boldsymbol{\mu}<em>{c}\end{array}\right), \quad \mathbf{\Sigma}=\left(\begin{array}{ccc}\mathbf{\Sigma}</em>{a a} &amp; \mathbf{\Sigma}<em>{a b} &amp; \mathbf{\Sigma}</em>{a c} \ \mathbf{\Sigma}<em>{b a} &amp; \mathbf{\Sigma}</em>{b b} &amp; \mathbf{\Sigma}<em>{b c} \ \mathbf{\Sigma}</em>{c a} &amp; \mathbf{\Sigma}<em>{c b} &amp; \mathbf{\Sigma}</em>{c c}\end{array}\right)
$$</p>
<p>のように分割される. 2.3節の結果を用いて$\mathbf{x}_c$を周辺化で消去した条件付き分布$p(\mathbf{x}_a|\mathbf{x}_b)$の式を求めよ．</p>
</div>
<p>$\mathbf{x}_c$を消去したときの同時分布$p(\mathbf{x}_a,\mathbf{x}_b)$は、平均ベクトルと共分散行列が</p>
<p>$$
\boldsymbol{\mu}=\left(\begin{array}{c}\boldsymbol{\mu}<em>{a} \ \boldsymbol{\mu}</em>{b} \end{array}\right), \quad \mathbf{\Sigma}=\left(\begin{array}{ccc}\mathbf{\Sigma}<em>{a a} &amp; \mathbf{\Sigma}</em>{a b} \ \mathbf{\Sigma}<em>{b a} &amp; \mathbf{\Sigma}</em>{b b} \end{array}\right)
$$</p>
<p>のガウス分布となる。よって条件付き分布$p(\mathbf{x}_a|\mathbf{x}_b)$もガウス分布であり、その平均は$(2.81)(2.82)$で与えられ、式は$(2.96)$となる。</p>
<h2 id="演習-226"><a class="header" href="#演習-226">演習 2.26</a></h2>
<div class="panel-primary">
<p>非常に有用な線形代数の結果であるWoodbury行列反転公式(Woodbury matrix inversion formula)は</p>
<p>$$
(\mathbf{A}+\mathbf{B C D})^{-1}=\mathbf{A}^{-1}-\mathbf{A}^{-1} \mathbf{B}\left(\mathbf{C}^{-1}+\mathbf{D} \mathbf{A}^{-1} \mathbf{B}\right)^{-1} \mathbf{D} \mathbf{A}^{-1} \tag{2.289}
$$</p>
<p>である．この両辺に$(\mathbf{A}+\mathbf{B C D})$を掛けて，この公式を証明せよ.</p>
</div>
<p>右辺に$(\mathbf{A}+\mathbf{B C D})$を掛けると</p>
<p>$$
\begin{aligned}
&amp; (\mathbf{A}+\mathbf{B C D}) (\mathbf{A}^{-1}-\mathbf{A}^{-1} \mathbf{B}\left(\mathbf{C}^{-1}+\mathbf{D} \mathbf{A}^{-1} \mathbf{B}\right)^{-1} \mathbf{D} \mathbf{A}^{-1}) \
&amp;= \mathbf{I} - \mathbf{B}\left(\mathbf{C}^{-1}+\mathbf{D} \mathbf{A}^{-1} \mathbf{B}\right)^{-1} \mathbf{D} \mathbf{A}^{-1} + \mathbf{B C D}\mathbf{A}^{-1} - \mathbf{B C D} \mathbf{A}^{-1} \mathbf{B}\left(\mathbf{C}^{-1}+\mathbf{D} \mathbf{A}^{-1} \mathbf{B}\right)^{-1} \mathbf{D} \mathbf{A}^{-1}\
&amp;= \mathbf{I} + \mathbf{B C D}\mathbf{A}^{-1} - \mathbf{B}\left(\mathbf{I}+\mathbf{C D} \mathbf{A}^{-1} \mathbf{B}\right) \left(\mathbf{C}^{-1}+\mathbf{D} \mathbf{A}^{-1} \mathbf{B}\right)^{-1} \mathbf{D} \mathbf{A}^{-1} \
&amp;= \mathbf{I} + \mathbf{B C D}\mathbf{A}^{-1} - \mathbf{B C}\left(\mathbf{C}^{-1}+\mathbf{D} \mathbf{A}^{-1} \mathbf{B}\right) \left(\mathbf{C}^{-1}+\mathbf{D} \mathbf{A}^{-1} \mathbf{B}\right)^{-1} \mathbf{D} \mathbf{A}^{-1} \
&amp;= \mathbf{I} + \mathbf{B C D}\mathbf{A}^{-1} - \mathbf{B C D}\mathbf{A}^{-1} \
&amp;= \mathbf{I}
\end{aligned}
$$</p>
<p>となり示された。</p>
<h2 id="演習-227"><a class="header" href="#演習-227">演習 2.27</a></h2>
<div class="panel-primary">
<p>$\mathbf{x}$と$\mathbf{z}$を２つの独立な確率ベクトル，すなわち，$p(\mathbf{x, z}) = p(\mathbf{x})p(\mathbf{z})$であるとする．これらの和$\mathbf{y}=\mathbf{x}+\mathbf{z}$の平均が，それぞれの変数について個別に求めた平均の和となることを示せ．同様に，$\mathbf{y}$の共分散行列が，$\mathbf{x}$と$\mathbf{z}$それぞれの共分散行列の和であることを示せ．これが，演習問題1.10の結果と一致することを確認せよ．</p>
</div>
<p>$\mathbb{E}[\mathbf{y}] = \mathbb{E}[\mathbf{x}+\mathbf{z}]=\int\int(\mathbf{x}+\mathbf{z})p(\mathbf{x},\mathbf{z})d\mathbf{x}d\mathbf{z}$</p>
<p>$\mathbf{x}$と$\mathbf{z}$は独立であるから
$$
\begin{aligned}
\int\int(\mathbf{x}+\mathbf{z})p(\mathbf{x},\mathbf{z})d\mathbf{x}d\mathbf{z} &amp;= \int\int(\mathbf{x}+\mathbf{z})p(\mathbf{x})p(\mathbf{z})d\mathbf{x}d\mathbf{z}\
&amp;= \int\int \mathbf{x}p(\mathbf{x})p(\mathbf{z})d\mathbf{x}d\mathbf{z} + \int\int \mathbf{z}p(\mathbf{z})p(\mathbf{x})d\mathbf{x}d\mathbf{z} \
&amp;=\int \mathbf{x}p(\mathbf{x})d\mathbf{x} + \int \mathbf{z}p(\mathbf{z})d\mathbf{z} \
&amp;=\mathbb{E}[\mathbf{x}]+\mathbb{E}[\mathbf{z}]
\end{aligned}
$$
以上より$\mathbb{E}[\mathbf{y}]=\mathbb{E}[\mathbf{x}]+\mathbb{E}[\mathbf{z}]$</p>
<p>共分散行列の定義より
$$
\begin{aligned}
cov[y]
&amp;= \mathbb{E}[(y-\mathbb{E}[y])(y-\mathbb{E}[y])^T]\
&amp;= \mathbb{E}[(x-\mathbb{E}[\mathbf{x}]+z-\mathbb{E}[z])(x-\mathbb{E}[\mathbf{x}]+z-\mathbb{E}[z])^T]\
\end{aligned}
$$
$\mathbf{x}-\mathbb{E}[\mathbf{x}]=A$、$\mathbf{z}-\mathbb{E}[\mathbf{z}]=B$と置くと
$$
\begin{aligned}
cov[\mathbf{y}]
&amp;= \mathbb{E}[(A+B)(A+B)^T]\
&amp;= \mathbb{E}[AA^T]+\mathbb{E}[AB^T]+\mathbb{E}[BA^T]+\mathbb{E}[BB^T]
\end{aligned}
$$
独立な変数同士の共分散は0になることから
$$
\operatorname{cov}[\mathbf{y}] = \mathbb{E}[AA^T]+\mathbb{E}[BB^T] = \operatorname{cov}[\mathbf{x}]+\operatorname{cov}[\mathbf{z}]
$$</p>
<h2 id="演習-228"><a class="header" href="#演習-228">演習 2.28</a></h2>
<div class="panel-primary">
<p>平均と共分散がそれぞれ
$$
\mathbb{E}[\mathbf{z}]=
\left(\begin{array}{c}\boldsymbol{\mu} \ \mathbf{A} \boldsymbol{\mu}+\mathbf{b}\end{array}\right) \tag{2.108}
$$
と
$$
\operatorname{cov}[\mathbf{z}]=\mathbf{R}^{-1}=\left(\begin{array}{cc}\mathbf{\Lambda}^{-1} &amp; \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathbf{T}} \ \mathbf{A} \mathbf{\Lambda}^{-1} &amp; \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathbf{T}}\end{array}\right) \tag{2.105}
$$
であるような，次の変数上の同時分布を考える．</p>
<p>$$
\mathbf{z}=\left(\begin{array}{l}\mathbf{x} \ \mathbf{y}\end{array}\right) \tag{2.190}
$$</p>
<p>$(2.92)\ \mathbb{E}[\mathbf{x}_a] = \boldsymbol{\mu}_a$と$(2.93)\ \operatorname{cov}[\mathbf{x}<em>a] = \mathbf{\Sigma}</em>{aa}$の結果を用いて，周辺分布$p(\mathbf{x})$が$\mathcal{N}(\mathbf{x}|\boldsymbol{\mu}, \mathbf{\Lambda}^{-1})\ (2.99)$となることを示せ．</p>
<p>同様に，
$$
\boldsymbol{\mu}<em>{a \mid b}=\boldsymbol{\mu}</em>{a}+\mathbf{\Sigma}<em>{ab} \mathbf{\Sigma}</em>{bb}^{-1}\left(\mathbf{x}<em>{b}-\boldsymbol{\mu}</em>{b}\right) \tag{2.81}
$$
と
$$
\mathbf{\Sigma}<em>{a \mid b}=\mathbf{\Sigma}</em>{a a}-\mathbf{\Sigma}<em>{a b} \mathbf{\Sigma}</em>{b b}^{-1} \mathbf{\Sigma}_{b a} \tag{2.82}
$$
の結果を用いて，条件付き分布$p(\mathbf{y}|\mathbf{x})$が</p>
<p>$$
p(\mathbf{y|x}) = \mathcal{N}(\mathbf{y}|\mathbf{Ax+b}, \mathbf{L}^{-1}) \tag{2.100}
$$</p>
<p>となることを示せ．</p>
</div>
<p>$(2.92)$と$(2.93)$を$(2.98)$に代入すると，$p(\mathbf{x})=\mathcal{N}(\mathbf{x}|\boldsymbol{\mu}, \mathbf{\Lambda}^{-1})$であることが簡単に示された．</p>
<p>また</p>
<p>$$
\begin{aligned}
\mathbb{E}[\mathbf{z}] &amp;= \left(
\begin{array}{c}
\mu\
\mathbf{A}\mu+\mathbf{b}\
\end{array}
\right)\
cov[\mathbf{z}] &amp;= \left(
\begin{array}{cc}
\Lambda^{-1} &amp; \Lambda^{-1}\mathbf{A}^\top \
\mathbf{A}\Lambda^{-1} &amp; \mathbf{L}^{-1} + \mathbf{A}\Lambda^{-1}\mathbf{A}^\top\
\end{array}
\right)
\end{aligned}
$$</p>
<p>を$(2.81)$と$(2.82)$に代入すると</p>
<p>$$
\begin{aligned}
\mu_{\mathbf{y}|\mathbf{x}} &amp;= \mu_{\mathbf{y}} + \Lambda_{\mathbf{yx}}\Lambda_{\mathbf{xx}}^{-1}(\mathbf{x} - \mu_{\mathbf{x}})\
&amp;= \mathbf{A}\mu + \mathbf{b} + \mathbf{A}\Lambda^{-1}\Lambda(\mathbf{x}-\mu)\
&amp;= \mathbf{A}\mu + \mathbf{b} + \mathbf{Ax} - \mathbf{A\mu}\
&amp;= \mathbf{Ax+b}\
\Sigma_{\mathbf{y}|\mathbf{x}} &amp;= \Sigma_{\mathbf{yy}} - \Sigma_{\mathbf{yx}}\Sigma_{xx}^{-1}\Sigma_{\mathbf{xy}}\
&amp;= \mathbf{L}^{-1} + \mathbf{A}\Lambda^{-1}\mathbf{A}^\top - \mathbf{A}\Lambda^{-1}\Lambda\Lambda^{-1}\mathbf{A}^\top \
&amp;= \mathbf{L}^{-1}\
\end{aligned}
$$</p>
<p>となって，$p(\mathbf{y|x}) = \mathcal{N}(\mathbf{y}|\mathbf{Ax+b}, \mathbf{L}^{-1})$も示された．</p>
<h2 id="演習-229"><a class="header" href="#演習-229">演習 2.29</a></h2>
<div class="panel-primary">
<p>分割行列の逆行列の公式</p>
<p>$$
\left(\begin{array}{cc}\mathbf{A} &amp; \mathbf{B} \ \mathbf{C} &amp; \mathbf{D}\end{array}\right)^{-1}=\left(\begin{array}{cc}\mathbf{M} &amp; -\mathbf{M B D}^{-1} \ -\mathbf{D}^{-1} \mathbf{C M} &amp; \mathbf{D}^{-1}+\mathbf{D}^{-1} \mathbf{C M B D}^{-1}\end{array}\right) \tag{2.76}
$$</p>
<p>を用いて，精度行列</p>
<p>$$
\mathbf{R}=\left(\begin{array}{cc}\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{L} \mathbf{A} &amp; -\mathbf{A}^{\mathrm{T}} \mathbf{L} \ -\mathbf{LA} &amp; \mathbf{L}\end{array}\right) \tag{2.104}
$$</p>
<p>の逆行列が，共分散行列</p>
<p>$$
\operatorname{cov}
[\mathbf{z}]=\mathbf{R}^{-1}=\left(\begin{array}{cc}\mathbf{\Lambda}^{-1} &amp; \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}} \ \mathbf{A} \mathbf{\Lambda}^{-1} &amp; \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}\end{array}\right) \tag{2.105}
$$</p>
<p>となることを示せ．</p>
</div>
<p>$$
\mathbf{M}=\left(\mathbf{A}-\mathbf{B} \mathbf{D}^{-1} \mathbf{C}\right)^{-1}
$$</p>
<p>$(2.76)$より、$\mathbf{R}^{-1}$について</p>
<p>$$\begin{aligned}( \text{左上} ) &amp;= ( \mathbf{\Lambda} + \mathbf{A}^T \mathbf{L} \mathbf{A} - (- \mathbf{A}^T \mathbf{L}) \mathbf{L}^{-1} \mathbf{L} \mathbf{A})^{-1} \&amp;= ( \mathbf{\Lambda} + \mathbf{A}^T \mathbf{L} \mathbf{A} - \mathbf{A}^T \mathbf{L} \mathbf{A} )^{-1} \&amp;= \mathbf{\Lambda}^{-1} \end{aligned}$$</p>
<p>$$\begin{aligned}( \text{右上} ) &amp;= - \mathbf{\Lambda}^{-1} (- \mathbf{A}^T \mathbf{L}) \mathbf{L}^{-1} \&amp;= \mathbf{\Lambda}^{-1} \mathbf{A}^T \end{aligned}$$</p>
<p>$$\begin{aligned}( \text{左下} ) &amp;= - \mathbf{L}^{-1} (- \mathbf{L} \mathbf{A}) \mathbf{\Lambda}^{-1} \&amp;= \mathbf{A} \mathbf{\Lambda}^{-1} \end{aligned}$$</p>
<p>$$\begin{aligned}( \text{右下} ) &amp;= \mathbf{L}^{-1} + \mathbf{L}^{-1} (- \mathbf{L} \mathbf{A}) \mathbf{\Lambda}^{-1} (- \mathbf{A}^T \mathbf{L}) \mathbf{L}^{-1} \&amp;= \mathbf{L}^{-1} + \mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^T \end{aligned}$$</p>
<p>これは共分散行列(2.105)と一致する。</p>
<h2 id="演習-230"><a class="header" href="#演習-230">演習 2.30</a></h2>
<div class="panel-primary">
<p>$$
\mathbb{E}[\mathbf{z}]=\mathbf{R}^{-1}\left(\begin{array}{c}\mathbf{\Lambda} \boldsymbol{\mu}-\mathbf{A}^{\mathrm{T}} \mathbf{L} \mathbf{b} \ \mathbf{Lb}\end{array}\right) \tag{2.107}
$$</p>
<p>に，</p>
<p>$$
\operatorname{cov}
[\mathbf{z}]=\mathbf{R}^{-1}=\left(\begin{array}{cc}\mathbf{\Lambda}^{-1} &amp; \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}} \ \mathbf{A} \mathbf{\Lambda}^{-1} &amp; \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}\end{array}\right) \tag{2.105}
$$
の結果を用いて，
$$
\mathbb{E}[\mathbf{z}]=\left(\begin{array}{c}\boldsymbol{\mu} \
\mathbf{A} \boldsymbol{\mu}+\mathbf{b}\end{array}\right) \tag{2.108}
$$
を確かめよ．</p>
</div>
<p>単純に計算するだけ</p>
<p>$$
\begin{aligned}
\mathbb{E}[\mathbf{z}] &amp;= \mathbf{R}^{-1}\left(\begin{array}{c}\mathbf{\Lambda} \boldsymbol{\mu}-\mathbf{A}^{\mathrm{T}} \mathbf{L} \mathbf{b} \ \mathbf{Lb}\end{array}\right) \
&amp;= \left(\begin{array}{cc}\mathbf{\Lambda}^{-1} &amp; \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}} \ \mathbf{A} \mathbf{\Lambda}^{-1} &amp; \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}\end{array}\right)\left(\begin{array}{c}\mathbf{\Lambda} \boldsymbol{\mu}-\mathbf{A}^{\mathrm{T}} \mathbf{L b} \ \mathbf{L b}\end{array}\right) \
&amp;=\left(\begin{array}{c}\mathbf{\Lambda}^{-1}\left(\mathbf{\Lambda} \boldsymbol{\mu}-\mathbf{A}^{\mathrm{T}} \mathbf{L b}\right)+\mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}} \mathbf{L b} \
\mathbf{A} \mathbf{\Lambda}^{-1}\left(\mathbf{\Lambda} \boldsymbol{\mu}-\mathbf{A}^{\mathrm{T}} \mathbf{L} \mathbf{b}\right)+\left(\mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}\right) \mathbf{L b}\end{array}\right) \
&amp;=\left(\begin{array}{c}\boldsymbol{\mu}-\mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}} \mathbf{L} \mathbf{b}+\mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}} \mathbf{L} \mathbf{b}
\ \mathbf{A} \boldsymbol{\mu}-\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}} \mathbf{L} \mathbf{b}+\mathbf{b}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}} \mathbf{L} \mathbf{b}\end{array}\right) \
&amp;=\left(\begin{array}{c}\boldsymbol{\mu} \
\mathbf{A} \boldsymbol{\mu}+\mathbf{b}\end{array}\right)
\end{aligned}
$$</p>
<h2 id="演習-231"><a class="header" href="#演習-231">演習 2.31</a></h2>
<div class="panel-primary">
<p>2つの多次元確率ベクトル$\mathbf{x}$と$\mathbf{z}$を考える．これらは，それぞれガウス分布$p(\mathbf{x}) = \mathcal{N}(\mathbf{x}| \boldsymbol{\mu}<em>{\mathbf{x}}, \mathbf{\Sigma_x})$と$p(\mathbf{z}) = \mathcal{N}(\mathbf{z}| \boldsymbol{\mu}</em>{\mathbf{z}}, \mathbf{\Sigma_z})$に従い，これらの和は$\mathbf{y} = \mathbf{x} + \mathbf{z}$であるとする．周辺分布$p(\mathbf{x})$と条件付き分布$p(\mathbf{y|x})$の積からなる線形ガウスモデルを用いて，</p>
<p>$$
\mathbb{E}[\mathbf{y}] =\mathbf{A} \boldsymbol{\mu}+\mathbf{b} \tag{2.109}
$$</p>
<p>$$
\operatorname{cov}[\mathbf{y}] = \mathbf{L}^{-1}+\mathbf{A}\mathbf{\Lambda}^{-1}\mathbf{A}^{\mathrm{T}} \tag{2.110}
$$</p>
<p>から周辺分布$p(\mathbf{y})$についての式を求めよ．</p>
</div>
<p>※ まず条件付き確率分布$p(\mathbf{y|x})$の解釈を考える。これは$\mathbf{x}$が与えられた中での$\mathbf{y}$の確率分布なので、$\mathbf{x}$は定数として考えることができる。さらに周辺分布$p(\mathbf{y})$は$p(\mathbf{y}) = p(\mathbf{y|x})p(\mathbf{x})$から計算することができる。ここの議論はPRMLテキストのP.88 2.3.3 <strong>ガウス変数に対するベイズの定理</strong>でしっかり行っている。また、周辺分布$p(\mathbf{x})$と条件付き分布$p(\mathbf{y|x})$がともにガウス分布であれば周辺分布$p(\mathbf{y})$もガウス分布になるので、平均と分散パラメータを求めてガウス分布の形の式にすれば良い。</p>
<p>条件付き確率分布$p(\mathbf{y|x})$は$\mathbf{x}$が与えられた（つまり定数とした）中での$\mathbf{y} = \mathbf{x}+\mathbf{z}$の確率分布を表しているので、$\mathbf{y}$の平均は$\boldsymbol{\mu}<em>{\mathbf{z}}$に$\mathbf{x}$を足したもの、$\mathbf{y}$の分散は$\mathbf{\Sigma}</em>{\mathbf{z}}$となるので、</p>
<p>$$
p(\mathbf{y|x}) = \mathcal{N}(\mathbf{y}| \boldsymbol{\mu}_{\mathbf{z}}+\mathbf{x}, \mathbf{\Sigma_z})
$$</p>
<p>と表すことができる。</p>
<p>以降、pp.88〜90の線形ガウスモデルの議論とまとめ$(2.113)$〜$(2.117)$と、この問題設定の</p>
<p>$$
\begin{aligned}
p(\mathbf{x}) &amp;= \mathcal{N}(\mathbf{x}|\boldsymbol{\mu}<em>{\mathbf{x}},\mathbf{\Sigma}</em>{\mathbf{x}}) \
p(\mathbf{y|x}) &amp;= \mathcal{N}(\mathbf{y}|\mathbf{x}+\boldsymbol{\mu}<em>{\mathbf{z}},\mathbf{\Sigma}</em>{\mathbf{z}})
\end{aligned}
$$
を比較すると、
$$
\boldsymbol{\mu} \to \boldsymbol{\mu}<em>{\mathbf{x}}, \mathbf{\Lambda}^{-1} \to \mathbf{\Sigma}</em>{\mathbf{x}}, \mathbf{A} \to \mathbf{I}, \mathbf{b} \to \boldsymbol{\mu}<em>{\mathbf{z}}, \mathbf{L}^{-1} \to \mathbf{\Sigma}</em>{\mathbf{z}}
$$
と置換することで、$(2.115)$式から
$$
p(\mathbf{y}) = \mathcal{N}(\mathbf{y}|\boldsymbol{\mu}<em>{\mathbf{x}}+\boldsymbol{\mu}</em>{\mathbf{z}}, \mathbf{\Sigma}<em>{\mathbf{x}}+\mathbf{\Sigma}</em>{\mathbf{z}})
$$</p>
<p>となる。これは演習2.27の結果と同じである。</p>
<h2 id="演習-232"><a class="header" href="#演習-232">演習 2.32</a></h2>
<div class="panel-primary">
<p>これと次の演習問題で，線形ガウスモデル中の二次形式の操作を練習し，また，本文中で導いた結果も検証する．周辺分布</p>
<p>$$
p(\mathbf{x})=\mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Lambda}^{-1}\right) \tag{2.99}
$$</p>
<p>と条件付き分布</p>
<p>$$
p(\mathbf{y} \mid \mathbf{x})=\mathcal{N}\left(\mathbf{y} \mid \mathbf{Ax}+\mathbf{b}, \mathbf{L}^{-1}\right) \tag{2.100}
$$</p>
<p>で定義される同時確率$p(\mathbf{x},\mathbf{y})$を考える．同時確率の指数部分の二次形式に，2.3節で述べた平方完成の技法を適用して，変数$\mathbf{x}$を積分消去した周辺分布$p(\mathbf{y})$の平均と共分散の式を求めよ．これには，Woodbury行列反転公式</p>
<p>$$
(\mathbf{A}+\mathbf{BCD})^{-1}=\mathbf{A}^{-1}-\mathbf{A}^{-1} \mathbf{B}\left(\mathbf{C}^{-1}+\mathbf{DA}^{-1} \mathbf{B}\right)^{-1} \mathbf{DA}^{-1} \tag{2.289}
$$</p>
<p>を用いる．これらが，2章の結果を用いて得た結果</p>
<p>$$
\mathbb{E}[\mathbf{y}] =\mathbf{A} \boldsymbol{\mu}+\mathbf{b} \tag{2.109}
$$</p>
<p>$$
\operatorname{cov}[\mathbf{y}] = \mathbf{L}^{-1}+\mathbf{A}\mathbf{\Lambda}^{-1}\mathbf{A}^{\mathrm{T}} \tag{2.110}
$$
と一致することを確かめよ．</p>
</div>
<p>※ 2.3.3 <strong>ガウス変数に対するベイズの定理</strong>における導出の流れに則りつつも、問題文の指示によれば途中から二次形式を使ったやり方で導出するよう求めているため、2.3.2 <strong>周辺ガウス分布</strong>で行ったような二次形式を使ったやり方で求めていきます。結果は当然変わらないですが、こちらの方が2.3.3の行列形式を使ったやり方よりも計算量がものすごく増えます。</p>
<p>演習2.31の内容と同様に、$p(\mathbf{x,y}) = p(\mathbf{y|x})p(\mathbf{x})$から</p>
<p>$$
\begin{aligned}
p(\mathbf{y}) &amp;= \int p(\mathbf{x}, \mathbf{y}) d\mathbf{x} \
&amp;= \int \mathcal{N}\left(\mathbf{y} \mid \mathbf{Ax}+\mathbf{b}, \mathbf{L}^{-1}\right) \mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Lambda}^{-1}\right) d\mathbf{x}
\end{aligned}
$$</p>
<p>となる。正規化係数を無視して指数部分だけを考えると</p>
<p>$$
\exp \left{-\frac{1}{2}\left(\mathbf{y}-\mathbf{Ax}-\mathbf{b}\right)^{\mathrm{T}}\mathbf{L}(\mathbf{y}-\mathbf{Ax}-\mathbf{b})-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}}\mathbf{\Lambda}(\mathbf{x}-\boldsymbol{\mu})\right}
$$</p>
<p>となるので、二次形式を展開すると</p>
<p>$$
\begin{aligned}
&amp;-\frac{1}{2}\left{\mathbf{y}^{\mathrm{T}}\mathbf{L}\mathbf{y}-2\mathbf{y}^{\mathrm{T}}\mathbf{L}(\mathbf{Ax}+\mathbf{b})+(\mathbf{Ax}+\mathbf{b})^{\mathrm{T}}\mathbf{L}(\mathbf{Ax}+\mathbf{b})+\mathbf{x}^{\mathrm{T}}\mathbf{\Lambda}\mathbf{x}-2\boldsymbol{\mu}^{\mathrm{T}} \mathbf{\Lambda} \mathbf{x}+\boldsymbol{\mu}^{\mathrm{T}} \mathbf{\Lambda} \boldsymbol{\mu} \right} \
=&amp;-\frac{1}{2}\left{\mathbf{y}^{\mathrm{T}} \mathbf{L} \mathbf{y}-2 \mathbf{y}^{\mathrm{T}} \mathbf{L} \mathbf{Ax}-2 \mathbf{y}^{\mathrm{T}} \mathbf{L} \mathbf{b}+\mathbf{x}^{\mathrm{T}} \mathbf{A}^{\mathrm{T}} \mathbf{LAx}+2 \mathbf{b}^{\mathrm{T}} \mathbf{LAx}+\mathbf{b}^{\mathrm{T}} \mathbf{Lb}\right. \
&amp;+\left.\mathbf{x}^{\mathrm{T}} \mathbf{\Lambda} \mathbf{x}-2 \boldsymbol{\mu}^{\mathrm{T}} \mathbf{\Lambda} \mathbf{x}+\boldsymbol{\mu}^{\mathrm{T}} \mathbf{A} \boldsymbol{\mu} \right}
\end{aligned}
$$</p>
<p>ここで2.3.2の技法に則り、$\mathbf{x}$を積分消去することを目標として$\mathbf{x}$の関わる項をまとめてから積分を容易にするために平方完成の技法を使う。</p>
<p>$\mathbf{x}$の関わる項を上式から抽出すると</p>
<p>$$
\begin{aligned}
&amp;-\frac{1}{2}\left{\mathbf{x}^{\mathrm{T}}\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right) \mathbf{x}-2\left(\left(\mathbf{y}^{\mathrm{T}}-\mathbf{b}^{\mathrm{T}}\right) \mathbf{LA}+\boldsymbol{\mu}^{\mathrm{T}} \mathbf{\Lambda}\right) \mathbf{x}\right} \
=&amp;-\frac{1}{2}(\mathbf{x}-\mathbf{m})^{\mathrm{T}}\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)(\mathbf{x}-\mathbf{m})+\frac{1}{2} \mathbf{m}^{\mathrm{T}}\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right) \mathbf{m}
\end{aligned}
$$</p>
<p>ここで$\mathbf{m}=\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)^{-1}(\mathbf{\Lambda}^{\mathrm{T}} \boldsymbol{\mu}+\mathbf{A}^{\mathrm{T}}\mathbf{L}(\mathbf{y}-\mathbf{b}))$とした。</p>
<p>指数内に戻して考えると</p>
<p>$$
\int\exp\left{
-\frac{1}{2}(\mathbf{x}-\mathbf{m})^{\mathrm{T}}\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)(\mathbf{x}-\mathbf{m})
\right}d\mathbf{x}
$$</p>
<p>この積分は正規化されていないガウス分布の標準的な二次形式になっているので、正規化係数の逆数になる。その逆数は$|(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}}\mathbf{L A})^{-1}|$、つまり共分散行列のみに依存することがガウス分布の性質から分かるので$\mathbf{x}$には依存しなくなっている。つまりこれは$\mathbf{x}$が積分によって消去されていることを意味する。</p>
<p>まだ$\frac{1}{2} \mathbf{m}^{\mathrm{T}}\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right) \mathbf{m}$部分が残っているので、これを$\mathbf{y}$に依存する項とあわせて再び計算すると（$\mathbf{y}$に依存しない項は$\text{const.}$とした）</p>
<p>$$
\begin{aligned}
&amp;=-\frac{1}{2}\left{\mathbf{y}^{\mathrm{T}} \mathbf{Ly}-2 \mathbf{y}^{\mathrm{T}} \mathbf{Lb}+\mathbf{m}^{\mathrm{T}}\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right) \mathbf{m}\right}+\text { const. } \
&amp;=-\frac{1}{2}\left{\mathbf{y}^{\mathrm{T}} \mathbf{Ly}-2 \mathbf{y}^{\mathrm{T}} \mathbf{Lb}+\mathbf{y}^{\mathrm{T}} \mathbf{LA}\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)^{-1} \mathbf{A}^{\mathrm{T}} \mathbf{Ly}-2 \mathbf{y}^{\mathrm{T}}\left[\mathbf{LA}\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)^{-1}\left(\mathbf{\Lambda} \mu-\mathbf{A}^{\mathrm{T}} \mathbf{Lb}\right)\right] + \text{const.} \right} \
&amp;=-\frac{1}{2} \mathbf{y}^{\mathrm{T}}\left[\mathbf{L}-\mathbf{LA}\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)^{-1} \mathbf{A}^{\mathrm{T}} \mathbf{L}\right] \mathbf{y}+\mathbf{y}^{\mathrm{T}}\left[\left(\mathbf{L}-\mathbf{LA}\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)^{-1} \mathbf{A}^{\mathrm{T}} \mathbf{L}\right)\mathbf{b}+ \mathbf{LA}\left(\mathbf{\Lambda}+\mathbf{A}^{T}\mathbf{LA}\right)^{-1}\mathbf{\Lambda} \boldsymbol{\mu}\right]
\end{aligned}
$$</p>
<p>これが周辺分布$p(\mathbf{y})$の指数部分となっているので、二次形式部分の$\mathbf{L}-\mathbf{LA}\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)^{-1} \mathbf{A}^{\mathrm{T}} \mathbf{L}$が$p(\mathbf{y})$の精度行列に相当することがわかる。</p>
<p>Woodburyの反転公式を使うと、共分散行列$\operatorname{cov}[\mathbf{y}]$は</p>
<p>$$
\operatorname{cov}[\mathbf{y}] = (\mathbf{L}-\mathbf{LA}\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)^{-1} \mathbf{A}^{\mathrm{T}} \mathbf{L})^{-1} = \mathbf{L}^{-1}+\mathbf{A}\mathbf{\Lambda}^{-1}\mathbf{A}^{\mathrm{T}}
$$</p>
<p>となる。</p>
<p>最後に平均$\mathbb{E}[\mathbf{y}]$は、$(2.71), (2.75), (2.89)$で議論したように、$\mathbf{y}^{\mathrm{T}}$の係数$\left[\left(\mathbf{L}-\mathbf{LA}\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)^{-1} \mathbf{A}^{\mathrm{T}} \mathbf{L}\right)\mathbf{b}+\mathbf{LA}\left(\mathbf{\Lambda}+\mathbf{A}^{T} \mathbf{LA}\right)^{-1}\mathbf{\Lambda} \boldsymbol{\mu}\right]$が${\operatorname{cov}[\mathbf{y}]}^{-1}\mathbb{E}[\mathbf{y}]$に一致することから求められる。</p>
<p>$$
\begin{aligned}
\mathbb{E}[\mathbf{y}] &amp;= \operatorname{cov}[\mathbf{y}]\left[\left(\mathbf{L}-\mathbf{LA}\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)^{-1} \mathbf{A}^{\mathrm{T}} \mathbf{L}\right)\mathbf{b}+\mathbf{LA}\left(\mathbf{\Lambda}+\mathbf{A}^{T} \mathbf{LA}\right)^{-1}\mathbf{\Lambda} \boldsymbol{\mu}\right] \
&amp;=\left(\mathbf{L}^{-1}+\mathbf{A}\mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}\right)\left(\left(\mathbf{L}-\mathbf{LA}(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA})^{-1} \mathbf{A}^{\mathrm{T}} \mathbf{L}\right) \mathbf{b}+\mathbf{LA}\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)^{-1} \mathbf{\Lambda} \boldsymbol{\mu}\right) \
&amp;=\left(\mathbf{L}^{-1}+\mathbf{A}\mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}\right)\left(\left(\mathbf{L}^{-1}+\mathbf{A}\mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}\right)^{-1} \mathbf{b}+\mathbf{LA}\left(\Lambda+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)^{-1} \mathbf{\Lambda} \boldsymbol{\mu}\right) \
&amp;=\mathbf{b}+\left(\mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}\right) \mathbf{LA}\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)^{-1} \mathbf{\Lambda} \boldsymbol{\mu} \
&amp;=\mathbf{b}+\left(\mathbf{A}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)\left(\mathbf{\Lambda}\left(\mathbf{I}+\mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)\right)^{-1} \mathbf{\Lambda} \boldsymbol{\mu} \
&amp;=\mathbf{b}+\left{\mathbf{A}\left(\mathbf{I}+\mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)\left(\mathbf{I}+\mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)^{-1} \mathbf{\Lambda}^{-1} \mathbf{\Lambda} \boldsymbol{\mu}\right} \
&amp;=\mathbf{A}\boldsymbol{\mu}+\mathbf{b}
\end{aligned}
$$</p>
<p>よって$(2.109)$の平均が求められた。</p>
<h2 id="演習-233"><a class="header" href="#演習-233">演習 2.33</a></h2>
<div class="panel-primary">
<p>演習問題2.32と同じ同時分布について考えるが，今度は，平方完成の技法によって，条件付き分布$p(\mathbf{x|y})$の平均と共分散の式を求める．この結果も，</p>
<p>$$
\mathbb{E}[\mathbf{x} \mid \mathbf{y}] =\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)^{-1}\left{\mathbf{A}^{\mathrm{T}} \mathbf{L}(\mathbf{y}-\mathbf{b})+\mathbf{\Lambda} \boldsymbol{\mu}\right} \tag{2.111}
$$</p>
<p>$$
\operatorname{cov}[\mathbf{x} \mid \mathbf{y}] =\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)^{-1} \tag{2.112}
$$</p>
<p>の式と一致することを確かめよ．</p>
</div>
<p>※条件付き分布$p(\mathbf{x\mid y})$の場合、$\mathbf{y}$は固定である。演習2.32と同様に考えると</p>
<p>$$
\begin{aligned}
p(\mathbf{x}\mid \mathbf{y}) &amp;= \frac{p(\mathbf{x},\mathbf{y})}{p(\mathbf{y})}\
&amp;= \frac{p(\mathbf{x}, \mathbf{y})}{\int p(\mathbf{x}, \mathbf{y}) d\mathbf{x}}
\end{aligned}
$$</p>
<p>であり、分母は$\mathbf{y}$が固定で$\mathbf{x}$について積分するので定数となる。よって、$\mathbf{y}$が固定された状態（定数とみなせる状態）で$p(\mathbf{x},\mathbf{y})$について平方完成し、指数部分を調べれば共分散$\operatorname{cov}[\mathbf{x} \mid \mathbf{y}]$が求まり、そこから平均$\mathbb{E}[\mathbf{x} \mid \mathbf{y}]$も求まる。</p>
<p>ここについては演習2.32と同じ流れで</p>
<p>$$
\begin{aligned}
p(\mathbf{x}\mid \mathbf{y}) &amp;= \frac{p(\mathbf{x}, \mathbf{y})}{\int p(\mathbf{x}, \mathbf{y}) d\mathbf{x}} \
&amp;= C_1 \mathcal{N}\left(\mathbf{y} \mid \mathbf{Ax}+\mathbf{b}, \mathbf{L}^{-1}\right) \mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Lambda}^{-1}\right) \
&amp;= C_2 \exp\left{-\frac{1}{2}\left{\mathbf{x}^{\mathrm{T}}\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right) \mathbf{x}-2\left(\left(\mathbf{y}^{\mathrm{T}}-\mathbf{b}^{\mathrm{T}}\right) \mathbf{LA}+\boldsymbol{\mu}^{\mathrm{T}} \mathbf{\Lambda}\right) \mathbf{x} + C_3 \right} \right}\
&amp;= C_4 \exp \left{ -\frac{1}{2}(\mathbf{x}-\mathbf{m})^{\mathrm{T}}\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)(\mathbf{x}-\mathbf{m})+\frac{1}{2} \mathbf{m}^{\mathrm{T}}\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right) \mathbf{m} \right}
\end{aligned}
$$</p>
<p>ここで$C_1$, $C_2$, $C_3$, $C_4$はそれぞれ定数で、$\mathbf{m}=\left(\mathbf{\Lambda}+\mathbf{A}^{\top} \mathbf{LA}\right)^{-1}(\mathbf{\Lambda} \boldsymbol{\mu}+\mathbf{A}^{\mathrm{T}}\mathbf{L}(\mathbf{y}-\mathbf{b}))$であるから、結局$p(\mathbf{x}\mid \mathbf{y})$の共分散は</p>
<p>$$
\operatorname{cov}[\mathbf{x} \mid \mathbf{y}] =\left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)^{-1} \tag{2.112}
$$</p>
<p>となり、</p>
<p>$$
\mathbb{E}[\mathbf{x} \mid \mathbf{y}] = \mathbf{m} = \left(\mathbf{\Lambda}+\mathbf{A}^{\mathrm{T}} \mathbf{LA}\right)^{-1}\left{\mathbf{A}^{\mathrm{T}} \mathbf{L}(\mathbf{y}-\mathbf{b})+\mathbf{\Lambda} \boldsymbol{\mu}\right}
$$</p>
<p>を得る。</p>
<h2 id="演習-234"><a class="header" href="#演習-234">演習 2.34</a></h2>
<div class="panel-primary">
<p>多変量ガウス分布の共分散行列の最尤推定解を求めるには，共分散行列が対称で正定値である制約の下で$\mathbf{\Sigma}$について対数尤度関数</p>
<p>$$\ln p(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Sigma})=-\frac{N D}{2} \ln (2 \pi)-\frac{N}{2} \ln |\mathbf{\Sigma}|-\frac{1}{2} \sum_{n=1}^{N}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}\right) \tag{2.118}
$$</p>
<p>を最大化しなくてはならない．ここでは，こうした制約を無視して，ただ最大化することにする．付録Cの$(C.21)$，$(C.26)$，および$(C.28)$の結果を用いて，対数尤度関数$(2.118)$を最大化する共分散行列$\mathbf{\Sigma}$が，サンプル共分散</p>
<p>$$
\mathbf{\Sigma}<em>{\mathbf{ML}}=\frac{1}{N} \sum</em>{n=1}^{N}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{\mathbf{ML}}\right)\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{\mathbf{ML}}\right)^{\mathrm{T}} \tag{2.122}
$$</p>
<p>となることを示せ．なお，（サンプル共分散が非特異なら）最終結果は対称かつ，正定値である必要がある．</p>
</div>
<p>※ 公式解答集では共分散行列$\mathbf{\Sigma}$が対称行列であること（$\mathbf{\Sigma}^{-1}=({\mathbf{\Sigma}^{-1}})^{\mathrm{T}}$）の制約を無視して〜と書いてあるのに、その対称性を利用した解答例になっているが、それを利用しなくても答えを出すことはできる。</p>
<p>$$
\begin{aligned}
\frac{\partial}{\partial \mathbf{\Sigma}} \ln p(\mathbf{X} | \boldsymbol{\mu}, \mathbf{\Sigma}) &amp;=-\frac{N}{2} \frac{\partial}{\partial \mathbf{\Sigma}} \ln |\mathbf{\Sigma}|-\frac{1}{2} \frac{\partial}{\partial \mathbf{\Sigma}}\left{\sum_{n=1}^{N}\left(\mathbf{x}_n-\boldsymbol{\mu}\right)^{\mathrm{T}} \Sigma^{-1} \left(\mathbf{x}<em>n-\boldsymbol{\mu} \right)\right} \
&amp;=-\frac{N}{2}\left(\mathbf{\Sigma}^{-1}\right)^{\mathrm{T}}-\frac{1}{2} \frac{\partial}{\partial \mathbf{\Sigma}}\left{\sum</em>{n=1}^{N} \operatorname{Tr}\left(\mathbf{\Sigma}^{-1}\left(\mathbf{x}_n-\boldsymbol{\mu}\right)\left(\mathbf{x}<em>n-\boldsymbol{\mu} \right)^{\mathrm{T}}\right)\right} \
&amp;=-\frac{N}{2}\left(\mathbf{\Sigma}^{-1}\right)^{\mathrm{T}} - \frac{1}{2} \frac{\partial}{\partial \mathbf{\Sigma}} \operatorname{Tr}\left( \mathbf{\Sigma}^{-1} \sum</em>{n=1}^{N}\left(\mathbf{x}_n-\boldsymbol{\mu}\right)\left(\mathbf{x}_n-\boldsymbol{\mu} \right)^{\mathrm{T}}\right)
\end{aligned}
$$</p>
<p>ここで第2項について、$\mathbf{A}=\mathbf{\Sigma}$, $\mathbf{B}=\sum_{n=1}^{N}\left(\mathbf{x}_n-\boldsymbol{\mu}\right)\left(\mathbf{x}_n-\boldsymbol{\mu} \right)^{\mathrm{T}}$とすると、求めるべきは$\displaystyle \frac{\partial}{\partial \mathbf{A}}{\mathrm{Tr}}(\mathbf{A}^{-1}\mathbf{B})$である。</p>
<p>$$
\begin{aligned}
\frac{\partial}{\partial A_{ij}}{\mathrm{Tr}}(\mathbf{A}^{-1}\mathbf{B}) =&amp; \operatorname{Tr}\left(\left(\frac{\partial}{\partial A_{i j}} \mathbf{A}^{-1} \right) \mathbf{B}\right) \
=&amp;- \operatorname{Tr}\left(\mathbf{A}^{-1}\left(\frac{\partial}{\partial A_{ij}} \mathbf{A}\right) \mathbf{A}^{-1} \mathbf{B}\right)\hspace{1em}(\because 付録(C.21))\
=&amp;-\operatorname{Tr}\left(\left(\frac{\partial}{\partial A_{ij}} \mathbf{A}\right) \mathbf{A}^{-1} \mathbf{B} \mathbf{A}^{-1}\right)\hspace{1em}(\because \operatorname{Tr}(\mathbf{ABCD}) = \operatorname{Tr}(\mathbf{BCDA}))
\end{aligned}
$$</p>
<p>なので、$\mathbf{C}=\mathbf{A^{-1}BA^{-1}}$とすると</p>
<p>$$
\begin{aligned} \operatorname{Tr}\left(\left(\frac{\partial}{\partial A_{ij}} \mathbf{A}\right) \mathbf{C}\right) &amp;=\sum_{s}\left(\left(\frac{\partial}{\partial A_{ij}} \mathbf{A}\right) \mathbf{C}\right)<em>{ss} \
&amp;= \sum</em>{s}\left(\sum_{t}\left(\frac{\partial}{\partial A_{ij}} \mathbf{A}\right)<em>{s t} c</em>{ts}\right) \
&amp;=\sum_{s, t} \delta_{is} \delta_{jt} c_{ts}=c_{ji}
\end{aligned}
$$</p>
<p>最後は$s=i$かつ$t=j$のときのみクロネッカーのデルタ$\delta_{is}\delta_{jt}$が$1$になり、その他は$0$となることを表している。
ここについては、付録(C.23)に書かれてある定理に則って</p>
<p>$$
\begin{aligned} \operatorname{Tr}\left(\left(\frac{\partial}{\partial A_{ij}} \mathbf{A}\right) \mathbf{C}\right)
&amp;=\frac{\partial}{\partial A_{ij}} \operatorname{Tr}\left(\mathbf{AC} \right) \
&amp;=c_{ji}
\end{aligned}
$$</p>
<p>としても良い。</p>
<p>（※ 一般に正方行列$\mathbf{F}$の行列の$ij$成分$f_{ij}$について$\operatorname{Tr}(\mathbf{F})=\sum_{i=1}f_{ii}$より、</p>
<p>$$
\frac{\partial}{\partial f_{ij}}\operatorname{Tr}(\mathbf{F}) = \frac{\partial f_{11}}{\partial f_{ij}}+\frac{\partial f_{22}}{\partial f_{ij}}+\cdots= \operatorname{Tr}\left(\frac{\partial}{\partial f_{ij}} \mathbf{F}\right)
$$</p>
<p>が成立する）</p>
<p>これより、$\displaystyle \frac{\partial}{\partial A_{ij}}{\mathrm{Tr}}(\mathbf{A}^{-1}\mathbf{B})=-c_{ji}$となるので、付録(C.24)のように</p>
<p>$$
\begin{aligned}
\frac{\partial}{\partial \mathbf{A}}{\mathrm{Tr}}(\mathbf{A}^{-1}\mathbf{B}) &amp;= -\mathbf{C}^{\mathrm T} \
&amp;=-(\mathbf{A^{-1}BA^{-1}})^{\mathrm T} \
&amp;=-(\mathbf{\Sigma^{-1}B\Sigma^{-1}})^{\mathrm T}
\end{aligned}
$$</p>
<p>よって、最大値を求めるために$\frac{\partial}{\partial \mathbf{\Sigma}} \ln p(\mathbf{X} | \boldsymbol{\mu}, \mathbf{\Sigma}) = 0$とすると</p>
<p>$$
\begin{aligned}
-\frac{N}{2}\left(\mathbf{\Sigma}^{-1}\right)^{\mathrm{T}} - \frac{1}{2} \frac{\partial}{\partial \mathbf{\Sigma}} \operatorname{Tr}\left( \mathbf{\Sigma}^{-1} \sum_{n=1}^{N}\left(\mathbf{x}_n-\boldsymbol{\mu}\right)\left(\mathbf{x}<em>n-\boldsymbol{\mu} \right)^{\mathrm{T}}\right)
=0 \
\frac{N}{2}\left(\mathbf{\Sigma}^{-1}\right)^{\mathrm{T}} = \frac{1}{2} \left( \mathbf{\Sigma^{-1}} \sum</em>{n=1}^{N}\left(\mathbf{x}_n-\boldsymbol{\mu}\right)\left(\mathbf{x}_n-\boldsymbol{\mu} \right)^{\mathrm{T}} \mathbf{\Sigma^{-1}}  \right) ^{\mathrm T}
\end{aligned}
$$</p>
<p>互いの転置をとって移項すると、</p>
<p>$$
\mathbf{\Sigma} = \frac{1}{N}\sum_{n=1}^{N}\left(\mathbf{x}_n-\boldsymbol{\mu}\right)\left(\mathbf{x}_n-\boldsymbol{\mu} \right)^{\mathrm{T}}
$$</p>
<p>となり、これは式$(2.122)$となる。また、これによって$\mathbf{\Sigma}$についての対称性を仮定せずに$\mathbf{\Sigma}$が対称行列になることがわかった。</p>
<h2 id="演習-235"><a class="header" href="#演習-235">演習 2.35</a></h2>
<div class="panel-primary">
<p>$$
\mathbb{E}[\mathbf{x}] = \boldsymbol{\mu} \tag{2.59}
$$</p>
<p>の結果を用いて，</p>
<p>$$
\mathbb{E}[\mathbf{xx}^{\mathrm{T}}] = \boldsymbol{\mu\mu}^{\mathrm{T}}+\mathbf{\Sigma} \tag{2.62}
$$</p>
<p>を証明せよ．そして，$(2.59)$と$(2.62)$の結果から，</p>
<p>$$
\mathbb{E}\left[\mathbf{x}<em>{n} \mathbf{x}</em>{m}^{\mathrm{T}}\right]=\boldsymbol{\mu} \boldsymbol{\mu}^{\mathrm{T}}+I_{n m} \mathbf{\Sigma} \tag{2.291}
$$</p>
<p>を示せ．ただし，$\mathbf{x}<em>{n}$は，平均が$\boldsymbol{\mu}$で共分散が$\mathbf{\Sigma}$のガウス分布からサンプリングされたデータ点，$I</em>{nm}$は単位行列の$(n,m)$要素を表す．これから，</p>
<p>$$
\mathbb{E}[\mathbf{\Sigma}_{\mathrm{ML}}] = \frac{N-1}{N}\mathbf{\Sigma} \tag{2.124}
$$</p>
<p>の結果を証明せよ.</p>
</div>
<p>$(2.59)$と$(2.62)$から、$m=n$であれば$\mathbb{E}\left[\mathbf{x}<em>{n} \mathbf{x}</em>{m}\right]=\boldsymbol{\mu} \boldsymbol{\mu}^{\mathrm{T}}+\mathbf{\Sigma}$となる。
一方で、$m \ne n$だったときには2つのデータセット$\mathbf{x}<em>n$と$\mathbf{x}<em>m$は独立なので$\mathbb{E}\left[\mathbf{x}</em>{n} \mathbf{x}</em>{m}\right]=\boldsymbol{\mu} \boldsymbol{\mu}^{\mathrm{T}}$となる。
よって、$I_{nm}$を使うと$\mathbb{E}[\mathbf{x}_n\mathbf{x}<em>m^{\mathrm T}] = \boldsymbol{\mu}\boldsymbol{\mu}^{\mathrm{T}} + I</em>{nm}\mathbf{\Sigma}$となる。</p>
<p>$(2.124)$式 $\displaystyle \mathbb{E}\left[\mathbf{\Sigma}_{\mathrm{ML}}\right]=\frac{N-1}{N} \mathbf{\Sigma}$を示す。</p>
<p>$(2.122)$式より</p>
<p>$$
\begin{aligned}
\mathbb{E}\left[\mathbf{\Sigma}<em>{\mathrm{ML}}\right] &amp;=\frac{1}{N} \sum</em>{n=1}^{N} \mathbb{E}\left[\left(\mathbf{x}<em>{n}-\frac{1}{N} \sum</em>{m=1}^{N} \mathbf{x}<em>{m}\right)\left(\mathbf{x}</em>{n}^{\mathrm{T}}-\frac{1}{N} \sum_{l=1}^{N} \mathbf{x}<em>{l}^{\mathrm{T}}\right)\right] \
&amp;=\frac{1}{N} \sum</em>{n=1}^{N} \mathbb{E}\left[\mathbf{x}<em>{n} \mathbf{x}</em>{n}^{\mathrm{T}}-\frac{2}{N} \mathbf{x}<em>{n} \sum</em>{m=1}^{N} \mathbf{x}<em>{m}^{\mathrm{T}}+\frac{1}{N^{2}} \sum</em>{m=1}^{N} \sum_{l=1}^{N} \mathbf{x}<em>{m} \mathbf{x}</em>{l}^{\mathrm{T}}\right] \
&amp;=\frac{1}{N}\sum_{n=1}^{N}\left{ \mathbb{E}[\mathbf{x}<em>n\mathbf{x}<em>n^{\mathrm{T}}]-\frac{2}{N}\mathbb{E}\left[ \mathbf{x}</em>{n} \sum</em>{m=1}^{N} \mathbf{x}<em>{m}^{\mathrm{T}} \right] +\frac{1}{N^2}\mathbb{E}\left[ \sum</em>{m=1}^{N} \sum_{l=1}^{N} \mathbf{x}<em>{m} \mathbf{x}</em>{l}^{\mathrm{T}} \right] \right} \
&amp;=\frac{1}{N}\sum_{n=1}^{N}\left{\boldsymbol{\mu} \boldsymbol{\mu}^{\mathrm{T}}+\mathbf{\Sigma}-2\left(\boldsymbol{\mu} \boldsymbol{\mu}^{\mathrm{T}}+\frac{1}{N} \mathbf{\Sigma}\right)+\boldsymbol{\mu} \boldsymbol{\mu}^{\mathrm{T}}+\frac{1}{N} \mathbf{\Sigma}\right} \
&amp;=\frac{N-1}{N} \mathbf{\Sigma}
\end{aligned}
$$</p>
<h2 id="演習-236"><a class="header" href="#演習-236">演習 2.36</a></h2>
<div class="panel-primary">
<p>$$
\begin{aligned}
\boldsymbol{\mu}<em>{\mathrm{ML}}^{(N)} &amp;=\frac{1}{N} \sum</em>{n=1}^{N} \mathbf{x}<em>{n} \
&amp;=\frac{1}{N} \mathbf{x}</em>{N}+\frac{1}{N} \sum_{n=1}^{N-1} \mathbf{x}<em>{n} \
&amp;=\frac{1}{N} \mathbf{x}</em>{N}+\frac{N-1}{N} \boldsymbol{\mu}<em>{\mathrm{ML}}^{(N-1)} \
&amp;=\boldsymbol{\mu}</em>{\mathrm{ML}}^{(N-1)}+\frac{1}{N}\left(\mathbf{x}<em>{N}-\boldsymbol{\mu}</em>{\mathrm{ML}}^{(N-1)}\right) \tag{2.126}
\end{aligned}
$$</p>
<p>を得たのと同様の手続きで，次の最尤推定の式から，1変数ガウス分布の分散の逐次推定の式を導出せよ．</p>
<p>$$
\sigma_{\mathrm{ML}}^{2}=\frac{1}{N} \sum_{n=1}^{N}\left(x_{n}-\mu\right)^{2} \tag{2.292}
$$</p>
<p>Robbins-Monroの逐次推定の式</p>
<p>$$
\theta^{(N)}=\theta^{(N-1)}-a_{N-1} \frac{\partial}{\partial \theta^{(N-1)}}\left[-\ln p\left(x_{N} \mid \theta^{(N-1)}\right)\right] \tag{2.135}
$$</p>
<p>にガウス分布を代入すると，これと同じ式になることを確かめ，ここから対応する係数$a_N$の式を求めよ．</p>
</div>
<p>※ この問題の主題は$(2.292)$式から$(2.126)$式のような手続きによって1変数ガウス分布の分散を求めるというのが前半で、後半はこの式とRobbins-Monroアルゴリズムによる$(2.135)$式が係数$a_N$を適切に定めることで同型になることを示すことである。</p>
<p>まずは$(2.292)$式から分散の逐次更新式を求める。
$$
\begin{aligned}
\sigma^2_{(N)}  &amp;=\frac{1}{N} \sum_{n=1}^{N}\left(x_{n}-\mu\right)^{2} \
&amp;=\frac{1}{N}\left{\sum_{n=1}^{N-1}\left(x_{n}-\mu\right)^{2}+\left(x_{N}-\mu\right)^{2}\right} \
&amp;=\frac{N-1}{N} \cdot \frac{1}{N-1} \sum_{n=1}^{N-1}\left(x_{n}-\mu \right)^{2}+\frac{1}{N}\left(x_{N}-\mu\right)^{2} \
&amp;=\frac{N-1}{N} \sigma^2_{(N-1)}+\frac{1}{N}\left(x_{N}-\mu \right)^{2} \
&amp;=\sigma^2_{(N-1)}+\frac{1}{N}\left( (x_N-\mu)^2-\sigma^2_{(N-1)}\right) \hspace{2em} \cdots (1)
\end{aligned}
$$</p>
<p>一方、Robbins-Monroの式ガウス分布の式を代入すると</p>
<p>$$
\begin{aligned}
\sigma_{(N)}^{2}
&amp;=\sigma_{(N-1)}^{2}-a_{N-1} \frac{\partial}{\partial \sigma_{(N-1)}^{2}}\left[-\ln \left{\frac{1}{\sqrt{2 \pi \sigma_{(N-1)}^{2}}} \exp \left(-\frac{\left(x_{N}-\mu\right)^{2}}{2 \sigma_{(N-1)}^{2}}\right)\right}\right]\
&amp;=\sigma_{(N-1)}^{2}-a_{N-1} \frac{\partial}{\partial \sigma_{(N-1)}^{2}}\left[\frac{1}{2} \ln \left(2 \pi \sigma_{(N-1)}^{2}\right)+\frac{\left(x_{N}-\mu\right)^{2}}{2 \sigma_{(N-1)}^{2}}\right] \
&amp;=\sigma_{(N-1)}^{2}-a_{N-1}\left{\frac{1}{2} \cdot \frac{1}{\sigma_{(N-1)}^{2}}-\frac{\left(x_{N}-\mu\right)^{2}}{2 \sigma_{(N-1)}^{4}}\right} \
&amp;=\sigma_{(N-1)}^{2}+\frac{a_{N-1}}{2 \sigma_{(N-1)}^{4}}\left{-\sigma^2_{(N-1)}+\left(x_{N}-\mu\right)^{2}\right} \hspace{2em} \cdots (2)
\end{aligned}
$$</p>
<p>$(1), (2)$式を比較すると$\displaystyle a_{N-1} = \frac{2\sigma^4_{(N-1)}}{N} \hspace{1em} \left( a_{N} = \frac{2\sigma^4_{(N)}}{N+1} \right)$が得られる。</p>
<blockquote>
<p>Robbins-Monroアルゴリズムについて本文の説明P.93はわかりにくいので補足すると、パラメータ$\theta$が与えられた時に、条件付き確率$p(z|\theta)$に従って、変数$z$が観測される状況を考えてください。図2.10でいうと、まずパラメータ$\theta$を決めると、$z$の観測値(青い点)が得られるという感じです。<br>
この時、$z$の観測値(青い点)の出方は、$z$の条件付き期待値$\mathrm{E}[z|\theta]=f(\theta)$(赤い曲線)に従います。<br>
今、$z$と$\theta$の大規模データはないので、私たちはこの条件付き期待値$\mathrm{E}[z|\theta]$(赤い曲線)の形を知りません。もし大規模データがあれば、直接曲線をフィッティングできますが、今は逐次的にのみ$z$が観測されるといった状態です。<br>
Robbins-Monroアルゴリズムで求めるのは、この$\mathrm{E}[z|\theta]$の値が$0$になるようなパラメータ$\theta$の値です。（というか、期待値が任意の値をとるようなパラメータを見つけるように簡単に拡張できます。(Robbins and Monro, 1951)）<br>
$(2.128)$のように条件つき分散(青点のちらばり具合)が有限であるなら, $(2.129)$のような簡単な手続きで$\theta$が求まります。<br>
つまり、ある$\theta$で$z$を観測してみて、$0$より大きければ$\theta$を小さく、小さければ大きくしてあげるだけです。もし$z$の観測の分散が無限に大きいならそれが無理なのもわかると思います。<br>
問題はこれを最尤推定にどうあてはめるかですが、とにかく、$0$に持っていきたい関数が条件付き期待値になっていれば良いのです。
パラメータの最尤推定は、尤度関数をパラメータで微分して$0$とおくことで、尤度関数が最小値をとるようなパラメータを求める方法です。なので、尤度関数をパラメータで微分したもの(0にしたいもの：$(2.133)$左辺)が上で言う$f(\theta)$になりそうです。そして、それが条件付き期待値の形になっていれば良いのですが、$(2.134)$のようにちゃんとなってますよ、ということです。</p>
</blockquote>
<h2 id="演習-237"><a class="header" href="#演習-237">演習 2.37</a></h2>
<div class="panel-primary">
<p>$(2.126)$を得たのと同様の手続きで、最尤推定の式</p>
<p>$$
\mathbf{\Sigma}<em>{\mathbf{ML}}=\frac{1}{N} \sum</em>{n=1}^{N}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{\mathbf{ML}}\right)\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{\mathbf{ML}}\right)^{\mathrm{T}} \tag{2.122}
$$</p>
<p>から，多変量ガウス分布の共分散の逐次推定の式を導出せよ．Robbins-Monroの逐次推定の式$(2.135)$にガウス分布を代入すると，これと同じ式になることを確かめ，ここから係数$a_N$の式を求めよ．</p>
</div>
<p>※<strong>問題設定が悪く、解けない可能性が高い</strong>。</p>
<p>$(2.122)$式</p>
<p>$$
\Sigma_{\mathrm{ML}} = \frac{1}{N}\sum_{n=1}^{N}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{\mathrm{ML}}\right)\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{\mathrm{ML}}\right)^{\mathrm{T}}
$$</p>
<p>から多次元ガウス分布の分散の逐次更新式を求める。</p>
<p>$$
\begin{aligned}
\Sigma_{(N)} &amp;= \frac{1}{N}\sum_{n=1}^{N}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}\right)\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}\right)^{\mathrm{T}} \
&amp;= \frac{N-1}{N}\cdot\frac{1}{N-1}\left{ \sum_{n=1}^{N-1}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}\right)\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}\right)^{\mathrm{T}}+\left(\mathbf{x}<em>{N}-\boldsymbol{\mu}\right)\left(\mathbf{x}</em>{N}-\boldsymbol{\mu}\right)^{\mathrm{T}} \right} \
&amp;= \frac{N-1}{N}\Sigma_{(N-1)}+\frac{1}{N}(\mathbf{x}<em>{N}-\boldsymbol{\mu})(\mathbf{x}</em>{N}-\boldsymbol{\mu})^{\mathrm{T}} \
&amp;= \Sigma_{(N-1)}+\frac{1}{N}\left{ (\mathbf{x}<em>{N}-\boldsymbol{\mu})(\mathbf{x}</em>{N}-\boldsymbol{\mu})^{\mathrm{T}} - \Sigma_{(N-1)}\right} \hspace{2em} \cdots (1)
\end{aligned}
$$</p>
<p>一方、Robbins-Monroの式に多変量ガウス分布を当てはめると、</p>
<p>$$
\Sigma_{(N)} = \Sigma_{(N-1)}-a_{N-1}\frac{\partial}{\partial \Sigma_{(N-1)}}\left[ -\ln \left{ \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma_{(N-1)}|^{1/2}}\exp\left{ -\frac{1}{2}(\mathbf{x}<em>N-\boldsymbol{\mu})^{\mathrm{T}}\Sigma</em>{(N-1)}(\mathbf{x}_N-\boldsymbol{\mu}) \right} \right} \right]\hspace{2em} \cdots (2)
$$</p>
<p>この微分項について考える。</p>
<p>$$
\begin{aligned}
&amp;\frac{\partial}{\partial \Sigma_{(N-1)}}\left[ -\ln \left{ \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma_{(N-1)}|^{1/2}}\exp\left{ -\frac{1}{2}(\mathbf{x}<em>N-\boldsymbol{\mu})^{\mathrm{T}}\Sigma</em>{(N-1)}(\mathbf{x}<em>N-\boldsymbol{\mu}) \right} \right} \right] \
=&amp; \frac{1}{2}\Sigma</em>{(N-1)}^{-1}-\frac{1}{2}\left{ \Sigma_{(N-1)}^{-1}(\mathbf{x}<em>{N}-\boldsymbol{\mu})(\mathbf{x}</em>{N}-\boldsymbol{\mu})^{\mathrm{T}}\Sigma_{(N-1)}^{-1} \right} \
=&amp; -\frac{1}{2}\Sigma_{(N-1)}^{-1}\left{ (\mathbf{x}<em>{N}-\boldsymbol{\mu})(\mathbf{x}</em>{N}-\boldsymbol{\mu})^{\mathrm{T}} -\Sigma_{(N-1)} \right}\Sigma_{(N-1)}^{-1}
\end{aligned}
$$</p>
<p>$(2)$式に当てはめれば</p>
<p>$$
\Sigma_{(N)} = \Sigma_{(N-1)}+\frac{a_{N-1}\Sigma_{(N-1)}^{-1}}{2}\left{ (\mathbf{x}<em>{N}-\boldsymbol{\mu})(\mathbf{x}</em>{N}-\boldsymbol{\mu})^{\mathrm{T}} -\Sigma_{(N-1)} \right}\Sigma_{(N-1)}^{-1}
$$</p>
<p>ここまではわかったけれど係数$a_{N-1}$がきれいな式にならない……。解答例では、追加で共分散行列がdiagonal（対角行列？）ならば$\displaystyle a_{N-1} = \frac{2}{N}\left( \Sigma_{(N-1)}\right)^2$とすれば良いと書いてあるが、仮にdiagonalでも合わない気がする。Googleで調べてみたけれどよくわからない。</p>
<blockquote>
<p>参考：https://math.stackexchange.com/questions/1558016/deriving-mle-for-covariance-matrix-using-robbins-monro</p>
</blockquote>
<h2 id="演習-238"><a class="header" href="#演習-238">演習 2.38</a></h2>
<div class="panel-primary">
<p>二次形式を平方完成することで，</p>
<p>$$
\mu_{N} =\frac{\sigma^{2}}{N \sigma_{0}^{2}+\sigma^{2}} \mu_{0}+\frac{N \sigma_{0}^{2}}{N \sigma_{0}^{2}+\sigma^{2}} \mu_{\mathrm{ML}} \tag{2.141}
$$</p>
<p>$$
\frac{1}{\sigma_{N}^{2}} =\frac{1}{\sigma_{0}^{2}}+\frac{N}{\sigma^{2}} \tag{2.142}
$$</p>
<p>の結果を導出せよ．</p>
</div>
<p>$$
p(\mu \mid \mathbf{x}) \propto p(\mathbf{x} \mid \mu) p(\mu) \tag{2.139}
$$</p>
<p>について、</p>
<p>$$
p(\mathbf{x} \mid \mu)=\prod_{n=1}^{N} p\left(x_{n} \mid \mu\right)=\frac{1}{\left(2 \pi \sigma^{2}\right)^{N / 2}} \exp \left{-\frac{1}{2 \sigma^{2}} \sum_{n=1}^{N}\left(x_{n}-\mu\right)^{2}\right} \tag{2.137}
$$</p>
<p>と$p(\mu) = \mathcal{N}(\mu | \mu_0, \sigma_0^2) \hspace{1em}(2.138)$式で変形する。</p>
<p><strong>$\mu$についての式</strong>としての$p(\mathbf{X}|\mu)p(\mu)$の指数部分について考えると</p>
<p>$$
\begin{aligned}
&amp; \exp \left{ -\frac{1}{2\sigma^2}\sum_{n=1}^{N}(x_n-\mu)^2 \right}\cdot \exp \left{ -\frac{1}{2\sigma_0^2}(\mu-\mu_0)^2 \right} \
=&amp;\exp \left { -\frac{1}{2\sigma^2} \sum_{n=1}^{N}(x_n^2 -2x_n \mu + \mu^2) - \frac{1}{2\sigma_0^2} (\mu^2 - 2\mu\mu_0 + \mu_0^2)\right} \
=&amp;\exp \left{ -\frac{1}{2}\left( \frac{N}{\sigma^2}+\frac{1}{\sigma_0^2}\right)\mu^2  + \left( \frac{1}{\sigma^2}\sum_{n=1}^{N}x_n+\frac{\mu_0}{\sigma_0^2} \right)\mu + \cdots  \right}
\end{aligned}
$$</p>
<p>となる。ここで、$\cdots$の部分は$\mu$に依存しない項なので考えなくても良い。一方で、$(2.140)$の右辺の指数部分について考えると、</p>
<p>$$
\begin{aligned}
&amp; \exp \left{ -\frac{1}{2\sigma_N^2}(\mu - \mu_N)^2 \right} \
=&amp; \exp \left{ -\frac{1}{2}\left( \frac{1}{\sigma_N^2} \right)\mu^2+\frac{\mu_N}{\sigma_N^2}\mu+\cdots \right}
\end{aligned}
$$</p>
<p>$\displaystyle \sum_{n=1}^{N}x_n = N\mu_{\mathrm{ML}}$に注意して、これら2つの式を比較すると</p>
<p>$$
\begin{aligned}
\frac{1}{\sigma_N^2}&amp;=\frac{1}{\sigma_0^2}+\frac{N}{\sigma^2},\hspace{2em}\left( \sigma_N^2 = \frac{\sigma^2\sigma_0^2}{\sigma^2+N\sigma_0^2} \right) \
\frac{\mu_N}{\sigma_N^2}&amp;=\frac{\mu_0}{\sigma_0^2}+\frac{N}{\sigma^2}\mu_{\mathrm{ML}}
\end{aligned}
$$</p>
<p>2式めを変形して</p>
<p>$$
\begin{aligned}
\mu_{N}&amp;=\left( \frac{\mu_0}{\sigma_0^2}+\frac{N\mu_{\mathrm{ML}}}{\sigma^2} \right)\sigma_{N}^2 \
&amp;=\left( \frac{\mu_0}{\sigma_0^2}+\frac{N\mu_{\mathrm{ML}}}{\sigma^2} \right) \left( \frac{\sigma^2\sigma_0^2}{\sigma^2+N\sigma_0^2} \right) \
&amp;=\frac{\sigma^{2}}{N \sigma_{0}^{2}+\sigma^{2}} \mu_{0}+\frac{N \sigma_{0}^{2}}{N \sigma_{0}^{2}+\sigma^{2}} \mu_{\mathrm{ML}}
\end{aligned}
$$</p>
<p><strong>ベイズ推定では、事前知識を決め打ちではなく分布の形で表す。</strong></p>
<h2 id="演習-239"><a class="header" href="#演習-239">演習 2.39</a></h2>
<div class="panel-primary">
<p>ガウス確率変数の平均の事後分布についての結果
$$
\mu_{N} =\frac{\sigma^{2}}{N \sigma_{0}^{2}+\sigma^{2}} \mu_{0}+\frac{N \sigma_{0}^{2}}{N \sigma_{0}^{2}+\sigma^{2}} \mu_{\mathrm{ML}} \tag{2.141}
$$</p>
<p>$$
\frac{1}{\sigma_{N}^{2}} =\frac{1}{\sigma_{0}^{2}}+\frac{N}{\sigma^{2}} \tag{2.142}
$$
を元に，最初の$N-1$個のデータ点の影響を分離し$\mu_{N}$と$\sigma^2_{N}$の逐次更新の式を求めよ．そして，事後分布$p\left(\mu | x_{1}, \ldots, x_{N-1}\right)=\mathcal{N}\left(\mu | \mu_{N-1}, \sigma_{N-1}^{2}\right)$に尤度関数$p(x_N|\mu) = \mathcal{N}(x_N|\mu, \sigma^2)$を掛けた後,平方完成と正規化をすることで，$N$個の観測値を得た後の事後分布と同じ結果を導出せよ．</p>
</div>
<p>問題文の通り、$N-1$個のデータ点との影響を分離する。ただし、分散$\sigma^2$は既知であることに注意する。</p>
<p>$(2.141)$と$(2.142)$式と、演習問題2.38の結果から、$\mu_0 \to \mu_{N-1}$, $\sigma_0 \to \sigma_{N-1}$, $\mu_{\mathrm{ML}}=x_N$, $N=1$の場合に当てはまるので、$\displaystyle \mu_N = \left( \frac{\mu_{N-1}}{\sigma_{N-1}^2}+\frac{x_N}{\sigma^2} \right)\sigma_N^2$の形になるはずである。これを示す。</p>
<p>$$
\begin{aligned}
\frac{1}{\sigma_N^2}&amp;=\frac{1}{\sigma_0^2}+\frac{N}{\sigma^2}=\frac{1}{\sigma_0^2}+\frac{N-1}{\sigma^2}+\frac{1}{\sigma^2} = \frac{1}{\sigma_{N-1}^2}+\frac{1}{\sigma^2} \
\sigma_N^2 &amp;= \frac{\sigma^2+\sigma_{N-1}^2}{\sigma^2\sigma_{N-1}^2}
\end{aligned}
$$</p>
<p>平均$\mu_N$について</p>
<p>$$
\begin{aligned}
\mu_N &amp;= \frac{\sigma^{2}}{N \sigma_{0}^{2}+\sigma^{2}} \mu_{0}+\frac{N \sigma_{0}^{2}}{N \sigma_{0}^{2}+\sigma^{2}} \mu_{\mathrm{ML}} \
&amp;= \frac{\mu_0}{\sigma_0^2}\sigma_N^2+\frac{\sigma_N^2}{\sigma^2}\left( \sum_{n=1}^{N-1}x_n + x_N \right) \
&amp;= \left( \frac{\mu_0}{\sigma_0^2}+\frac{1}{\sigma^2}\sum_{n=1}^{N-1}x_n \right)\sigma_N^2 + \frac{x_N}{\sigma^2}\sigma_N^2
\end{aligned}
$$</p>
<p>ここで$\displaystyle \frac{\mu_0}{\sigma_0^2}+\frac{1}{\sigma^2}\sum_{n=1}^{N-1}x_n$について、演習2.38の途中式で出てきたように</p>
<p>$$
\frac{\mu_0}{\sigma_0^2}+\frac{1}{\sigma^2}\sum_{n=1}^{N-1}x_n = \frac{\mu_{N-1}}{\sigma_{N-1}^2}
$$</p>
<p>と書けるので、</p>
<p>$$
\mu_N = \left( \frac{\mu_{N-1}}{\sigma_{N-1}^2} + \frac{x_N}{\sigma^2} \right)\sigma_N^2, \hspace{2em}\left( \frac{\mu_N}{\sigma_N^2} = \frac{\mu_{N-1}}{\sigma_{N-1}^2}+\frac{x_N}{\sigma^2} \right)
$$</p>
<p>となる。これが逐次更新の式となる。</p>
<p>事後分布に尤度関数をかけて、平方完成と正規化〜の部分は演習問題2.38の解き方と同じであり、得られる結果は上記となる。</p>
<h2 id="演習-240"><a class="header" href="#演習-240">演習 2.40</a></h2>
<div class="panel-primary">
<p>$D$次元ガウス確率変数$\mathbf{x}$を考える．この分布$\mathcal{N}(\mathbf{x}|\boldsymbol{\mu}, \mathbf{\Sigma})$の共分散$\mathbf{\Sigma}$は既知としたとき，観測値集合$\mathbf{X} = { \mathbf{x}<em>1, \ldots, \mathbf{x}<em>N }$から平均$\boldsymbol{\mu}$を推定したいとする．事前分布$p(\boldsymbol{\mu})=\mathcal{N}\left(\boldsymbol{\mu} | \boldsymbol{\mu}</em>{0}, \mathbf{\Sigma</em>{0}} \right)$について、これに対応する事後分布$p(\boldsymbol{\mu} | \mathbf{X})$を求めよ．</p>
</div>
<p>事後分布$p(\boldsymbol{\mu} | \mathbf{X})$は事前分布$\prod_{n=1}^{N} p\left(\mathbf{x}_{n} | \boldsymbol{\mu}, \mathbf{\Sigma}\right)$に尤度関数$p(\boldsymbol{\mu})$をかけたものに比例するので、</p>
<p>$$
p(\boldsymbol{\mu} | \mathbf{X}) \propto \prod_{n=1}^{N} p\left(\mathbf{x}_{n} | \boldsymbol{\mu}, \mathbf{\Sigma}\right) p(\boldsymbol{\mu})
$$</p>
<p>先の演習問題と同様に指数部分を考えると</p>
<p>$$
\begin{aligned}
&amp;-\frac{1}{2}\left(\boldsymbol{\mu}-\boldsymbol{\mu}<em>{0}\right)^{\mathrm{T}} \mathbf{\Sigma}</em>{0}^{-1}\left(\boldsymbol{\mu}-\boldsymbol{\mu}<em>{0}\right)-\frac{1}{2} \sum</em>{n=1}^{N}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}\right) \
=&amp;-\frac{1}{2} \boldsymbol{\mu}^{\mathrm{T}}\left(\mathbf{\Sigma}<em>{0}^{-1}+N \mathbf{\Sigma}^{-1}\right) \boldsymbol{\mu}+\boldsymbol{\mu}^{\mathbf{T}}\left(\mathbf{\Sigma}</em>{0}^{-1} \boldsymbol{\mu}<em>{0}+\mathbf{\Sigma}^{-1} \sum</em>{n=1}^{N} \mathbf{x}_{n}\right)+\mathrm{const.}
\end{aligned}
$$
ここで、$\mathrm{const.}$は$\boldsymbol{\mu}$と独立な項であり、正規化時に吸収される。</p>
<p>求める事後分布の形を$\mathcal{N}(\boldsymbol{\mu}|\boldsymbol{\mu}<em>N,\mathbf{\Sigma}</em>{N})$とすると、この指数部分は</p>
<p>$$
-\frac{1}{2}(\boldsymbol{\mu}-\boldsymbol{\mu}_N)^{\mathrm{T}}\mathbf{\Sigma}_N^{-1}(\boldsymbol{\mu}-\boldsymbol{\mu}_N)=-\frac{1}{2}\boldsymbol{\mu}^{\mathrm{T}}\mathbf{\Sigma}_N^{-1}\boldsymbol{\mu}+\boldsymbol{\mu}^{\mathrm{T}}\mathbf{\Sigma}_N^{-1}\boldsymbol{\mu}_N+\textrm{const.}
$$</p>
<p>となるので、まず$\boldsymbol{\mu}$の二次形式部分について</p>
<p>$$
\mathbf{\Sigma}<em>{N}^{-1} =\mathbf{\Sigma}</em>{0}^{-1}+N \mathbf{\Sigma}^{-1}
$$</p>
<p>となり、$\boldsymbol{\mu}^{\mathrm{T}}$の係数について
$$
\begin{aligned}
\boldsymbol{\mu}<em>{N} &amp;=\mathbf{\Sigma}<em>N\left(\mathbf{\Sigma}</em>{0}^{-1} \boldsymbol{\mu}</em>{0}+\mathbf{\Sigma}^{-1} N \boldsymbol{\mu}<em>{\mathrm{ML}}\right) \
&amp;=\left(\mathbf{\Sigma}</em>{0}^{-1}+N \boldsymbol{\Sigma}^{-1}\right)^{-1}\left(\mathbf{\Sigma}<em>{0}^{-1} \boldsymbol{\mu}</em>{0}+\mathbf{\Sigma}^{-1} N \boldsymbol{\mu}_{\mathrm{ML}}\right)
\end{aligned}
$$</p>
<p>となる。ただし、ここで$\displaystyle \sum_{n=1}^{N}\mathbf{x}<em>n=N\boldsymbol{\mu}</em>{\mathrm{ML}}$とした。</p>
<p>すなわち、事後分布$p(\boldsymbol{\mu} | \mathbf{X})$は$\mathcal{N}(\boldsymbol{\mu}|\boldsymbol{\mu}<em>N,\mathbf{\Sigma}</em>{N}) = \mathcal{N}(\boldsymbol{\mu}\mid \left(\mathbf{\Sigma}<em>{0}^{-1}+N \boldsymbol{\Sigma}^{-1}\right)^{-1}\left(\mathbf{\Sigma}</em>{0}^{-1} \boldsymbol{\mu}<em>{0}+\mathbf{\Sigma}^{-1} N \boldsymbol{\mu}</em>{\mathrm{ML}}\right), (\mathbf{\Sigma}_{0}^{-1}+N \mathbf{\Sigma}^{-1})^{-1})$の形で書ける。</p>
<h2 id="演習-241"><a class="header" href="#演習-241">演習 2.41</a></h2>
<div class="panel-primary">
<p>ガンマ関数</p>
<p>$$
\Gamma(a) \equiv \int_{0}^{\infty} u^{a-1} e^{-u} \mathrm{d} u \tag{1.141}
$$</p>
<p>の定義から，ガンマ分布</p>
<p>$$
\operatorname{Gam}(\lambda | a, b)=\frac{1}{\Gamma(a)} b^{a} \lambda^{a-1} \exp (-b \lambda) \tag{2.146}
$$</p>
<p>が正規化されていることを示せ．</p>
</div>
<p>ガンマ分布が正規化されている$\displaystyle \left(\int_0^{\infty}\frac{1}{\Gamma(a)} b^{a} \lambda^{a-1} \exp (-b \lambda) d\lambda = 1\right)$ことを示す。</p>
<p>これには$b\lambda = u$という変数変換を使うと（$d\lambda = b^{-1}du$）、</p>
<p>$$
\begin{aligned}
&amp;\int_0^{\infty}\frac{1}{\Gamma(a)} b^{a} \lambda^{a-1} \exp (-b \lambda) d\lambda \
=&amp;\frac{1}{\Gamma(a)}\int_0^{\infty}b^a\lambda^{a-1}\exp(-u)b^{-1}du \
=&amp;\frac{1}{\Gamma(a)}\int_0^{\infty}u^{a-1}e^{-u}du = \frac{\Gamma(a)}{\Gamma(a)} = 1
\end{aligned}
$$</p>
<p>となるので、正規化されていることが示された。</p>
<h2 id="演習-242"><a class="header" href="#演習-242">演習 2.42</a></h2>
<div class="panel-primary">
<p>ガンマ分布</p>
<p>$$
\operatorname{Gam}(\lambda | a, b)=\frac{1}{\Gamma(a)} b^{a} \lambda^{a-1} \exp (-b \lambda) \tag{2.146}
$$</p>
<p>の平均，分散，およびモードを求めよ．</p>
</div>
<p>まず，平均を求める．</p>
<p>$$
\begin{aligned}
\mathbb E \left[\lambda \right] =&amp; \int_0^{\infty} \frac{1}{\Gamma(a)} b^{a} \lambda^{a-1} \exp (-b \lambda) \lambda \mathrm{d} \lambda \
=&amp; \frac{1}{\Gamma(a)} \int_0^{\infty} b^{a} \lambda^a \exp (-b \lambda) \mathrm{d} \lambda \
\end{aligned}
$$</p>
<p>ここで$b\lambda = u$と変数変換すると，$\mathrm{d} \lambda = b^{-1} \mathrm{d} u$であるから</p>
<p>$$
\begin{aligned}
\mathbb E \left[\lambda \right] =&amp; \frac{1}{\Gamma(a)} \int_0^{\infty} u^a \exp (-u) b^{-1} \mathrm{d} u \
=&amp; \frac{1}{\Gamma(a)} b^{-1} \int_0^{\infty} u^a e^{-u} \mathrm{d} u \
=&amp; \frac{1}{\Gamma(a)} b^{-1} \Gamma(a+1) \
=&amp; \frac{1}{\Gamma(a)} b^{-1} a\Gamma(a) \
=&amp; \frac{a}{b}
\end{aligned}
$$</p>
<p>となる．</p>
<p>次に，分散を求める．</p>
<p>$$
\begin{aligned}
\mathbb E \left[\lambda^2 \right] =&amp; \int_0^{\infty} \frac{1}{\Gamma(a)} b^{a} \lambda^{a-1} \exp (-b \lambda) \lambda^2 \mathrm{d} \lambda \
=&amp; \frac{1}{\Gamma(a)} \int_0^{\infty} b^{a} \lambda^{a+1} \exp (-b \lambda) \mathrm{d} \lambda \
\end{aligned}
$$</p>
<p>ここで$b\lambda = u$と変数変換すると，$\mathrm{d} \lambda = b^{-1} \mathrm{d} u$であるから</p>
<p>$$
\begin{aligned}
\mathbb E \left[\lambda^2 \right] =&amp; \frac{1}{\Gamma(a)} \int_0^{\infty} b^{-1} u^{a+1} \exp (-u) b^{-1} \mathrm{d} u \
=&amp; \frac{1}{\Gamma(a)} b^{-2} \int_0^{\infty} u^{a+1} e^{-u} \mathrm{d} u \
=&amp; \frac{1}{\Gamma(a)} b^{-2} \Gamma(a+2) \
=&amp; \frac{1}{\Gamma(a)} b^{-2} (a+1)\Gamma(a+1) \
=&amp; \frac{1}{\Gamma(a)} b^{-2} a(a+1)\Gamma(a) \
=&amp; \frac{a(a+1)}{b^2}
\end{aligned}
$$</p>
<p>となる．したがって分散は</p>
<p>$$
\begin{aligned}
\operatorname{var}\left[\lambda \right] =&amp; \mathbb E \left[\lambda^2 \right] - \mathbb E \left[\lambda \right]^2 \
=&amp; \frac{a(a+1)}{b^2} - \frac{a^2}{b^2} \
=&amp; \frac{a}{b^2}
\end{aligned}
$$</p>
<p>となる．</p>
<p>次に，モードを求める．モードは分布の停留点を求めれば良いので$(2.146)$を$\lambda$で微分して0とおく．</p>
<p>$$
\begin{aligned}
&amp; \frac{\mathrm{d}}{\mathrm{d}\lambda} \operatorname{Gam}(\lambda | a, b) = 0 \
\Leftrightarrow ~ &amp; \frac{\mathrm{d}}{\mathrm{d}\lambda} \frac{1}{\Gamma(a)} b^{a} \lambda^{a-1} \exp (-b \lambda) = 0 \
\Leftrightarrow ~ &amp; \frac{\mathrm{d}}{\mathrm{d}\lambda} \lambda^{a-1} e^{-b \lambda} = 0 \
\Leftrightarrow ~ &amp; (a-1) \lambda^{a-2} e^{-b \lambda} + \lambda^{a-1} e^{-b \lambda} (-b) = 0 \
\Leftrightarrow ~ &amp; \lambda^{a-2} e^{-b \lambda} (a-1-b\lambda) = 0 \
\Leftrightarrow ~ &amp; \lambda = \frac{a-1}{b} \
\end{aligned}
$$</p>
<p>したがってモードは$\displaystyle \frac{a-1}{b}$となる．</p>
<h2 id="演習-243"><a class="header" href="#演習-243">演習 2.43</a></h2>
<div class="panel-primary">
<p>次の分布は，1変数ガウス分布を一般化したものである．</p>
<p>$$
p\left(x | \sigma^{2}, q\right)=\frac{q}{2\left(2 \sigma^{2}\right)^{1 / q} \Gamma(1 / q)} \exp \left(-\frac{|x|^{q}}{2 \sigma^{2}}\right) \tag{2.293}
$$</p>
<p>この分布が，次のように正規化されていることを示せ．</p>
<p>$$
\int_{-\infty}^{\infty} p\left(x | \sigma^{2}, q\right) \mathrm{d} x=1 \tag{2.294}
$$
そして,$q=2$で$(2.293)$がガウス分布となることを示せ．目的変数が$t=y(\mathbf{x}, \mathbf{w})+\epsilon$で，ランダムノイズ変数$\epsilon$が分布$(2.293)$に従う回帰モデルを考える．入力ベクトル集合$\mathbf{X} = { \mathbf{x}_1, \cdots, \mathbf{x}_N }$と，目的変数に相当する観測値が$\mathbf{t} = (t_1, \ldots, t_N)^{\mathrm{T}}$であるとき，$\mathbf{w}$と$\sigma^2$についての対数尤度関数が</p>
<p>$$
\ln p\left(\mathbf{t} | \mathbf{X}, \mathbf{w}, \sigma^{2}\right)=-\frac{1}{2 \sigma^{2}} \sum_{n=1}^{N}\left|y\left(\mathbf{x}<em>{n}, \mathbf{w}\right)-t</em>{n}\right|^{q}-\frac{N}{q} \ln \left(2 \sigma^{2}\right)+\mathrm{const} \tag{2.295}
$$</p>
<p>になることを示せ．ただし，$\mathrm{const}$は$\mathbf{w}$と$\sigma^2$の両方と独立な項である．なお，これは$\mathbf{w}$の関数として見た場合，1.5.5節で扱った$L_q$誤差関数である．</p>
</div>
<p>まず$(2.293)$について</p>
<p>$$
\begin{aligned}
\int_{-\infty}^{\infty} p(x | \sigma^2, q)dx &amp;= \int_{-\infty}^{\infty} \frac{q}{2\left(2 \sigma^{2}\right)^{1 / q} \Gamma(1 / q)} \exp \left(-\frac{|x|^{q}}{2 \sigma^{2}}\right) d x
\
&amp;=\frac{q}{2\left(2 \sigma^{2}\right)^{1 / q} \Gamma(1 / q)} \cdot 2 \int_{0}^{\infty} \exp \left(-\frac{|x|^{q}}{2 \sigma^{2}}\right) d x \
&amp;=\frac{q}{\left(2 \sigma^{2}\right)^{1 / q} \Gamma(1 / q)} \int_{0}^{\infty} \exp \left(-\frac{x^{q}}{2 \sigma^{2}}\right) d x
\end{aligned}
$$</p>
<p>ここで$\displaystyle u = \frac{x^q}{2\sigma^2}$とおくと、$\displaystyle du = \frac{q}{2\sigma^2}x^{q-1}dx$, $x=(2\sigma^2u)^{1/q}$となるので、</p>
<p>$$
\begin{aligned}
\int_{0}^{\infty} \exp \left(-\frac{x^{2}}{2 \sigma^{2}}\right) d x
&amp;=\int_{0}^{\infty} \frac{2 \sigma^{2}}{q x^{q-1}} e^{-u} du \
&amp;=\frac{2 \sigma^{2}}{q} \int_{0}^{\infty} \frac{1}{\left(2 \sigma^{2} u\right)^{\frac{q-1}{q}}} e^{-u} d u \
&amp;=\frac{2 \sigma^{2}}{q} \cdot \frac{\left(2 \sigma^{2}\right)^{\frac{1}{q}}}{2 \sigma^{2}} \int_{0}^{\infty} u^{\frac{1}{q}-1} e^{-u} d u \
&amp;=\frac{\left(2 \sigma^{2}\right)^{\frac{1}{q}}}{q} \Gamma(1 / q)
\end{aligned}
$$</p>
<p>よって</p>
<p>$$
\int_{-\infty}^{\infty} p\left(x \mid \sigma^{2}, q\right) d x=\frac{q}{\left(2 \sigma^{2}\right)^{1 / q} \Gamma(1 / q)} \cdot \frac{\left(2 \sigma^{2}\right)^{\frac{1}{q}}}{q} \Gamma(1 / q)=1
$$</p>
<p>正規化されていることが示された。
また、$q=2$のとき$\Gamma(1/2) = \sqrt{\pi}$を代入すれば</p>
<p>$$
\int_{-\infty}^{\infty} p\left(x \mid \sigma^{2}, 2 \right) dx = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left( -\frac{x^2}{2\sigma^2}\right)
$$</p>
<p>となり、ガウス分布の形になる。</p>
<p>最後に、ランダムノイズが$\epsilon$が$p(x|\sigma^2,q)$に従うとすると、$\epsilon = t-y(\mathbf{x}, \mathbf{w})$が$(2.293)$式の$x$に相当する。
$\mathbf{X} = {\mathbf{x}_1, \ldots, \mathbf{x}_N}$を入力として目的変数に相当する観測値が$\mathbf{t} = (t_1, \ldots, t_N)^{\mathrm{T}}$とするとき、1.2.5節での議論から</p>
<p>$$
p(\mathbf{t} | \mathbf{X}, \mathbf{w}, \sigma^2) = \prod_{n=1}^N p(t_n | \mathbf{x}_n, \mathbf{w}, \sigma^2)
$$</p>
<p>となるので、これの対数尤度は</p>
<p>$$
\begin{aligned}
\ln p\left(\mathbf{t} | \mathbf{X}, \mathbf{w}, \sigma^2 \right) &amp;=\ln \left{\prod_{n=1}^{N} \frac{q}{2\left(2 \sigma^{2}\right)^{1 / q} \Gamma(1 / q)} \exp \left(-\frac{\mid t_n-y\left(\mathbf{x}<em>n, \mathbf{w} \right) \mid^q}{2 \sigma^{2}}\right)\right } \
&amp;=-\frac{1}{2 \sigma^{2}} \sum</em>{n=1}^{N}\left|y\left(\mathbf{x}<em>n, \mathbf{w} \right)-t</em>{n}\right|^{q}-\frac{N}{q} \ln \left(2 \sigma^{2}\right)+\operatorname{const}
\end{aligned}
$$</p>
<p>となる($\textrm{const}$は$\mathbf{w}$と$\sigma^2$の両方に独立な項である)。</p>
<blockquote>
<p>$(2.295)$を、パラメータ $\mathbf w$ についての式と見れば、1.5.5節の$L_q$損失(ミンコフスキー損失)と同値になります。</p>
<p>1.2.5節にあるように、回帰において、二乗損失($L_2$損失)の総和を最小化するパラメータ $\mathbf w$ を求める（＝最小二乗法）ことと、データのノイズにガウス分布を仮定して最尤推定によりパラメータ $\mathbf w$を求めることとは、$\mathbf w$ を決定する意味では結果的に同じになりました。</p>
<p>これと同様に、回帰問題において、$L_q$損失を最小化することと、ノイズに一般化ガウス分布を仮定して最尤推定することとは、$\mathbf w$を決定する意味においては同じ事になります。</p>
</blockquote>
<h2 id="演習-244"><a class="header" href="#演習-244">演習 2.44</a></h2>
<div class="panel-primary">
<p>1変数ガウス分布$\mathcal{N}\left(x | \mu, \tau^{-1}\right)$について考える．共役事前分布はガウス–ガンマ分布</p>
<p>$$
p(\mu, \lambda)=\mathcal{N}\left(\mu \mid \mu_{0},(\beta \lambda)^{-1}\right) \operatorname{Gam}(\lambda \mid a, b) \tag{2.154}
$$</p>
<p>で，独立同分布な観測値集合が$\mathbf{x}=\left{x_{1}, \ldots, x_{N}\right}$であるとする．事後分布も，事前分布と同じガウスーガンマ分布になることを示し，各パラメータに対する事後分布の式を書き下せ．</p>
</div>
<p>事後分布は$p(\mu,\lambda|\mathbf{X})$、尤度関数は$p(\mathbf{X}|\mu, \lambda)$と書ける。
このとき共役事前分布$p(\mu,\lambda)$と合わせて、ベイズの定理から
$$
p(\mu,\lambda|\mathbf{X}) \propto p(\mathbf{X}|\mu, \lambda)p(\mu,\lambda)
$$
と書ける。また、$(2.152)$式のように尤度関数は</p>
<p>$$
\begin{aligned}
p(\mathbf{X} \mid \mu, \lambda)&amp;=\prod_{n=1}^{N}\left(\frac{\lambda}{2 \pi}\right)^{1 / 2} \exp \left{-\frac{\lambda}{2}\left(x_{n}-\mu\right)^{2}\right} \
&amp;\propto\left[\lambda^{1 / 2} \exp \left(-\frac{\lambda \mu^{2}}{2}\right)\right]^{N} \exp \left{\lambda \mu \sum_{n=1}^{N} x_{n}-\frac{\lambda}{2} \sum_{n=1}^{N} x_{n}^{2}\right}
\end{aligned}
$$</p>
<p>の形で書ける。これと</p>
<p>$$
p(\mu, \lambda) \propto (\beta\lambda)^{1/2}\exp\left( -\frac{\beta\lambda}{2}(\mu-\mu_0)^2 \right)\lambda^{a-1}\exp(-b\lambda)
$$</p>
<p>を利用して、事後分布$p(\mu,\lambda|\mathbf{X})$の指数部を計算してみてガウス-ガンマ分布のそれと同型になることを示せば良い（正規化定数を求めるのはキツイ）。</p>
<p>$$
\begin{aligned}
p(\mu,\lambda|\mathbf{X}) &amp;= \lambda^{\frac{N}{2}}(\beta \lambda)^{\frac{1}{2}} \lambda^{a-1} \exp \left{-\frac{N \lambda \mu^{2}}{2}-\frac{\beta \lambda}{2}\left(\mu-\mu_{0}\right)^{2}-b \lambda+\lambda \mu \sum_{n=1}^{N} x_{n}-\frac{\lambda}{2} \sum_{n=1}^{N} x_{n}^{2}\right} \
&amp;= \beta^{\frac{1}{2}} \lambda^{\frac{N}{2}+\frac{1}{2}+a-1} \exp \left{\mu^{2} \left( -\frac{N \lambda}{2}-\frac{\beta \lambda}{2}\right)+\mu\left(\beta \lambda \mu_{0}+\lambda \sum_{n=1}^{N} x_{n}\right)-b \lambda-\frac{\lambda}{2} \sum_{n=1}^{N} x_{n}^{2}-\frac{\beta \lambda}{2} \mu_{0}^{2}\right} \
&amp;=\beta^{\frac{1}{2}} \lambda^{\frac{N}{2}+\frac{1}{2}+a-1} \exp \left{ -\frac{\lambda(N+\beta)}{2} \mu^{2}+\mu \lambda\left(\beta \mu_{0}+\sum_{n=1}^{N} x_{n}\right)-\lambda\left(b+\frac{1}{2} \sum_{n=1}^{N} x_{n}^{2}-\frac{\beta}{2} \mu_{0}^{2}\right)\right} \
&amp;=\beta^{\frac{1}{2}} \lambda^{\frac{N}{2}+\frac{1}{2}+a-1} \exp \left{-\frac{\lambda(N+\beta)}{2}\left(\mu-\frac{\beta\mu_0+\sum_{n=1}^{N} x_{n}}{N+\beta}\right)^{2}\right}
\exp \left{-\lambda\left(b+\frac{1}{2} \sum_{n=1}^N x_{n}^{2}+\frac{\beta}{2} \mu_{0}^{2}-\frac{\left(\beta\mu_0+\sum_{n=1}^{N} x_{n}\right)^{2}}{2(N+\beta)}\right)\right} \
&amp;\propto (\lambda(N+\beta))^{\frac{1}{2}}\exp \left{-\frac{\lambda(N+\beta)}{2}\left(\mu-\frac{\beta\mu_0+\sum_{n=1}^{N} x_{n}}{N+\beta}\right)^{2}\right}\lambda^{a+\frac{N}{2}-1}\exp \left{-\left(b+\frac{1}{2} \sum_{n=1}^N x_{n}^{2}+\frac{\beta}{2} \mu_{0}^{2}-\frac{\left(\beta\mu_0+\sum_{n=1}^{N} x_{n}\right)^{2}}{2(N+\beta)}\right)\lambda\right}
\end{aligned}
$$</p>
<p>これは
$$
\begin{aligned}
\mu_{N} &amp;=\frac{\beta \mu_{0}+\sum_{n=1}^{N} x_{n}}{N+\beta} \
\lambda_{N}^{-1} &amp;= (\lambda(N+\beta))^{-1} \
a_{N} &amp;=a+\frac{N}{2} \
b_{N} &amp;=b+\frac{1}{2} \sum_{n=1}^N x_{n}^{2}+\frac{\beta}{2} \mu_{0}^{2}-\frac{\left(\beta\mu_0+\sum_{n=1}^{N} x_{n}\right)^{2}}{2(N+\beta)}
\end{aligned}
$$
としたときの$\mathcal{N}(\mu|\mu_N,\lambda_N^{-1})\operatorname{Gam}(\lambda|a_N, b_N)$の指数部分と同型になる。</p>
<p>したがって、事後分布もガウス-ガンマ分布の形になっていることが示された。</p>
<h2 id="演習-245"><a class="header" href="#演習-245">演習 2.45</a></h2>
<div class="panel-primary">
<p>$$
\mathcal{W}(\mathbf{\Lambda} \mid \mathbf{W}, \nu)=B|\mathbf{\Lambda}|^{(\nu-D-1) / 2} \exp \left(-\frac{1}{2} \operatorname{Tr}\left(\mathbf{W}^{-1} \mathbf{\Lambda}\right)\right) \tag{2.155}
$$</p>
<p>で定義されたウィシャート分布が確かに，多変量ガウス分布の精度行列の共役事前分布であることを確かめよ．</p>
</div>
<p>事前分布と尤度と掛け合わせて事後分布を求めたとき，目的とする変数についての形式が変わらないような事前分布が共役事前分布である．したがって，尤度を求め，精度$\mathbf \Lambda$についてウィシャート分布の形になっていることを確認すれば良い．</p>
<p>$\mathbf{X} = {\mathbf{x}_1, \mathbf{x}_2, \cdots,\mathbf{x}_N}$が与えられたとき，精度$\mathbf \Lambda$のガウス分布$\mathcal{N}(\mathbf{X}|\boldsymbol{\mu},\mathbf{\Lambda}^{-1})$の尤度関数は</p>
<p>$$
\begin{aligned}
&amp; \prod_{n=1}^N \mathcal{N}(\mathbf{x}<em>n|\boldsymbol{\mu},\mathbf{\Lambda}^{-1}) \
\propto &amp; |\mathbf{\Lambda}|^{\frac{1}{2}} \exp{\left( -\frac{1}{2} \sum</em>{n=1}^N (\mathbf{x}_n - \boldsymbol{\mu})^{\mathrm{T}} \mathbf{\Lambda} (\mathbf{x}<em>n - \boldsymbol{\mu}) \right)} \
=&amp; |\mathbf{\Lambda}|^{\frac{1}{2}} \exp{\left( -\frac{1}{2} \sum</em>{n=1}^N \operatorname{Tr} \left[(\mathbf{x}_n - \boldsymbol{\mu})^{\mathrm{T}} \mathbf{\Lambda} (\mathbf{x}<em>n - \boldsymbol{\mu})  \right]\right)} \
=&amp; |\mathbf{\Lambda}|^{\frac{1}{2}} \exp{\left( -\frac{1}{2} \sum</em>{n=1}^N \operatorname{Tr} \left[(\mathbf{x}_n - \boldsymbol{\mu}) (\mathbf{x}<em>n - \boldsymbol{\mu})^{\mathrm{T}} \mathbf{\Lambda} \right]\right)} \
=&amp; |\mathbf{\Lambda}|^{\frac{1}{2}} \exp{\left( -\frac{1}{2} \operatorname{Tr} \left[ \sum</em>{n=1}^N (\mathbf{x}_n - \boldsymbol{\mu}) (\mathbf{x}_n - \boldsymbol{\mu})^{\mathrm{T}} \mathbf{\Lambda} \right]\right)} \
=&amp; |\mathbf{\Lambda}|^{\frac{1}{2}} \exp{\left( -\frac{1}{2} \operatorname{Tr} \left[\mathrm{S} \mathbf{\Lambda} \right]\right)} \
\end{aligned}
$$</p>
<p>となり，ウィシャート分布と同じ形であることがわかる．ただし$\displaystyle \sum_{n=1}^N (\mathbf{x}_n - \boldsymbol{\mu}) (\mathbf{x}_n - \boldsymbol{\mu})^{\mathrm{T}} = \mathrm{S}$とおいた．</p>
<p>以上より，ウィシャート分布$(2.156)$は多変量ガウス分布の精度行列の共役事前分布となっていることが確認できた．</p>
<h2 id="演習-246"><a class="header" href="#演習-246">演習 2.46</a></h2>
<div class="panel-primary">
<p>$$
\begin{aligned}
p(x|\mu,a,b)&amp;=\int_{0}^{\infty} \mathcal{N} (x | \mu, \tau^{-1}) \operatorname{Gam}(\tau | a, b) d \tau \
&amp;=\int_{0}^{\infty} \frac{b^{a} e^{(-b \tau)} \tau^{a-1}}{\Gamma(a)}\left(\frac{\tau}{2 \pi}\right)^{1 / 2} \exp \left{-\frac{\tau}{2}(x-\mu)^{2}\right} \mathrm{d} \tau\
&amp;=\frac{b^{a}}{\Gamma(a)}\left(\frac{1}{2 \pi}\right)^{\frac{1}{2}} \int_{0}^{\infty} \tau^{a-\frac{1}{2}} \exp \left{-\tau\left(b+\frac{(x-\mu)^{2}}{2}\right)\right} d \tau \tag{2.146}
\end{aligned}
$$</p>
<p>の積分を計算し</p>
<p>$$
\operatorname{St}(x \mid \mu, \lambda, \nu)=\frac{\Gamma(\nu / 2+1 / 2)}{\Gamma(\nu / 2)}\left(\frac{\lambda}{\pi \nu}\right)^{1 / 2}\left[1+\frac{\lambda(x-\mu)^{2}}{\nu}\right]^{-\nu / 2-1 / 2} \tag{2.159}
$$</p>
<p>になることを確かめよ．</p>
</div>
<p>まず$(2.158)$式に$\displaystyle \operatorname{Gam} ( \lambda |a, b)=\frac{1}{r(a)} b^{a} \lambda^{a-1} \exp (-b \lambda), \quad \Gamma(x)=\int_{0}^{\infty} u^{x-1} e^{-u} dx$を適用する。</p>
<p>$$
\begin{aligned}
&amp;\int_{0}^{\infty} \mathcal{N} (x | \mu, \tau^{-1}) \operatorname{Gam}(\tau | a, b) d \tau \
=&amp;\int_{0}^{\infty} \frac{\sqrt{\tau}}{\sqrt{2 \pi}} \exp \left{ -\frac{\tau}{2}(x-\mu)^{2}\right} \frac{b^{a}}{\Gamma(a)} \tau^{a-1} \exp (-b \tau) d \tau \
=&amp;\frac{b^{a}}{\Gamma(a)}\left(\frac{1}{2 \pi}\right)^{\frac{1}{2}} \int_{0}^{\infty} \tau^{a-\frac{1}{2}} \exp \left{-\tau\left(b+\frac{(x-\mu)^{2}}{2}\right)\right} d \tau
\end{aligned}
$$
ここで、$\displaystyle b+\frac{(x-\mu)^2}{2}=c$とおき、$\tau c = u$とすると、$d\tau = c^{-1}du$</p>
<p>$$
\begin{aligned}
=&amp;\frac{b^{a}}{\Gamma(a)}\left(\frac{1}{2 \pi}\right)^{\frac{1}{2}} \int_{0}^{\infty}\left(\frac{u}{c}\right)^{a-\frac{1}{2}} \exp (-u) \cdot c^{-1} d u \
=&amp;\frac{b^{a}}{\Gamma(a)}\left(\frac{1}{2 \pi}\right)^{\frac{1}{2}} c^{-a-\frac{1}{2}} \int_{0}^{\infty} u^{a-\frac{1}{2}} e^{-u} d u \
=&amp;\frac{b^{a}}{\Gamma(a)}\left(\frac{1}{2 \pi}\right)^{\frac{1}{2}}\left[b+\frac{(x-\mu)^{2}}{2}\right]^{-a-\frac{1}{2}} \Gamma\left(a+\frac{1}{2}\right)
\end{aligned}
$$</p>
<p>よって$(2.158)$の式変形が示された。次に$\displaystyle \nu=2a,\quad \lambda=\frac{a}{b}$とおくと, $\displaystyle b^a = \left( \frac{\nu}{2\lambda} \right)^\frac{\nu}{2}= \left( \frac{2\lambda}{\nu} \right)^{- \frac{\nu}{2}}$</p>
<p>$$
\begin{aligned}
&amp;=\frac{\Gamma\left(\frac{\nu}{2}+\frac{1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)}\left(\frac{1}{2 \pi}\right)^{\frac{1}{2}}\left( \frac{2\lambda}{\nu} \right)^{- \frac{\nu}{2}}\left[\frac{\nu}{2 \lambda}+\frac{(x-\mu)^{2}}{2}\right]^{-\frac{\nu}{2}-\frac{1}{2}} \
&amp;=\frac{\Gamma\left(\frac{\nu}{2}+\frac{1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)} \left(\frac{\lambda}{\pi \nu}\right)^{\frac{1}{2}}  \left(\frac{2 \lambda}{\nu} \cdot \frac{\nu+\lambda(x-\mu)^{2}}{2 \lambda}\right)^{-\frac{\nu}{2}-\frac{1}{2}} \
&amp;=\frac{\Gamma\left(\frac{\nu}{2}+\frac{1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)}\left(\frac{\lambda}{\pi \nu}\right)^{\frac{1}{2}}\left[1+\frac{\lambda(x-\mu)^{2}}{\nu}\right]^{-\frac{\nu}{2}-\frac{1}{2}}
\end{aligned}
$$
となり、$(2.159)$式に変形できることが示された。</p>
<h2 id="演習-247"><a class="header" href="#演習-247">演習 2.47</a></h2>
<div class="panel-primary">
<p>$\nu \to \infty$の極限で，t分布</p>
<p>$$
\operatorname{St}(x \mid \mu, \lambda, \nu)=\frac{\Gamma(\nu / 2+1 / 2)}{\Gamma(\nu / 2)}\left(\frac{\lambda}{\pi \nu}\right)^{1 / 2}\left[1+\frac{\lambda(x-\mu)^{2}}{\nu}\right]^{-\nu / 2-1 / 2} \tag{2.159}
$$</p>
<p>がガウス分布になることを示せ．ヒント: 正規化係数を無視し，$x$への依存性だけに注目する．</p>
</div>
<p>スチューデントのt分布が$\nu \to \infty$の極限で$\displaystyle \mathcal{N}(x|\mu, \lambda^{-1})=\sqrt{\frac{\lambda}{2\pi}} \exp \left{ -\frac{\lambda(x-\mu)^2}{2} \right}$になることを示す。</p>
<p>$\displaystyle \left[1+\frac{\lambda(x-\mu)^{2}}{\nu}\right]^{-\frac{\nu}{2}-\frac{1}{2}}$の部分を$\exp(\cdot)$で表すように変形させることを意識する。また、$\displaystyle \frac{\Gamma\left(\frac{\nu}{2}+\frac{1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)\left(\frac{\nu}{2}\right)^{1/2}}$が$\nu \to \infty$の極限で1に収束することを示す。このために$\displaystyle \lim_{n\to\infty}\frac{\Gamma(x+n)}{\Gamma(n)n^x} = 1$であることを示す（※公式の解答集や問題文のヒントから、ここはどうせ規格化定数で吸収するので考えなくても良いことになっているけれど、一応示す）。</p>
<p>まずガンマ関数の定義から</p>
<p>$$
\Gamma(x) = \lim_{n\to\infty}\frac{(n-1)!n^x}{\prod_{k=0}^{n-1}(x+k)}
$$</p>
<p>逆数をとって</p>
<p>$$
\frac{1}{\Gamma(x)}=\lim_{n\to\infty}\frac{\prod_{k=0}^{n-1}(x+k)}{(n-1)!n^x}
$$</p>
<p>よって</p>
<p>$$
\begin{aligned}
1 &amp;= \lim_{n\to\infty}\Gamma(x)\frac{\prod_{k=0}^{n-1}(x+k)}{(n-1)!n^x} \
&amp;= \lim_{n\to\infty}\frac{\Gamma(x)\Gamma(x+n)}{\Gamma(n)\Gamma(x)n^x} \
&amp;= \lim_{n\to\infty}\frac{\Gamma(x+n)}{\Gamma(n)n^x}
\end{aligned}
$$</p>
<p>ここで、$x = 1/2, n=\nu/2$とすれば、$\displaystyle \lim_{n\to\infty}\frac{\Gamma\left(\frac{\nu}{2}+\frac{1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)\left(\frac{\nu}{2}\right)^{1/2}} = 1$となることが示された。</p>
<p>ちなみにガンマ関数の定義をWikipediaにならって</p>
<p>$$
\Gamma(x) = \lim_{n\to\infty}\frac{n!n^x}{\prod_{k=0}^{n}(x+k)}
$$</p>
<p>とした場合、上の式変形を進めていくと</p>
<p>$$
1 = \lim_{n\to\infty}\left( 1+\frac{x}{n+1}\right)\frac{\Gamma(x+n)}{\Gamma(n)n^x}
$$</p>
<p>となる。$n\to\infty$で$\displaystyle 1+\frac{x}{n+1} \to 1$となることを使えば結果は同様となる（ということであってるかな……？）。</p>
<p>続いて$\displaystyle \left[1+\frac{\lambda(x-\mu)^{2}}{\nu}\right]^{-\frac{\nu}{2}-\frac{1}{2}}$ について</p>
<p>$$
\left[1+\frac{\lambda(x-\mu)^{2}}{\nu}\right]^{-\frac{\nu}{2}-\frac{1}{2}}= \exp \left{ -\frac{\nu + 1}{2}\ln \left( 1+ \frac{\lambda(x-\mu)^2}{\nu}\right)\right}
$$</p>
<p>ここでテイラー展開$\ln (1+\epsilon) = \epsilon+O(\epsilon^2)$を用いると</p>
<p>$$
\begin{aligned}
&amp;\exp \left{ -\frac{\nu + 1}{2}\ln \left( 1+ \frac{\lambda(x-\mu)^2}{\nu}\right)\right} \
=&amp;\exp\left{ -\frac{\nu+1}{\nu}\cdot \left( \frac{\lambda(x-\mu)^2}{2}+O(\nu^{-2})\right)\right} \
=&amp;\exp\left{ -\frac{\nu+1}{\nu}\cdot \frac{\lambda(x-\mu)^2}{2}+O(\nu^{-1})\right}
\end{aligned}
$$</p>
<p>$\nu \to \infty$の極限で、上式は$\displaystyle \exp\left{ -\frac{\lambda(x-\mu)^2}{2}\right}$となる。</p>
<p>したがって以上をまとめると、$\nu \to \infty$の極限で</p>
<p>$$
\begin{aligned}
&amp;\lim_{\nu\to\infty}\frac{\Gamma\left(\frac{\nu}{2}+\frac{1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)}\left(\frac{\lambda}{\pi \nu}\right)^{\frac{1}{2}}\left[1+\frac{\lambda(x-\mu)^{2}}{\nu}\right]^{-\frac{\nu}{2}-\frac{1}{2}} \
=&amp;\lim_{\nu\to\infty}\frac{\Gamma\left(\frac{\nu}{2}+\frac{1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)\left(\frac{\nu}{2}\right)^\frac{1}{2}} \left(\frac{\lambda}{2\pi}\right)^{\frac{1}{2}}\left[1+\frac{\lambda(x-\mu)^{2}}{\nu}\right]^{-\frac{\nu}{2}-\frac{1}{2}} \
=&amp;1\cdot\left(\frac{\lambda}{2\pi}\right)^{\frac{1}{2}}\exp\left{ -\frac{\lambda(x-\mu)^2}{2}\right} \
=&amp;\mathcal{N}(x|\mu,\lambda^{-1})
\end{aligned}
$$</p>
<p>となる。よってガウス分布となることが示された。</p>
<p><strong>別解</strong></p>
<p>ネイピア数の定義($\lim_{n\to\infty}(1 + n^{-1})^{n}=e$)に基づいて示す方法</p>
<p>$$
\begin{aligned}
\lim_{\nu\to\infty}\left[1+\frac{\lambda(x-\mu)^{2}}{\nu}\right]^{-\frac{\nu+1}{2}}=&amp; \lim_{\nu\to\infty}\left{\left[1+\frac{\lambda(x-\mu)^{2}}{\nu}\right]^{\frac{\nu}{\lambda(x-\mu)^{2}}}\right}^{\frac{\lambda(x-\mu)^{2}}{\nu}({-\frac{\nu+1}{2}})}\
=&amp;\displaystyle \exp\left{ -\frac{\lambda(x-\mu)^2}{2}\right}
\end{aligned}
$$</p>
<p>{} の中身は $\nu\to\infty$ で exp になり {} の外側の指数部分は $\nu\to\infty$ で $\frac{\lambda(x-\mu)^{2}}{2}$ となることを用いた</p>
<h2 id="演習-248"><a class="header" href="#演習-248">演習 2.48</a></h2>
<div class="panel-primary">
<p>1変数のスチューデントのt分布</p>
<p>$$
\operatorname{St}(x \mid \mu, \lambda, \nu)=\frac{\Gamma(\nu / 2+1 / 2)}{\Gamma(\nu / 2)}\left(\frac{\lambda}{\pi \nu}\right)^{1 / 2}\left[1+\frac{\lambda(x-\mu)^{2}}{\nu}\right]^{-\nu / 2-1 / 2} \tag{2.159}
$$</p>
<p>の導出で用いたのと同様の手続きで，スチューデントのt分布の多変量形式である</p>
<p>$$
\operatorname{St}(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Lambda}, \nu)=\frac{\Gamma(D / 2+\nu / 2)}{\Gamma(\nu / 2)} \frac{|\mathbf{\Lambda}|^{1 / 2}}{(\pi \nu)^{D / 2}}\left[1+\frac{\Delta^{2}}{\nu}\right]^{-D / 2-\nu / 2} \tag{2.162}
$$</p>
<p>の結果を，</p>
<p>$$
\operatorname{St}(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Lambda}, \nu)=\int_{0}^{\infty} \mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu},(\eta \mathbf{\Lambda})^{-1}\right) \operatorname{Gam}(\eta \mid \nu / 2, \nu / 2) \mathrm{d} \eta \tag{2.161}
$$</p>
<p>の変数$\eta$を周辺化することで確かめよ．定義$(2.161)$を用いて，積分変数の順序を交換することで，多変量t分布が正規化されていることを示せ．</p>
</div>
<p>$(2.161)$の積分表現から$(2.162)$を得ることを目的とする。途中で以下の定理を用いる。</p>
<blockquote>
<p>任意の$n\times n$行列$\mathbf{A}$と任意のスカラー値$k$に対して
$|k\mathbf{A}| = k^n|\mathbf{A}|$
が成り立つ（統計のための行列代数P.217, 系13.2.4）</p>
</blockquote>
<p>途中でマハラノビス距離$\Delta^2 = (\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}}\mathbf{\Lambda}(\mathbf{x}-\boldsymbol{\mu})$を用いると</p>
<p>$$
\begin{aligned}
\operatorname{St}(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Lambda}, v)
&amp;=\int_{0}^{\infty} \mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu},(\eta \mathbf{\Lambda})^{-1}\right) \operatorname{Gam}\left(\eta \left| \frac{\nu}{2}, \frac{\nu}{2}\right.\right) d \eta \
&amp;=\int_{0}^{\infty} \frac{1}{(2 \pi)^{D/2}}|\eta \mathbf{\Lambda}|^{\frac{1}{2}} \exp \left{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}}(\eta \mathbf{\Lambda})(\mathbf{x}-\boldsymbol{\mu})\right} \frac{1}{\Gamma\left(\frac{\nu}{2}\right)}\left(\frac{\nu}{2}\right)^{\frac{\nu}{2}} \eta^{\frac{\nu}{2}-1} \exp \left{-\frac{\nu}{2} \eta\right} d\eta \
&amp;=\frac{|\mathbf{\Lambda}|^{\frac{1}{2}}}{(2 \pi)^{D/2}} \frac{\left(\frac{\nu}{2}\right)^{\nu/2}}{\Gamma\left(\frac{\nu}{2}\right)} \int_{0}^{\infty} \eta^{\frac{D}{2}} \cdot \eta^{\frac{\nu}{2}-1} \exp \left{-\frac{\nu}{2} \eta-\frac{\eta}{2} \Delta^{2}\right} d\eta
\end{aligned}
$$
ここで$\displaystyle \tau = \left( \frac{\nu}{2}+\frac{\Delta^2}{2} \right)$とおくと、$\displaystyle d\tau = \left( \frac{\nu}{2}+\frac{\Delta^2}{2}\right)d\eta$で、</p>
<p>$$
\begin{aligned}
(与式)
&amp;=\frac{|\mathbf{\Lambda}|^{\frac{1}{2}}}{(2 \pi)^{D/2}} \frac{\left(\frac{\nu}{2}\right)^{\nu/2}}{\Gamma\left(\frac{\nu}{2}\right)}\left(\frac{\nu}{2}+\frac{\Delta^{2}}{2}\right)^{-\frac{D}{2}-\frac{\nu}{2}} \int_{0}^{\infty} \tau^{\frac{D}{2}+\frac{\nu}{2}-1} e^{-\tau} d \tau \
&amp;=\frac{|\mathbf{\Lambda}|^{\frac{1}{2}}}{(2 \pi)^{D/2}} \frac{\left(\frac{\nu}{2}\right)^{\nu/2}}{\Gamma\left(\frac{\nu}{2}\right)}\left(\frac{\nu}{2}+\frac{\Delta^{2}}{2}\right)^{-\frac{D}{2}-\frac{\nu}{2}} \Gamma\left(\frac{D}{2}+\frac{\nu}{2}\right) \
&amp;=\frac{\Gamma\left(\frac{D}{2}+\frac{\nu}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)} \cdot \frac{\left(\frac{\nu}{2}\right)^{\nu/2}}{(2 \pi)^{D/2}} | \mathbf{\Lambda} |^{\frac{1}{2}}\left(\frac{\nu}{2}\right)^{-\frac{D}{2}-\frac{\nu}{2}}\left[1+\frac{\Delta^{2}}{\nu}\right]^{-\frac{D}{2}-\frac{\nu}{2}} \
&amp;=\frac{\Gamma\left(\frac{D}{2}+\frac{\nu}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)} \frac{|\mathbf{\Lambda}|^{\frac{1}{2}}}{(\nu \pi)^{D/2}}\left[1+\frac{\Delta^{2}}{\nu}\right]^{-\frac{D}{2}-\frac{\nu}{2}}
\end{aligned}
$$</p>
<p>よって$(2.162)$式が得られた。</p>
<p>また、この多変数t分布が正規化されていることを示す。すなわち、</p>
<p>$$
\int_{-\infty}^{\infty}\operatorname{St}(\mathbf{x}|\boldsymbol{\mu},\mathbf{\Lambda},\nu) d\mathbf{x} = 1
$$</p>
<p>であることを示す。（問題文のヒントから）積分が交換可能であることを利用すると、定義から</p>
<p>$$
\begin{aligned}
&amp; \int_{-\infty}^{\infty} \operatorname{St}(\mathbf{x}|\boldsymbol{\mu},\mathbf{\Lambda},\nu) d\mathbf{x} \
=&amp; \int_{-\infty}^{\infty} \int_{0}^{\infty} \mathcal{N}\left(\mathbf{x} | \boldsymbol{\mu},\left(\eta\mathbf{\Lambda}^{-1}\right)\right) \operatorname{Gam}\left(\eta \left| \frac{\nu}{2}, \frac{\nu}{2}\right.\right) d\eta d\mathbf{x} \
=&amp; \int_{0}^{\infty} \underbrace{\int_{-\infty}^{\infty} \mathcal{N}\left(\mathbf{x} | \boldsymbol{\mu},\left(\eta \mathbf{\Lambda}^{-1}\right)\right) d\mathbf{x}}<em>{1} \operatorname{Gam}\left(n \left| \frac{\nu}{2}, \frac{\nu}{2}\right.\right) d \eta \
=&amp; \int</em>{0}^{\infty} \operatorname{Gam}\left(\eta \left| \frac{\nu}{2}, \frac{\nu}{2}\right.\right) d\eta \
=&amp; 1
\end{aligned}
$$</p>
<p>ガンマ分布が正規化されていることは演習問題2.41で示した。</p>
<h2 id="演習-249"><a class="header" href="#演習-249">演習 2.49</a></h2>
<div class="panel-primary">
<p>ガウス分布とガンマ分布のたたみ込みである多変量スチューデントt分布の定義$(2.161)$を用いて，$(2.162)$で定義される多変量t分布の平均，共分散，およびモード</p>
<p>$$
\mathbb{E}[\mathbf{x}] = \boldsymbol{\mu} \hspace{2em} (\mu &gt; 1のとき)\tag{2.164}
$$</p>
<p>$$
\operatorname{cov}[\mathbf{x}] = \frac{\nu}{\nu -2} \mathbf{\Lambda}^{-1} \hspace{2em} (\mu &gt; 2のとき)\tag{2.165}
$$</p>
<p>$$
\operatorname{mode}[\mathbf{x}] = \boldsymbol{\mu} \tag{2.166}
$$</p>
<p>を確かめよ．</p>
</div>
<p>$$
\begin{aligned}
\mathbb{E}[\mathbf{x}] &amp;= \int_{0}^{\infty} \underbrace{\int
\mathcal{N}\left(\mathbf{x} | \boldsymbol{\mu},\left(\eta\mathbf{\Lambda}\right)^{-1}\right) \mathbf{x} d\mathbf{x}}<em>{\boldsymbol{\mu}(\because{(2.58)式})}\operatorname{Gam}\left(\eta \left| \frac{\nu}{2}, \frac{\nu}{2}\right.\right) d\eta  \
&amp;= \int</em>{0}^{\infty} \boldsymbol{\mu}\operatorname{Gam}\left(\eta \left| \frac{\nu}{2}, \frac{\nu}{2}\right.\right) d\eta \
&amp;=\boldsymbol{\mu} (\because 演習2.41)
\end{aligned}
$$</p>
<hr />
<p>$$
\begin{aligned}
\operatorname{cov}[\mathbf{x}] &amp;= \int\int_{0}^{\infty}
\mathcal{N}\left(\mathbf{x} | \boldsymbol{\mu},\left(\eta\mathbf{\Lambda}\right)^{-1}\right) \operatorname{Gam}\left(\eta \left| \frac{\nu}{2}, \frac{\nu}{2}\right.\right) d\eta (\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^\mathrm{T} d\mathbf{x} \
&amp;= \int_{0}^{\infty}\underbrace{\int
\mathcal{N}\left(\mathbf{x} | \boldsymbol{\mu},\left(\eta\mathbf{\Lambda}\right)^{-1}\right) (\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^\mathrm{T} d\mathbf{x}}<em>{\left(\eta\mathbf{\Lambda}\right)^{-1}} \operatorname{Gam}\left(\eta \left| \frac{\nu}{2}, \frac{\nu}{2}\right.\right) d\eta  \
&amp;=\int</em>{0}^{\infty}\left(\eta\mathbf{\Lambda}\right)^{-1} \frac{1}{\Gamma(\frac{\nu}{2})}\left(\frac{\nu}{2}\right)^\frac{\nu}{2}\eta^{\left(\frac{\nu}{2}-1\right)} e^{{-\frac{\nu}{2}}\eta} d\eta \
&amp;=\mathbf{\Lambda}^{-1} \frac{\frac{\nu}{2}^\frac{\nu}{2}}{\Gamma(\frac{\nu}{2})} \int_{0}^{\infty}\eta^{\left(\frac{\nu}{2}-2\right)}e^{-\frac{\nu}{2}\eta}d\eta \\
&amp;ここで\frac{\nu}{2}\eta = uとおく。d\eta=\frac{2}{\nu}du \\
&amp;=\mathbf{\Lambda}^{-1} \frac{\frac{\nu}{2}^\frac{\nu}{2}}{\Gamma(\frac{\nu}{2})} \int_{0}^{\infty}\left(\frac{2u}{\nu}\right)^{\left(\frac{\nu}{2}-2\right)}e^{-u}\frac{2}{\nu}du \
&amp;=\mathbf{\Lambda}^{-1} \frac{\frac{\nu}{2}^\frac{\nu}{2}}{\Gamma(\frac{\nu}{2})}\left(\frac{2}{\nu}\right)^{\left(\frac{\nu}{2}-1\right)} \underbrace{\int_{0}^{\infty}u^{\left(\frac{\nu}{2}-2\right)}e^{-u}du}_{\Gamma(\frac{\nu}{2}-1)} \
&amp;=\mathbf{\Lambda}^{-1} \frac{\Gamma(\frac{\nu}{2}-1)}{\Gamma(\frac{\nu}{2})}\frac{\nu}{2} \
&amp;=\mathbf{\Lambda}^{-1} \frac{\Gamma(\frac{\nu}{2}-1)}{\Gamma(\frac{\nu}{2}-1)\left(\frac{\nu}{2}-1\right)}\frac{\nu}{2}\
&amp;=\frac{\nu}{\nu-2}\Lambda^{-1}\
\end{aligned}
$$</p>
<hr />
<p>$mode[x]=\boldsymbol{\mu}$ を示す</p>
<p>$$
\begin{aligned}\
\operatorname{St}(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Lambda}, \nu)=\frac{\Gamma(D / 2+\nu / 2)}{\Gamma(\nu / 2)} \frac{|\mathbf{\Lambda}|^{1 / 2}}{(\pi \nu)^{D / 2}}\left[1+\frac{\Delta^{2}}{\nu}\right]^{-D / 2-\nu / 2}\
\end{aligned}
$$</p>
<p>このようにかけるので</p>
<p>$$
\begin{aligned}
&amp;\frac{\partial}{\partial\mathbf{x}}\operatorname{St}(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Lambda}, \nu)\propto \frac{\partial}{\partial\mathbf{x}}\left[1+\frac{\Delta^{2}}{\nu}\right]^{-D / 2-\nu / 2}\
&amp;=\left(-\frac{D}{2}-\frac{\nu}{2}\right)\left[1+\frac{\Delta^2}{\nu}\right]^{-D/2-\nu/2-1}\frac{2}{\nu}\mathbf{\Lambda}(\mathbf{x}-\boldsymbol{\mu})\
\end{aligned}
$$
これが$\mathbf{0}$となるのは$\mathbf{x}=\boldsymbol{\mu}$のとき</p>
<h2 id="演習-250"><a class="header" href="#演習-250">演習 2.50</a></h2>
<div class="panel-primary">
<p>$\nu \to \infty$の極限で，多変量スチューデントt分布</p>
<p>$$
\operatorname{St}(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Lambda}, \nu)=\frac{\Gamma(D / 2+\nu / 2)}{\Gamma(\nu / 2)} \frac{|\mathbf{\Lambda}|^{1 / 2}}{(\pi \nu)^{D / 2}}\left[1+\frac{\Delta^{2}}{\nu}\right]^{-D / 2-\nu / 2} \tag{2.162}
$$</p>
<p>が，平均が$\boldsymbol{\mu}$で精度が$\mathbf{\Lambda}$のガウス分布になることを示せ．</p>
</div>
<p>$\lim_{\nu \to \infty} \operatorname{St}(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Lambda}, \nu)=\mathcal{N}\left(\mathbf{x} | \boldsymbol{\mu},\mathbf{\Lambda}\right)$を示す。スチューデントのt分布の形は</p>
<p>$$
\operatorname{St}(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Lambda}, \nu)=\frac{\Gamma(D / 2+\nu / 2)}{\Gamma(\nu / 2)} \frac{|\mathbf{\Lambda}|^{1 / 2}}{(\pi \nu)^{D / 2}}\left[1+\frac{\Delta^{2}}{\nu}\right]^{-D / 2-\nu / 2}
$$</p>
<p>ここで$\displaystyle \lim_{\nu \to \infty}\frac{\Gamma(D/2+\nu/2)}{\Gamma(\nu/2)\left(\nu/2\right)^{D/2}}=1$であるので(演習2.47を参照)、</p>
<p>$$
\begin{aligned}\
\lim_{\nu\to\infty}\operatorname{St}(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Lambda}, \nu) &amp;= \lim_{\nu\to\infty}\frac{|\mathbf{\Lambda}|^{1 / 2}}{(2\pi)^{D / 2}}\left[1+\frac{\Delta^{2}}{\nu}\right]^{-D / 2-\nu / 2} \
&amp;=\frac{|\mathbf{\Lambda}|^{1 / 2}}{(2\pi)^{D / 2}}\lim_{\nu\to\infty}\underbrace{\left[1+\frac{\Delta^{2}}{\nu}\right]^{-D / 2}}_{\to1}\left[1+\frac{\Delta^{2}}{\nu}\right]^{-\nu / 2}
\end{aligned}
$$</p>
<p>ここで$\displaystyle u=-\frac{\nu}{2}$と置換し$\displaystyle \exp(x)=\lim_{n\to\infty}\left[1+\frac{x}{n}\right]^{n}=\lim_{n\to-\infty}\left[1+\frac{x}{n}\right]^{n}$を用いると</p>
<p>$$
\begin{aligned}
\lim_{\nu\to\infty}\operatorname{St}(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Lambda}, \nu) &amp;= \frac{|\mathbf{\Lambda}|^{1 / 2}}{(2\pi)^{D / 2}}\lim_{u\to-\infty}\left[1-\frac{\Delta^{2}}{2u}\right]^{u}\
&amp;=\frac{|\mathbf{\Lambda}|^{1 / 2}}{(2\pi)^{D / 2}}\exp\left(-\frac{\Delta^{2}}{2}\right)\
&amp;=\mathcal{N}\left(\mathbf{x} | \boldsymbol{\mu},\mathbf{\Lambda}\right)
\end{aligned}
$$</p>
<h2 id="演習-251"><a class="header" href="#演習-251">演習 2.51</a></h2>
<div class="panel-primary">
<p>本章の周期変数の議論で用いた，いろいろな三角関数の公式は，次の関係を用いて容易に証明できる．</p>
<p>$$
\exp(iA) = \cos A+i\sin A \tag{2.296}
$$</p>
<p>ただし$i$は$-1$の平方根である．</p>
<p>$$
\cos^2 A+\sin^2 A = 1 \tag{2.177}
$$</p>
<p>の結果を</p>
<p>$$
\exp(iA) \exp(-iA)=1 \tag{2.297}
$$</p>
<p>から証明せよ．同様に</p>
<p>$$
\cos (A-B)=\Re \exp {i(A-B)} \tag{2.298}
$$</p>
<p>を用いて</p>
<p>$$
\cos A \cos B+\sin A \sin B=\cos (A-B) \tag{2.178}
$$</p>
<p>を証明せよ．ただし，$\Re$は実部を示す．最後に，$\sin (A-B) = \Im \exp{i(A-B)}$から，</p>
<p>$$
\sin (A-B)=\sin A \cos B-\cos A \sin B \tag{2.183}
$$</p>
<p>を証明せよ．ただし，$\Im$は虚部を示す．</p>
</div>
(2.296)を(2.297)に代入すると
<p>$$\begin{aligned}( \text{左辺} ) &amp;= \exp(iA)\exp(-iA) \&amp;= (\cos A + i \sin iA)(\cos A - i \sin iA) \&amp;= \cos^2 A + \sin^2 A \end{aligned}$$
よって、(2.177)は示された。</p>
<p>(2.298)より</p>
<p>$$\begin{aligned} \cos (A-B) &amp;= \Re \exp { i(A-B) } \&amp;= \Re \exp(iA) \exp(-iB) \&amp;= \Re (\cos A + i \sin iA)(\cos B - i \sin iB) \&amp;= \cos A \cos B + \sin A \sin B \end{aligned}$$</p>
<p>また</p>
<p>$$\begin{aligned} \sin (A-B) &amp;= \Im \exp { i(A-B) } \&amp;= \Im \exp(iA) \exp(-iB) \&amp;= \Im (\cos A + i \sin iA)(\cos B - i \sin iB) \&amp;= \sin A \cos B - \cos A \sin B \end{aligned}$$</p>
<h2 id="演習-252"><a class="header" href="#演習-252">演習 2.52</a></h2>
<div class="panel-primary">
<p>フォン・ミーゼス分布</p>
<p>$$
p\left(\theta \mid \theta_{0}, m\right)=\frac{1}{2 \pi I_{0}(m)} \exp \left{m \cos \left(\theta-\theta_{0}\right)\right} \tag{2.179}
$$</p>
<p>は，$m$が大きいとき，モード$\theta_0$の周囲で鋭く尖る．$\xi = m^{1/2}(\theta-\theta_0)$と定義し，余弦関数のテイラー展開が</p>
<p>$$
\cos \alpha = 1-\frac{\alpha^2}{2}+O(\alpha^4)　\tag{2.299}
$$
であるとして，$m \to \infty$でフォン・ミーゼス分布がガウス分布になることを示せ．</p>
</div>
<p>$\xi = m^{1/2}(\theta-\theta_0)$より、$\theta-\theta_0 = \xi m^{-1/2}$。</p>
<p>これをフォン・ミーゼス分布(2.179)に代入すると、</p>
<p>$$\begin{aligned} p\left(\theta \mid \theta_{0}, m\right) &amp;\propto \exp \left{m \cos \left(\theta-\theta_{0}\right)\right} \&amp;= \exp \left{m \cos \left(\xi m^{-1/2}\right)\right} \&amp;= \exp \left{m \left( 1 - \frac{1}{2} \xi^2 m^{-1} + O(\xi^4 m^{-2}) \right) \right} \&amp;\propto \exp \left( -\frac{\xi^2}{2} \right) \&amp;= \exp \left{ - \frac{m(\theta - \theta_{0})^2}{2} \right} \end{aligned}$$</p>
<p>これはガウス分布の形になっているので示された。</p>
<h2 id="演習-253"><a class="header" href="#演習-253">演習 2.53</a></h2>
<div class="panel-primary">
<p>三角関数の公式
$$
\sin (A-B)=\sin A \cos B-\cos A \sin B \tag{2.183}
$$
を用いて，$\theta_0$についての</p>
<p>$$
\sum_{n=1}^{N} \sin \left(\theta_{n}-\theta_{0}\right)=0 \tag{2.182}
$$</p>
<p>の解が</p>
<p>$$
\theta_{0}^{\mathrm{ML}}=\tan ^{-1}\left{\frac{\sum_{n} \sin \theta_{n}}{\sum_{n} \cos \theta_{n}}\right} \tag{2.184}
$$</p>
<p>となることを示せ．</p>
</div>
<p>$(2.183)$で示されている加法定理$\sin(A-B) = \sin A \cos B- \cos A \sin B$を用いて$(2.182)$式を解いて</p>
<p>$$
\sum_{n=1}^{N}\sin(\theta_n-\theta_0)=0 \
$$
$$
\sum_{n=1}^{N}\left( \sin \theta_n \cos \theta_0 -\cos \theta_n \sin\theta_0 \right) =0 \
$$
$$sin\theta_0\sum_{n=1}^{N}\cos(\theta_n)=cos\theta_0\sum_{n=1}^{N}\sin(\theta_n)$$
$$
\tan \theta_0 = \frac{\sum_{n}\sin\theta_n}{\sum_{n}\cos\theta_n}
$$</p>
<p>なので、最尤推定量$\theta_0^{\mathrm{ML}}$は</p>
<p>$$
\theta_0^{\mathrm{ML}} = \tan^{-1}\left{ \frac{\sum_{n}\sin\theta_n}{\sum_{n}\cos\theta_n} \right}
$$</p>
<p>となり、$(2.184)$式を得ることができた。</p>
<h2 id="演習-254"><a class="header" href="#演習-254">演習 2.54</a></h2>
<div class="panel-primary">
<p>フォン・ミーゼス分布
$$
p\left(\theta \mid \theta_{0}, m\right)=\frac{1}{2 \pi I_{0}(m)} \exp \left{m \cos \left(\theta-\theta_{0}\right)\right} \tag{2.179}
$$
の1階と2階の導関数を求め，さらに$m&gt;0$で$I_0(m) &gt; 0$であることを用いて，分布は$\theta = \theta_0$で最大になり，$\theta = \theta_0 + \pi, (\mathrm{mod}\ 2\pi)$で最小になることを示せ．</p>
</div>
<p>1階の導関数は
$$
\frac{\partial}{\partial \theta} p\left(\theta \mid \theta_{0}, m\right)=\frac{1}{2 \pi I_{0}(m)} \exp \left(m \cos \left(\theta-\theta_{0}\right)\right)\left(-m \sin \left(\theta-\theta_{0}\right)\right)
$$
2階の導関数は
$$
\begin{aligned}
\frac{\partial^{2}}{\partial \theta^{2}} p\left(\theta \mid \theta_{0}, m\right) &amp;=
\frac{1}{2 \pi I_{0}(m)} \exp \left(m \cos\left(\theta-\theta_{0}\right)\left(-m \sin \left(\theta-\theta_{0}\right)\right)\left(-m \sin \left(\theta-\theta_{0}\right)\right)\right. \ &amp;\left.\left.+\exp \left(m\cos \left(\theta-\theta_{0}\right)\right)( -m\cos \left(\theta-\theta_{0}\right)\right)\right} \ &amp;=\frac{1}{2 \pi I_{0}(m)} \exp \left(m\cos \left(\theta-\theta_{0}\right)\right)\left(m^{2} \sin ^{2}\left(\theta-\theta_{0}\right)-m \cos \left(\theta-\theta_{0}\right)\right)
\end{aligned}
$$</p>
<p>上の結果より$\displaystyle \frac{\partial p}{\partial \theta} = 0$となるのは$\displaystyle \frac{1}{2\pi I_0(m)}\exp (m \cos (\theta - \theta_0))&gt;0, m&gt;0$より$\sin(\theta - \theta_0)=0$のとき、つまり$\theta = \theta_0, \theta = \theta_0 + \pi\ (\textrm{mod}\ 2\pi)$のとき。</p>
<p>$\theta = \theta_0$では</p>
<p>$$
\frac{\partial^{2} p}{\partial \theta^{2}}=\frac{1}{2 \pi I_{0}(m)} \exp \left(m \cos \left(\theta-\theta_{0}\right)\right)(-m)&lt;0
$$</p>
<p>なので、$\theta = \theta_0$が$P(\theta \mid \theta_0,m)$の極大点。</p>
<p>$\theta = \theta_0 + \pi$では</p>
<p>$$\frac{\partial^2 p}{\partial \theta^2} = \frac{1}{2\pi I_0(m)}\exp (m \cos (\theta - \theta_0))$$</p>
<p>$m&gt;0$より$\theta = \theta_0 + \pi\ (\mathrm{mod}\ 2\pi)$が$P(\theta \mid \theta_0, m)$の極小点となる。</p>
<h2 id="演習-255"><a class="header" href="#演習-255">演習 2.55</a></h2>
<div class="panel-primary">
<p>$$
\bar{x}<em>{1}=\bar{r} \cos \bar{\theta}=\frac{1}{N} \sum</em>{n=1}^{N} \cos \theta_{n}, \quad \bar{x}<em>{2}=\bar{r} \sin \bar{\theta}=\frac{1}{N} \sum</em>{n=1}^{N} \sin \theta_{n} \tag{2.168}
$$
の結果を，</p>
<p>$$
\theta_{0}^{\mathrm{ML}}=\tan ^{-1}\left{\frac{\sum_{n} \sin \theta_{n}}{\sum_{n} \cos \theta_{n}}\right} \tag{2.184}
$$</p>
<p>と三角関数の公式</p>
<p>$$
\cos A \cos B+\sin A \sin B=\cos (A-B) \tag{2.178}
$$</p>
<p>と共に用いて，フォン・ミーゼス分布の集中度の最尤推定解$m_{\mathrm{ML}}$が，$A(m_{\mathrm{ML}})=\bar{r}$を満たすことを示せ．ただし，$\bar{r}$は，図2.17のように，2次元ユークリッド平面中の単位ベクトルによって表した観測値の平均の半径である．</p>
</div>
(2.187)より
<p>$$\begin{aligned} A(m_{\mathrm{ML}}) &amp;= \left( \frac{1}{N} \sum_{n=1}^{N} \cos \theta_n \right) \cos \theta_{0}^{\mathrm{ML}} + \left( \frac{1}{N} \sum_{n=1}^{N} \sin \theta_n \right) \sin \theta_{0}^{\mathrm{ML}} \&amp;= \bar{r} \cos \bar{\theta} \cos \theta_{0}^{\mathrm{ML}} + \bar{r} \sin \bar{\theta} \sin \theta_{0}^{\mathrm{ML}} \&amp;= \bar{r} \cos \theta_{0}^{\mathrm{ML}} \cos \theta_{0}^{\mathrm{ML}} + \bar{r} \sin \theta_{0}^{\mathrm{ML}} \sin \theta_{0}^{\mathrm{ML}} \&amp;= \bar{r} \end{aligned}$$</p>
<h2 id="演習-256"><a class="header" href="#演習-256">演習 2.56</a></h2>
<div class="panel-primary">
<p>ベータ分布</p>
<p>$$
\operatorname{Beta}(\mu \mid a, b)=\frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} \mu^{a-1}(1-\mu)^{b-1} \tag{2.13}
$$</p>
<p>ガンマ分布</p>
<p>$$
\operatorname{Gam}(\lambda \mid a, b)=\frac{1}{\Gamma(a)} b^{a} \lambda^{a-1} \exp (-b \lambda) \tag{2.146}
$$</p>
<p>およびフォン・ミーゼス分布
$$
p\left(\theta \mid \theta_{0}, m\right)=\frac{1}{2 \pi I_{0}(m)} \exp \left{m \cos \left(\theta-\theta_{0}\right)\right} \tag{2.179}
$$
を指数型分布族の形</p>
<p>$$
p(\mathbf{x} \mid \boldsymbol{\eta})=h(\mathbf{x}) g(\boldsymbol{\eta}) \exp \left{\boldsymbol{\eta}^{\mathbf{T}} \mathbf{u}(\mathbf{x})\right} \tag{2.194}
$$</p>
<p>に変形し，これらの分布の自然パラメータを求めよ．</p>
</div>
<p>※ 与えられた各分布の形を無理やり指数型分布族の一般形に変形していけば求まる。パラメータの変数名は各分布にしたがって適切に置き換える。</p>
<p>まずベータ分布は</p>
<p>$$
\begin{aligned}
\frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} \mu^{a-1}(1-\mu)^{b-1}
&amp;= \frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)}\exp\left{ (a-1)\ln \mu +(b-1) \ln(1-\mu) \right}
\
&amp;= \frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)}\exp\left{ \left(\begin{array}{c}a-1 \ b-1\end{array}\right)^{\mathrm{T}} \left(\begin{array}{c}\ln \mu \ \ln (1-\mu)\end{array}\right) \right}
\end{aligned}
$$</p>
<p>これより、一般形と照らし合わせると、$\mathbf{x}\to\mu$, $\eta \to a,b$として</p>
<p>$$
h(\mu) = 1,\ g(a,b) = \frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)},\ \boldsymbol{\eta}(a,b) = \left(\begin{array}{c}a-1 \ b-1\end{array}\right),\ \mathbf{u}(\mu) = \left(\begin{array}{c}\ln \mu \ \ln (1-\mu)\end{array}\right)
$$</p>
<p>同様にしてガンマ分布は</p>
<p>$$
\begin{aligned}
\frac{1}{\Gamma(a)} b^{a} \lambda^{a-1} \exp (-b \lambda) &amp;= \frac{1}{\Gamma(a)} b^{a} \exp {(a-1)\ln\lambda - b\lambda} \
&amp;= \frac{b^a}{\Gamma(a)} \exp \left{ \begin{pmatrix}a-1 \ -b \end{pmatrix}^{\mathrm{T}} \begin{pmatrix}\ln \lambda \ \lambda \end{pmatrix}\right}
\end{aligned}
$$</p>
<p>これより</p>
<p>$$
h(\lambda) = 1,\ g(a,b) = \frac{b^a}{\Gamma(a)},\ \boldsymbol{\eta}(a,b) = \left(\begin{array}{c}a-1 \ -b\end{array}\right),\ \mathbf{u}(\lambda) = \left(\begin{array}{c}\ln \lambda \ \lambda \end{array}\right)
$$</p>
<p>フォン・ミーゼス分布は</p>
<p>$$
\begin{aligned}
\frac{1}{2 \pi I_{0}(m)} \exp \left{m \cos \left(\theta-\theta_{0}\right)\right}
&amp;= \frac{1}{2 \pi I_{0}(m)} \exp { m\cos \theta \cos\theta_0 + m\sin\theta \sin\theta_0 } \
&amp;= \frac{1}{2 \pi I_{0}(m)} \exp \left{ \left(\begin{array}{c}m\cos\theta_0 \ m\sin\theta_0 \end{array}\right)^{\mathrm{T}} \left(\begin{array}{c}\cos\theta \ \sin\theta \end{array}\right) \right}
\end{aligned}
$$</p>
<p>これより、
$$
h(\theta) = 1,\ g(\theta_0, m) = \frac{1}{2\pi I_0(m)},\ \boldsymbol{\eta}(\theta_0, m) = \left(\begin{array}{c}m\cos\theta_0 \ m\sin\theta_0 \end{array}\right),\ \mathbf{u}(\theta) = \left(\begin{array}{c}\cos\theta \ \sin\theta \end{array}\right)
$$</p>
<h2 id="演習-257"><a class="header" href="#演習-257">演習 2.57</a></h2>
<div class="panel-primary">
<p>多変量ガウス分布は，指数型分布族の形式</p>
<p>$$
p(\mathbf{x} \mid \boldsymbol{\eta})=h(\mathbf{x}) g(\boldsymbol{\eta}) \exp \left{\boldsymbol{\eta}^{\mathbf{T}} \mathbf{u}(\mathbf{x})\right} \tag{2.194}
$$</p>
<p>に変形できることを示し，$(2.220)–(2.223)$と同様に，$\boldsymbol{\eta}, \mathbf{u}(\mathbf{x}), h(\mathbf{x})$および$g(\boldsymbol{\eta})$の式を導出せよ．</p>
</div>
<p>多変量ガウス分布の式</p>
<p>$$
\begin{aligned}
N(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Sigma})
&amp;=\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\mathbf{\Sigma}|^{1 / 2}} \exp \left{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu}) \mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})^{\mathrm{T}}\right} \
&amp;=\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\mathbf{\Sigma}|^{1 / 2}} \exp\left{-\frac{1}{2} \boldsymbol{\mu}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \boldsymbol{\mu}\right} \exp \left{-\frac{1}{2} \mathbf{x}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \mathbf{x}+\mathbf{x}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \boldsymbol{\mu}\right}
\
&amp;= \frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\mathbf{\Sigma}|^{1 / 2}} \exp\left{-\frac{1}{2} \boldsymbol{\mu}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \boldsymbol{\mu}\right} \exp \left{-\frac{1}{2}\operatorname{Tr}[\mathbf{xx}^{\mathrm{T}}\mathbf{\Sigma}^{-1}] + \boldsymbol{\mu}^{\mathrm{T}}\mathbf{\Sigma}^{-1}\mathbf{x}\right}
\end{aligned}
$$
からはじめて、指数型分布族の一般形と比較すると、
$$
h(\mathbf{x}) = (2\pi)^{-D/2}, g(\boldsymbol{\eta}) = |\mathbf{\Sigma}|^{-1 / 2}\exp\left{-\frac{1}{2} \boldsymbol{\mu}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \boldsymbol{\mu}\right}
$$
となる。（$g(\boldsymbol{\eta})$が$\boldsymbol{\eta}$の関数になっていないことについては後述。）</p>
<p>指数部分について$\boldsymbol{\eta}(\boldsymbol{\mu}, \mathbf{\Sigma})$と$\mathbf{u}(\mathbf{x})$の形に分離できるようにすることを考える。</p>
<p>まず$\boldsymbol{\mu}^{\mathrm{T}}\mathbf{\Sigma}^{-1}\mathbf{x}$を作るため、</p>
<p>$$
\boldsymbol{\eta}(\mu, \mathbf{\Sigma})^{\mathrm{T}} =
\begin{pmatrix}
\mathbf{A} \
\mathbf{\Sigma}^{-1}\boldsymbol{\mu}
\end{pmatrix}^{\mathrm{T}},\ \mathbf{u}(\mathbf{x}) =
\begin{pmatrix}
\mathbf{B} \
\mathbf{x}
\end{pmatrix}
$$
とすれば（<strong>区分行列</strong>（または<strong>分割行列、ブロック行列</strong>とも）$\mathbf{A}, \mathbf{B}$の上下の位置は逆でもよい）、$\boldsymbol{\eta}^{\mathrm{T}}\mathbf{u} = \mathbf{A}^{\mathrm{T}}\mathbf{B}+\boldsymbol{\mu}^{\mathrm{T}}\mathbf{\Sigma}^{-1}\mathbf{x}$となるので、$\exp$の第2項ができる（$\because \mathbf{\Sigma}^{-1} = (\mathbf{\Sigma}^{-1})^{\mathrm{T}}$）。</p>
<p>問題は$\mathbf{A}^{\mathrm{T}}\mathbf{B} = \operatorname{Tr}[\mathbf{xx}^{\mathrm{T}}\mathbf{\Sigma}^{-1}]$となるような区分行列$\mathbf{A}, \mathbf{B}$を求めることである。</p>
<p>ここで$\mathbf{xx}^{\mathrm{T}} = \mathbf{M}$とおくと</p>
<p>$$
\begin{aligned}
\operatorname{Tr}[\mathbf{xx}^{\mathrm{T}}\mathbf{\Sigma}^{-1}] &amp;= \sum_{j=1}^{D}\mathbf{M}<em>j\mathbf{\Sigma}^{-1}<em>j \
&amp;= \begin{pmatrix}\mathbf{m}</em>{1}^{\mathrm{T}} \mathbf{m}</em>{2}^{\mathrm{T}}  \ldots \mathbf{m}<em>{n}^{\mathrm{T}}\end{pmatrix} \begin{pmatrix}\boldsymbol{\sigma}</em>{1}^{-1} \ \boldsymbol{\sigma}<em>{2}^{-1} \ \vdots \ \boldsymbol{\sigma}</em>{n}^{-1}\end{pmatrix} \
&amp;= \begin{pmatrix}\mathbf{m}<em>{1} \ \mathbf{m}</em>{2} \ \vdots \ \mathbf{m}<em>{n}\end{pmatrix}^{\mathrm{T}}\begin{pmatrix}\boldsymbol{\sigma}</em>{1}^{-1} \ \boldsymbol{\sigma}<em>{2}^{-1} \ \vdots \ \boldsymbol{\sigma}</em>{n}^{-1}\end{pmatrix} \
&amp;= (\operatorname{vec}(\mathbf{M}))^{\mathrm{T}}(\operatorname{vec}(\mathbf{\mathbf{\Sigma}^{-1}})) \
&amp;= (\operatorname{vec}(\mathbf{xx}^{\mathrm{T}}))^{\mathrm{T}}(\operatorname{vec}(\mathbf{\mathbf{\Sigma}^{-1}}))
\end{aligned}
$$</p>
<p>ここで$\operatorname{vec}(\mathbf{A})$は任意の$m\times n$行列$\mathbf{A}$について要素$\mathbf{A} = { a_{ij} }$を$mn$次元列ベクトルに再配列したものである（※<strong>vec作用素</strong>と呼ばれる。統計のための行列代数下巻 16.2と定理16.2.2を参照。）</p>
<blockquote>
<p>たとえば， $m = 2$, $n = 3$ の行列$\mathbf{A}$にvec作用素を適用すると，
$$
\operatorname{vec}(\mathbf{A}) = \begin{pmatrix} \mathbf{a}<em>{1} \ \mathbf{a}</em>{2} \ \vdots \ \mathbf{a}<em>{n} \end{pmatrix}=\begin{pmatrix}a</em>{11} \ a_{21} \ a_{12} \ a_{22} \ a_{13} \ a_{23}\end{pmatrix}
$$
である</p>
</blockquote>
<p>これを用いると、求める$\boldsymbol{\eta}(\boldsymbol{\mu}, \mathbf{\Sigma}), \mathbf{u}(\mathbf{x})$は</p>
<p>$$
\boldsymbol{\eta}(\boldsymbol{\mu}, \mathbf{\Sigma}) =
\begin{pmatrix}
\operatorname{vec}(\mathbf{\Sigma}^{-1}) \
\mathbf{\Sigma}^{-1}\boldsymbol{\mu}
\end{pmatrix},\ \mathbf{u}(\mathbf{x}) =
\begin{pmatrix}
-\frac{1}{2}\operatorname{vec}(\mathbf{xx}^{\mathrm{T}}) \
\mathbf{x}
\end{pmatrix}
$$</p>
<p>となる。</p>
<p>（$g(\boldsymbol{\eta})$が$\boldsymbol{\eta}$についての式になっていないけれど大丈夫か？という疑問が残るが、vecを作用させる前の$\mathbf{\Sigma}^{-1}$を$\eta_1$,$\mathbf{\Sigma}^{-1}\boldsymbol{\mu}$を$\eta_2$とすれば, $\eta_1, \eta_2$の関数として$g(\boldsymbol{\eta})$を表現することは可能なのでセーフという理屈らしい……）</p>
<h2 id="演習-258"><a class="header" href="#演習-258">演習 2.58</a></h2>
<div class="panel-primary">
<p>$$
-\nabla \ln g(\boldsymbol{\eta})=\mathbb{E}[\mathbf{u}(\mathbf{x})] \tag{2.226}
$$</p>
<p>は，指数型分布族では$\ln g(\boldsymbol{\eta})$の負の勾配が，$\mathbf{u}(\mathbf{x})$の期待値になることを示している．</p>
<p>$$
g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left{\boldsymbol{\eta}^{\mathrm{T}} \mathbf{u}(\mathbf{x})\right} \mathrm{d} \mathbf{x}=1 \tag{2.195}
$$</p>
<p>の2階微分を取ることで</p>
<p>$$
-\nabla \nabla \ln g(\boldsymbol{\eta})=\mathbb{E}\left[\mathbf{u}(\mathbf{x}) \mathbf{u}(\mathbf{x})^{\mathrm{T}}\right]-\mathbb{E}[\mathbf{u}(\mathbf{x})] \mathbb{E}\left[\mathbf{u}(\mathbf{x})^{\mathrm{T}}\right]=\operatorname{cov}[\mathbf{u}(\mathbf{x})] \tag{2.300}
$$</p>
<p>を示せ．</p>
</div>
<p>※ 先に<strong>スカラーについてのベクトルでの微分</strong>、<strong>ベクトルについてのベクトルでの微分</strong>のやり方を知っておく必要がある。
$g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left{\boldsymbol{\eta}^{\mathrm{T}}\mathbf{u}(\mathbf{x})\right}\mathrm{d}\mathbf{x} = 1$なので$g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left{\boldsymbol{\eta}^{\mathrm{T}}\mathbf{u}(\mathbf{x})\right}\mathbf{u}(\mathbf{x})$はスカラーである。一方で$\mathbf{u}(\mathbf{x})$は列ベクトルである。$\boldsymbol{\eta}$でそれぞれを微分する場合、得られる結果はそれぞれ列ベクトルと行列になることに注意する。
スカラーをベクトルで微分する場合は
$$
\frac{\partial f(\mathbf{x})}{\partial \mathbf{x}}=
\begin{pmatrix}
\frac{\partial f}{\partial x_{1}} \ \vdots \ \frac{\partial f}{\partial x_{D}}
\end{pmatrix}
$$
これは便宜的に以下のように考えることができる。
$$
\frac{\partial}{\partial \mathbf{x}} f(\mathbf{x})=
\begin{pmatrix}
\frac{\partial}{\partial x_{1}} \ \vdots \ \frac{\partial}{\partial x_{D}}
\end{pmatrix} f(\mathbf{x})
$$</p>
<p>これより、</p>
<p>$$
\begin{aligned}
\frac{\partial}{\partial \boldsymbol{\eta}}\underbrace{\exp (\boldsymbol{\eta}^{\mathrm{T}}\mathbf{u}(\mathbf{x}))}<em>{\textrm{scalar}}
&amp;= \begin{pmatrix}
\frac{\partial}{\partial \eta</em>{1}} \ \vdots \ \frac{\partial}{\partial \eta_{D}}
\end{pmatrix} \exp \left( \sum_{i=1}^{D}\eta_{i}u_i(\mathbf{x}) \right) \
&amp;= \begin{pmatrix}
\exp \left( \sum_{i=1}^{D}\eta_{i}u_i(\mathbf{x}) \right) u_1(\mathbf{x}) \ \vdots \ \exp \left( \sum_{i=1}^{D}\eta_{i}u_i(\mathbf{x}) \right) u_D(\mathbf{x})
\end{pmatrix} \
&amp;= \exp (\boldsymbol{\eta}^{\mathrm{T}}\mathbf{u}(\mathbf{x})) \mathbf{u}(\mathbf{x})
\end{aligned}
$$
となる。また、ベクトルをベクトルで微分する場合は、教科書によって微分する変数側を行ベクトルとするか、微分される関数側を行ベクトルとするか2通りの表現があるが、ここでは変数側を行ベクトルとする。微分される関数を$\mathbf{f}(\mathbf{x})$とすると
$$
\frac{\partial}{\partial \mathbf{x}} \mathbf{f}(\mathbf{x})=
\begin{pmatrix}
\frac{\partial f_1}{\partial x_{1}} &amp; \cdots &amp;\frac{\partial f_1}{\partial x_{D}} \ \vdots &amp; &amp; \vdots  \ \frac{\partial f_D}{\partial x_1} &amp; \ldots &amp; \frac{\partial f_D}{\partial x_D}
\end{pmatrix}
$$
これは便宜的に以下のように考えることができる。
$$
\frac{\partial}{\partial \mathbf{x}} \mathbf{f}(\mathbf{x})=
\begin{pmatrix}
\frac{\partial}{\partial x_{1}} \ \vdots \ \frac{\partial}{\partial x_{D}}
\end{pmatrix}
\begin{pmatrix}
f_1(\mathbf{x}) &amp; \ldots &amp; f_D(\mathbf{x})
\end{pmatrix}
$$
これより
$$
\begin{aligned}
\frac{\partial}{\partial \boldsymbol{\eta}}\underbrace{\exp (\boldsymbol{\eta}^{\mathrm{T}}\mathbf{u}(\mathbf{x}))\mathbf{u}(\mathbf{x})}<em>{\textrm{vector}}
&amp;= \begin{pmatrix}
\frac{\partial}{\partial \eta</em>{1}} \ \vdots \ \frac{\partial}{\partial \eta_{D}}
\end{pmatrix}
\begin{pmatrix}
\exp (\boldsymbol{\eta}^{\mathrm{T}}\mathbf{u}(\mathbf{x})) u_1(\mathbf{x}) &amp; \ldots &amp; \exp (\boldsymbol{\eta}^{\mathrm{T}}\mathbf{u}(\mathbf{x})) u_D(\mathbf{x})
\end{pmatrix} \
&amp;= \begin{pmatrix}
\exp (\boldsymbol{\eta}^{\mathrm{T}}\mathbf{u}(\mathbf{x})) u_1(\mathbf{x}) u_1(\mathbf{x}) &amp; \ldots &amp; \exp (\boldsymbol{\eta}^{\mathrm{T}}\mathbf{u}(\mathbf{x})) u_D(\mathbf{x}) u_1(\mathbf{x})
\ \vdots &amp; \ddots &amp; \vdots \
\exp (\boldsymbol{\eta}^{\mathrm{T}}\mathbf{u}(\mathbf{x})) u_1(\mathbf{x}) u_D(\mathbf{x}) &amp; \ldots &amp; \exp (\boldsymbol{\eta}^{\mathrm{T}}\mathbf{u}(\mathbf{x})) u_D(\mathbf{x}) u_D(\mathbf{x})
\end{pmatrix} \
&amp;= \exp (\boldsymbol{\eta}^{\mathrm{T}}\mathbf{u}(\mathbf{x})) \mathbf{u}(\mathbf{x}) \mathbf{u}(\mathbf{x})^{\mathrm{T}}
\end{aligned}
$$
となる。</p>
<p>※ PRML P.110の$(2.194), (2.195)$を用いて$(2.224)-(2.226)$までの式変形をまず検証する。</p>
<p>$(2.195)$の両辺を$\boldsymbol{\eta}$で微分すると</p>
<p>$$
\begin{aligned}
\nabla g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left{\boldsymbol{\eta}^{\mathrm{T}}\mathbf{u}(\mathbf{x})\right} \mathrm{d}\mathbf{x}
&amp;+g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left{\boldsymbol{\eta}^{\mathrm{T}}\mathbf{u}(\mathbf{x})\right}\mathbf{u}(\mathbf{x}) \mathrm{d}\mathbf{x} = 0 \
\nabla g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left{\boldsymbol{\eta}^{\mathrm{T}}\mathbf{u}(\mathbf{x})\right} \mathrm{d}\mathbf{x} &amp;=
-g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left{\boldsymbol{\eta}^{\mathrm{T}}\mathbf{u}(\mathbf{x})\right}\mathbf{u}(\mathbf{x}) \mathrm{d}\mathbf{x} \
-\frac{\nabla g(\boldsymbol{\eta})}{g(\boldsymbol{\eta})} &amp;= \frac{\int h(\mathbf{x}) \exp \left{\boldsymbol{\eta}^{\mathrm{T}}\mathbf{u}(\mathbf{x})\right}\mathbf{u}(\mathbf{x}) \mathrm{d}\mathbf{x}}{\int h(\mathbf{x}) \exp \left{\boldsymbol{\eta}^{\mathrm{T}}\mathbf{u}(\mathbf{x})\right}\mathrm{d}\mathbf{x}}
\end{aligned}
$$</p>
<p>ここで再度$(2.195)$式を使うと</p>
<p>$$
-\frac{\nabla g(\boldsymbol{\eta})}{g(\boldsymbol{\eta})} = g(\boldsymbol{\eta})\int h(\mathbf{x}) \exp \left{\boldsymbol{\eta}^{\mathrm{T}}\mathbf{u}(\mathbf{x})\right}\mathbf{u}(\mathbf{x}) \mathrm{d}\mathbf{x} = \mathbb{E}[\mathbf{u}(\mathbf{x})]
$$</p>
<p>これより$(2.226)$式の$-\nabla \ln g(\boldsymbol{\eta}) = \mathbb{E}[\mathbf{u}(\mathbf{x})]$が得られる。</p>
<p>続いて2階微分を行うと</p>
<p>$$
\begin{aligned}
-\nabla \nabla \ln g(\boldsymbol{\eta}) &amp;=\nabla g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left{\boldsymbol{\eta}^{\mathrm{T}} \mathbf{u}(\mathbf{x})\right} \mathbf{u}(\mathbf{x})^{\mathrm{T}} \mathrm{d}\mathbf{x} \
&amp;+g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left{\boldsymbol{\eta}^{\mathrm{T}} \mathbf{u}(\mathbf{x})\right} \mathbf{u}(\mathbf{x}) \mathbf{u}(\mathbf{x})^{\mathrm{T}} \mathrm{d}\mathbf{x} \
&amp;=-\mathbb{E}[\mathbf{u}(\mathbf{x})] g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left{\boldsymbol{\eta}^{\mathrm{T}} \mathbf{u}(\mathbf{x})\right} \mathbf{u}(\mathbf{x})^{\mathrm{T}} \mathrm{d}\mathbf{x} \
&amp;+\mathbb{E}\left[\mathbf{u}(\mathbf{x}) \mathbf{u}(\mathbf{x})^{\mathrm{T}}\right] \ &amp;=\mathbb{E}\left[\mathbf{u}(\mathbf{x}) \mathbf{u}(\mathbf{x})^{\mathrm{T}}\right]-\mathbb{E}[\mathbf{u}(\mathbf{x})]\mathbb{E}\left[\mathbf{u}(\mathbf{x})^{\mathrm{T}}\right] \
&amp;=\operatorname{cov}[\mathbf{u}(\mathbf{x})]
\end{aligned}
$$</p>
<p>となり、共分散が求まる。</p>
<h2 id="演習-259"><a class="header" href="#演習-259">演習 2.59</a></h2>
<div class="panel-primary">
<p>$f(x)$が正規化されていれば，密度</p>
<p>$$
p(x|\sigma) = \frac{1}{\sigma}f\left( \frac{x}{\sigma} \right) \tag{2.236}
$$</p>
<p>も正規化されていることを，$y=x/\sigma$と変数を変換することで示せ．</p>
</div>
<p>$\displaystyle \int f(x) dx = 1$が成立するときに$\displaystyle \int p(x | \sigma) dx = 1$であることを示せば良い。</p>
<p>$y=x/\sigma$とすると、$dy = \frac{1}{\sigma} dx$なので</p>
<p>$$
\int p(x|\sigma) dx = \int \sigma \cdot \frac{1}{\sigma}f(y) dy = \int f(y) dy = 1 \hspace{1em} \left( \because \int f(x)dx = 1\right)
$$</p>
<p>したがって$p(x|\sigma)$が正規化されていることが示された。</p>
<h2 id="演習-260"><a class="header" href="#演習-260">演習 2.60</a></h2>
<div class="panel-primary">
<p>空間$\mathbf{x}$がいくつかの固定された領域に分割されているとする．このとき，$i$番目の領域中での密度$p(\mathbf{x})$は一定の値$h_i$であり，領域$i$の体積が$\Delta_i$であるような，ヒストグラム型の密度モデルを考える．$N$個の$\mathbf{x}$の観測値集合があり，領域$i$に入る観測値が、$n_i$個であるとする．このとき，密度の正規化制約条件をラグランジュ乗数法によって実現し，${ h_i }$の最尤推定量の式を導出せよ．</p>
</div>
<p>※ ノンパラメトリック法に関連した問題だが、手法として最尤推定を使うので、2.2 多値変数の手法を見直しながらラグランジュ未定乗数法で対数尤度関数の最大化を目指し、${ h_i }$の最尤推定量を求めていく。しかし、その上で観測データ点$\mathbf{x}_n$が領域$j$にあることを示す変数$j(n)$を使うというのは発想として難しいと思われる。</p>
<p>観測データ点$\mathbf{x}_n$が領域$j$にあることを示す変数$j(n)$を定義すると、点$\mathbf{x}<em>n$での密度$p(\mathbf{x})$の値は$h</em>{j(n)}$で与えられることになる。これを用いると、観測データセット$\mathbf{X} = { \mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n}$が発生する尤度関数は</p>
<p>$$
\prod_{n=1}^{N}p(\mathbf{x}<em>n) = \prod</em>{n=1}^{N}h_{j(n)}
$$</p>
<p>と表せる。すると、この対数尤度関数は</p>
<p>$$
\ln \prod_{n=1}^{N}h_{j(n)} = \sum_{n=1}^{N}\ln h_{j(n)}
$$</p>
<p>となる。</p>
<p>${h_i}$の最尤推定解を求めるには、制約条件として$\displaystyle \sum_{i=1}^{N} h_i \Delta_i = 1$となることを考慮しつつ、$h_i$についての対数尤度関数$\displaystyle \sum_{n=1}^{N}\ln h_{j(n)}$を最大化する必要がある。すなわちラグランジュ乗数$\lambda$を使って</p>
<p>$$
L = \sum_{n=1}^{N}\ln h_{j(n)} + \lambda\left( \sum_{i=1}^{N} h_i \Delta_i - 1 \right)
$$</p>
<p>を最大化する。$h_i$についての導関数を0とおくと</p>
<p>$$
\frac{\partial L}{\partial h_i} = \frac{\partial}{\partial h_i}\sum_{n=1}^{N}\ln h_{j(n)}+\lambda \Delta_i = 0
$$</p>
<p>ここで$\displaystyle \frac{\partial}{\partial h_i}\sum_{n=1}^{N}\ln h_{j(n)}$について、$j(n)=i$となるような観測値は問題文の設定から$n_i$個なので</p>
<p>$$
\frac{\partial}{\partial h_i}\sum_{n=1}^{N}\ln h_{j(n)} = n_i \cdot \frac{1}{h_i}
$$</p>
<p>よって$\displaystyle \frac{n_i}{h_i}+\lambda \Delta_i = 0$。変形すると$n_i + \lambda \Delta_i h_i= 0$。</p>
<p>$i$について和を取ると$\sum_i (n_i + \lambda \Delta_i h_i)= N + \lambda = 0$となるので、$\lambda = -N$が求まる。</p>
<p>以上から、$h_i$の最尤推定解の式は</p>
<p>$$
h_i = -\frac{n_i}{\lambda \Delta_i} = \frac{n_i}{N\Delta_i}
$$</p>
<p>となる。</p>
<h2 id="演習-261"><a class="header" href="#演習-261">演習 2.61</a></h2>
<div class="panel-primary">
<p>$K$近傍密度モデルでは，全空間上での積分が発散する変則分布になることを示せ．</p>
</div>
<p>【解法1】 ざっくりとしたやり方</p>
<p>ある$D$次元ユークリッド空間上での未知の確率密度$p(\mathbf{x})$から観測値の集合が得られていて、この集合から$p(\mathbf{x})$の値を推定したいとする。$(2.246)$までの議論から、K近傍法では密度$p(\mathbf{x})$を推定したい点$\mathbf{x}$を中心とした半径$r$の$D$次元小球の体積$V(r)$中に$K$個のデータ点が存在する時（これは言い換えれば$r$は$N$個の観測値からなるデータセット内で$\mathbf{x}$から見て$K$番目に近い点までの距離$r$をとった時）、密度$p(\mathbf{x})$の推定値が</p>
<p>$$
p(\mathbf{x}) = \frac{K}{NV(r)}
$$</p>
<p>で与えられることになる。</p>
<p>この問題の題意は「全空間上での積分が発散する」ことを示すことなので、$\displaystyle \int p(\mathbf{x})d\mathbf{x} \to \infty$を示せば良い。</p>
<p>ここで、この$p(\mathbf{x})$を極座標中で考えると、もし十分に大きな半径の値$r$をとったとき、上式から$p(\mathbf{x}) \propto r^{-D}$となることがわかる。また、$D$次元直交座標から極座標に変換するときのヤコビアンは（参考：https://wasan.hatenablog.com/entry/20110321/1300733907）</p>
<p>$$
\begin{aligned}
\left|\frac{\partial\left(x_{1}, \cdots, x_{D}\right)}{\partial\left(r, \theta_{1}, \cdots, \theta_{D-1}\right)}\right| &amp;=\left|\frac{\partial\left(x_{1}, \cdots, x_{D}\right)}{\partial\left(x_{1}, \rho, \theta_{2}, \cdots, \theta_{D-1}\right)}\right|\left|\frac{\partial\left(x_{1}, \rho, \theta_{2}, \cdots, \theta_{D-1}\right)}{\partial\left(r, \theta_{1}, \cdots, \theta_{D-1}\right)}\right| \ &amp;=\left{\rho^{D-2} \prod_{i=2}^{D-1}\left(\sin \theta_{i}\right)^{D-i-1}\right} r \ &amp;=r\left(r \sin \theta_{1}\right)^{D-2} \prod_{i=2}^{D-1}\left(\sin \theta_{i}\right)^{D-i-1} \quad\left(\because \rho=r \sin \theta_{1}\right) \ &amp;=r^{D-1} \prod_{i=1}^{D-1}\left(\sin \theta_{i}\right)^{D-i-1} \end{aligned}
$$</p>
<p>より、</p>
<p>$$
d\mathbf{x} = d x_{1} \cdots d x_{D}=r^{D-1}\left{\prod_{i=1}^{D-1}\left(\sin \theta_{i}\right)^{D-i-1}\right} d r d \theta_{1} \cdots d \theta_{D-1}
$$
となるので、</p>
<p>$$
\int p(\mathbf{x})d \mathbf{x} \propto \int r^{-D}d\mathbf{x} \propto \int r^{-D}r^{D-1} dr d \theta_{1} \cdots d \theta_{D-1} \propto \int r^{-1}dr = \ln r
$$</p>
<p>これは$r\to \infty$の無限大で$\ln r \to \infty$ となるため全空間上での積分は発散することが示された。すなわち、$K$近傍密度モデルは変則分布であることが示された。</p>
<p>【解法2】より厳密な解き方</p>
<p>https://qiita.com/r-takahama/items/cc03cffd49ad4732ebd3 に書かれてあります。</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="prml第3章演習問題解答"><a class="header" href="#prml第3章演習問題解答">PRML第3章演習問題解答</a></h1>
<head>
<style>
  div.panel-primary {
	border: 1px solid #000;
    margin: 10px 5px;
    padding: 16px 10px 0px;
  }
</style>
</head>
<h2 id="演習-31"><a class="header" href="#演習-31">演習 3.1</a></h2>
<div class="panel-primary">
<p>tanh関数とロジステイックシグモイド関数</p>
<p>$$
\sigma(a)=\frac{1}{1+\exp (-a)} \tag{3.6}
$$</p>
<p>は次のように関係付けられることを示せ．</p>
<p>$$
\tanh(a) = 2\sigma(2a)-1 \tag{3.100}
$$</p>
<p>さらに，次の形のロジステイックシグモイド関数の線形結合</p>
<p>$$
y(x, \mathbf{w})=w_{0}+\sum_{j=1}^{M} w_{j} \sigma\left(\frac{x-\mu_{j}}{s}\right) \tag{3.101}
$$</p>
<p>は次の形の$\tanh$関数の線形結合</p>
<p>$$
y(x, \mathbf{u})=u_{0}+\sum_{j=1}^{M} u_{j} \tanh \left(\frac{x-\mu_{j}}{2 s}\right) \tag{3.102}
$$</p>
<p>と等価であることを示し，新しいパラメータ${ u_0, \ldots, u_M}$ともとのパラメータ${ w_0, \ldots, w_M}$を関係付ける式を求めよ．</p>
</div>
<p>双曲線関数$\sinh$と$\cosh$関数を使うと
$$
\sinh a = \frac{e^a - e^{-a}}{2},\quad \cosh a = \frac{e^a + e^{-a}}{2}
$$
であるから、
$$
\tanh a = \frac{\sinh a}{\cosh a} = \frac{e^a - e^{-a}}{e^a + e^{-a}}
$$
である。これと$(3.100)$式の右辺を計算すると
$$
2\sigma(2a) -1 = \frac{2}{1+e^{-2a}}-1 = \frac{1-e^{-2a}}{1+e^{-2a}} = \frac{e^a - e^{-a}}{e^a + e^{-a}}
$$
となるので、$\tanh a = 2\sigma(2a) -1$が示された。</p>
<p>また$(3.100)$の関係式から$\displaystyle \sigma(a) = \frac{1}{2}\left{ \tanh \left(\frac{a}{2}\right) + 1\right}$となるので、</p>
<p>$$
\begin{aligned}
w_0+\sum_{j=1}^M w_j \sigma\left( \frac{x-\mu_j}{s} \right) &amp;= w_0 + \sum_{j=1}^M \left{ \frac{w_j}{2} \tanh \left( \frac{x-\mu_j}{2s} \right) + \frac{w_j}{2} \right} \
&amp;= w_0 + \sum_{j=1}^M \frac{w_j}{2} + \sum_{j=1}^M \frac{w_j}{2} \tanh \left( \frac{x-\mu_j}{2s} \right)
\end{aligned}
$$
これと$(3.102)$式の形を比較すれば
$$
u_0 = w_0 + \sum_{j=1}^M \frac{w_j}{2},\quad u_j = \frac{w_j}{2}
$$
と関係付けることができる。</p>
<h2 id="演習-32"><a class="header" href="#演習-32">演習 3.2</a></h2>
<div class="panel-primary">
<p>行列</p>
<p>$$
\mathbf{\Phi}\left(\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm{T}} \tag{3.103}
$$</p>
<p>は任意のベクトル$\mathbf{v}$を$\mathbf{\Phi}$の列ベクトルで張られる空間の上に正射影することを示せ．そしてこの結果を使って，最小二乗解</p>
<p>$$
\mathbf{w}_{\mathrm{ML}}=\left(\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm{T}} \textsf{t} \tag{3.15}
$$</p>
<p>は図3.2で示した多様体$\mathcal{S}$の上にベクトル$\mathsf{t}$を正射影することに対応していることを示せ．</p>
<p><img src="/attachment/5f4281e56b27b0172dd66336" alt="PRML_Fig3.2.png" /><br>
Figure 3.2</p>
</div>
<p>【前半】</p>
<p>この問題で線形空間（多様体）$\mathcal{S}$は行列$\mathbf{\Phi}$の$j$番目の列ベクトル$\boldsymbol{\varphi}<em>j$を基底としている。つまり、任意のスカラー$x_j$を用いて$\displaystyle \sum</em>{j=1}^M x_j\boldsymbol{\varphi}_j$の形で書けるベクトルは線形空間$\mathcal{S}$に含まれる。</p>
<p>あるベクトル$\mathbf{v}$の線形空間$\mathcal{S}$への正射影とは、次の2つを満たすベクトル$\mathbf{v}^{\prime}$のことである。</p>
<ol>
<li>
<p>ベクトル$\mathbf{v}^{\prime}$が$\mathcal{S}$上に存在する。すなわち$\displaystyle \mathbf{v}^{\prime} = \sum_{j=1}^M x_j\boldsymbol{\varphi}_j=\mathbf{\Phi}\mathbf{x}$と書ける。</p>
</li>
<li>
<p>ベクトル$\mathbf{v}-\mathbf{v}^{\prime}$が線形空間$\mathcal{S}$と直交する。すなわち任意の$j$について$\boldsymbol{\varphi}_j^{\mathrm{T}}(\mathbf{v}-\mathbf{v}^{\prime})=0$つまり$\displaystyle \mathbf{\Phi}^{\mathrm{T}}(\mathbf{v}-\mathbf{v}^{\prime})=0$が成立する。</p>
</li>
</ol>
<p>以上を踏まえて、まずベクトル$\mathbf{v}$に$\mathbf{\Phi}\left(\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm{T}}$を作用させた$\mathbf{v}^{\prime}$を考える。すなわち
$$
\begin{aligned}
\mathbf{v}^{\prime} &amp;= \mathbf{\Phi}\left(\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm{T}}\mathbf{v} = \mathbf{\Phi}\tilde{{\mathbf{v}}}=\sum_{j=1}^M \tilde{v_j} \boldsymbol{\varphi}_j
\end{aligned}
$$
とする。ここで$\boldsymbol{\varphi}_j$は$\mathbf{\Phi}$の$j$番目の列ベクトルで$\tilde{\mathbf{v}} \equiv \left(\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm{T}}\mathbf{v}$とする。$\tilde{v_j}$はベクトル$\tilde{\mathbf{v}}$の$j$番目の要素である（スカラー）。これは$\boldsymbol{\varphi}_j$についての線形結合となっているので、上の正射影の条件1を満たしている。</p>
<p>ちなみに</p>
<p>$$
\Phi=\begin{pmatrix}\mathbf{\phi}<em>{0}\left(\mathbf{x}</em>{1}\right) &amp; \phi_{1}\left(\mathbf{x}<em>{1}\right) &amp; \cdots &amp; \phi</em>{M-1}\left(\mathbf{x}<em>{1}\right) \ \phi</em>{0}\left(\mathbf{x}<em>{2}\right) &amp; \phi</em>{1}\left(\mathbf{x}<em>{2}\right) &amp; \cdots &amp; \phi</em>{M-1}\left(\mathbf{x}<em>{2}\right) \ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \ \phi</em>{0}\left(\mathbf{x}<em>{N}\right) &amp; \phi</em>{1}\left(\mathbf{x}<em>{N}\right) &amp; \cdots &amp; \phi</em>{M-1}\left(\mathbf{x}_{N}\right)\end{pmatrix} = (\boldsymbol{\varphi}_1\ \boldsymbol{\varphi}<em>2\ \cdots \ \boldsymbol{\varphi}</em>{M})  \tag{3.16}
$$</p>
<p>としたので番号は1つずつずれているけれど気にしないでOK。</p>
<p>次に正射影の条件2のために$\mathbf{v}-\mathbf{v}^{\prime}$と$\mathcal{S}$の直交性を調べる。</p>
<p>$$
\begin{aligned}
\mathbf{\Phi}^{\mathrm{T}}(\mathbf{v}-\mathbf{v}^{\prime}) &amp;= \mathbf{\Phi}^{\mathrm{T}}(\mathbf{I} - \mathbf{\Phi}(\mathbf{\Phi}^{\mathrm{T}}\mathbf{\Phi})^{-1}\mathbf{\Phi}^{\mathrm{T}})\mathbf{v} \
&amp;=\mathbf{\Phi}^{\mathrm{T}}\mathbf{v} - \mathbf{\Phi}^{\mathrm{T}}\mathbf{v} \
&amp;= \mathbf{0}
\end{aligned}
$$
であるから、$\mathbf{v}-\mathbf{v}^{\prime}$と$\mathcal{S}$は直交していることが示された。以上条件1と2が成立しているので、$\mathbf{v}^{\prime}$は$\mathbf{v}$の$\mathcal{S}$への正射影であることが示された。</p>
<p>（参考：https://python.atelierkobato.com/projection/）</p>
<p>【後半】</p>
<p>続いて、3.1.2節の流れから$n$番目の要素が$y(\mathbf{x}_n,\mathbf{w})$で与えられる$N$次元ベクトル$\mathsf{y}$を定義すると、$\mathsf{y}$の構成は</p>
<p>$$
\mathsf{y} = \begin{pmatrix}y(\mathbf{x}_1,\mathbf{w}) \ \vdots \ y(\mathbf{x}_n,\mathbf{w})\end{pmatrix} = \begin{pmatrix}\mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}(\mathbf{x}_1) \ \vdots \ \mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}(\mathbf{x}_n)\end{pmatrix} = \begin{pmatrix}\boldsymbol{\phi}(\mathbf{x}_1)^{\mathrm{T}}\mathbf{w} \ \vdots \ \boldsymbol{\phi}(\mathbf{x}_n)^{\mathrm{T}}\mathbf{w}\end{pmatrix} = \mathbf{\Phi}\mathbf{w}
$$</p>
<p>となっている。これと$(3.15)$から$\mathbf{w}_{\mathrm{ML}} = (\mathbf{\Phi}^{\mathrm{T}}\mathbf{\Phi})^{-1}\mathbf{\Phi}^{\mathrm{T}} \mathsf{t}$を代入してみると</p>
<p>$$
\mathsf{y} = \mathbf{\Phi}\mathbf{w}_{\mathrm{ML}} = \mathbf{\Phi}(\mathbf{\Phi}^{\mathrm{T}}\mathbf{\Phi})^{-1}\mathbf{\Phi}^{\mathrm{T}} \mathsf{t}
$$</p>
<p>となる。ここで、この数式と【前半】の議論より、$\mathsf{y}$がベクトル$\mathsf{t}$の線形空間$\mathcal{S}$への正射影であることは明らかになっている。したがって題意（最小二乗解$\mathsf{y}$はデータベクトル$\mathsf{t}$の部分空間$\mathcal{S}$上への正射影に対応する）は示された。</p>
<blockquote>
<p>統計のための行列代数（上）の第12章 射影と射影行列に詳しい議論が載っている。</p>
</blockquote>
<h2 id="演習-33"><a class="header" href="#演習-33">演習 3.3</a></h2>
<div class="panel-primary">
<p>それぞれのデータ点$t_n$に重み要素$r_n&gt;0$が割り当てられており，二乗和誤差関数が</p>
<p>$$
E_{D}(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N} r_{n}\left{t_{n}-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right}^{2} \tag{3.104}
$$</p>
<p>となるデータ集合を考える．このとき，この誤差関数を最小にする解$\mathbf{w}^{*}$についての式を求めよ．また，(i)ノイズの分散がデータに依存する場合，(ii)データ点に重複がある場合に照らして，それぞれ重み付き二乗和誤差関数の解釈を与えよ．</p>
</div>
<p>誤差関数を最小にする解を求めたいので、$(3.104)$式を$\mathbf{w}$で微分する。その前に$(3.104)$式を行列形式で書き直したい。</p>
<p>$\displaystyle \mathbf{R} = \operatorname{diag}(r_1,r_2,\ldots,r_n)$（つまり対角成分が$r_1,r_2,\ldots,r_n$で残りが$0$の行列）とすると、$(3.104)$式は次のように書ける。</p>
<p>$$
\begin{aligned}
E_{D}(\mathbf{w}) &amp;=\frac{1}{2} \sum_{n=1}^{N} r_{n}\left{t_{n}-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}<em>{n}\right)\right}^{2} \
&amp;=\frac{1}{2}\left{\left(\begin{array}{c}
t</em>{1} \
\vdots \
t_{n}
\end{array}\right)-\left(\boldsymbol{\phi}\left(\mathbf{x}<em>{1}\right), \cdots, \boldsymbol{\phi}\left(\mathbf{x}</em>{n}\right)\right)\left(\begin{array}{c}
w_{1} \
\vdots \
w_{n}
\end{array}\right)\right}^{\mathrm{T}} \mathbf{R} \left{\left(\begin{array}{c}
t_{1} \
\vdots \
t_{n}
\end{array}\right)-\left(\boldsymbol{\phi}\left(\mathbf{x}<em>{1}\right), \cdots, \boldsymbol{\phi}\left(\mathbf{x}</em>{n}\right)\right)\left(\begin{array}{c}
w_{1} \
\vdots \
w_{n}
\end{array}\right)\right} \
&amp;=\frac{1}{2}(\mathsf{t}-\mathbf{\Phi} \mathbf{w})^{\mathrm{T}} \mathbf{R}(\mathsf{t}-\mathbf{\Phi} \mathbf{w})
\end{aligned}
$$
$\mathbf{w}$で微分すると
$$
\begin{aligned}
\frac{\partial E_D}{\partial \mathbf{w}} &amp;=\frac{1}{2}\left{-2 \mathbf{\Phi}^{\mathrm{T}} \mathbf{R}(\mathsf{t}-\mathbf{\Phi} \mathbf{w})\right} \quad \left(\because \frac{\partial}{\partial \mathbf{s}}(\mathbf{x}-\mathbf{A} \mathbf{s})^{\mathrm{T}} \mathbf{W}(\mathbf{x}-\mathbf{A} \mathbf{s})=-2 \mathbf{A}^{\mathrm{T}} \mathbf{W}(\mathbf{x}-\mathbf{A} \mathbf{s}),\ \textrm{if}\ \mathbf{W} \textrm{is symmetric.}\right)\
&amp;=-\mathbf{\Phi}^{\mathrm{T}} \mathbf{R}(\mathsf{t}-\mathbf{\Phi} \mathbf{w})=0 \
&amp; \mathbf{\Phi}^{\mathrm{T}} \mathbf{R} \mathsf{t}=\mathbf{\Phi}^{\mathrm{T}} \mathbf{R} \mathbf{\Phi} \mathbf{w}^{<em>} \
\therefore \quad \mathbf{w}^{</em>}&amp;=\left(\mathbf{\Phi}^{\mathrm{T}} \mathbf{R} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm{T}} \mathbf{R} \mathsf{t}
\end{aligned}
$$
なお行列の微分には https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf の$(84)$の公式を使った。また、こうして得られた$\mathbf{w}^{*}$は単位行列$\mathbf{R} = \mathbf{I}$とすれば$(3.15)$の正規方程式と一致する。</p>
<ol>
<li>
<p>ノイズの分散がデータに依存する場合というのは$(3.8)$において全データの分散の逆数（精度）が$\beta$で一定ではなく、データごとに$y_n$と変化することを意味する。このとき$(3.8)-(3.12)$の導出に沿って対数尤度関数をとると
$$
\begin{aligned}
\ln p(\mathsf{t} \mid \mathbf{x}, \mathbf{w}, \mathbf{y}) &amp;=\sum_{n=1}^{N} \ln \mathcal{N}\left(t_n \mid \mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}<em>{n}\right), y</em>{n}^{-1}\right) \
&amp;=\sum_{n=1}^{N} \ln \left{\frac{y_{n}^{1 / 2}}{(2 \pi)^{1 / 2}} \exp \left(-\frac{y_{n}\left(t_{n}-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}<em>{n}\right)\right)^{2}}{2}\right)\right} \
&amp;=\frac{1}{2} \sum</em>{n=1}^{N} \ln y_{n}-\frac{N}{2} \ln (2 \pi)-\frac{1}{2} \sum_{n=1}^{N} y_{n}\left{t_{n}-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right}^{2}
\end{aligned}
$$
となる。ここで$y_n = r_n$とすればまさに重み付き二乗和誤差関数として$(3.104)$が現れていることがわかる。</p>
</li>
<li>
<p>データ点に重複がある場合、そのデータ点についての<strong>実効的な</strong>数として見なすことができる。尤度関数$\ln p(\mathsf{t}\mid \mathbf{w},\beta)$の最大化は二乗和誤差関数$E_D(\mathbf{w})$の最小化と等価であることを考えれば、例えば$N$個の点のうち$(\mathbf{x}_1, t_1), (\mathbf{x}_2, t_2)$のみが同じだった場合に$r_1=1, r_2=0$とおけば実質1つカウントとして見なせるし、そうしないこともできる……ってことかな？</p>
</li>
</ol>
<h2 id="演習-34"><a class="header" href="#演習-34">演習 3.4</a></h2>
<div class="panel-primary">
<p>次の形の線形モデル</p>
<p>$$
y(\mathbf{x}, \mathbf{w})=w_{0}+\sum_{i=1}^{D} w_{i} x_{i} \tag{3.105}
$$</p>
<p>と二乗和誤差関数</p>
<p>$$
E_{D}(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N}\left{y\left(\mathbf{x}<em>{n}, \mathbf{w}\right)-t</em>{n}\right}^{2} \tag{3.106}
$$</p>
<p>を考える．平均$0$，分散$\sigma^2$のガウスノイズ$\epsilon_i$が独立にそれぞれの入力変数$x_i$に加わるものとする．$\mathbb{E}[\epsilon_i] = 0$と$\mathbb{E}\left[\epsilon_{i} \epsilon_{j}\right]=\delta_{i j} \sigma^{2}$の2つの性質を用いて，$E_D$のノイズ分布に関する平均を最小にすることは，ノイズのない入力変数に対する二乗和誤差と荷重減衰の正則化項の和を最小にすることと等価であることを示せ．ただし，正則化項にバイアスパラメータ$w_0$は含めない．</p>
</div>
<p>※ 線形モデルの入力ベクトル$\mathbf{x}$の各次元$x_i$にノイズ$\epsilon_i$が加わったときの二乗和誤差について、その二乗和誤差のノイズ$\epsilon_i$についての期待値を取ったものを最小化することが、$\mathbf{w}$の正則化項を考慮した最小二乗法と同じ形式になることを示します。</p>
<p>ガウスノイズ$\epsilon_{i}$が入力変数$x_i$に加えられるので
$$
\begin{aligned}
\tilde{y}(\mathbf{x}<em>n,\mathbf{w}) &amp;= w_0 + \sum</em>{i=1}^D w_{ni}(x_{ni}+\epsilon_{ni}) \
&amp;= y(\mathbf{x}<em>n,\mathbf{w})+\sum</em>{i=1}^D w_i \epsilon_{ni}
\end{aligned}
$$
となる。これの二乗和誤差関数は
$$
\begin{aligned}
\tilde{E}<em>D(\mathbf{w}) &amp;= \frac{1}{2}\sum</em>{n=1}^N\left{
\tilde{y}(\mathbf{x}<em>n,\mathbf{w}) - t_n
\right}^2 \
&amp;=\frac{1}{2}\sum</em>{n=1}^N\left{
\tilde{y}<em>n^2-2\tilde{y}<em>nt_n+t_n
\right}^2 \
&amp;=\frac{1}{2}\sum</em>{n=1}^N\left{
y_n^2+2y_n\sum</em>{i=1}^D{w_i\epsilon_{ni}}+\left(\sum_{i=1}^D w_i \epsilon_{ni} \right)^2 -2 y_n t_n-2\sum_{i=1}^D w_i\epsilon_{ni} + t_n^2
\right} \quad \cdots (<em>)
\end{aligned}
$$
と展開できる。ここで$\tilde{E}_D(\mathbf{w})$についての期待値$\mathbb{E}\left[\tilde{E}_D(\mathbf{w})\right]$をとると$\mathbb{E}[\epsilon_i]=0$より$(</em>)$の第2項と第5項は0になる。また、第3項については$\mathbb{E}[\epsilon_i \epsilon_j]=\delta_{ij}\sigma^2$から
$$
\mathbb{E}\left[\left( \sum_{i=1}^D w_i \epsilon_{ni}\right)^2\right] = \sum_{i=1}^D w_i^2\sigma^2
$$
となるので
$$
\mathbb{E}\left[\tilde{E}<em>D(\mathbf{w})\right] = E_D(\mathbf{w}) + \frac{1}{2}\sum</em>{i=1}^D w_i^2\sigma^2
$$
と表せる。さらに$(3.25)$のような重みベクトルの二乗和$E_W(\mathbf{w})=\frac{1}{2}\mathbf{w}^{\mathrm{T}}\mathbf{w}$を用いると
$$
\mathbb{E}\left[\tilde{E}_D(\mathbf{w})\right] = E_D(\mathbf{w}) + \frac{\sigma^2}{2}\mathbf{w}^{\mathrm{T}}\mathbf{w}
$$
となる。これは、ノイズのない入力変数に対する二乗和誤差$E_D(\mathbf{w})$と$\lambda=\sigma^2$にしたときの荷重減衰の正則化項の和$E_W(\mathbf{w})$の和の形になっている。したがって題意が示された。</p>
<blockquote>
<p>この問題の意味を考えてみましょう。簡単のため、入力が一次元$x_1$だけだったとします。
入力$x_1$にノイズ$\epsilon_1$が加わると、そのノイズもパラメータ$w_1$倍されて、誤差に含まれることになります。二乗誤差なので$w_1^2$倍になります。ノイズの分布に関して二乗誤差の期待値を取ると、ノイズの平均はゼロなので、平均の影響は消えてしまうのですが、分散の影響は$\sigma^2 \epsilon_1^2$として残ります。つまり、分散が大きいほど、二乗誤差が大きくなります。そのため、この二乗誤差を最小化しようとすると、$w_1^2$を小さくする方向に力が働く(正則化)ことになります。</p>
<p>入力が多次元だったとしても、基本的な考えは同じです。ノイズの平均と共分散がゼロなので、ほとんどの項がうまく消えてしまいます。入力がノイズで撹乱されるほど(分散$\sigma^2$が大きいほど)、二乗和誤差は増加します。</p>
</blockquote>
<h2 id="演習-35"><a class="header" href="#演習-35">演習 3.5</a></h2>
<div class="panel-primary">
<p>付録Eに示したラグランジュ未定乗数法を用いて，正則化誤差関数</p>
<p>$$
\frac{1}{2} \sum_{n=1}^{N}\left{t_{n}-\mathbf{w}^{\mathrm{T}} \phi\left(\mathrm{x}<em>{n}\right)\right}^{2}+\frac{\lambda}{2} \sum</em>{j=1}^{M}\left|w_{j}\right|^{q} \tag{3.29}
$$</p>
<p>の最小化と，正則化されていない二乗和誤差</p>
<p>$$
E_{D}(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N}\left{t_{n}-\mathbf{w}^{\mathrm{T}} \phi\left(\mathbf{x}_{n}\right)\right}^{2} \tag{3.12}
$$</p>
<p>の制約条件</p>
<p>$$
\sum_{j=1}^{M}|w_j|^q \le \eta \tag{3.30}
$$</p>
<p>下での最小化が等価であることを示せ．そして，パラメータ$\eta$と$\lambda$の関係を議論せよ．</p>
</div>
<p>※ ヒントとしてラグランジュの未定乗数法を使うと書いてあるのですぐに導けるが、このヒントがないとなぜ等価なのかわかりにくいかもしれない、という問題。</p>
<p>制約条件$(3.30)$が不等式なので付録Eのラグランジュ未定乗数法の不等式制約の場合を参考にする。</p>
<p>$(3.30)$を変形すると$\displaystyle \frac{1}{2}\left( \sum_{j=1}^M|w_j|^q - \eta \right) \leq 0$である。（後で$(3.29)$に合わせるために$\frac{1}{2}$をわざとつけている）</p>
<p>ラグランジュの未定乗数法を用いると</p>
<p>$$
\begin{aligned}
L(\mathbf{w},\lambda) &amp;= \frac{1}{2}\sum_{j=1}^{M}\left{ t_n - \mathbf{w}^{\mathrm{T}}\phi(\mathbf{x}<em>n)\right}^2 + \frac{\lambda}{2}\left( \sum</em>{j=1}^M|w_j|^q - \eta \right) \
&amp;= \frac{1}{2}\sum_{j=1}^{M}\left{ t_n - \mathbf{w}^{\mathrm{T}}\phi(\mathbf{x}<em>n)\right}^2 + \frac{\lambda}{2}\sum</em>{j=1}^M|w_j|^q - \frac{\lambda\eta}{2}
\end{aligned}
$$</p>
<p>これについて$\displaystyle \frac{\partial}{\partial\mathbf{w}}\left( - \frac{\lambda\eta}{2} \right) = 0$なので、$L(\mathbf{w},\lambda)$を$\mathbf{w}$について最小化させることと、$(3.29)$式の$\mathbf{w}$についての最小化は等価であることが示された。</p>
<p>またラグランジュの未定乗数法の不等式制約におけるKarush-Kuhn-Tucker条件は
$$
\left{
\begin{array}{l}
\frac{1}{2}\left( \sum_{j=1}^M|w_j|^q - \eta \right) \leq 0 \
\lambda \geq 0 \
\frac{\lambda}{2}\left( \sum_{j=1}^M|w_j|^q - \eta \right) = 0
\end{array}
\right.
$$
となる。最後の等式が$L(\mathbf{w},\lambda)$を$\mathbf{w}$について最小化させた$\mathbf{w}^<em>$で成立する必要がある。
すなわち、$\mathbf{w}^</em>$で$\displaystyle \eta = \sum_{j=1}^M|w_j|^q$となる。これより、$q$は正則化項の形状（lasso, ridge...）を示している。$\lambda$は正則化項の大きさを表している。</p>
<blockquote>
<p>とても重要な回帰におけるパラメータ正則化(もしくは重み減衰)の問題。パラメータ正則化は非常に重要なテクニックなのですが、よくある下の図を見てなんとなく理解したつもりになっている人が多い気がします。<br>
<img src="/attachment/611f4b7fe07f664fed1981a3" width="400px"><br>
$q=2$のときの正則化項を加えた$(3.24)$の誤差関数を最小化することが上の図の通りになることを考察してみます。制約条件$(3.30)$はパラメータ$\mathbf{w}$が図の領域のオレンジ部分$\sum_{j=1}^{M}|w_j|^q \leq \eta$内に存在しなければならないことを表しています。青線は正則化されていない二乗和誤差関数$(3.26)$式の$E_D(\mathbf{w})$の等高線表示です。ラグランジュの未定乗数法を使うことで、領域$(3.30)$の制限下での誤差関数$E_D(\mathbf{w})$の最小化ができるようになり、これが$(3.27)$（$q=2$の場合）、$(3.29)$(一般形)と等価になります。</p>
</blockquote>
<h2 id="演習-36"><a class="header" href="#演習-36">演習 3.6</a></h2>
<div class="panel-primary">
<p>ガウス分布に従う複数の目標変数$\mathbf{t}$を持つ次の形の線形基底関数モデルを考える．</p>
<p>$$
p(\mathbf{t} | \mathbf{W}, \mathbf{\Sigma})=\mathcal{N}(\mathbf{t} | \mathbf{y}(\mathbf{x}, \mathbf{W}), \mathbf{\Sigma}) \tag{3.107}
$$</p>
<p>ただし，</p>
<p>$$
\mathbf{y}(\mathbf{x}, \mathbf{W}) = \mathbf{W}^{\mathrm{T}}\boldsymbol{\phi}(\mathbf{x}) \tag{3.108}
$$</p>
<p>である．入力基底ベクトルの$\boldsymbol{\phi}(\mathbf{x}_n)\ (n=1,\ldots,N)$とそれに対応する目標ベクトル$\mathbf{t}<em>n$が訓練データ集合として与えられるとき，パラメータ行列$\mathbf{W}$の最尤推定解$\mathbf{W}</em>{\mathrm{ML}}$のそれぞれの列が，等方性のノイズ分布に対する解の</p>
<p>$$
\mathbf{w}_{\mathrm{ML}}=\left(\Phi^{\mathrm{T}} \Phi\right)^{-1} \Phi^{\mathrm{T}} \mathbf{t} \tag{3.15}
$$</p>
<p>の形の式で与えられることを示せ．これは共分散行列$\mathbf{\Sigma}$にはよらないことに注意せよ．さらに，$\mathbf{\Sigma}$の最尤推定解が</p>
<p>$$
\mathbf{\Sigma}=\frac{1}{N} \sum_{n=1}^{N}\left(\mathbf{t}<em>{n}-\mathbf{W}</em>{\mathrm{ML}}^{\mathrm{T}} \phi\left(\mathbf{x}<em>{n}\right)\right)\left(\mathbf{t}</em>{n}-\mathbf{W}<em>{\mathrm{ML}}^{\mathrm{T}} \phi\left(\mathbf{x}</em>{n}\right)\right)^{\mathrm{T}} \tag{3.109}
$$</p>
<p>で与えられることを示せ．</p>
</div>
<p>※ <strong>3.1.5 出力変数が多次元の場合</strong>　を参考にしてすすめる。また、途中の行列の微分については付録だけでは足りないので https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf を参考にする。</p>
<p>$\mathbf{y}(\mathbf{X},\mathbf{W})=\mathbf{W}^{\mathrm T}\boldsymbol{\phi}(\mathbf{x})$について、$\mathbf{y}$は$K$次元列ベクトル、$\mathbf{W}$は$M\times K$のパラメータ行列、$\boldsymbol{\phi}(\mathbf{x})$は$\boldsymbol{\phi}_j(\mathbf{x})$を$j$番目の要素に持つ$M$次元の列ベクトルである。また、$n$番目の観測値$\mathbf{t}_n$は$K$次元の列ベクトル、観測値の集合$\mathbf{t}_1, \mathbf{t}_2, \cdots \mathbf{t}_N$をまとめて$n$番目の行が$\mathbf{t}_n^{\mathrm T}$となる$N\times K$行列$\mathbf{T}$とする。</p>
<p>$(3.107)$について最尤推定解を得るために対数尤度関数を考えると</p>
<p>$$
\begin{aligned}
\ln p(\mathbf{t} \mid \mathbf{W}, \mathbf{\Sigma}) &amp;=\ln \prod_{n=1}^{N} \mathcal{N}\left(\mathbf{t}<em>{n} \mid \mathbf{W}^{\mathrm T} \boldsymbol{\phi}(\mathbf{x}), \mathbf{\Sigma}\right) \
&amp;=\sum</em>{n=1}^{N} \ln \mathcal{N}\left(\mathbf{t}<em>{n} \mid \mathbf{W}^{\mathrm T} \boldsymbol{\phi}(\mathbf{x}), \mathbf{\Sigma}\right) \
&amp;=\sum</em>{n=1}^{N} \ln \left{\frac{1}{(2 \pi)^{\frac{K}{2}}} \frac{1}{\left|\mathbf{\Sigma}\right|^{\frac{1}{2}}} \exp \left(-\frac{1}{2}\left(\mathbf{t}<em>{n}-\mathbf{W}^{\mathrm T} \boldsymbol{\phi}\left(\mathbf{x}</em>{n}\right)\right)^{\mathrm T} \mathbf{\Sigma}^{-1} \left(\mathbf{t}<em>{n}-\mathbf{W}^{\mathrm T} \boldsymbol{\phi}\left(\mathbf{x}</em>{n}\right)\right)\right)\right}\
&amp;=-\frac{N K}{2} \ln (2 \pi)-\frac{N}{2} \ln |\mathbf{\Sigma}|-\frac{1}{2} \sum_{n=1}^{N}\left(\mathbf{t}<em>{n}-\mathbf{W}^{\mathrm T} \boldsymbol{\phi}\left(\mathbf{x}</em>{n}\right)\right)^{\mathrm T} \mathbf{\Sigma}^{-1} \left(\mathbf{t}<em>{n}-\mathbf{W}^{\mathrm T} \boldsymbol{\phi}\left(\mathbf{x}</em>{n}\right)\right)
\end{aligned}
$$</p>
<p>$\mathbf{W}_{\mathrm{ML}}$を求めるために$\mathbf{W}$について偏微分すると</p>
<p>$$
\begin{aligned}
\frac{\partial}{\partial \mathbf{W}} \ln p(t \mid \mathbf{W}, \Sigma) &amp;=-\frac{1}{2} \sum_{n=1}^{N} \frac{\partial}{\partial \mathbf{W}}\left{\left(\mathbf{t}<em>{n}-\mathbf{W}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}</em>{n}\right)\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1} \left(\mathbf{t}<em>{n}-\mathbf{W}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}</em>{n}\right)\right)\right} \
&amp;= -\frac{1}{2}\sum_{n=1}^N \left( \mathbf{\Sigma}^{-1} + (\mathbf{\Sigma}^{-1})^{\mathrm T}\right) \left(\mathbf{t}<em>{n}-\mathbf{W}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}</em>{n}\right)\right) \left(- \boldsymbol{\phi}\left(\mathbf{x}<em>{n}\right)^{\mathrm T}\right) \quad \left(\because \frac{\partial}{\partial \mathbf{X}}(\mathbf{X} \mathbf{b}+\mathbf{c})^{\mathrm T} \mathbf{D}(\mathbf{X} \mathbf{b}+\mathbf{c})=\left(\mathbf{D}+\mathbf{D}^{\mathrm T}\right)(\mathbf{X} \mathbf{b}+\mathbf{c}) \mathbf{b}^{\mathrm T} \right)\
&amp;= \sum</em>{n=1}^N \mathbf{\Sigma}^{-1}\left(\mathbf{t}<em>{n}-\mathbf{W}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}</em>{n}\right)\right) \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)^{\mathrm T} \quad \left( \because \mathbf{\Sigma}^{-1} = (\mathbf{\Sigma}^{-1})^{\mathrm T} \right)
\end{aligned}
$$</p>
<p>これを$0$とすると
$$
\sum_{n=1}^N \mathbf{\Sigma}^{-1}\mathbf{t}<em>n\boldsymbol{\phi}(\mathbf{x}<em>n)^{\mathrm T} = \sum</em>{n=1}^N \mathbf{\Sigma}^{-1}\left( \mathbf{W}</em>{\mathrm{ML}}^{\mathrm T} \boldsymbol{\phi}(\mathbf{x}<em>n)\right) \boldsymbol{\phi}(\mathbf{x}<em>n)^{\mathrm T}
$$
両辺に$\mathbf{\Sigma}$をかけると
$$
\sum</em>{n=1}^N \mathbf{t}<em>n\boldsymbol{\phi}(\mathbf{x}<em>n)^{\mathrm T} =
\mathbf{W}</em>{\mathrm{ML}}^{\mathrm T} \sum</em>{n=1}^N \boldsymbol{\phi}(\mathbf{x}<em>n)\boldsymbol{\phi}(\mathbf{x}<em>n)^{\mathrm T}
$$
これは行列形式で書くと
$$
\left( \mathbf{t}<em>1, \mathbf{t}<em>2 \cdots, \mathbf{t}<em>n\right)\begin{pmatrix}
\boldsymbol{\phi}\left(\mathbf{x}</em>{1}\right)^{\mathrm{T}} \
\boldsymbol{\phi}\left(\mathbf{x}</em>{2}\right)^{\mathrm{T}} \
\vdots \
\boldsymbol{\phi}\left(\mathbf{x}</em>{N}\right)^{\mathrm{T}}
\end{pmatrix} = \mathbf{W}</em>{\mathrm{ML}}^{\mathrm T} \left(\boldsymbol{\phi}\left(\mathbf{x}</em>{1}\right), \boldsymbol{\phi}\left(\mathbf{x}</em>{2}\right),  \ldots, \boldsymbol{\phi}\left(\mathbf{x}<em>{N}\right)\right)\begin{pmatrix}
\boldsymbol{\phi}\left(\mathbf{x}</em>{1}\right)^{\mathrm{T}} \
\boldsymbol{\phi}\left(\mathbf{x}<em>{2}\right)^{\mathrm{T}} \
\vdots \
\boldsymbol{\phi}\left(\mathbf{x}</em>{N}\right)^{\mathrm{T}}
\end{pmatrix}
$$
と書き直せる。ここで、計画行列が
$$
\mathbf{\Phi}=\begin{pmatrix}
\boldsymbol{\phi}\left(\mathbf{x}<em>{1}\right)^{\mathrm{T}} \
\boldsymbol{\phi}\left(\mathbf{x}</em>{2}\right)^{\mathrm{T}} \
\vdots \
\boldsymbol{\phi}\left(\mathbf{x}<em>{N}\right)^{\mathrm{T}}
\end{pmatrix}, \quad \mathbf{\Phi}^{\mathrm{T}}=\left(\boldsymbol{\phi}\left(\mathbf{x}</em>{1}\right), \boldsymbol{\phi}\left(\mathbf{x}<em>{2}\right),  \ldots , \boldsymbol{\phi}\left(\mathbf{x}</em>{N}\right)\right)
$$
のように書けることを考えると、
$$
\mathbf{T}^{\mathrm T}\mathbf{\Phi} = \mathbf{W}<em>{\mathrm{ML}}^{\mathrm T}\mathbf{\Phi}^{\mathrm T}\mathbf{\Phi}
$$
と書き表せる。これより$\mathbf{W}</em>{\mathrm{ML}}^{\mathrm T}=\mathbf{T}^{\mathrm T}\mathbf{\Phi}\left(\mathbf{\Phi}^{\mathrm T}\mathbf{\Phi}\right)^{-1}$なので、$\mathbf{W}<em>{\mathrm{ML}}$を求めるよう変換していくと
$$
\begin{aligned}
\mathbf{W}</em>{\mathrm{ML}} &amp;=\left(\mathbf{T}^{\mathrm T}\mathbf{\Phi}\left(\mathbf{\Phi}^{\mathrm T}\mathbf{\Phi}\right)^{-1}\right)^{\mathrm T} \
&amp;=\left(\left(\mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right)^{-1}\right)^{\mathrm T}\left(\mathbf{T}^{\mathrm T} \mathbf{\Phi}\right)^{\mathrm T} \quad \left(\because (\mathbf{AB})^{\mathrm T} = \mathbf{B}^{\mathrm T}\mathbf{A}^{\mathrm T}\right)\
&amp;=\left(\mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm T} \mathbf{T}
\end{aligned}
$$
これは$(3.15)$式のような形で求まっている。また$\mathbf{\Sigma}$によらず決まることが分かる。</p>
<p>$\mathbf{\Sigma}$についての最尤推定解は<a href="../PRML%E3%81%AE%E6%BC%94%E7%BF%92%E5%95%8F%E9%A1%8C%E8%A7%A3%E7%AD%94%E9%9B%86%20%E7%AC%AC2%E7%AB%A0%202.61%E3%81%BE%E3%81%A7#%E6%BC%94%E7%BF%92%202.34">演習問題2.34と同じ手続き</a>で求めることができるので省略。</p>
<h2 id="演習-37"><a class="header" href="#演習-37">演習 3.7</a></h2>
<div class="panel-primary">
<p>$\mathbf{m}_N$と$\mathbf{S}_N$がそれぞれ</p>
<p>$$
\mathbf{m}<em>{N} =\mathbf{S}</em>{N}\left(\mathbf{S}<em>{0}^{-1} \mathbf{m}</em>{0}+\beta \mathbf{\Phi}^{\mathrm{T}} \textsf{t}\right) \tag{3.50}
$$
$$
\mathbf{S}<em>{N}^{-1} =\mathbf{S}</em>{0}^{-1}+\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \tag{3.51}
$$</p>
<p>で定義される線形基底関数モデルを考える．平方完成を用いて，このモデルのパラメータ$\mathbf{w}$の事後分布が
$$
p(\mathbf{w}| \textsf{t})=\mathcal{N}\left(\mathbf{w} \mid \mathbf{m}<em>{N}, \mathbf{S}</em>{N}\right) \tag{3.49}
$$
で与えられることを確かめよ．</p>
</div>
<p>（事後分布）$\propto$（尤度関数）×（事前分布）の関係式、すなわち$p(\mathbf{w}|\textsf{t}) \propto p(\textsf{t}|\mathbf{w}) p(\mathbf{w})$を使って計算する。正規化係数部分は吸収されるので、指数部分だけに着目する。</p>
<p>$$
\begin{aligned}
p(\mathbf{w}|\textsf{t}) &amp;= \prod_{n=1}^{N}\mathcal{N}(t_n|\mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}(\mathbf{x}_n))\mathcal{N}(\mathbf{w}|\mathbf{m}_0,\mathbf{S}<em>0) \
&amp;\propto \left( \prod</em>{n=1}^N \exp\left[ -\frac{1}{2} \left{ t_n - \mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x}_n) \right}^2 \beta \right] \right) \exp \left[ -\frac{1}{2} (\mathbf{w} - \mathbf{m}_0)^{\mathrm{T}} \mathbf{S}_0^{-1}(\mathbf{w} - \mathbf{m}<em>0) \right] \
&amp;= \exp\left[  -\frac{1}{2} \left{ \beta \sum</em>{n=1}^N \left( t_n - \mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x}_n)) \right)^2 + (\mathbf{w} - \mathbf{m}_0)^{\mathrm{T}} \mathbf{S}_0^{-1}(\mathbf{w} - \mathbf{m}_0) \right} \right] \
\end{aligned}
$$</p>
<p>ここで、指数部分のみ着目すると</p>
<p>$$
\begin{aligned}
&amp; \beta \sum_{n=1}^N \left( t_n - \mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}(\mathbf{x}_n) \right)^2 + (\mathbf{w} - \mathbf{m}_0)^{\mathrm{T}}\mathbf{S}_0^{-1}(\mathbf{w} - \mathbf{m}_0)\
= &amp;
\beta \begin{pmatrix}
t_1 - \mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}(\mathbf{x}_1) \
\vdots\
t_N - \mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}(\mathbf{x}_N)
\end{pmatrix}^{\mathrm{T}} \begin{pmatrix}
t_1 - \mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}(\mathbf{x}_1) \
\vdots\
t_N - \mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}(\mathbf{x}_N)
\end{pmatrix}
+ (\mathbf{w} - \mathbf{m}_0)^{\mathrm{T}}\mathbf{S}_0^{-1}(\mathbf{w} - \mathbf{m}_0) \
= &amp;
\beta \begin{pmatrix}
t_1 - \boldsymbol{\phi}(\mathbf{x}_1)^{\mathrm{T}}\mathbf{w} \
\vdots\
t_N - \boldsymbol{\phi}(\mathbf{x}_N)^{\mathrm{T}}\mathbf{w}
\end{pmatrix}^{\mathrm{T}} \begin{pmatrix}
t_1 - \boldsymbol{\phi}(\mathbf{x}_1)^{\mathrm{T}}\mathbf{w} \
\vdots\
t_N - \boldsymbol{\phi}(\mathbf{x}_N)^{\mathrm{T}}\mathbf{w}
\end{pmatrix}
+ (\mathbf{w} - \mathbf{m}_0)^{\mathrm{T}}\mathbf{S}_0^{-1}(\mathbf{w} - \mathbf{m}_0) \hspace{1em} (\because \mathbf{a}^{\mathrm{T}}\mathbf{b} = \mathbf{b}^{\mathrm{T}}\mathbf{a})\
\end{aligned}
$$</p>
<p>ここで、$(3.16)$式の計画行列が$\displaystyle \mathbf{\Phi} = \begin{pmatrix} \boldsymbol{\phi}(\mathbf{x}_1)^{\mathrm{T}} \ \boldsymbol{\phi}(\mathbf{x}_2)^{\mathrm{T}} \ \vdots \ \boldsymbol{\phi}(\mathbf{x}_N)^{\mathrm{T}} \end{pmatrix}$と書けることを利用すると、上の式は以下のように変形できる。</p>
<p>$$
\begin{aligned}
&amp;= \beta (\textsf{t} - \mathbf{\Phi} \mathbf{w})^{\mathrm{T}}(\textsf{t} - \mathbf{\Phi} \mathbf{w}) + (\mathbf{w} - \mathbf{m}_0)^{\mathrm{T}}\mathbf{S}_0^{-1}(\mathbf{w} - \mathbf{m}_0)\
&amp;= \beta (\mathbf{w}^{\mathrm{T}}\mathbf{\Phi}^{\mathrm{T}}\mathbf{\Phi} \mathbf{w} - \mathbf{w}^{\mathrm{T}}\mathbf{\Phi}^{\mathrm{T}}\textsf{t} - \textsf{t}^{\mathrm{T}}\mathbf{\Phi} \mathbf{w} ) + \mathbf{w}^{\mathrm{T}}\mathbf{S}_0^{-1}\mathbf{w} - \mathbf{w}^{\mathrm{T}}\mathbf{S}_0^{-1}\mathbf{m}_0 - \mathbf{m}_0^{\mathrm{T}}\mathbf{S}_0^{-1}\mathbf{w} + \text{const.} \
&amp;= \mathbf{w}^{\mathrm{T}}(\mathbf{S}_0^{-1} + \beta \mathbf{\Phi}^{\mathrm{T}}\mathbf{\Phi})\mathbf{w} - \mathbf{w}^{\mathrm{T}}(\mathbf{S}_0^{-1}\mathbf{m}_0 + \beta \mathbf{\Phi}^{\mathrm{T}} \textsf{t}) - (\mathbf{S}_0^{-1}\mathbf{m}_0 + \beta \mathbf{\Phi}^{\mathrm{T}} \textsf{t})^{\mathrm{T}}\mathbf{w} + \text{const.}
\end{aligned}
$$</p>
<p>$\mathbf{S}_0$は共分散なので対称行列である。この逆行列も対称行列である。よって$(\mathbf{S}_0^{-1})^{\mathrm{T}} = \mathbf{S}_0^{-1}$となることに注意する。</p>
<p>一方で、事後分布の形$\mathcal{N}(\mathbf{w}|\mathbf{m}_N, \mathbf{S}_N)$の指数部分を同様に展開すると</p>
<p>$$
\mathbf{w}^{\mathrm{T}}\mathbf{S}_N^{-1}\mathbf{w} - \mathbf{w}^{\mathrm{T}}(\mathbf{S}_N^{-1}\mathbf{m}_N) - (\mathbf{S}_N^{-1}\mathbf{m}_N)^{\mathrm{T}}\mathbf{w} + \text{const.}
$$</p>
<p>となる。これらの係数を比較すれば</p>
<p>$$
\begin{aligned}
\mathbf{S}_N^{-1} &amp;= \mathbf{S}_0^{-1} + \beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \
\mathbf{m}_N &amp;= \mathbf{S}_N(\mathbf{S}_0^{-1} \mathbf{m}_0 + \beta \mathbf{\Phi}^{\mathrm{T}}\textsf{t})
\end{aligned}
$$</p>
<p>すなわち$(3.50)$, $(3.51)$式が導けた。</p>
<h2 id="演習-38"><a class="header" href="#演習-38">演習 3.8</a></h2>
<div class="panel-primary">
<p>3.1節の線形基底関数モデルを考える．そして，すでに$N$個のデータ点が観測され，$\mathbf{w}$の事後分布が
$$
p(\mathbf{w}|\textsf{t})=\mathcal{N}\left(\mathbf{w} \mid \mathbf{m}<em>{N}, \mathbf{S}</em>{N}\right) \tag{3.49}
$$
で与えられるとする．この事後分布は次に観測されるデータの事前確率とみなすことができる．追加のデータ点$(\mathbf{x}<em>{N+1}, t</em>{N+1})$を考え，指数関数の中で平方完成することにより，事後確率が再び$(3.49)$の形式で与えられ，$\mathbf{S}<em>N$を$\mathbf{S}</em>{N+1}$に，$\mathbf{m}<em>N$を$\mathbf{m}</em>{N+1}$にそれぞれ置き換えたものになることを示せ．</p>
</div>
<p>事後分布は尤度関数と事前分布の掛け算に比例するので</p>
<p>$$
\begin{aligned}
p(\mathbf{w|t})&amp;\propto p(\mathbf{t|w})\cdot p(\mathbf{w})
\end{aligned}
$$</p>
<p>が満たされることがわかる．また事前分布は$p(\mathbf{w}) = \mathcal{N}(\mathbf{w} \mid \mathbf{m}<em>{N},\mathbf{S}</em>{N})$で、尤度関数が$p(t_{N+1}\mathbf{|w}) = \prod_{n=1}^{N}\mathcal{N}({t_{N+1}\mid \mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}<em>{N+1}}, \beta^{-1})$なので、
$$
\begin{aligned}
p(\mathbf{w|t})&amp;\propto p(t</em>{N+1}|\mathbf{w}) \cdot p(\mathbf{w})\
p(t_{N+1}|\mathbf{w}) \cdot p(\mathbf{w}) &amp;\propto \text{exp}\left(-\frac{1}{2}(\mathbf{w-m_{N}})^{\mathbf{T}}\mathbf{S}<em>{N}^{-1}(\mathbf{w-m</em>{N}}) - \frac{\beta}{2}(t_{N+1} - \mathbf{w^{T}}\boldsymbol{\phi}<em>{N+1})^{2}\right)\
\end{aligned}
$$
となる（ここで、$\mathbf{\phi}</em>{N+1} = \mathbf{\phi}(\mathbf{x}_{N+1})$と表記している）．指数部分だけを見ていくと</p>
<p>$$
\begin{aligned}
&amp;\quad(\mathbf{w-m_{N}})^{\mathrm{T}}\mathbf{S}<em>{N}^{-1}(\mathbf{w-m</em>{N}}) + \beta(t_{N+1} - \mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}<em>{N+1})^{2}\
&amp;= \mathbf{w}^{\mathrm{T}}\mathbf{S}</em>{N}^{-1}\mathbf{w} -2\mathbf{w}^{\mathrm{T}}\mathbf{S}<em>{N}^{-1}\mathbf{m</em>{N}} +\beta\mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}<em>{N+1}^{\mathrm{T}}\boldsymbol{\phi}</em>{N+1}\mathbf{w} - 2\beta\mathbf{w^{T}}\boldsymbol{\phi}<em>{N+1}t</em>{N+1} + \text{const.}\
&amp;= \mathbf{w}^{\mathrm{T}}(\mathbf{S}<em>{N}^{-1}+\beta\boldsymbol{\phi}</em>{N+1}\boldsymbol{\phi}<em>{N+1}^{\mathrm{T}})\mathbf{w} -2\mathbf{w}^{\mathrm{T}}(\mathbf{S}</em>{N}^{-1}\mathbf{m_{N}} + \beta\boldsymbol{\phi}<em>{N+1}t</em>{N+1}) + \text{const.}\
\end{aligned}
$$</p>
<p>となるので、演習問題3.7の最後の部分と比較すれば</p>
<p>$$
\begin{aligned}
\therefore \quad \mathbf{S}<em>{N+1}^{-1} &amp;= \mathbf{S}</em>{N}^{-1}+\beta\boldsymbol{\phi}<em>{N+1}\boldsymbol{\phi}</em>{N+1}^{\mathrm{T}}\
\mathbf{m}<em>{N+1} &amp;= \mathbf{S}</em>{N+1}(\mathbf{S}<em>{N}^{-1}\mathbf{m}</em>{N} + \beta\boldsymbol{\phi}<em>{N+1}t</em>{N+1})
\end{aligned}
$$</p>
<p>となって、題意は示された．</p>
<h2 id="演習-39"><a class="header" href="#演習-39">演習 3.9</a></h2>
<div class="panel-primary">
<p>上記の問題を平方完成ではなく，線形ガウスモデルの一般的な結果</p>
<p>$$
p(\mathbf{x} \mid \mathbf{y})=\mathcal{N}\left(\mathbf{x} \mid \mathbf{\Sigma}\left{\mathbf{A}^{\mathrm{T}} \mathbf{L}(\mathbf{y}-\mathbf{b})+\mathbf{\Lambda} \boldsymbol{\mu}\right}, \mathbf{\Sigma}\right) \tag{2.116}
$$</p>
<p>を用いて示せ．</p>
</div>
<p>$$
\begin{aligned}
\
(2.112) \quad p(\mathbf{x}) &amp;= \mathcal{N}(\mathbf{x|}\boldsymbol{\mu,\Lambda^{-1}})\
(2.114) \quad p(\mathbf{y|x}) &amp;= \mathcal{N}(\mathbf{y|Ax+b,L}^{-1})\
(2.116) \quad p(\mathbf{x|y}) &amp;= \mathcal{N}(\mathbf{x|}\boldsymbol{\Sigma}{\mathbf{A^{T}L(y-b)+\boldsymbol{\Lambda\mu}}},\boldsymbol{\Sigma})\quad(\boldsymbol{\Sigma=(\Lambda+\mathbf{A^{T}LA})^{-1}})
\
\end{aligned}
$$</p>
<p>を使って、演習3.8をもう一度解ける、観察すれば$\mathbf{w}\leftrightarrow\mathbf{x}\quad,t_{N+1}\leftrightarrow\mathbf{y}$で関係付けることができるので</p>
<p>$$
\begin{aligned}
p(\mathbf{w}) &amp;= \mathcal{N}(\mathbf{w|m_{N},S_{N}})\
p(t_{N+1}\mathbf{|w}) &amp;= \mathcal{N}({t_{N+1}\mathbf{|w^{T}}\boldsymbol{\phi}_{N+1}}, \beta^{-1})\
\end{aligned}
$$</p>
<p>と比較すれば、さらに各パラメータを次のように決めることができる．</p>
<p>$$
\begin{aligned}
\boldsymbol{\mu}&amp;=\mathbf{m_{N}}\
\boldsymbol{\Lambda}&amp;=\mathbf{S}<em>{N}^{-1}\
\mathbf{A}&amp;=\boldsymbol{\phi}</em>{N+1}^{T}\
\mathbf{b}&amp;=0\
\mathbf{L}&amp;=\beta\
\end{aligned}
$$</p>
<p>$(2.116)$に代入すれば</p>
<p>$$
\begin{aligned}
p(\mathbf{w}|t_{N+1}) &amp;= \mathcal{N}(\mathbf{w|m}<em>{N+1}, \mathbf{S}</em>{N+1})\
\mathbf{S}<em>{N+1}^{-1} &amp;= \boldsymbol{\Sigma}^{-1} \
&amp;=\boldsymbol{\Lambda}+\mathbf{A^{T}LA}\
&amp;= \mathbf{S}</em>{N}^{-1}+\beta\boldsymbol{\phi}<em>{N+1}^{T}\boldsymbol{\phi}</em>{N+1}\
\mathbf{m}<em>{N+1} &amp;= \boldsymbol{\Sigma}{\textbf{A}^\textbf{T}\textbf{L(y}-\textbf{b)} + \boldsymbol{\Lambda\mu}}\
&amp;=\textbf{S}</em>{N+1}(\textbf{S}<em>{N}^{-1}\textbf{m}</em>{N} + \beta\boldsymbol{\phi}<em>{N+1}t</em>{N+1})\
\end{aligned}
$$</p>
<p>となって、題意は示された．</p>
<h2 id="演習-310"><a class="header" href="#演習-310">演習 3.10</a></h2>
<div class="panel-primary">
<p>$$
p(\mathbf{y})=\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \boldsymbol{\mu}+\mathbf{b}, \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathbf{T}}\right) \tag{2.115}
$$
の結果を用いて</p>
<p>$$
p(t \mid \mathbf{t}, \alpha, \beta)=\int p(t \mid \mathbf{w}, \beta) p(\mathbf{w} \mid \mathbf{t}, \alpha, \beta) \mathrm{d} \mathbf{w} \tag{3.57}
$$</p>
<p>の積分を評価し，ベイズ線形回帰モデルの予測分布が
$$
p(t \mid \mathbf{x}, \mathbf{t}, \alpha, \beta)=\mathcal{N}\left(t \mid \mathbf{m}<em>{N}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x}), \sigma</em>{N}^{2}(\mathbf{x})\right) \tag{3.58}
$$
で与えられることを確かめよ．ただし，入力に依存する分散は
$$
\sigma_{N}^{2}(\mathbf{x})=\frac{1}{\beta}+\boldsymbol{\phi}(\mathbf{x})^{\mathrm{T}} \mathbf{S}_{N} \boldsymbol{\phi}(\mathbf{x}) \tag{3.59}
$$
で与えられる．</p>
</div>
<p>$(3.57)$の条件付き分布と事後分布は, それぞれ以下の式で表される。</p>
<p>$$
p \left (t \mid \mathbf w,\beta \right ) = \mathcal { N } \left (t \mid \mathbf w^\textrm T \boldsymbol \phi (\mathbf x),\beta^{-1} \right ) \tag{3.3, 3.8}
$$</p>
<p>$$
p \left (\mathbf w \mid \mathbf t,\alpha,\beta \right ) = \mathcal { N } \left (\mathbf w \mid \mathbf m _ N,\mathbf S_N \right )
\tag{3.49}
$$</p>
<p>ここで, $(2.115)$の式は,</p>
<p>$$
p \left (\mathbf x \right ) = \mathcal{N} \left( \boldsymbol{\mu},\mathbf{\Lambda}^{-1} \right) \tag{2.113}
$$</p>
<p>$$
p \left (\mathbf y \mid \mathbf x \right ) = \mathcal{N} \left (\mathbf{y} \mid \mathbf{A} \mathbf x + \mathbf b,\mathbf L^{-1} \right ) \tag{2.114}
$$</p>
<p>が与えられた際の周辺分布だったことに注意して, $(2.113)$から$(2.115)$について,</p>
<p>$$
\mathbf y \rightarrow \textit t, \quad \mathbf x \rightarrow \mathbf w, \quad \boldsymbol \mu \rightarrow \mathbf m_N, \quad \mathbf{\Lambda}^{-1} \rightarrow \mathbf S_N,\quad  \mathbf A \rightarrow \boldsymbol \phi (\mathbf x)^\textrm T,\quad \mathbf L^{-1}→\beta^{-1}
$$</p>
<p>と置き換えると, $(3.57)$を評価できる。</p>
<p>したがって, $(2.115)$にそれぞれを代入すると,</p>
<p>$$
p \left (t \mid \mathbf x,\mathbf t,\alpha,\beta \right ) = \mathcal { N } \left (t \mid \mathbf m^\textrm T_N \boldsymbol \phi (\mathbf x),\sigma ^2_N (\mathbf x) \right ) \tag{3.58}
$$
と求まる。ここで, 入力に依存する分散は</p>
<p>$$
\sigma ^ 2 _ N (\mathbf x) = \frac{1}{\beta}+\boldsymbol \phi (\mathbf x)^\textrm T \mathbf S_N \boldsymbol \phi (\mathbf x) \tag{3.59}
$$
である。</p>
<h2 id="演習-311"><a class="header" href="#演習-311">演習 3.11</a></h2>
<div class="panel-primary">
<p>データ集合のサイズが増えるにつれて，モデルパラメータの事後分布に関する不確かさが減少することについて説明した．次の行列の公式（付録C参照）</p>
<p>$$
\left(\mathbf{M}+\mathbf{vv}^{\mathrm{T}}\right)^{-1}=\mathbf{M}^{-1}-\frac{\left(\mathbf{M}^{-1} \mathbf{v}\right)\left(\mathbf{v}^{\mathrm{T}} \mathbf{M}^{-1}\right)}{1+\mathbf{v}^{\mathrm{T}} \mathbf{M}^{-1} \mathbf{v}} \tag{3.110}
$$</p>
<p>を用いて，
$$
\sigma_{N}^{2}(\mathbf{x})=\frac{1}{\beta}+\boldsymbol{\phi}(\mathbf{x})^{\mathrm{T}} \mathbf{S}<em>{N} \boldsymbol{\phi}(\mathbf{x}) \tag{3.59}
$$
の線形回帰モデルに関する不確かさ$\sigma</em>{N}^{2}(\mathbf{x})$が</p>
<p>$$
\sigma_{N+1}^{2}(\mathbf{x}) \leq \sigma_{N}^{2}(\mathbf{x}) \tag{3.111}
$$</p>
<p>を満たすことを示せ．</p>
</div>
<p>演習問題3.8で示したように、新しい点$(\mathbf{x}<em>{N+1}, t</em>{N+1})$が与えられたとき、$\mathcal{N}(\mathbf{w}\mid \mathbf{m}<em>N, \mathbf{S}<em>N)$に対する事後分布は$\mathcal{N}(\mathbf{w}\mid \mathbf{m}</em>{N+1}, \mathbf{S}</em>{N+1})$と書け、</p>
<p>$$
\begin{aligned}
\mathbf{S}<em>{N+1}&amp;=\left(\mathbf{S}</em>{N}^{-1}+\beta \boldsymbol{\phi}<em>{N+1} \boldsymbol{\phi}</em>{N+1}^{\mathrm{T}}\right)^{-1} \
\mathbf{m}<em>{N+1}&amp;=\mathbf{S}</em>{N+1}\left(\mathbf{S}<em>{N}^{-1} \mathbf{m}</em>{N}+\beta \boldsymbol{\phi}<em>{N+1} t</em>{N+1}\right)
\end{aligned}
$$</p>
<p>で与えられる（$\boldsymbol{\phi}<em>{N+1} = \boldsymbol{\phi}(\mathbf{x}</em>{N+1})$である）。</p>
<p>問いとしては$\sigma_N^{2}(\mathbf{x})-\sigma_{N+1}^{2}(\mathbf{x}) \ge 0$であることを証明すれば良い。これはまず</p>
<p>$$
\begin{aligned}
\sigma_N^{2}(\mathbf{x})-\sigma_{N+1}^{2}(\mathbf{x}) &amp;= \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\mathbf{S}<em>N\boldsymbol{\phi}(\mathbf{x}) - \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\mathbf{S}</em>{N+1}\boldsymbol{\phi}(\mathbf{x}) \
&amp;= \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}(\mathbf{S}<em>{N} - \mathbf{S}</em>{N+1})\boldsymbol{\phi}(\mathbf{x})
\end{aligned}
$$</p>
<p>であるから、$\mathbf{S}<em>{N} - \mathbf{S}</em>{N+1}$を計算すると</p>
<p>$$
\begin{aligned}
\mathbf{S}<em>{N} - \mathbf{S}</em>{N+1} &amp;= \mathbf{S}<em>{N} - \left(\mathbf{S}</em>{N}^{-1} + \beta \boldsymbol{\phi}<em>{N+1} \boldsymbol{\phi}</em>{N+1}^{\mathrm{T}}\right)^{-1} \
&amp;= \mathbf{S}<em>{N} - \left(\mathbf{S}</em>{N}^{-1} + \boldsymbol{\psi}<em>{N+1} \boldsymbol{\psi}</em>{N+1}^{\mathrm{T}} \right) \quad (\boldsymbol{\psi}<em>{N+1} = \beta^{1/2} \boldsymbol{\phi}</em>{N+1})\
&amp;=\frac{\left(\mathbf{S}<em>{N} \boldsymbol{\psi}</em>{N+1}\right)\left(\boldsymbol{\psi}<em>{N+1}^{\mathrm{T}} \mathbf{S}</em>{N}\right)}{1+ \boldsymbol{\psi}<em>{N+1}^{\mathrm{T}} \mathbf{S}</em>{N} \boldsymbol{\psi}<em>{N+1}} \
&amp;=\frac{\beta \mathbf{S}</em>{N} \boldsymbol{\phi}<em>{N+1} \boldsymbol{\phi}</em>{N+1}^{\mathrm{T}} \mathbf{S}<em>{N}}{1+\beta \boldsymbol{\phi}</em>{N+1}^{\mathrm{T}} \mathbf{S}<em>{N} \boldsymbol{\phi}</em>{N+1}}
\end{aligned}
$$</p>
<p>これより
$$
\begin{aligned}
\sigma_N^{2}(\mathbf{x})-\sigma_{N+1}^{2}(\mathbf{x}) &amp;= \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\mathbf{S}<em>N\boldsymbol{\phi}(\mathbf{x}) - \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\mathbf{S}</em>{N+1}\boldsymbol{\phi}(\mathbf{x}) \
&amp;= \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}(\mathbf{S}<em>{N} - \mathbf{S}</em>{N+1})\boldsymbol{\phi}(\mathbf{x}) \
&amp;= \frac{\beta \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\mathbf{S}<em>{N} \boldsymbol{\phi}</em>{N+1} \boldsymbol{\phi}<em>{N+1}^{\mathrm{T}} \mathbf{S}</em>{N} \boldsymbol{\phi}(\mathbf{x})}{1+\beta \boldsymbol{\phi}<em>{N+1}^{\mathrm{T}} \mathbf{S}</em>{N} \boldsymbol{\phi}<em>{N+1}} \
&amp;= \frac{\beta \left| \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\mathbf{S}</em>{N} \boldsymbol{\phi}<em>{N+1} \right|^2}{1+\beta \boldsymbol{\phi}</em>{N+1}^{\mathrm{T}} \mathbf{S}<em>{N} \boldsymbol{\phi}</em>{N+1}}
\end{aligned}
$$</p>
<p>ここで、$\mathbf{S}<em>{N}$は実対称行列であり（$(3.54)$式から $\mathbf{S}</em>{N} = \left( \alpha \mathbf{I} + \beta\mathbf{\Phi}^{\mathrm T}\mathbf{\Phi} \right)^{-1}$）、このため正定値行列である（後述）。すなわち任意のベクトル$\mathbf{x}$において$\mathbf{x}^{\mathrm T}\mathbf{S}<em>{N}\mathbf{x} \gt 0$が成立するため、$\boldsymbol{\phi}</em>{N+1}^{\mathrm{T}} \mathbf{S}<em>{N} \boldsymbol{\phi}</em>{N+1} \gt 0$となる（また当然ながら$\beta \gt 0$）。これより、</p>
<p>$$
\sigma_{N+1}^{2}(\mathbf{x}) \leq \sigma_{N}^{2}(\mathbf{x}) \tag{3.111}
$$</p>
<p>が証明された。</p>
<p>※ 最後の正定値行列であることについて、定義として、任意の零ベクトルでないベクトル$\mathbf{x}$について行列$\mathbf{P}$が</p>
<p>$$
\mathbf{x}^{\mathrm T}\mathbf{Px} \gt 0
$$</p>
<p>を満たすならば、$\mathbf{P}$は正定値行列である。また、<a href="https://www.iwanttobeacat.com/entry/2019/12/31/152102">その逆行列$\mathbf{P}^{-1}$も正定値行列</a>である。</p>
<p>そこで$\mathbf{S}<em>{N} = \left( \alpha \mathbf{I} + \beta\mathbf{\Phi}^{\mathrm T}\mathbf{\Phi} \right)^{-1}$について、逆行列$\mathbf{S}</em>{N}^{-1} = \alpha \mathbf{I} + \beta\mathbf{\Phi}^{\mathrm T}\mathbf{\Phi}$が正定値行列であることを示す。</p>
<p>まず$\alpha \mathbf{I}$は正定値行列である（$\because \mathbf{x}^{\mathrm T}(\alpha \mathbf{I})\mathbf{x} = \alpha \mathbf{x}^{\mathrm T}\mathbf{x} = \alpha |\mathbf{x}|^{2} \gt 0$）。また、$\mathbf{x}^{\mathrm T}(\beta\mathbf{\Phi}^{\mathrm T}\mathbf{\Phi})\mathbf{x} = \beta |\mathbf{\Phi x}|^2 \ge 0$が成り立つ（これは半正定値）。したがって線形結合である$\mathbf{S}_N^{-1}$も正定値行列であるので、$\mathbf{S}_N$が正定値行列であることが示された。</p>
<blockquote>
<p>むしろ等号成立条件がわからないけど</p>
</blockquote>
<h2 id="演習-312"><a class="header" href="#演習-312">演習 3.12</a></h2>
<div class="panel-primary">
<p>2.3.6節で，平均および精度（分散の逆数）がともに未知のガウス分布に対応する共役事前分布は正規ガンマ分布であることを述べた．この性質は，線形回帰モデルの条件付きガウス分布$p(t|\mathbf{x}, \mathbf{w}, \beta)$の場合にも成り立つ．尤度関数が
$$
p(\mathsf{t} \mid \mathbf{X}, \mathbf{w}, \beta)=\prod_{n=1}^{N} \mathcal{N}\left(t_{n} \mid \mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right), \beta^{-1}\right) \tag{3.10}
$$
で与えられるとき，$\mathbf{w}$と$\beta$の共役事前分布が</p>
<p>$$
p(\mathbf{w}, \beta)=\mathcal{N}\left(\mathbf{w} | \mathbf{m}<em>{0}, \beta^{-1} \mathbf{S}</em>{0}\right) \operatorname{Gam}\left(\beta | a_{0}, b_{0}\right) \tag{3.112}
$$</p>
<p>で与えられることを示せ．さらに，対応する事後分布が同様に</p>
<p>$$
p(\mathbf{w}, \beta | \mathbf{t})=\mathcal{N}\left(\mathbf{w} | \mathbf{m}<em>{N}, \beta^{-1} \mathbf{S}</em>{N}\right) \operatorname{Gam}\left(\beta | a_{N}, b_{N}\right) \tag{3.113}
$$</p>
<p>の関数形で与えられることを示し，パラメータ$\mathbf{m}<em>{N}, \mathbf{S}</em>{N}, a_N, b_N$についての式を求めよ．</p>
</div>
<p>（事後確率）$\propto$（尤度関数）$\times$（共役事前分布）なので、$p(\mathbf{w},\beta\mid \mathsf{t},\mathbf{x}) \propto p(\mathbf{t} \mid \mathbf{x}, \mathbf{w}, \beta)p(\mathbf{w},\beta)$である。</p>
<blockquote>
<p>実際には入力変数$\mathbf{x}$をモデル化しようとしていないので、$\mathbf{x}$は条件としてしか現れないので$p(\mathbf{w},\beta\mid t)\propto p(\mathsf{t}\mid \mathbf{w},\beta)p(\mathbf{w},\beta)$である。</p>
</blockquote>
<p>P.98を読むことで、この共役事前分布も正規-ガンマ関数として書けることがわかる。よって、$p(\mathbf{w}, \beta)=\mathcal{N}\left(\mathbf{w} | \mathbf{m}<em>{0}, \beta^{-1} \mathbf{S}</em>{0}\right) \operatorname{Gam}\left(\beta | a_{0}, b_{0}\right)$の形で書くことができる。</p>
<blockquote>
<p>本当にこれで示せたことになるのか？感はある</p>
</blockquote>
<p>これが与えられているとき、対数で事後分布を考えると
$$
\ln p(\mathbf{w},\beta\mid \mathsf{t}) = \ln p(\mathsf{t}\mid \mathbf{x},\mathbf{w},\beta) + \ln p(\mathbf{w},\beta) + \textrm{const.}
$$
と書くことができる。これを展開すると</p>
<p>$$
\begin{aligned}
\ln p(\mathbf{w}, \beta \mid \mathbf{t})=&amp; \sum_{n=1}^{N} \ln p\left(t_{n} \mid \mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}<em>{n}\right), \beta^{-1}\right) + \ln p(\mathbf{w}, \beta)\
=&amp; \frac{M}{2} \ln \beta-\frac{1}{2} \ln \left|\mathbf{S}</em>{0}\right|-\frac{\beta}{2}\left(\mathbf{w}-\mathbf{m}<em>{0}\right)^{\mathrm{T}} \mathbf{S}</em>{0}^{-1}\left(\mathbf{w}-\mathbf{m}<em>{0}\right) - b</em>{0} \beta+\left(a_{0}-1\right) \ln \beta \
+&amp; \frac{N}{2} \ln \beta-\frac{\beta}{2} \sum_{n=1}^{N}\left{\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}<em>{n}\right)-t</em>{n}\right}^{2}+\text{const.}
\end{aligned} \tag{*}
$$</p>
<p>この事後分布も正規-ガンマ分布の形で分解できることが示されている。すなわち$p(\mathbf{w},\beta\mid \mathsf{t}) = p(\mathbf{w} \mid \beta, \mathsf{t})p(\beta\mid \mathsf{t})$と分解でき、$\mathbf{w}$に依存する$p(\mathbf{w} \mid \beta, \mathsf{t})$がガウス分布の形$\mathcal{N}(\mathbf{w}\mid \mathbf{m}_N,\beta^{-1}\mathbf{S}_N)$になるはずである。</p>
<p>(*)式を$\mathbf{w}$についての関数としてまとめ直すと、$\displaystyle \sum_{n=1}^{N}\left{\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}<em>{n}\right)-t</em>{n}\right}^{2} = (\mathbf{\Phi}\mathbf{w} - \mathsf{t})^{\mathrm T}(\mathbf{\Phi}\mathbf{w} - \mathsf{t})$を利用して</p>
<p>$$
\ln p(\mathbf{w} \mid \beta, \mathbf{t})=-\frac{\beta}{2} \mathbf{w}^{\mathrm{T}}\left[\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}+\mathbf{S}<em>{0}^{-1}\right] \mathbf{w}+\mathbf{w}^{\mathrm{T}}\left[\beta \mathbf{S}</em>{0}^{-1} \mathbf{m}_{0}+\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{t}\right]+\mathrm{const.}
$$</p>
<p>となる。よって、$\mathcal{N}(\mathbf{w}\mid \mathbf{m}_N, \mathbf{S}_N)$と比較すると（P.84のように$\mathbf{S}_N$から求める）、</p>
<p>$$
\begin{aligned}
\mathbf{S}<em>{N}^{-1} &amp;=\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} + \mathbf{S}</em>{0}^{-1} \
\mathbf{m}<em>{N} &amp;=\mathbf{S}</em>{N}\left[\mathbf{S}<em>{0}^{-1} \mathbf{m}</em>{0}+\mathbf{\Phi}^{\mathrm{T}} \mathsf{t}\right]
\end{aligned}
$$</p>
<p>となる（$(3.50), (3.51)$と似ているようで異なる）。</p>
<p>次に、$p(\beta\mid \mathsf{t})$がガンマ分布の形になるはずなので(*)の$\beta$に依存する残りの項をまとめる。ただし、$\frac{M}{2}\ln \beta$は上記のガウス分布の正規化定数に相当するので$\ln p(\beta\mid \mathsf{t})$には現れなくなることに注意する。</p>
<p>$$
\begin{aligned}
\ln p(\beta \mid \mathbf{t}) &amp;= \ln p(\mathbf{w},\beta\mid \mathsf{t}) - \ln p(\mathbf{w}\mid \beta, \mathsf{t})
\ &amp;= -\frac{\beta}{2} \mathbf{m}<em>{0}^{\mathrm{T}} \mathbf{S}</em>{0}^{-1} \mathbf{m}<em>{0}+\frac{\beta}{2} \mathbf{m}</em>{N}^{\mathrm{T}} \mathbf{S}<em>{N}^{-1} \mathbf{m}</em>{N} + \frac{N}{2} \ln \beta-b_{0} \beta+\left(a_{0}-1\right) \ln \beta-\frac{\beta}{2} \sum_{n=1}^{N} t_{n}^{2}+\mathrm{const.} \
&amp;=\left(\frac{N}{2}+a_0 -1\right)\ln \beta - \frac{1}{2} \left( \mathbf{m}<em>{0}^{\mathrm{T}} \mathbf{S}</em>{0}^{-1} \mathbf{m}<em>{0} - \mathbf{m}</em>{N}^{\mathrm{T}} \mathbf{S}<em>{N}^{-1} \mathbf{m}</em>{N} + \sum_{n=1}^{N} t_{n}^{2} \right)\beta - b_0\beta + \mathrm{const.}
\end{aligned}
$$</p>
<p>これがガンマ分布の対数形$\ln \operatorname{Gam}(\beta\mid a_N, b_N) = (a_N -1)\ln \beta - b_N \beta + \mathrm{const.}$の式と同形になるべきなので、両式を比較することで</p>
<p>$$
\begin{aligned}
a_{N}&amp;=a_{0}+\frac{N}{2} \
b_{N}&amp;=b_{0}+\frac{1}{2}\left(\mathbf{m}<em>{0}^{\mathrm{T}} \mathbf{S}</em>{0}^{-1} \mathbf{m}<em>{0}-\mathbf{m}</em>{N}^{\mathrm{T}} \mathbf{S}<em>{N}^{-1} \mathbf{m}</em>{N}+\sum_{n=1}^{N} t_{n}^{2}\right)
\end{aligned}
$$</p>
<p>が得られる。</p>
<h2 id="演習-313"><a class="header" href="#演習-313">演習 3.13</a></h2>
<div class="panel-primary">
<p>演習問題3.12で議論したモデルに対する予測分布$p(t|\mathbf{x},\mathsf{t})$が次の形のスチューデントのt分布</p>
<p>$$
p(t | \mathbf{x}, \mathsf{t})=\operatorname{St}(t | \mu, \lambda, \nu) \tag{3.114}
$$</p>
<p>で与えられることを示し，$\mu, \lambda, \nu$についての式を求めよ．</p>
</div>
<p>$$ \tag{3.8}
p(t \mid \mathbf{x}, \mathbf{w}, \beta)=\mathcal{N}\left(t \mid y(\mathbf{x}, \mathbf{w}), \beta^{-1}\right)
$$</p>
<p>$$ \tag{3.113}
p(\mathbf{w}, \beta \mid \mathsf{t}, \mathbf{X}) = \mathcal{N}\left(\mathbf{w} \mid \mathbf{m}<em>{N}, \beta^{-1} \mathbf{S}</em>{N}\right) \operatorname{Gam}\left(\beta \mid a_{N}, b_{N}\right)
$$</p>
<p>と$3.3.2$節の議論より，予測分布は</p>
<p>$$
\begin{aligned}
p(t \mid \mathbf{x}, \mathbf{X}, \mathsf{t})
&amp;= \iint p(t \mid \mathbf{x}, \mathbf{w}, \beta) p(\mathbf{w}, \beta \mid \mathsf{t}, \mathbf{X}) \mathrm{d}\mathbf{w} \mathrm{d}\beta \
&amp;= \iint \mathcal{N}\left(t \mid \phi(\mathbf{x})^{\mathrm{T}} \mathbf{w}, \beta^{-1}\right) \mathcal{N}\left(\mathbf{w} \mid \mathbf{m}<em>{N}, \beta^{-1} \mathbf{S}</em>{N}\right) \mathrm{d} \mathbf{w} \operatorname{Gam}\left(\beta \mid a_{N}, b_{N}\right) \mathrm{d} \beta
\end{aligned}
$$</p>
<p>となる．</p>
<p>ここで，$\mathbf{w}$についての積分は線形ガウスモデルなので，公式</p>
<p>$$ \tag{2.113}
p(\mathbf{x})=\mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Lambda}^{-1}\right)
$$</p>
<p>$$ \tag{2.114}
p(\mathbf{y} \mid \mathbf{x})=\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \mathbf{x}+\mathbf{b}, \mathbf{L}^{-1}\right)
$$</p>
<p>$$ \tag{2.115}
p(\mathbf{y})=\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \boldsymbol{\mu}+\mathbf{b}, \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathbf{T}}\right)
$$</p>
<p>を用いる．</p>
<p>$$ \tag{3.49}
p(\mathbf{w} \mid \mathbf{t})=\mathcal{N}\left(\mathbf{w} \mid \mathbf{m}<em>{N}, \mathbf{S}</em>{N}\right)
$$</p>
<p>を$(2.113)$に，</p>
<p>$$ \tag{3.8}
p(t \mid \mathbf{x}, \mathbf{w}, \beta)=\mathcal{N}\left(t \mid y(\mathbf{x}, \mathbf{w}), \beta^{-1}\right)
$$</p>
<p>$$ \tag{3.3}
y(\mathbf{x}, \mathbf{w})=\sum_{j=0}^{M-1} w_{j} \phi_{j}(\mathbf{x})=\mathbf{w}^{\mathrm{T}} \phi(\mathbf{x})
$$</p>
<p>を$(2.114)$に適応するために</p>
<p>$$
\begin{aligned}
\mathbf{x} \Rightarrow \mathbf{w} \quad \boldsymbol{\mu} \Rightarrow \mathbf{m}<em>{N} \quad \mathbf{\Lambda}^{-1} \Rightarrow \mathbf{S}</em>{N} \quad \mathbf{y} \Rightarrow t \quad \mathbf{A} \Rightarrow \phi(\mathbf{x})^{\mathrm{T}}=\phi^{\mathrm{T}} \quad \mathbf{b} \Rightarrow \mathbf{0} \quad \mathbf{L}^{-1} \Rightarrow \beta^{-1}
\end{aligned}
$$</p>
<p>と置き換えると，$(2.115)$より</p>
<p>$$
\begin{aligned}
p(t \mid \beta) &amp;=\mathcal{N}\left(t \mid \phi^{\mathrm{T}} \mathbf{m}<em>{N}, \beta^{-1}+\phi^{\mathrm{T}} \mathbf{S}</em>{N} \phi\right) \
&amp;=\mathcal{N}\left(t \mid \phi^{\mathrm{T}} \mathbf{m}<em>{N}, \beta^{-1}\left(1+\phi^{\mathrm{T}}\left(\mathbf{S}</em>{0}+\phi^{\mathrm{T}} \phi\right)^{-1} \phi\right)\right)
\end{aligned}
$$</p>
<p>となる．ただし，演習問題$3.12$より，$\beta \mathbf{S}_{N}^{-1} = \beta\left[ \mathbf{S}_0^{-1} + \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \right]$を用いた．</p>
<p>したがって予測分布は</p>
<p>$$
p(t \mid \mathbf{x}, \mathbf{X}, \mathbf{t}) = \int \mathcal{N}\left(t \mid \phi^{\mathrm{T}} \mathbf{m}<em>{N}, \beta^{-1} s\right) \operatorname{Gam}\left(\beta \mid a</em>{N}, b_{N}\right) \mathrm{d} \beta
$$</p>
<p>と変形できる．ただし</p>
<p>$$
s=1+\boldsymbol{\phi}^{\mathrm{T}}\left(\mathbf{S}_{0}+\boldsymbol{\phi}^{\mathrm{T}} \boldsymbol{\phi}\right)^{-1} \boldsymbol{\phi}
$$</p>
<p>とおいた．ここで，スチューデントのt分布の式</p>
<p>$$ \tag{2.158}
\begin{aligned}
p(x \mid \mu, a, b) &amp;=\int_{0}^{\infty} \mathcal{N}\left(x \mid \mu, \tau^{-1}\right) \operatorname{Gam}(\tau \mid a, b) \mathrm{d} \tau \
&amp;=\int_{0}^{\infty} \frac{b^{a} e^{(-b \tau)} \tau^{a-1}}{\Gamma(a)}\left(\frac{\tau}{2 \pi}\right)^{1 / 2} \exp \left{-\frac{\tau}{2}(x-\mu)^{2}\right} \mathrm{d} \tau \
&amp;=\frac{b^{a}}{\Gamma(a)}\left(\frac{1}{2 \pi}\right)^{1 / 2}\left[b+\frac{(x-\mu)^{2}}{2}\right]^{-a-1 / 2} \Gamma(a+1 / 2)
\end{aligned}
$$</p>
<p>$$ \tag{2.159}
\operatorname{St}(x \mid \mu, \lambda, \nu)=\frac{\Gamma(\nu / 2+1 / 2)}{\Gamma(\nu / 2)}\left(\frac{\lambda}{\pi \nu}\right)^{1 / 2}\left[1+\frac{\lambda(x-\mu)^{2}}{\nu}\right]^{-\nu / 2-1 / 2}
$$</p>
<p>を参考にすると</p>
<p>$$
\begin{aligned}
p(t \mid \mathbf{x}, \mathbf{X}, \mathbf{t})
&amp;= \int_{0}^{\infty} \frac{b_N^{a_N} e^{(-b_N \beta)} \beta^{a_N-1}}{\Gamma(a_N)}\left(\frac{\beta s^{-1}}{2 \pi}\right)^{1 / 2} \exp \left{-\frac{\beta s^{-1}}{2}(t-\phi^{\mathrm{T}} \mathbf{m}<em>{N})^{2}\right} \mathrm{d} \beta  \
&amp;=\frac{b_N^{a_N}}{\Gamma(a_N)}\left(\frac{s^{-1}}{2 \pi}\right)^{1 / 2} \int</em>{0}^{\infty} \beta^{(a_N + \frac{1}{2})-1} \exp \left{-\left(b_N + \frac{s^{-1}}{2}(t-\phi^{\mathrm{T}} \mathbf{m}_{N})^{2} \right)\beta \right} \mathrm{d} \beta  \
\end{aligned}
$$</p>
<p>ここで$\displaystyle u = \left(b_N + \frac{s^{-1}}{2}(t-\phi^{\mathrm{T}} \mathbf{m}<em>{N})^{2} \right)\beta$と変数変換すると，$\displaystyle \mathrm{d}u = \left(b_N + \frac{s^{-1}}{2}(t-\phi^{\mathrm{T}} \mathbf{m}</em>{N})^{2} \right) \mathrm{d}\beta$と積分範囲に注意すると</p>
<p>$$
\begin{aligned}
p(t \mid \mathbf{x}, \mathbf{X}, \mathbf{t})
&amp;= \frac{b_N^{a_N}}{\Gamma(a_N)}\left(\frac{s^{-1}}{2 \pi}\right)^{1 / 2} \int_{0}^{\infty} \beta^{(a_N + \frac{1}{2})-1} \exp \left{-\left(b_N + \frac{s^{-1}}{2}(t-\phi^{\mathrm{T}} \mathbf{m}<em>{N})^{2} \right)\beta \right} \mathrm{d} \beta  \
&amp;= \frac{b_N^{a_N}}{\Gamma(a_N)}\left(\frac{s^{-1}}{2 \pi}\right)^{1 / 2} \int</em>{0}^{\infty} \left(b_N + \frac{s^{-1}}{2}(t-\phi^{\mathrm{T}} \mathbf{m}<em>{N})^{2} \right)^{-\left{(a_N + \frac{1}{2})-1\right}-1} u^{(a_N + \frac{1}{2})-1} e^{-u} \mathrm{d}u  \
&amp;= \frac{b_N^{a_N}}{\Gamma(a_N)}\left(\frac{s^{-1}}{2 \pi}\right)^{1 / 2} \left[b_N + \frac{s^{-1}}{2}(t-\phi^{\mathrm{T}} \mathbf{m}</em>{N})^{2} \right]^{-a_N - \frac{1}{2}} \Gamma\left(a_N + \frac{1}{2}\right)  \
&amp;= \frac{\Gamma\left(a_N + 1/2\right)}{\Gamma(a_N)}\left(\frac{s^{-1}}{2 \pi}\right)^{1 / 2} b_N^{a_N} \left[b_N + \frac{s^{-1}}{2}(t-\phi^{\mathrm{T}} \mathbf{m}<em>{N})^{2} \right]^{-a_N - 1/2}  \
&amp;= \frac{\Gamma\left(a_N + 1/2\right)}{\Gamma(a_N)}\left(\frac{s^{-1}}{2 \pi}\right)^{1 / 2} b_N^{a_N} b_N^{-a_N - 1/2} \left[1 + \frac{a_N}{b_N} \frac{s^{-1}}{2a_N}(t-\phi^{\mathrm{T}} \mathbf{m}</em>{N})^{2} \right]^{-a_N - 1/2}  \
&amp;= \frac{\Gamma\left(a_N + 1/2\right)}{\Gamma(a_N)}\left(\frac{a_N}{b_N}\frac{s^{-1}}{2 a_N\pi}\right)^{1 / 2} \left[1 + \frac{a_N}{b_N} \frac{s^{-1}}{2a_N}(t-\phi^{\mathrm{T}} \mathbf{m}_{N})^{2} \right]^{-a_N - 1/2}  \
&amp;= \operatorname{St}(t \mid \mu, \lambda, \nu)
\end{aligned}
$$</p>
<p>となることがわかる．ただし</p>
<p>$$
\begin{aligned}
\mu &amp;= \phi^{\mathrm{T}} \mathbf{m}<em>{N} \
\lambda &amp;= \frac{a</em>{N}}{b_{N}} s^{-1} \
\nu &amp;= 2 a_{N}
\end{aligned}
$$</p>
<p>である．</p>
<h2 id="演習-314"><a class="header" href="#演習-314">演習 3.14</a></h2>
<div class="panel-primary">
<p>この演習問題では，
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\beta \boldsymbol{\phi}(\mathbf{x})^{\mathrm{T}} \mathbf{S}_{N} \boldsymbol{\phi}\left(\mathbf{x}^{\prime}\right) \tag{3.62}
$$
で定義される等価カーネルのより深い性質を調べよう．ただし，$\mathbf{S}<em>N$は
$$
\mathbf{S}</em>{N}^{-1}=\alpha \mathbf{I}+\beta \Phi^{\mathrm{T}} \boldsymbol{\Phi} \tag{3.54}
$$
で定義される．基底関数$\phi_j(\mathbf{x})$は線形独立であると仮定し，データ点の数$N$は基底関数の数$M$よりも大きいものとする．さらに，基底関数の1つは定数，すなわち$\phi_0(\mathbf{x})=1$とするこれらの基底関数の適当な線形結合を取り，同じ空間を張る新しい基底関数集合$\psi_j(\mathbf{x})$を生成することができる．ただし，新しい基底関数は正規直交である．</p>
<p>$$
\sum_{n=1}^{N} \psi_{j}\left(\mathbf{x}<em>{n}\right) \psi</em>{k}\left(\mathbf{x}<em>{n}\right)=I</em>{j k} \tag{3.115}
$$</p>
<p>$I_{jk}$は$j=k$のとき$1$を取り，それ以外は$0$を取る．また，$\psi_0(\mathbf{x})=1$と定義する．このとき$\alpha=0$に対して，等価カーネルが$k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\boldsymbol{\psi}(\mathbf{x})^{\mathrm{T}} \boldsymbol{\psi}\left(\mathbf{x}^{\prime}\right)$と書けることを示せ．ただし，$\boldsymbol{\psi}=(\psi_0,\ldots,\psi_M)^{\mathrm{T}}$である．そしてこの結果を用いて，上記のカーネルが</p>
<p>$$
\sum_{n=1}^{N} k\left(\mathbf{x}, \mathbf{x}_{n}\right)=1 \tag{3.116}
$$</p>
<p>を満たすことを示せ．</p>
</div>
<p>$$
\alpha = 0より
\mathbf{S}_{N}^{-1}=\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}
$$
また、基底関数の適当な線型結合をとり互いに正規直交な新しい基底関数集合$\psi_j(\mathbf{x})$を</p>
<p>$$
\boldsymbol{\psi(\mathbf{x})}=\mathbf{A}\boldsymbol{\phi(\mathbf{x})}
$$
とおく。また、これを用いて$\mathbf{\Psi}$を</p>
<p>$$
\mathbf{\Psi}=\mathbf{\Phi}\mathbf{A}^{\mathrm{T}} \
\mathbf{\Psi}(\mathbf{A}^{\mathrm{T}})^{-1} =\mathbf{\Phi}
$$</p>
<p>と定義する。ここで$(3.115)$を用いると$\mathbf{\Phi}^\mathrm{T}\mathbf{\Phi}=\mathbf{I}なので$</p>
<p>$$
\begin{aligned}
\mathbf{S}_{N}^{-1}&amp;=\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\
&amp;= \beta\mathbf{A}^{-1}\mathbf{\Psi}^{\mathrm{T}}\mathbf{\Psi}(\mathbf{A}^\mathrm{T})^{-1}\
&amp;= \beta(\mathbf{A}^\mathrm{T}\mathbf{A})^{-1}
\end{aligned}
$$
これを$(3.62)$に代入すると</p>
<p>$$
\begin{aligned}
k\left(\mathbf{x}, \mathbf{x}'\right)&amp;=\boldsymbol{\phi}^\mathrm{T}(\mathbf{x})\mathbf{A}^\mathrm{T}\mathbf{A}\boldsymbol{\phi}(\mathbf{x}') \
&amp;= \boldsymbol{\psi}^\mathrm{T}(\mathbf{x})\boldsymbol{\psi}(\mathbf{x}')
\end{aligned}
$$
が得られる。</p>
<p>また、これを$(3.116)$に代入すると</p>
<p>$$
\begin{aligned}
(3.116) &amp;= \sum_{n=1}^{N} k\left(\mathbf{x}, \mathbf{x}<em>n\right) \
&amp;= \sum</em>{n=1}^{N}\boldsymbol{\psi}^\mathrm{T}(\mathbf{x})\boldsymbol{\psi}(\mathbf{x}<em>n) \
&amp;= \sum</em>{n=1}^{N} \sum_{i=0}^{M-1} \psi_i(\mathbf{x})\psi_i(\mathbf{x}<em>n) \
&amp;= \sum</em>{i=0}^{M-1}\psi_i(\mathbf{x})\sum_{n=1}^{N}\psi_i(\mathbf{x}_n)
\end{aligned}
$$
ここで、$(3.115)$の$k=0$のときを考えると$\psi_0(\mathbf{x})=1$なので</p>
<p>$$
\begin{aligned}
\sum_{n=1}^{N}\psi_j(\mathbf{x}_n)\psi_0(\mathbf{x}<em>n)&amp;=\sum</em>{n=1}^{N}\psi_j(\mathbf{x}<em>n) = \mathbf{I}</em>{j0}
\end{aligned}
$$
よって</p>
<p>$$
\begin{aligned}
\sum_{i=0}^{M-1}\psi_i(\mathbf{x})\sum_{n=1}^{N}\psi_i(\mathbf{x}<em>n)=\sum</em>{i=0}^{M-1}\psi_i(\mathbf{x})\mathbf{I}_{i0}=\psi_0(\mathbf{x}) = 1
\end{aligned}
$$
以上より</p>
<p>$$
\begin{aligned}
\sum_{n=1}^{N}k(\mathbf{x},\mathbf{x}_n) = 1
\end{aligned}
$$
を満たすことを示した。</p>
<h2 id="演習-315"><a class="header" href="#演習-315">演習 3.15</a></h2>
<div class="panel-primary">
<p>線形基底関数からなる回帰モデルの超パラメータ$\alpha,\ \beta$をエビデンスの枠組みを用いて決定する場合を考える．
$$
E\left(\mathbf{m}<em>{N}\right)=\frac{\beta}{2}\left|\mathbf{t}-\Phi \mathbf{m}</em>{N}\right|^{2}+\frac{\alpha}{2} \mathbf{m}<em>{N}^{\mathrm{T}} \mathbf{m}</em>{N} \tag{3.82}
$$
で定義される関数$E(\mathbf{m}_N)$が関係式$2E(\mathbf{m}_N)=N$を満たすことを示せ．</p>
</div>
<p>$(3.92)$式と$(3.95)$式を代入するだけで答えが出る</p>
<p>$$
\alpha = \frac{ \gamma }{ \mathbf{m}<em>{N}^\mathrm{T} \mathbf{m}</em>{N} } \tag{3.92}
$$</p>
<p>$$
\beta = (N-\gamma) \left{\sum_{n=1}^{N}\left{t_n-\mathbf{m}_{N}^\mathrm{T}\boldsymbol{\phi}(\mathbf{x}_n)\right}^2\right}^{-1} \tag{3.95}
$$</p>
<p>これらを$(3.82)$式に代入すると</p>
<p>$$
\begin{aligned}
E(\mathbf{m}<em>N) &amp;= \frac{(N-\gamma)\left|\mathbf{t}-\Phi \mathbf{m}</em>{N}\right|^{2}}{2 \sum_{n=1}^{N}\left{t_n-\mathbf{m}<em>{N}^\mathrm{T}\boldsymbol{\phi}(\mathbf{x}<em>n)\right}^2}+\frac{\gamma \mathbf{m}</em>{N}^\mathrm{T}\mathbf{m}</em>{N}}{2\mathbf{m}<em>{N}^\mathrm{T}\mathbf{m}</em>{N}} \
&amp;=\frac{N-\gamma}{2}+\frac{\gamma}{2} \
&amp;= \frac{N}{2}
\end{aligned}
$$
よって$2E(\mathbf{m}_N)=N$が示せた。</p>
<h2 id="演習-316"><a class="header" href="#演習-316">演習 3.16</a></h2>
<div class="panel-primary">
<p>$$
p(\mathbf{t} \mid \alpha, \beta)=\int p(\mathbf{t} \mid \mathbf{w}, \beta) p(\mathbf{w} \mid \alpha) \mathrm{d} \mathbf{w} \tag{3.77}
$$
の積分の評価に
$$
p(\mathbf{y})=\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \boldsymbol{\mu}+\mathbf{b}, \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathbf{T}}\right) \tag{2.115}
$$
を直接用いて，
$$
\ln p(\mathbf{t} \mid \alpha, \beta)=\frac{M}{2} \ln \alpha+\frac{N}{2} \ln \beta-E\left(\mathbf{m}_{N}\right)-\frac{1}{2} \ln |\mathbf{A}|-\frac{N}{2} \ln (2 \pi) \tag{3.86}
$$
で与えられる線形回帰モデルの対数エビデンス関数$p(\mathbf{t}|\alpha, \beta)$の結果を導け．</p>
</div>
<p>※ $(2.115)$式を適用するところまではそこまで難しくないが、$-  \frac {1}{2} \ln  \left | \beta^{-1} \mathbf I_N+\alpha ^{-1} \mathbf{\Phi}\mathbf{\Phi}^{\mathrm T} \right |$と$-\frac{1}{2} \mathsf{t}^{\mathrm T} \left( \beta^{-1} \mathbf{I}_N+\alpha ^{-1} \mathbf{\Phi}\mathbf{\Phi}^{\mathrm T} \right)^{-1} \mathsf{t}$を$(3.86)$式のように変形するところが非常にテクニカル。付録CのWoodburyの公式(C.7)や<a href="MASP/%E8%A1%8C%E5%88%97%E5%BC%8F#%5Cmathbf%7BR%7D%2B%5Cmathbf%7BSTU%7D%E3%81%AE%E5%BD%A2%E3%81%AE%E8%A1%8C%E5%88%97%E3%81%AE%E8%A1%8C%E5%88%97%E5%BC%8F">行列式についての変形の定理</a>(C.14)を利用する必要がある。</p>
<p>$(3.77)$式の積分を計算する。$p(\mathsf{t}\mid \mathbf{w}, \beta)$は$(3.10)$式から、$p(\mathbf{w}|\alpha)$は$(3.52)$式から与えられる。</p>
<p>$(3.52)$式より$p(\mathbf{w}\mid \alpha) = \mathcal{N}(\mathbf{w}\mid \mathbf{0},\alpha^{-1}\mathbf{I}_M)$である。また、$(3.10)$式から</p>
<p>$$
\begin{aligned}
p(\mathsf{t} \mid \mathbf{w}, \beta) &amp;=\prod_{n=1}^{N} \mathcal{N}\left(t_n \mid \mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}<em>{n}\right), \beta^{-1}\right) \
&amp;=\prod</em>{n=1}^{N}\left(\frac{\beta}{2 \pi}\right)^{\frac{1}{2}} \exp \left{-\frac{\beta}{2}\left(t_n-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}<em>{n}\right)\right)^{2}\right} \
&amp;=\left(\frac{\beta}{2 \pi}\right)^{\frac{N}{2}} \exp \left{-\frac{\beta}{2} \sum</em>{n=1}^{N}\left(t_n-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right)^{2}\right} \
&amp;=\left(\frac{\beta}{2 \pi}\right)^{\frac{N}{2}} \exp \left{-\frac{\beta}{2}\left(\mathsf{t}-\mathbf{\Phi}\mathbf{w}\right)^{\mathrm{T}}\left(\mathsf{t}-\mathbf{\Phi}\mathbf{w}\right)\right} \
&amp;=\mathcal{N}(\mathsf{t}\mid \mathbf{\Phi}\mathbf{w},\beta^{-1}\mathbf{I}_N)
\end{aligned}
$$
となる。注意点として、$\mathbf{\Phi}$は$N\times M$行列、$\mathbf{w}$は$M$次元の列ベクトルである。</p>
<p>ここで、問題文のヒントにしたがって</p>
<p>$$
\begin{aligned}
\mathbf y \rightarrow \mathbf t, \quad \mathbf x \rightarrow\mathbf w, \quad \boldsymbol \mu\rightarrow\mathbf 0, \quad \mathbf{\Lambda}^{-1}\rightarrow\alpha ^{-1}\mathbf I_M,\quad  \mathbf A\rightarrow\mathbf{\Phi} ,\quad \mathbf L^{-1}\rightarrow\beta^{-1} \mathbf I_N
\end{aligned}
$$</p>
<p>と置き換えると、$(2.115)$式を使って$p(\mathsf{t}\mid \alpha, \beta)$を求めることができる。これより</p>
<p>$$
p\left(\mathsf{t} \mid \alpha,\beta \right) = \mathcal{N}\left(\mathsf{t}\mid \mathbf{0}, ~ \beta^{-1} \mathbf{I}_N+\alpha^{-1} \mathbf{\Phi\Phi}^{\mathrm T} \right)
$$</p>
<p>と求まる。これについて対数をとって展開していくと（$(2.43)$の対数表現にあてはめて）</p>
<p>$$
\ln p\left( \mathsf{t} \mid \alpha,\beta \right) = -\frac {N}{2} \ln \left ( 2 \pi \right )  -  \frac {1}{2} \ln  \left | \beta^{-1} \mathbf I_N+\alpha ^{-1} \mathbf{\Phi}\mathbf{\Phi}^{\mathrm T} \right | -\frac{1}{2} \mathsf{t}^{\mathrm T} \left( \beta^{-1} \mathbf{I}_N+\alpha ^{-1} \mathbf{\Phi}\mathbf{\Phi}^{\mathrm T} \right)^{-1} \mathsf{t}
$$
となる。この第2項と第3項について計算していく。</p>
<p>まず第2項について</p>
<p>$$
\begin{aligned}
\left|\beta^{-1} \mathbf{I}<em>{N}+\alpha^{-1} \mathbf{\Phi\Phi}^{\mathrm T}\right| &amp;=\beta^{-N} \cdot \beta^{N}\left|\beta^{-1} \mathbf{I}</em>{N}+\alpha^{-1} \mathbf{\Phi\Phi}^{\mathrm T}\right| \
&amp;=\beta^{-N}\left|\mathbf{I}<em>{N}+\beta \alpha^{-1} \mathbf{\Phi\Phi}^{\mathrm T}\right| \quad\left(\because k^{N}|\mathbf{N}|=|k \mathbf{N}|\right) \
&amp;=\beta^{-N}\left|\mathbf{I}</em>{M}+\beta \alpha^{-1} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right| \quad(\because \text {Appendix}\ (\text{C}.14)) \
&amp;=\beta^{-N} \alpha^{-M}\left|\alpha \mathbf{I}_{M}+\beta \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right|\quad\left(\because k^M|\mathbf{M}|=|k \mathbf{M}|\right) \
&amp;=\beta^{-N} \alpha^{-M}|\mathbf{A}|\quad(\because \mathbf{A} = \alpha \mathbf{I}_M+\beta\mathbf{\Phi}^{\mathrm T}\mathbf{\Phi}\quad (3.81))
\end{aligned}
$$
が得られる。ここで、以下の定理を用いた。</p>
<blockquote>
<p>任意の$n\times n$行列$\mathbf{A}$と任意のスカラー値$k$に対して
$|k\mathbf{A}| = k^n|\mathbf{A}|$
が成り立つ（統計のための行列代数P.217, 系13.2.4）</p>
</blockquote>
<p>また対数を取った時の第3項についてはまず
$$
-\frac{1}{2} \mathsf{t}^{\mathrm T} \left( \beta^{-1} \mathbf{I}_N+\alpha ^{-1} \mathbf{\Phi}\mathbf{\Phi}^{\mathrm T} \right)^{-1} \mathsf{t} = -\frac{1}{2} \mathsf{t}^{\mathrm T} \left( \beta^{-1} \mathbf{I}_N+\mathbf{\Phi}(\alpha ^{-1} \mathbf{I}_M)\mathbf{\Phi}^{\mathrm T} \right)^{-1} \mathsf{t}
$$
としてからWoodburyの公式</p>
<p>$$
\left ( \mathbf A + \mathbf {BD}^{-1} \mathbf C \right ) ^{-1} = \mathbf A^{-1}-\mathbf A^{-1} \mathbf{B} \left ( \mathbf D + \mathbf{CA}^{-1}\mathbf B \right ) ^{-1}\mathbf{CA}^{-1} \tag {C.7}
$$</p>
<p>に当てはめると</p>
<p>$$
\begin{aligned}
-\frac{1}{2} \mathsf{t}^{\mathrm{T}}\left(\beta^{-1} \mathbf{I}<em>{N}+\alpha^{-1} \mathbf{\Phi} \Phi^{\mathrm{T}}\right)^{-1} \mathsf{t} &amp;=-\frac{1}{2} \mathsf{t}^{\mathrm{T}}\left[\beta \mathbf{I}</em>{N}-\beta\mathbf{I}<em>{N} \mathbf{\Phi}\left(\alpha \mathbf{I}</em>{M}+\mathbf{\Phi}^{\mathrm{T}}(\beta\mathbf{I}<em>N)\mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm{T}}(\beta\mathbf{I}<em>N)\right] \mathsf{t} \
&amp;=-\frac{1}{2} \mathsf{t}^{\mathrm{T}}\left[\beta \mathbf{I}</em>{N}-\beta \mathbf{\Phi}\left(\alpha \mathbf{I}</em>{M}+\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm{T}} \beta\right] \mathsf{t} \
&amp;=-\frac{\beta}{2} \mathsf{t}^{\mathrm{T}} \mathsf{t}+\frac{\beta^{2}}{2} \mathsf{t}^{\mathrm{T}} \mathbf{\Phi} \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}} \mathsf{t} \
&amp;=-\frac{\beta}{2} \mathsf{t}^{\mathrm{T}} \mathsf{t}+\frac{1}{2} \mathbf{m}<em>{N}^{\mathrm{T}} \mathbf{A} \mathbf{m}</em>{N} \quad (\because \mathbf{m}<em>{N}=\beta \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}} \mathsf{t}, (\mathbf{A}^{-1})^{\mathrm{T}} = (\mathbf{A}^{\mathrm{T}})^{-1} = \mathbf{A}^{-1}) \
&amp;=-\frac{1}{2}\left(\beta \mathsf{t}^{\mathrm{T}} \mathsf{t}-2 \mathbf{m}</em>{N}^{\mathrm{T}} \mathbf{A} \mathbf{m}<em>{N}+\mathbf{m}</em>{N}^{\mathrm{T}} \mathbf{A} \mathbf{m}<em>{N}\right) \
&amp;=-\frac{1}{2}\left(\beta \mathsf{t}^{\mathrm{T}} \mathsf{t}-2 \mathbf{m}</em>{N}^{\mathrm{T}} \mathbf{A}\left(\beta \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}} \mathsf{t}\right)+\mathbf{m}<em>{N}^{\mathrm{T}}\left(\alpha \mathbf{I}</em>{M}+\beta \mathbf{\Phi}^{\mathrm{T}} \boldsymbol{\Phi}\right) \mathbf{m}<em>{N}\right) \
&amp;=-\frac{1}{2}\left(\beta \mathsf{t}^{\mathrm{T}} \mathsf{t}-2 \mathbf{m}</em>{N}^{\mathrm{T}} \mathbf{\Phi}^{\mathrm{T}} \mathsf{t} \beta+\beta \mathbf{m}<em>{N}^{\mathrm{T}} \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \mathbf{m}</em>{N}+\alpha \mathbf{m}<em>{N}^{\mathrm{T}} \mathbf{m}</em>{N}\right) \
&amp;=-\frac{1}{2}\left(\beta\left(\mathsf{t}-\mathbf{\Phi} \mathbf{m}<em>{N}\right)^{\mathrm{T}}\left(\mathsf{t}-\mathbf{\Phi} \mathbf{m}</em>{N}\right)+\alpha \mathbf{m}<em>{N}^{\mathrm{T}} \mathbf{m}</em>{N}\right) \
&amp;=-\frac{\beta}{2}\left|\mathsf{t}-\mathbf{\Phi} \mathbf{m}<em>{N}\right|^{2}-\frac{1}{2} \alpha \mathbf{m}</em>{N}^{\mathrm{T}} \mathbf{m}_{N} \
&amp;=-E(\mathbf{m}_N)\quad(\because\ (3.82))
\end{aligned}
$$</p>
<p>以上から</p>
<p>$$
\ln p \left ( \mathsf{t} \mid \alpha,\beta \right ) =\frac {M}{2} \ln \alpha + \frac {N}{2} \ln \beta - E \left ( \mathbf m_N \right ) -\frac{1}{2} \ln \left | \mathbf A \right | -\frac {N}{2} \ln \left ( 2 \pi \right )  \tag{3.86}
$$</p>
<p>を導出することができた。</p>
<h2 id="演習-317"><a class="header" href="#演習-317">演習 3.17</a></h2>
<div class="panel-primary">
<p>ベイズ線形回帰モデルに対するエビデンス関数が
$$
p(\mathbf{t} \mid \alpha, \beta)=\left(\frac{\beta}{2 \pi}\right)^{N / 2}\left(\frac{\alpha}{2 \pi}\right)^{M / 2} \int \exp {-E(\mathbf{w})} \mathrm{d} \mathbf{w} \tag{3.78}
$$
の形式で書けることを示せ．ただし，
$$
\begin{aligned}
E(\mathbf{w}) &amp;=\beta E_{D}(\mathbf{w})+\alpha E_{W}(\mathbf{w}) \
&amp;=\frac{\beta}{2}|\mathbf{t}-\mathbf{\Phi} \mathbf{w}|^{2}+\frac{\alpha}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w}
\end{aligned} \tag{3.79}
$$
で定義される．</p>
</div>
<p>※演習問題3.16よりも簡単。</p>
<p>$p(\mathsf{t}\mid \alpha, \beta) = \int p(\mathsf{t}\mid \mathbf{w},\beta)p(\mathbf{w}\mid\alpha)d\mathbf{w}\quad (3.77)$を求める。<strong>演習3.16</strong>で示した通り</p>
<p>$$
p(\mathsf{t}\mid \mathbf{w},\beta) = \left(\frac{\beta}{2 \pi}\right)^{\frac{N}{2}} \exp \left{-\frac{\beta}{2}(\mathsf{t}-\boldsymbol{\Phi} \mathbf{w})^{\mathrm{T}}(\mathsf{t}-\boldsymbol{\Phi} \mathbf{w})\right}\
$$
$$
\begin{aligned}
p(\mathbf{w}\mid\alpha) &amp;= \mathcal{N}(\mathbf{w}\mid \mathbf{0}, \alpha^{-1}\mathbf{I}_M) \
&amp;=\left( \frac{\alpha}{2\pi} \right)^{\frac{M}{2}}\exp \left{ -\frac{1}{2}\mathbf{w}^{\mathrm{T}}(\alpha^{-1}\mathbf{I}_M)^{-1}\mathbf{w}\right} \
&amp;=\left( \frac{\alpha}{2\pi} \right)^{\frac{M}{2}}\exp \left{ -\frac{\alpha}{2}\mathbf{w}^{\mathrm{T}}\mathbf{w}\right}
\end{aligned}
$$
なのでこれらを代入すると
$$
\begin{aligned}
p(\mathbf{t} \mid \alpha, \beta)&amp;=\int\left(\frac{\beta}{2 \pi}\right)^{N / 2}\left(\frac{\alpha}{2 \pi}\right)^{M / 2} \exp \left{-\frac{\beta}{2}(\mathsf{t}-\mathbf{\Phi}\mathbf{w})^{\mathrm{T}}(\mathsf{t}-\mathbf{\Phi}\mathbf{w}) - \frac{\alpha}{2}\mathbf{w}^{\mathrm{T}}\mathbf{w}\right} \mathrm{d} \mathbf{w} \
&amp;=\left(\frac{\beta}{2 \pi}\right)^{N / 2}\left(\frac{\alpha}{2 \pi}\right)^{M / 2}\int \exp\left{-\frac{\beta}{2}||\mathsf{t}-\mathbf{\Phi}\mathbf{w}||^{2} - \frac{\alpha}{2}\mathbf{w}^{\mathrm{T}}\mathbf{w}\right} \mathrm{d} \mathbf{w}
\end{aligned}
$$
となる。これは$(3.78)$,$(3.79)$の形になっている。</p>
<h2 id="演習-318"><a class="header" href="#演習-318">演習 3.18</a></h2>
<div class="panel-primary">
<p>$\mathbf{w}$に関して平方完成することにより，</p>
<p>$$
\begin{aligned}
E(\mathbf{w}) &amp;=\beta E_{D}(\mathbf{w})+\alpha E_{W}(\mathbf{w}) \
&amp;=\frac{\beta}{2}|\mathbf{t}-\mathbf{\Phi} \mathbf{w}|^{2}+\frac{\alpha}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w}
\end{aligned} \tag{3.79}
$$</p>
<p>で定義されるベイズ線形回帰の誤差関数が</p>
<p>$$
E(\mathbf{w})=E\left(\mathbf{m}<em>{N}\right)+\frac{1}{2}\left(\mathbf{w}-\mathbf{m}</em>{N}\right)^{\mathrm{T}} \mathbf{A}\left(\mathbf{w}-\mathbf{m}_{N}\right) \tag{3.80}
$$</p>
<p>の形で書けることを示せ．</p>
</div>
<p>※誘導に従って平方完成して式変形していくだけ。$\mathbf{A} = \alpha \mathbf{I}_M+\beta\mathbf{\Phi}^{\mathrm T}\mathbf{\Phi}=(\alpha \mathbf{I}_M+\mathbf{\Phi}^{\mathrm T}(\beta\mathbf{I}_M)\mathbf{\Phi})$と$(3.84)$式の定義$\mathbf{m}_N=\beta \mathbf{A}^{-1}\mathbf{\Phi}^{\mathrm T}\mathsf{t}$を途中で導入する。</p>
<p>$$
\begin{aligned}
E(\mathbf{w}) &amp;= \frac{\beta}{2} ||\mathbf{t}-\mathbf{\Phi} \mathbf{w}|^{2}+\frac{\alpha}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w} \
&amp;=\frac{\beta}{2}\left(\mathbf{t}^{\mathrm{T}} \mathbf{t}-2 \mathbf{t}^{\mathrm{T}} \mathbf{\Phi} \mathbf{w}+\mathbf{w}^{\mathrm{T}} \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \mathbf{w}\right)+\frac{\alpha}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w} \
&amp;=\frac{1}{2}\left(\mathbf{w}^{\mathrm{T}}\mathbf{\Phi}^{\mathrm{T}}(\beta\mathbf{I}_M)\mathbf{\Phi} \mathbf{w}+\mathbf{w}^{\mathrm{T}}(\alpha \mathbf{I}_M)\mathbf{w}-2\beta\mathbf{t}^{\mathrm{T}} \mathbf{\Phi} \mathbf{w}+\beta \mathbf{t}^{\mathrm{T}} \mathbf{t} \right) \
&amp;=\frac{1}{2}\left( \mathbf{w}^{\mathrm{T}}\mathbf{A}\mathbf{w}-2\beta\mathbf{t}^{\mathrm{T}} \mathbf{\Phi} \mathbf{w}+\beta \mathbf{t}^{\mathrm{T}} \mathbf{t}\right) \
&amp;=\frac{1}{2}\left( \mathbf{w}^{\mathrm{T}}\mathbf{A}\mathbf{w}-2\mathbf{m}_N^{\mathrm T}\mathbf{A}^{\mathrm T}\mathbf{\Phi}^{-1}\mathbf{\Phi} \mathbf{w}+\beta \mathbf{t}^{\mathrm{T}} \mathbf{t}\right) \
&amp;=\frac{1}{2}\left( \mathbf{w}^{\mathrm{T}}\mathbf{A}\mathbf{w}-2\mathbf{m}_N^{\mathrm T}\mathbf{A}^{\mathrm T}\mathbf{w}+\mathbf{m}_N^{\mathrm T}\mathbf{A}\mathbf{m}_N\right) - \frac{1}{2}\mathbf{m}_N^{\mathrm T} \mathbf{A} \mathbf{m}_N + \frac{\beta}{2}\mathbf{t}^{\mathrm{T}} \mathbf{t} \
&amp;=\frac{1}{2}(\mathbf{w} - \mathbf{m}_N)^{\mathrm T}\mathbf{A}(\mathbf{w} - \mathbf{m}_N)- \frac{1}{2}\mathbf{m}_N^{\mathrm T} \mathbf{A} \mathbf{m}_N + \frac{\beta}{2}\mathbf{t}^{\mathrm{T}} \mathbf{t}
\end{aligned}
$$</p>
<p>ここで、$\displaystyle -\frac{1}{2}\mathbf{m}_N^{\mathrm T} \mathbf{A} \mathbf{m}_N + \frac{\beta}{2}\mathbf{t}^{\mathrm{T}} \mathbf{t}$については<strong>演習問題3.16</strong>の後半の式変形と同じなので
$$
\begin{aligned}
-\frac{1}{2}\mathbf{m}_N^{\mathrm T} \mathbf{A} \mathbf{m}<em>N + \frac{\beta}{2}\mathbf{t}^{\mathrm{T}} \mathbf{t} &amp;= \frac{\alpha}{2}\mathbf{m}<em>N^{\mathrm T}\mathbf{m}<em>N + \frac{\beta}{2}\left|\mathsf{t}-\mathbf{\Phi} \mathbf{m}</em>{N}\right|^{2}\
&amp;=E(\mathbf{m}<em>N)
\end{aligned}
$$
となるので、結果として$(3.80)$式
$$
E(\mathbf{w})=E\left(\mathbf{m}</em>{N}\right)+\frac{1}{2}\left(\mathbf{w}-\mathbf{m}</em>{N}\right)^{\mathrm{T}} \mathbf{A}\left(\mathbf{w}-\mathbf{m}</em>{N}\right) \tag{3.80}
$$
が成立する。</p>
<h2 id="演習-319"><a class="header" href="#演習-319">演習 3.19</a></h2>
<div class="panel-primary">
<p>ベイズ線形回帰モデルの$\mathbf{w}$に関する積分が</p>
<p>$$
\int \exp {-E(\mathbf{w})} \mathrm{d} \mathbf{w} =\exp \left{-E\left(\mathbf{m}_{N}\right)\right}(2 \pi)^{M / 2}|\mathbf{A}|^{-1 / 2}
\tag{3.85}
$$</p>
<p>で与えられることを示せ．したがって，対数周辺尤度が
$$
\ln p(\mathbf{t} \mid \alpha, \beta)=\frac{M}{2} \ln \alpha+\frac{N}{2} \ln \beta-E\left(\mathbf{m}_{N}\right)-\frac{1}{2} \ln |\mathbf{A}|-\frac{N}{2} \ln (2 \pi) \tag{3.86}
$$
で与えられることを示せ．</p>
</div>
<p>$(3.85)$の積分が成立することを示す。</p>
<p>$$
E(\mathbf{w})=E\left(\mathbf{m}<em>{N}\right)+\frac{1}{2}\left(\mathbf{w}-\mathbf{m}</em>{N}\right)^{\mathrm{T}} \mathbf{A}\left(\mathbf{w}-\mathbf{m}_{N}\right)
\tag{3.80}
$$</p>
<p>$$
E\left(\mathbf{m}<em>{N}\right)=\frac{\beta}{2}\left|\mathbf{t}-\mathbf{\Phi} \mathbf{m}</em>{N}\right|^{2}+\frac{\alpha}{2} \mathbf{m}<em>{N}^{\mathrm{T}} \mathbf{m}</em>{N}
\tag{3.82}
$$</p>
<p>$(3.80)$式から$(3.85)$が成り立つことを示す。$(3.82)$より$E\left(\mathbf{m}_{N}\right)$は$\mathbf{w}$の関数ではないため積分の外に出すことができる。</p>
<p>$$
\begin{aligned}
\int \exp {-E(\mathbf{w})} \mathrm{d} \mathbf{w} =\exp \left{-E\left(\mathbf{m}<em>{N}\right)\right} \int \exp \left{-\frac{1}{2}\left(\mathbf{w}-\mathbf{m}</em>{N}\right)^{\mathrm{T}} \mathbf{A}\left(\mathbf{w}-\mathbf{m}<em>{N}\right)\right} \mathrm{d} \mathbf{w}
\end{aligned}
$$
今$\mathbf{w}$の次元は$M$であるので、正規化された多次元ガウス分布の形
$$
\frac{1}{(2 \pi)^{M / 2}} \frac{1}{|\mathbf{A}|^{1 / 2}}\int \exp \left{-\frac{1}{2}\left(\mathbf{w}-\mathbf{m}</em>{N}\right)^{\mathrm{T}} \mathbf{A}\left(\mathbf{w}-\mathbf{m}_{N}\right)\right} \mathrm{d} \mathbf{w} = 1
$$
から正規化係数部分を取り出せば</p>
<p>$$
\int \exp {-E(\mathbf{w})} \mathrm{d} \mathbf{w} =\exp \left{-E\left(\mathbf{m}_{N}\right)\right}(2 \pi)^{M / 2}|\mathbf{A}|^{-1 / 2}
\tag{3.85}
$$
$(3.85)$を示すことができる。</p>
<p>対数周辺尤度は</p>
<p>$$
p(\mathsf{t} | \alpha, \beta)=\left(\frac{\beta}{2 \pi}\right)^{N / 2}\left(\frac{\alpha}{2 \pi}\right)^{M / 2} \int \exp {-E(\mathbf{w})} \mathrm{d} \mathbf{w}
\tag{3.78}
$$</p>
<p>で表すことができ、$(3.85)$の結果と合わせると、</p>
<p>$$
p(\mathsf{t} | \alpha, \beta)=\left(\frac{\beta}{2 \pi}\right)^{N / 2}\left(\frac{\alpha}{2 \pi}\right)^{M / 2} \exp \left{-E\left(\mathbf{m}_{N}\right)\right}(2 \pi)^{M / 2}|\mathbf{A}|^{-1 / 2}
$$
この等式に対数を取ってやると、</p>
<p>$$
\ln p(\mathsf{t} | \alpha, \beta)=\frac{M}{2} \ln \alpha+\frac{N}{2} \ln \beta-E\left(\mathbf{m}_{N}\right)-\frac{1}{2} \ln |\mathbf{A}|-\frac{N}{2} \ln (2 \pi)
\tag{3.86}
$$
よって対数周辺尤度が$(3.86)$で与えられることが示された。</p>
<h2 id="演習-320"><a class="header" href="#演習-320">演習 3.20</a></h2>
<div class="panel-primary">
<p>対数周辺尤度関数</p>
<p>$$
\ln p(\mathbf{t} \mid \alpha, \beta)=\frac{M}{2} \ln \alpha+\frac{N}{2} \ln \beta-E\left(\mathbf{m}_{N}\right)-\frac{1}{2} \ln |\mathbf{A}|-\frac{N}{2} \ln (2 \pi) \tag{3.86}
$$</p>
<p>の$\alpha$に関する最大化が再推定方程式</p>
<p>$$
\alpha = \frac{\gamma}{\mathbf{m}_N^{\mathrm{T}}\mathbf{m}<em>N},\quad \gamma = \sum</em>{i}\frac{\lambda_i}{\alpha+\lambda_i} \tag{3.92}
$$
に帰着されることを示すのに必要なすべての段階を$(3.86)$から始めて確かめよ．</p>
</div>
<p>※<strong>3.5.2 エビデンス関数の最大化</strong>をなぞるだけ。</p>
<p>$(3.86)$式を$\alpha$で偏微分する。そのために，まず次の固有ベクトル方程式を考える。
$$
\left(\beta \mathbf{\Phi}^{\mathrm T}\mathbf{\Phi} \right)\mathbf{u}_i = \lambda_i \mathbf{u}_i \tag{3.87}
$$
また、$\alpha \mathbf{I}_M$についての固有値は当然$\alpha$であり、$(\alpha \mathbf{I}<em>M)\mathbf{u}<em>i = \alpha \mathbf{u}<em>i$のように書けるので、この2式を足せば
$$
\left(\alpha \mathbf{I}<em>M + \beta \mathbf{\Phi}^{\mathrm T}\mathbf{\Phi} \right)\mathbf{u}<em>i = (\alpha + \lambda_i)\mathbf{u}<em>i
$$
となるので、$\mathbf{A}$は固有値$\alpha + \lambda_i$を持つことがわかる。ここで，$(3.86)$に含まれる$\ln |\mathbf{A}|$の項の$\alpha$に関する導関数を考えると
$$
\frac{d}{d \alpha} \ln |\mathbf{A}|=\frac{d}{d \alpha} \ln \prod</em>{i=1}^M \left(\lambda</em>{i}+\alpha\right)=\frac{d}{d \alpha} \sum</em>{i=1}^M \ln \left(\lambda</em>{i}+\alpha\right)=\sum</em>{i=1}^M \frac{1}{\lambda</em>{i}+\alpha} \tag{3.88}
$$
が得られる。これより，$(3.86)$の$\alpha$に関する停留点は
$$
0 = \frac{M}{2\alpha}-\frac{1}{2}\mathbf{m}<em>N^{\mathrm T}\mathbf{m}<em>N-\frac{1}{2}\sum</em>{i=1}^M \frac{1}{\lambda_i+\alpha} \tag{3.89}
$$
を満たす。$2\alpha$を掛け，式を整理すれば
$$
\alpha\mathbf{m}<em>N^{\mathrm T}\mathbf{m}<em>N = M - \alpha \sum</em>{i=1}^M \frac{1}{\lambda_i + \alpha} = \sum</em>{i=1}^M \left( 1- \frac{1}{\lambda_i + \alpha} \right) = \sum</em>{i=1}^M \frac{\lambda_i}{\lambda_i + \alpha} \equiv \gamma
$$
が得られる。よって
$$
\alpha = \frac{\gamma}{\mathbf{m}_N^{\mathrm T}\mathbf{m}_N} \tag{3.92}
$$
となる。</p>
<h2 id="演習-321"><a class="header" href="#演習-321">演習 3.21</a></h2>
<div class="panel-primary">
<p>$(3.92)$はエビデンスの枠組みにおける最適な$\alpha$の値である．この結果は，次の等式を使って導出することもできる．</p>
<p>$$
\frac{d}{d \alpha} \ln |\mathbf{A}|=\operatorname{Tr}\left(\mathbf{A}^{-1} \frac{d}{d \alpha} \mathbf{A}\right) \tag{3.117}
$$</p>
<p>実対称行列$\mathbf{A}$の固有値展開，および$\mathbf{A}$の行列式とトレースの固有値表現の標準的結果（付録C参照）を用いて，この等式を証明せよ．そして，$(3.117)$を用いて，$(3.86)$から$(3.92)$を導け．</p>
</div>
<p>※$(3.117)$を証明する。付録Cも参照。
まず$\mathbf{A} = \alpha \mathbf{I}_M + \beta\mathbf{\Phi}^{\mathrm T}\mathbf{\Phi}$であり、$\mathbf{A}\mathbf{u}_i = \lambda_i \mathbf{u}_i$となるような固有値$\lambda_i$と固有ベクトル$\mathbf{u}_i$が存在する。この2つはそれぞれ$\alpha$に依存する。</p>
<p>$\mathbf{U} = (\mathbf{u}_1,\mathbf{u}_2,\ldots,\mathbf{u}_M)$とすると$\mathbf{AU} = \mathbf{U\Lambda}$と書くことができる。ここで$\mathbf{\Lambda}$は$\lambda_i$を対角成分とする$M\times M$対角行列である（付録Cの(C.38)）。</p>
<p>実対称行列$\mathbf{A}$についての$\mathbf{U}$は正規直交行列となるようにとることができるので(C.29)〜(C.36)、$\mathbf{U}^{\mathrm T}\mathbf{U}=\mathbf{I}$, よって$\mathbf{U}^{\mathrm T} = \mathbf{U}^{-1}$となる。これより$\mathbf{A} = \mathbf{U\Lambda U}^{-1}$が得られ、
$$
|\mathbf{A}| = |\mathbf{U}||\mathbf{\Lambda}||\mathbf{U}^{-1}|=|\mathbf{\Lambda}|=\prod_{i=1}^M \lambda_i
$$
となることが分かる。</p>
<p>一方で
$$
\operatorname{Tr}(\mathbf{A}) = \operatorname{Tr}(\mathbf{U\Lambda U}^{-1}) = \operatorname{Tr}(\mathbf{U}^{-1}\mathbf{U\Lambda}) = \operatorname{Tr}(\mathbf{\Lambda}) = \sum_{i=1}^M \lambda_i
$$
である。</p>
<p>以上から$(3.117)$の左辺について変形すると
$$
\frac{d}{d \alpha} \ln |\mathbf{A}|=\frac{d}{d \alpha} \ln \prod_{i=1}^{M} \lambda_{i}=\frac{d}{d \alpha} \sum_{i=1}^{M} \ln \lambda_{i}=\sum_{i=1}^{M} \frac{1}{\lambda_{i}} \frac{d}{d \alpha} \lambda_{i}
$$
となる。続いて右辺について</p>
<p>$$
\begin{aligned}
\operatorname{Tr}\left(\mathbf{A}^{-1} \frac{d}{d \alpha} \mathbf{A}\right) &amp;=\operatorname{Tr}\left(\sum_{i=1}^{M} \frac{1}{\lambda_{i}} \mathbf{u}<em>{i} \mathbf{u}</em>{i}^{\mathrm T} \frac{d}{d \alpha} \sum_{j=1}^{M} \lambda_{j} \mathbf{u}<em>{j} \mathbf{u}</em>{j}^{\mathrm T}\right) \
&amp;=\operatorname{Tr}\left(\sum_{i=1}^{M} \frac{1}{\lambda_{i}} \mathbf{u}<em>{i} \mathbf{u}</em>{i}^{\mathrm T}\left{\sum_{j=1}^{M}\left(\frac{d \lambda_{j}}{d \alpha} \mathbf{u}<em>{j} \mathbf{u}</em>{j}^{\mathrm T}+\lambda_{j} \frac{d \mathbf{u}<em>{j}}{d \alpha} \mathbf{u}</em>{j}^{\mathrm T}+\lambda_j \mathbf{u}<em>{j} \frac{d \mathbf{u}</em>{j}^{\mathrm T}}{d \alpha}\right)\right}\right) \
&amp;=\operatorname{Tr}\left(\sum_{i=1}^{M} \frac{1}{\lambda_{i}} \mathbf{u}<em>{i} \mathbf{u}</em>{i}^{\mathrm T} \sum_{j=1}^{M} \frac{d \lambda_{j}}{d \alpha} \mathbf{u}<em>{j} \mathbf{u}</em>{j}^{\mathrm T}\right)+\operatorname{Tr}\left(\sum_{i=1}^{M} \frac{1}{\lambda_{i}} \mathbf{u}<em>{i} \mathbf{u}</em>{i}^{\mathrm T} \left( \sum_{j=1}^{M} \lambda_{j} \frac{d \mathbf{u}<em>{j}}{d \alpha} \mathbf{u}</em>{j}^{\mathrm T}+\lambda_j \mathbf{u}<em>{j} \frac{d \mathbf{u}</em>{j}^{\mathrm T}}{d \alpha}\right) \right)\
&amp;=\operatorname{Tr}\left(\sum_{i=1}^{M} \sum_{j=1}^{M} \frac{1}{\lambda_{i}} \frac{d \lambda_{j}}{d \alpha} \mathbf{u}<em>{i} \mathbf{u}</em>{i}^{\mathrm T} \mathbf{u}<em>{j} \mathbf{u}</em>{j}^{\mathrm T}\right)+\operatorname{Tr}\left(\sum_{i=1}^{M} \sum_{j=1}^{M} \frac{2\lambda_{j}}{\lambda_{i}} \mathbf{u}<em>{i} \mathbf{u}</em>{i}^{\mathrm T} \mathbf{u}<em>{j} \frac{d \mathbf{u}</em>{j}^{\mathrm T}}{d \alpha} \right) \quad \left(\because \sum_i \alpha_i\sum_j \beta_j = \sum_i \sum_j \alpha_i \beta_j \right)\
&amp;=\operatorname{Tr}\left(\sum_{i=1}^{M} \frac{1}{\lambda_{i}} \frac{d \lambda_{i}}{d \alpha} \mathbf{u}<em>{i} \mathbf{u}</em>{i}^{\mathrm T}\right)+\operatorname{Tr}\left(\sum_{i=1}^{M} 2 \mathbf{u}<em>{i} \frac{d \mathbf{u}</em>{i}^{\mathrm T}}{d \alpha}\right) \quad \left( \because \mathbf{u}<em>i^{\mathrm T}\mathbf{u}<em>j = \delta</em>{ij}より, i=jの項だけが残る \right)\
&amp;=\sum</em>{i=1}^{M} \frac{1}{\lambda_{i}} \frac{d \lambda_{i}}{d \alpha}+\operatorname{Tr}\left(\frac{d}{d \alpha}\left(\frac{d \mathbf{u}<em>{i}}{d \alpha} \mathbf{u}</em>{i}^{\mathrm T}+\mathbf{u}<em>{i} \frac{d \mathbf{u}</em>{i}^{\mathrm T}}{d \alpha}\right)\right) \
&amp;=\sum_{i=1}^{M} \frac{1}{\lambda_{i}} \frac{d \lambda_{i}}{d \alpha}+\operatorname{Tr}\left(\frac{d}{d \alpha} \sum_{i=1}^{M} \mathbf{u}<em>{i} \mathbf{u}</em>{i}^{\mathrm T}\right) \
&amp;=\sum_{i=1}^{M} \frac{1}{\lambda_{i}} \frac{d \lambda_{i}}{d \alpha}+\operatorname{Tr}\left(\frac{d}{d \alpha} \mathbf{I}<em>{M}\right) \
&amp;=\sum</em>{i=1}^{M} \frac{1}{\lambda_{i}} \frac{d \lambda_{i}}{d \alpha}
\end{aligned}
$$</p>
<p>以上の式変形から
$$
\frac{d}{d \alpha} \ln |\mathbf{A}| = \operatorname{Tr}\left(\mathbf{A}^{-1} \frac{d}{d \alpha} \mathbf{A}\right) \tag{3.117}
$$
が示された。</p>
<blockquote>
<p>統計のための行列代数第15章 15.8 <strong>行列式と逆行列と随伴行列の一次偏導関数</strong>（P.365）の話によれば、余因子行列を使って$(3.117)$式を証明することもできるらしい。</p>
</blockquote>
<p>後半の$(3.92)$の導出は演習問題3.20とほぼ同じなので省略。</p>
<h2 id="演習-322"><a class="header" href="#演習-322">演習 3.22</a></h2>
<div class="panel-primary">
<p>対数周辺尤度関数</p>
<p>$$
\ln p(\mathbf{t} \mid \alpha, \beta)=\frac{M}{2} \ln \alpha+\frac{N}{2} \ln \beta-E\left(\mathbf{m}_{N}\right)-\frac{1}{2} \ln |\mathbf{A}|-\frac{N}{2} \ln (2 \pi) \tag{3.86}
$$</p>
<p>の$\beta$に関する最大化が再推定方程式</p>
<p>$$
\frac{1}{\beta}=\frac{1}{N-\gamma} \sum_{n=1}^{N}\left{t_{n}-\mathbf{m}<em>{N}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}</em>{n}\right)\right}^{2} \tag{3.95}
$$</p>
<p>に帰着されることを示すのにすべての段階を，$(3.86)$から始めて確かめよ．</p>
</div>
<p>※P.168をなぞるだけ</p>
<p>$(3.86)$の$\ln p(\mathsf{t}\mid \alpha, \beta)$を$\beta$で偏微分する。準備として、$\displaystyle \frac{\partial}{\partial \beta}\ln |\mathbf{A}|$について、$\beta$と$\lambda_i$は比例するので$\displaystyle \frac{\partial \lambda_i}{\partial \beta} = \frac{\lambda_i}{\beta}$より</p>
<p>$$
\frac{d}{d \beta} \ln |\mathbf{A}|=\frac{d}{d \beta} \sum_{i} \ln \left(\lambda_{i}+\alpha\right)=\frac{1}{\beta} \sum_{i} \frac{\lambda_{i}}{\lambda_{i}+\alpha}=\frac{\gamma}{\beta}
$$
が得られる。したがって, 周辺尤度の停留点は
$$
0=\frac{N}{2 \beta}-\frac{1}{2} \sum_{n=1}^{N}\left{t_{n}-\mathbf{m}<em>{N}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}</em>{n}\right)\right}^{2}-\frac{\gamma}{2 \beta} \tag{3.94}
$$
これを整理すれば
$$
\frac{1}{\beta}=\frac{1}{N-\gamma} \sum_{n=1}^{N}\left{t_{n}-\mathbf{m}<em>{N}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}</em>{n}\right)\right}^{2} \tag{3.95}
$$
が得られる。</p>
<blockquote>
<p>$\mathbf{m}_N$は$\alpha, \beta$に依存しているので本当は$E(\mathbf{m}_N)$も$\beta$で偏微分するともっと複雑な式になるが（$\partial \mathbf{m}_N/\partial \beta$の項を考える必要が出てくる）、P.168の$\alpha$のときのように繰り返し法で解くことを想定しているので$\mathbf{m}_N$の$\beta$依存性は考慮しなくてよいことになっている。</p>
</blockquote>
<h2 id="演習-323"><a class="header" href="#演習-323">演習 3.23</a></h2>
<div class="panel-primary">
<p>演習問題3.12で説明したモデルに対するデータの周辺確率（言い換えるとモデルエビデンス）が</p>
<p>$$
p(\mathsf{t})=\frac{1}{(2 \pi)^{N / 2}} \frac{b_{0}^{a_{0}}}{b_{N}^{a_{N}}} \frac{\Gamma\left(a_{N}\right)}{\Gamma\left(a_{0}\right)} \frac{\left|\mathbf{S}<em>{N}\right|^{1 / 2}}{\left|\mathbf{S}</em>{0}\right|^{1 / 2}} \tag{3.118}
$$</p>
<p>で与えられることを示せ．まず最初に$\mathbf{w}$に関して周辺化し，そして次に$\beta$に関して周辺化するとよい．</p>
</div>
<p>ベイズの定理と周辺確率から
$$
p(t)=\iint p(\mathsf{t}, \mathbf{w}, \beta) d\mathbf{w} d \beta=\iint p(\mathsf{t} \mid \mathbf{w}, \beta) p(\mathbf{w}, \beta) d\mathbf{w} d\beta
$$
と書くことができる。
演習問題3.12でやったように、$p(\mathsf{t} \mid \mathbf{w}, \beta)$は尤度関数、$p(\mathbf{w},\beta)$は共役事前分布となる（正規-ガンマ分布）。</p>
<p>$p(\mathsf{t} \mid \mathbf{w}, \beta)$は演習問題3.16でやったように$\mathcal{N}(\mathsf{t}\mid \mathbf{\Phi}\mathbf{w},\beta^{-1}\mathbf{I}<em>N)$と表すことができる。よってこれらの式を使うと、
$$
\begin{aligned}
p(\mathsf{t}) &amp;=\iint \mathcal{N}\left(\mathsf{t} \mid \mathbf{\Phi} \mathbf{w}, \beta^{-1} \mathbf{I}</em>{N}\right) \mathcal{N}\left(\mathbf{w} \mid \mathbf{m}<em>{0}, \beta^{-1}  \mathbf{S}</em>{0}\right) {\operatorname{Gam}}\left(\beta \mid a_{0}, b_{0}\right) d \mathbf{w} d \beta \
&amp;=\iint\left(\frac{\beta}{2 \pi}\right)^{\frac{N}{2}} \exp \left{-\frac{\beta}{2}(\mathsf{t}-\mathbf{\Phi} \mathbf{w})^{\mathrm{T}}(\mathsf{t}-\mathbf{\Phi} \mathbf{w})\right}\left(\frac{\beta}{2 \pi}\right)^{\frac{M}{2}} \frac{1}{\left| \mathbf{S}<em>{0}\right|^{\frac{1}{2}}} \exp \left{-\frac{\beta}{2}\left(\mathbf{w}-\mathbf{m}</em>{0}\right)^{\mathrm{T}}  \mathbf{S}<em>{0}^{-1}\left(\mathbf{w}-\mathbf{m}</em>{0}\right)\right}\Gamma\left(a_{0}\right)^{-1} b_{0}^{a_{0}} \beta^{a_{0}-1} \exp \left(-b_{0} \beta\right) d \mathbf{w} d \beta \
&amp;=\frac{b_{0}^{a_{0}}}{\Gamma\left(a_{0}\right)(2 \pi)^{\frac{M+N}{2}}\left| \mathbf{S}<em>{0}\right|^{\frac{1}{2}}} \iint \beta^{\frac{M}{2}+\frac{N}{2}+a</em>{0}-1} \exp \left[-\frac{\beta}{2}\left{\mathbf{w}^{\mathrm{T}}\left(\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}+ \mathbf{S}<em>{0}^{-1}\right) \mathbf{w}-2 \mathbf{w}^{\mathrm{T}}\left(\mathbf{\Phi}^{\mathrm{T}} \mathsf{t}+ \mathbf{S}</em>{0}^{-1} \mathbf{m}<em>{0}\right)\right}\right] \exp \left[-\frac{\beta}{2}\left(\mathsf{t}^{\mathrm{T}} \mathsf{t}+\mathbf{m}</em>{0}^{\mathrm{T}}  \mathbf{S}<em>{0}^{-1} \mathbf{m}</em>{0}\right)\right] \exp \left(-b_{0} \beta\right) d \mathbf{w} d \beta
\end{aligned}
$$
演習問題3.12で求めた$\mathbf{S}<em>{N}^{-1}=\mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}+\mathbf{S}</em>{0}^{-1}, \quad \mathbf{m}<em>{N}=\mathbf{S}</em>{N}\left(\mathbf{\Phi}^{\mathrm T} \mathsf{t}+\mathbf{S}<em>{0}^{-1} \mathbf{m}</em>{0}\right)$（※教科書$(3.50), (3.51)$のものとは異なるので注意）を使ってこれを書き換えると</p>
<p>$$
\begin{aligned}
p(\mathsf{t})&amp;=\frac{b_{0}^{a_{0}}}{\Gamma\left(a_{0}\right)(2 \pi)^{\frac{M+N}{2}}\left| \mathbf{S}<em>{0}\right|^{\frac{1}{2}}} \iint \beta^{\frac{M}{2}+\frac{N}{2}+a</em>{0}-1} \exp \left[-\frac{\beta}{2}\left{\mathbf{w}^{\mathrm{T}}  \mathbf{S}<em>{N}^{-1} \mathbf{w}-2 \mathbf{w}^{\mathrm{T}}  \mathbf{S}</em>{N}^{-1} \mathbf{m}<em>{N}\right}\right] \exp \left[-\frac{\beta}{2}\left(\mathsf{t}^{\mathrm{T}} \mathsf{t}+\mathbf{m}</em>{0}^{\mathrm{T}}  \mathbf{S}<em>{0}^{-1} \mathbf{m}</em>{0}\right)\right] \exp \left(-b_{0} \beta\right) d \mathbf{w} d \beta \
&amp;=\frac{b_{0}^{a_{0}}}{\Gamma\left(a_{0}\right)(2 \pi)^{\frac{M+N}{2}}\left| \mathbf{S}<em>{0}\right|^{\frac{1}{2}}} \iint \beta^{\frac{M}{2}+\frac{N}{2}+a</em>{0}-1} \exp \left[-\frac{\beta}{2}\left(\mathbf{w}-\mathbf{m}<em>{N}\right)^{\mathrm{T}}  \mathbf{S}</em>{N}^{-1}\left(\mathbf{w}-\mathbf{m}<em>{N}\right)\right] \exp \left[-\frac{\beta}{2}\left(\mathsf{t}^{\mathrm{T}} \mathsf{t}+\mathbf{m}</em>{0}^{\mathrm{T}}  \mathbf{S}<em>{0}^{-1} \mathbf{m}</em>{0}-\mathbf{m}<em>{N}^{\mathrm{T}}  \mathbf{S}</em>{N}^{-1} \mathbf{m}<em>{N}\right)\right] \exp \left(-b</em>{0} \beta\right) d \mathbf{w} d \beta \
\end{aligned}
$$
そしてさらに演習問題3.12で求めた$\displaystyle a_{N}=a_{0}+\frac{N}{2},\quad b_{N}=b_{0}+\frac{1}{2}\left(\mathbf{m}<em>{0}^{\mathrm{T}} \mathbf{S}</em>{0}^{-1} \mathbf{m}<em>{0}-\mathbf{m}</em>{N}^{\mathrm{T}} \mathbf{S}<em>{N}^{-1} \mathbf{m}</em>{N}+\mathsf{t}^{\mathrm{T}} \mathsf{t}\right)$を使うと</p>
<p>$$
\begin{aligned}
p(\mathsf{t}) &amp;= \underbrace{\frac{\beta^{\frac{M}{2}}}{(2 \pi)^{\frac{M}{2}}|\mathbf{S}<em>N|^{\frac{1}{2}}} \int \exp \left[-\frac{\beta}{2}\left(\mathbf{w}-\mathbf{m}</em>{N}\right)^{\mathrm{T}} \mathbf{S}<em>{N}^{-1}\left(\mathbf{w}-\mathbf{m}</em>{N}\right)\right] d \mathbf{w}}<em>{\text{Normal distribution, equal to 1}} \cdot \frac{\left|\mathbf{S}</em>{N}\right|^{\frac{1}{2}} b_{0}^{a_{0}}}{\Gamma\left(a_{0}\right)(2 \pi)^{\frac{N}{2}}\left|\mathbf{S}<em>{0}\right|^{\frac{1}{2}}} \underbrace{\int \beta^{a</em>{N}-1} \exp \left(-b_{N} \beta\right) d \beta}<em>{\text{Gamma distribution (not normalized)}} \
&amp;= \frac{b</em>{0}^{a_{0}}\left|\mathbf{S}<em>{N}\right|^{\frac{1}{2}}}{\Gamma\left(a</em>{0}\right)(2 \pi)^{\frac{N}{2}}\left|\mathbf{S}<em>{0}\right|^{\frac{1}{2}}} \frac{\Gamma\left(a</em>{N}\right)}{b_{N}^{a_{N}}} \
&amp;=\frac{1}{(2 \pi)^{N / 2}} \frac{b_{0}^{a_{0}}}{b_{N}^{a_{N}}} \frac{\Gamma\left(a_{N}\right)}{\Gamma\left(a_{0}\right)} \frac{\left|\mathbf{S}<em>{N}\right|^{1 / 2}}{\left|\mathbf{S}</em>{0}\right|^{1 / 2}}
\end{aligned}
$$</p>
<p>以上から$(3.118)$式となることが示された。</p>
<h2 id="演習-324"><a class="header" href="#演習-324">演習 3.24</a></h2>
<div class="panel-primary">
<p>次の形のベイズの定理に事前，事後分布と尤度関数を代入して上記の$(3.118)$が成立することを示せ．</p>
<p>$$
p(\mathsf{t})=\frac{p(\mathsf{t} | \mathbf{w}, \beta) p(\mathbf{w}, \beta)}{p(\mathbf{w}, \beta | \mathsf{t})} \tag{3.119}
$$</p>
</div>
<p>$(3.119)$の分母は演習問題3.12の$(3.113)$の$p(\mathbf{w}, \beta | \mathbf{t})=\mathcal{N}\left(\mathbf{w} | \mathbf{m}<em>{N}, \beta^{-1} \mathbf{S}</em>{N}\right) \operatorname{Gam}\left(\beta | a_{N}, b_{N}\right)$で、分子の$p(\mathsf{t} | \mathbf{w}, \beta)$は演習問題3.16の$\mathcal{N}(\mathsf{t}\mid \mathbf{\Phi}\mathbf{w},\beta^{-1}\mathbf{I}<em>N)$で、$p(\mathbf{w}, \beta)$は$\mathcal{N}\left(\mathbf{w} \mid \mathbf{m}</em>{0}, \beta^{-1}  \mathbf{S}<em>{0}\right) {\operatorname{Gam}}\left(\beta \mid a</em>{0}, b_{0}\right)$で、それぞれ与えられる。これらを代入して展開する。まず分母について計算すると</p>
<p>$$
\begin{aligned}
p(\mathbf{w}, \beta | \mathbf{t})&amp;=\mathcal{N}\left(\mathbf{w} | \mathbf{m}<em>{N}, \beta^{-1} \mathbf{S}</em>{N}\right) \operatorname{Gam}\left(\beta | a_{N}, b_{N}\right) \
&amp;=\left(\frac{\beta}{2 \pi}\right)^{M / 2}\left|\mathbf{S}<em>{N}\right|^{-1 / 2} \exp \left(-\frac{\beta}{2}\left(\mathbf{w}^{\mathrm{T}} \mathbf{S}</em>{N}^{-1} \mathbf{w}-\mathbf{w}^{\mathrm{T}} \mathbf{S}<em>{N}^{-1} \mathbf{m}</em>{N}-\mathbf{m}<em>{N}^{\mathrm{T}} \mathbf{S}</em>{N}^{-1} \mathbf{w} +\mathbf{m}<em>{N}^{\mathrm{T}} \mathbf{S}</em>{N}^{-1} \mathbf{m}<em>{N}\right)\right) \Gamma\left(a</em>{N}\right)^{-1} b_{N}^{a_{N}} \beta^{a_{N}-1} \exp \left(-b_{N} \beta\right) \
&amp;= \left(\frac{\beta}{2 \pi}\right)^{M / 2}\left|\mathbf{S}<em>{N}\right|^{-1 / 2} \exp \left(-\frac{\beta}{2}\left(\mathbf{w}^{\mathrm{T}} \mathbf{S}</em>{0}^{-1} \mathbf{w}+\mathbf{w}^{\mathrm{T}} \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \mathbf{w}-\mathbf{w}^{\mathrm{T}} \mathbf{S}<em>{0}^{-1} \mathbf{m}</em>{0} - \mathbf{w}^{\mathrm{T}} \mathbf{\Phi}^{\mathrm{T}} \mathbf{t}-\mathbf{m}<em>{0}^{\mathrm{T}} \mathbf{S}</em>{N}^{-1} \mathbf{w}-\mathbf{t}^{\mathrm{T}} \mathbf{\Phi} \mathbf{w}+\mathbf{m}<em>{N}^{\mathrm{T}} \mathbf{S}</em>{N}^{-1} \mathbf{m}<em>{N}\right)\right) \ &amp;\quad \ \Gamma\left(a</em>{N}\right)^{-1} b_{N}^{a_{N}} \beta^{a_{0}+N / 2-1} \exp \left(-\left(b_{0}+\frac{1}{2}\left(\mathbf{m}<em>{0}^{\mathrm{T}} \mathbf{S}</em>{0}^{-1} \mathbf{m}<em>{0}-\mathbf{m}</em>{N}^{\mathrm{T}} \mathbf{S}<em>{N}^{-1} \mathbf{m}</em>{N}+\mathbf{t}^{\mathrm{T}} \mathbf{t}\right)\right) \beta\right) \
&amp;=\left(\frac{\beta}{2 \pi}\right)^{M / 2}\left|\mathbf{S}<em>{N}\right|^{-1 / 2} \exp \left(-\frac{\beta}{2}\left(\left(\mathbf{w}-\mathbf{m}</em>{0}\right)^{\mathrm{T}} \mathbf{S}<em>{0}^{-1}\left(\mathbf{w}-\mathbf{m}</em>{0}\right)+|\mathbf{t}-\Phi \mathbf{w}|^{2}\right)\right) \Gamma\left(a_{N}\right)^{-1} b_{N}^{a_{N}} \beta^{a_{0}+N / 2-1} \exp \left(-b_{0} \beta\right)
\end{aligned}
$$</p>
<p>一方で分子は
$$
\begin{aligned}
p(\mathsf{t} | \mathbf{w}, \beta) p(\mathbf{w}, \beta) &amp;= \left(\frac{\beta}{2 \pi}\right)^{N / 2} \exp \left(-\frac{\beta}{2}|\mathbf{t}-\Phi \mathbf{w}|^{2}\right) \left(\frac{\beta}{2 \pi}\right)^{M / 2}\left|\mathbf{S}<em>{0}\right|^{-1 / 2} \exp \left(-\frac{\beta}{2}\left(\mathbf{w}-\mathbf{m}</em>{0}\right)^{\mathrm{T}} \mathbf{S}<em>{0}^{-1}\left(\mathbf{w}-\mathbf{m}</em>{0}\right)\right) \
&amp;\quad\ \Gamma\left(a_{0}\right)^{-1} b_{0}^{a_{0}} \beta^{a_{0}-1} \exp \left(-b_{0} \beta\right)
\end{aligned}
$$</p>
<p>よってこれらを用いて約分すると</p>
<p>$$
\begin{aligned}
p(\mathsf{t}) &amp;= \frac{p(\mathsf{t} | \mathbf{w}, \beta) p(\mathbf{w}, \beta)}{p(\mathbf{w}, \beta | \mathsf{t})} \
&amp;= \frac{\left(\frac{\beta}{2 \pi}\right)^{N / 2}\left|\mathbf{S}<em>{0}\right|^{-1 / 2}\Gamma\left(a</em>{0}\right)^{-1} b_{0}^{a_{0}} \beta^{a_{0}-1}}{\left|\mathbf{S}<em>{N}\right|^{-1 / 2}\Gamma\left(a</em>{N}\right)^{-1} b_{N}^{a_{N}} \beta^{a_{0}+N / 2-1}} \
&amp;= \frac{1}{(2\pi)^{N/2}}\frac{\left|\mathbf{S}<em>{0}\right|^{-1 / 2}\Gamma\left(a</em>{0}\right)^{-1} b_{0}^{a_{0}}}{\left|\mathbf{S}<em>{N}\right|^{-1 / 2}\Gamma\left(a</em>{N}\right)^{-1} b_{N}^{a_{N}}} \
&amp;=\frac{1}{(2 \pi)^{N / 2}} \frac{b_{0}^{a_{0}}}{b_{N}^{a_{N}}} \frac{\Gamma\left(a_{N}\right)}{\Gamma\left(a_{0}\right)} \frac{\left|\mathbf{S}<em>{N}\right|^{1 / 2}}{\left|\mathbf{S}</em>{0}\right|^{1 / 2}}
\end{aligned}
$$</p>
<p>となり、$(3.118)$式が得られることが確認された。</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="prml第4章演習問題解答"><a class="header" href="#prml第4章演習問題解答">PRML第4章演習問題解答</a></h1>
<head>
<style>
  div.panel-primary {
	border: 1px solid #000;
    margin: 10px 5px;
    padding: 16px 10px 0px;
  }
</style>
</head>
<h2 id="演習-41"><a class="header" href="#演習-41">演習 4.1</a></h2>
<div class="panel-primary">
<p>データ${ \mathbf{x}_n}$の集合が与えられ，<strong>凸包</strong>(convex hull)とは以下の式で与えられるすべての点の集合であると定義することができる．</p>
<p>$$
\mathbf{x} = \sum_{n}\alpha_n \mathbf{x}_n \tag{4.156}
$$</p>
<p>ここで$\alpha_n \geq 0$であり，$\sum_n \alpha_n = 1$である．第2のデータ${ \mathbf{y}_n}$の集合とそれに対応する凸包を考える．もしすべてのデータ$\mathbf{x}_n$に対し$\hat{\mathbf{w}}^{\mathrm{T}}\mathbf{x}_n + w_0 &gt; 0$を満たし，すべてのデータ$\mathbf{y}_n$に対し$\hat{\mathbf{w}}^{\mathrm{T}}\mathbf{y}_n + w_0 &lt; 0$を満たすベクトル$\hat{\mathbf{w}}$とスカラー$w_0$が存在するなら，定義によりこれら2つのデータの集合は線形分離可能である．それらの凸包が重なる場合，2つのデータの集合は線形分離可能ではないことを示せ．また，逆に2つのデータの集合が線形分離可能な場合，それらの凸包が重ならないことを示せ．</p>
</div>
<p>※<a href="https://ja.wikipedia.org/wiki/%E5%87%B8%E5%8C%85">凸包 (Wikipedia)</a>とは与えられた集合を含む最小の凸集合である。例えば$X$がユークリッド平面内の有界な点集合のとき、その凸包は直観的には$X$をゴム膜で包んだときにゴム膜が作る図形として視認することができる。つまり「与えられたデータを包み込む最小の集合」なので、凸包が2つのデータ集合間で重なっている場合には、線形分離不可能であるイメージが直観的に理解できる。矛盾を示す際には、両方にまたがる交差領域を使う。</p>
<p>「<strong>それらの凸包が重なる場合，2つのデータの集合は線形分離可能ではないことを示せ</strong>」について</p>
<p>凸包が重なるというのは、2つのデータ集合${\mathbf{x}_n}$と${\mathbf{y}_m}$が存在し、それぞれについての凸包のintersect（交差）領域が存在することを意味する。そしてそのようなintersect上の点$\mathbf{z}$を</p>
<p>$$
\mathbf{z}=\sum_{n} \alpha_{n} \mathbf{x}<em>{n}=\sum</em>{m} \beta_{m} \mathbf{y}_{m}
$$</p>
<p>のように書くことができる。ここで$\beta_m$はすべての$m$について$\beta_m \geq 0$であり、$\sum_m \beta_m = 1$を満たす。</p>
<p>ここで、凸包が重なる場合であっても2つのデータ集合${\mathbf{x}_n}$と${\mathbf{y}_m}$が線形分離可能であると仮定する。このとき、2つのintersect上に存在する点$\mathbf{z}$について</p>
<p>$$
\begin{aligned}
\hat{\mathbf{w}}^{\mathrm T}\mathbf{z} + w_0 &amp;= \sum_n \alpha_n \hat{\mathbf{w}}^{\mathrm T}\mathbf{x}_n + w_0 \
&amp;= \sum_n \alpha_n \hat{\mathbf{w}}^{\mathrm T}\mathbf{x}<em>n + \underbrace{\left( \sum_n\alpha_n \right)}</em>{1}w_0 \
&amp;= \sum_n \alpha_n \left( \hat{\mathbf{w}}^{\mathrm T}\mathbf{x}_n + w_0 \right) &gt; 0 \quad (\because \hat{\mathbf{w}}^{\mathrm T}\mathbf{x}_n + w_0 &gt; 0)
\end{aligned}
$$</p>
<p>一方で</p>
<p>$$
\begin{aligned}
\hat{\mathbf{w}}^{\mathrm T}\mathbf{z} + w_0 &amp;= \sum_m \beta_m \hat{\mathbf{w}}^{\mathrm T}\mathbf{y}_m + w_0 \
&amp;= \sum_m \beta_m \hat{\mathbf{w}}^{\mathrm T}\mathbf{y}<em>m + \underbrace{\left( \sum_m \beta_m \right)}</em>{1}w_0 \
&amp;= \sum_m \beta_m \left( \hat{\mathbf{w}}^{\mathrm T}\mathbf{y}_m + w_0 \right) &lt; 0 \quad (\because \hat{\mathbf{w}}^{\mathrm T}\mathbf{y}_m + w_0 &lt; 0)
\end{aligned}
$$</p>
<p>となる。これは矛盾するので仮定は誤りであり、凸包が重なる場合には2つのデータ集合${\mathbf{x}_n}$と${\mathbf{y}_m}$は線形分離可能ではないことが示された。また、この対偶を取ることで「<strong>2つのデータの集合が線形分離可能な場合，それらの凸包が重ならない</strong>」ことも自動的に示される。</p>
<h2 id="演習-42"><a class="header" href="#演習-42">演習 4.2</a></h2>
<div class="panel-primary">
<p>二乗和誤差関数
$$
E_{D}(\widetilde{\mathbf{W}})=\frac{1}{2} \operatorname{Tr}\left{(\widetilde{\mathbf{X}} \widetilde{\mathbf{W}}-\mathbf{T})^{\mathrm{T}}(\widetilde{\mathbf{X}} \widetilde{\mathbf{W}}-\mathbf{T})\right} \tag{4.15}
$$
の最小化を考える．学習データ集合におけるすべての目的変数ベクトルが線形制約</p>
<p>$$
\mathbf{a}^{\mathrm{T}} \mathbf{t}_{n}+b=0 \tag{4.157}
$$</p>
<p>を満たすと仮定する．ここで，$\mathbf{t}_n$は$(4.15)$における行列$\mathbf{T}$の$n$番目の行に相当する．この制約の結果として，最小二乗解
$$
\mathbf{y}(\mathbf{x})=\widetilde{\mathbf{W}}^{\mathrm{T}} \widetilde{\mathbf{x}}=\mathbf{T}^{\mathrm{T}}\left(\tilde{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\widetilde{\mathbf{x}},\quad \text{where}\quad \tilde{\mathbf{X}}^{\dagger} = \left(\tilde{\mathbf{X}}^{\mathrm{T}} \tilde{\mathbf{X}}\right)^{-1} \tilde{\mathbf{X}}^{\mathrm{T}} \tag{4.17}
$$
によって与えられるモデルの予測$\mathbf{y}(\mathbf{x})$の要素もまたこの制約を満たす，つまり，以下の式を満たすことを示せ．</p>
<p>$$
\mathbf{a}^{\mathrm{T}} \mathbf{y}(\mathbf{x})+b=0 \tag{4.158}
$$</p>
<p>上記を示すため，パラメータ$w_0$がバイアスとしての役割を持つように，基底関数が$\phi_0(\mathbf{x})=0$であると仮定する．</p>
</div>
<p>まずそれぞれの行列の形を確認する。</p>
<p>$$
\mathbf{\widetilde{X}}=\begin{pmatrix}
1 &amp; \mathbf{x}<em>1^{\mathrm{T}} \
1 &amp; \mathbf{x}<em>2^{\mathrm{T}} \
\vdots &amp; \vdots \
1 &amp; \mathbf{x}<em>n^{\mathrm{T}} \
\end{pmatrix} = \begin{pmatrix}\mathbf{1} &amp; \mathbf{X}\end{pmatrix},\quad
\mathbf{\widetilde{W}}=\begin{pmatrix}
w</em>{10} &amp; w</em>{20} &amp; \cdots &amp; w</em>{K0} \
\mathbf{w}_1 &amp; \mathbf{w}_2 &amp; \cdots &amp; \mathbf{w}_K \
\end{pmatrix}= \begin{pmatrix}\mathbf{w}_0^{\mathrm{T}} \ \mathbf{W}\end{pmatrix}
$$
$\mathbf{\widetilde{X}}$は$N\times (D+1)$行列, $\mathbf{\widetilde{W}}$は$(D+1)\times K$行列である。$\mathbf{w}_0$はバイアスパラメータのベクトルである。また$\mathbf{1}$は$N$次元の全要素が$1$の列ベクトルを表しており、ダミー入力$x_0 = 1$を明示的に表示するために入れている。</p>
<p>$\mathbf{T}$は$N\times K$行列で
$$
\mathbf{T}=\begin{pmatrix}
\mathbf{t}_1^{\mathrm{T}} \
\mathbf{t}_2^{\mathrm{T}} \
\vdots \
\mathbf{t}_n^{\mathrm{T}}
\end{pmatrix}
$$
と書ける。</p>
<p>これらからバイアスパラメータとダミー入力を分離して$(4.15)$の誤差二乗和関数$E_D(\widetilde{\mathbf{W}})$を表すと</p>
<p>$$
\begin{aligned}
E_{D}(\widetilde{\mathbf{W}})&amp;=\frac{1}{2} \operatorname{Tr}\left{(\widetilde{\mathbf{X}} \widetilde{\mathbf{W}}-\mathbf{T})^{\mathrm{T}}(\widetilde{\mathbf{X}} \widetilde{\mathbf{W}}-\mathbf{T})\right} \
&amp;=\frac{1}{2} \operatorname{Tr}\left{
(\mathbf{XW}+\mathbf{1}\mathbf{w}_0^{\mathrm{T}}-\mathbf{T})^{\mathrm{T}}(\mathbf{XW}+\mathbf{1}\mathbf{w}_0^{\mathrm{T}}-\mathbf{T})
\right}
\end{aligned}
$$</p>
<p>となる。</p>
<p>$E_D(\widetilde{\mathbf{W}})$の最小化のためにまず$\mathbf{w}_0$について微分すると</p>
<p>$$
\begin{aligned}
\frac{\partial E_D}{\partial \mathbf{w}_0} &amp;= \frac{1}{2}\cdot 2\left( \mathbf{XW}+\mathbf{1}\mathbf{w}_0^{\mathrm{T}}-\mathbf{T} \right)^{\mathrm{T}}\mathbf{1} \
&amp;=\left( \mathbf{XW} - \mathbf{T} \right)^{\mathrm{T}}\mathbf{1} + \mathbf{w}_0 \mathbf{1}^{\mathrm{T}}\mathbf{1} \
&amp;=\left( \mathbf{XW} - \mathbf{T} \right)^{\mathrm{T}}\mathbf{1} + N\mathbf{w}_0
\end{aligned}
$$
これが$\mathbf{0}$となれば良いので
$$
\begin{aligned}
\mathbf{w}_0 &amp;= \frac{1}{N}\left( \mathbf{T} - \mathbf{XW} \right)^{\mathrm{T}}\mathbf{1} \
&amp;= \frac{1}{N}\mathbf{T}^{\mathrm{T}}\mathbf{1} - \frac{1}{N}\mathbf{W}^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{1} \
&amp;:= \overline{\mathbf{t}}-\mathbf{W}^{\mathrm{T}}\overline{\mathbf{x}}
\end{aligned}
$$
ここで、以降の略記のために$\displaystyle \overline{\mathbf{t}} = \frac{1}{N}\mathbf{T}^{\mathrm{T}}\mathbf{1},\quad \overline{\mathbf{x}} = \frac{1}{N}\mathbf{X}^{\mathrm{T}}\mathbf{1}$とした。</p>
<p>$E_D(\widetilde{\mathbf{W}})$の$\mathbf{w}<em>0$をこれで書き換えると
$$
\begin{aligned}
E</em>{D}(\mathbf{W}) &amp;= \frac{1}{2} \operatorname{Tr}\left{(\mathbf{X} \mathbf{W}+\overline{\mathbf{T}}-\overline{\mathbf{X}} \mathbf{W}-\mathbf{T})^{\mathrm{T}}(\mathbf{X} \mathbf{W}+\overline{\mathbf{T}}-\overline{\mathbf{X}} \mathbf{W}-\mathbf{T})\right} \
&amp;= \frac{1}{2} \operatorname{Tr}\left{\left((\mathbf{X} - \overline{\mathbf{X}})\mathbf{W} - (\mathbf{T} - \overline{\mathbf{T}})\right)^{\mathrm{T}}\left((\mathbf{X} - \overline{\mathbf{X}})\mathbf{W} - (\mathbf{T} - \overline{\mathbf{T}})\right)\right}
\end{aligned}
$$
ここで略記のために$\displaystyle \overline{\mathbf{T}}=\mathbf{1} \overline{\mathbf{t}}^{\mathrm{T}}, \quad \overline{\mathbf{X}}=\mathbf{1} \overline{\mathbf{x}}^{\mathrm{T}}$とした。</p>
<p>これについても$\mathbf{W}$について微分して$\mathbf{0}$をとると
$$
\frac{\partial E_{D}}{\partial \mathbf{W}} = \frac{1}{2}\cdot 2 (\mathbf{X} - \overline{\mathbf{X}})^{\mathrm{T}}\left((\mathbf{X} - \overline{\mathbf{X}})\mathbf{W} - (\mathbf{T} - \overline{\mathbf{T}})\right) = \mathbf{0}
$$
$\widehat{\mathbf{X}}=\mathbf{X}-\overline{\mathbf{X}}, \widehat{\mathbf{T}}=\mathbf{T}-\overline{\mathbf{T}}$と書き直すと
$$
\mathbf{W}=\left(\widehat{\mathbf{X}}^{\mathrm{T}} \widehat{\mathbf{X}}\right)^{-1} \widehat{\mathbf{X}}^{\mathrm{T}} \widehat{\mathbf{T}}=\widehat{\mathbf{X}}^{\dagger} \widehat{\mathbf{T}}
$$
となる。</p>
<p>以上から、新しい入力$\mathbf{x}^{<em>}$が得られたときの予測値$\mathbf{y}(\mathbf{x}^{</em>})$は</p>
<p>$$
\begin{aligned}
\mathbf{y}\left(\mathbf{x}^{<em>}\right) &amp;= \widetilde{\mathbf{W}}^{\mathrm{T}} \widetilde{\mathbf{x}^{</em>}}\
&amp;= \begin{pmatrix}\mathbf{w}<em>0 &amp; \mathbf{W}^{\mathrm{T}}\end{pmatrix} \begin{pmatrix}1 \ \mathbf{x}^{*}\end{pmatrix} \
&amp;=\mathbf{W}^{\mathrm{T}} \mathbf{x}^{\star}+\mathbf{w}</em>{0} \
&amp;=\mathbf{W}^{\mathrm{T}} \mathbf{x}^{\star}+\overline{\mathbf{t}}-\mathbf{W}^{\mathrm{T}} \overline{\mathbf{x}} \
&amp;=\overline{\mathbf{t}}+\widehat{\mathbf{T}}^{\mathrm{T}}\left(\widehat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right)
\end{aligned}
$$
となる。</p>
<p>一方、$(4.157)$式の線形制約条件は$\overline{\mathbf{t}}$について適用すると、以下のように書き直せる
$$
\begin{aligned}
\mathbf{a}^{\mathrm{T}}\overline{\mathbf{t}} + b &amp;= \frac{1}{N}(\mathbf{a}^{\mathrm{T}}\mathbf{T}^{\mathrm{T}}\mathbf{1} + Nb) \
&amp;= \frac{1}{N}\sum_{n=1}^N(\mathbf{a}^{\mathrm{T}}\mathbf{t}_n + b) \
&amp;= 0
\end{aligned}
$$
すなわち$\mathbf{a}^{\mathrm{T}}\overline{\mathbf{t}} = -b$が得られる。また、$\mathbf{a}^{\mathrm{T}}\mathbf{t}_n + b = 0$を$\mathbf{T}^{\mathrm{T}}$について適用すると
$$
\mathbf{a}^{\mathrm{T}}\mathbf{T}^{\mathrm{T}} = \begin{pmatrix} -b &amp; -b &amp; \cdots &amp; -b \end{pmatrix} = -b\mathbf{1}^{\mathrm{T}}
$$
となる。</p>
<p>以上を用いて、$(4.157)$式の線形制約が成立しているとき、モデルの予測$\mathbf{y}(\mathbf{x}^{*})$について$(4.158)$の左辺を計算してみると
$$
\begin{aligned}
\mathbf{a}^{\mathrm{T}} \mathbf{y}\left(\mathbf{x}^{\star}\right) + b &amp;=\mathbf{a}^{\mathrm{T}} \overline{\mathbf{t}}+\mathbf{a}^{\mathrm{T}} \widehat{\mathbf{T}}^{\mathrm{T}}\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) + b \
&amp;= \mathbf{a}^{\mathrm{T}} \widehat{\mathbf{T}}^{\mathrm{T}}\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \
&amp;= \mathbf{a}^{\mathrm{T}} (\mathbf{T} - \overline{\mathbf{T}})^{\mathrm{T}}\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \
&amp;= (\mathbf{a}^{\mathrm{T}}\mathbf{T}^{\mathrm{T}} - \mathbf{a}^{\mathrm{T}}\overline{\mathbf{T}}^{\mathrm{T}})\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \
&amp;= (\mathbf{a}^{\mathrm{T}}\mathbf{T}^{\mathrm{T}} - \mathbf{a}^{\mathrm{T}}\overline{\mathbf{t}}\mathbf{1}^{\mathrm{T}})\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \
&amp;= (-b\mathbf{1}^{\mathrm{T}} + b\mathbf{1}^{\mathrm{T}})\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \
&amp;= \mathbf{0}^{\mathrm{T}} \left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \
&amp;= 0
\end{aligned}
$$
よって、$(4.158)$式の$\mathbf{a}^{\mathrm{T}} \mathbf{y}\left(\mathbf{x}^{\star}\right) + b = 0$が示された。</p>
<blockquote>
<p>目標値の分布が超平面に載るという拘束条件下で誤差関数を最小二乗和にしてクラス分類すると、その予測値の分布も超平面上に載るということである。</p>
</blockquote>
<h2 id="演習-43"><a class="header" href="#演習-43">演習 4.3</a></h2>
<div class="panel-primary">
<p>演習問題4.2の結果を拡張し，多次元線形制約が目的変数ベクトルによって同時に満たされる場合，同じ制約が線形モデルの最小二乗予測によっても満たされることを示せ．</p>
</div>
<p>演習問題4.2の仮定を拡張する。
「多次元線形制約が目的変数ベクトルによって同時に満たされる場合」というのは、$(4.157)$式の線形制約が</p>
<p>$$
\mathbf{A}\mathbf{t}_n+\mathbf{b} = \mathbf{0}
$$</p>
<p>となることを表す。ここで、$\mathbf{A}$は行列で$\mathbf{b}$はベクトルであり、$\mathbf{A}$と$\mathbf{b}$の各行が1つの線形制約を表すようなものである。つまり$l$を多次元線形制約の次元数として</p>
<p>$$
\mathbf{A} = \begin{pmatrix}
\mathbf{a}_1^{\mathrm T} \
\mathbf{a}_2^{\mathrm T} \
\vdots \
\mathbf{a}_l^{\mathrm T}
\end{pmatrix}, \quad
\mathbf{b} = \begin{pmatrix}
b_1 \
b_2 \
\vdots \
b_l
\end{pmatrix}
$$</p>
<p>となる。</p>
<p>題意「同じ制約が線形モデルの最小二乗予測によっても満たされることを示せ」は、$\mathbf{A}\mathbf{t}_n+\mathbf{b} = \mathbf{0}$が成り立つ中で$\mathbf{A}\mathbf{y}(\mathbf{x}^{\star})+\mathbf{b}$が$\mathbf{0}$となることを示すことなので、これを演習問題4.2の最後の式に当てはめると</p>
<p>$$
\begin{aligned}
\mathbf{A}^{\mathrm{T}} \mathbf{y}\left(\mathbf{x}^{\star}\right) + \mathbf{b} &amp;=\mathbf{A}^{\mathrm{T}} \overline{\mathbf{t}}+\mathbf{A}^{\mathrm{T}} \widehat{\mathbf{T}}^{\mathrm{T}}\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) + \mathbf{b} \
&amp;= \mathbf{A}^{\mathrm{T}} \widehat{\mathbf{T}}^{\mathrm{T}}\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \
&amp;= \mathbf{A}^{\mathrm{T}} (\mathbf{T} - \overline{\mathbf{T}})^{\mathrm{T}}\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \
&amp;= (\mathbf{A}^{\mathrm{T}}\mathbf{T}^{\mathrm{T}} - \mathbf{A}^{\mathrm{T}}\overline{\mathbf{T}}^{\mathrm{T}})\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \
&amp;= (\mathbf{A}^{\mathrm{T}}\mathbf{T}^{\mathrm{T}} - \mathbf{A}^{\mathrm{T}}\overline{\mathbf{t}}\mathbf{1}^{\mathrm{T}})\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \
&amp;= (-\mathbf{b}\mathbf{1}^{\mathrm{T}} + \mathbf{b}\mathbf{1}^{\mathrm{T}})\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \
&amp;= \mathbf{0}^{\mathrm{T}} \left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \
&amp;= \mathbf{0}
\end{aligned}
$$</p>
<p>よって$\mathbf{A}\mathbf{y}(\mathbf{x}^{\star})+\mathbf{b} = \mathbf{0}$となるので、題意が示された。</p>
<h2 id="演習-44"><a class="header" href="#演習-44">演習 4.4</a></h2>
<div class="panel-primary">
<p>制約$\mathbf{w}^{\mathrm{T}}\mathbf{w}=1$を満たすようにラグランジュ乗数を利用し，
$$
m_{2}-m_{1}=\mathbf{w}^{\mathrm{T}}\left(\mathbf{m}<em>{2}-\mathbf{m}</em>{1}\right) \tag{4.22}
$$
によって与えられるクラス分離規準を$\mathbf{w}$に関して最大化すれば$\mathbf{w} \propto (\mathbf{m}_2-\mathbf{m}_1)$となることを示せ．</p>
</div>
<p>$\mathbf{w}^\mathrm{T}\mathbf{w} =1$の制約の下でラグランジュの未定乗数法を用いると</p>
<p>$$
\begin{aligned}
L &amp;= \mathbf{w}^\mathrm{T}( \mathbf{m}_2-\mathbf{m}_1)+\lambda(\mathbf{w}^\mathrm{T}\mathbf{w}-1) \
\frac{\partial L}{\partial \mathbf{w}}&amp;= \mathbf{m}_2-\mathbf{m}_1+2\lambda \mathbf{w}
\end{aligned}
$$
これが$\mathbf{0}$になる点を考えると</p>
<p>$$
\mathbf{w} = -\frac{1}{2\lambda}(\mathbf{m}_2-\mathbf{m}_1)
$$
となり$\mathbf{w}\propto(\mathbf{m}_2-\mathbf{m}_1)$であることが示せた。</p>
<h2 id="演習-45"><a class="header" href="#演習-45">演習 4.5</a></h2>
<div class="panel-primary">
<p>$$
y = \mathbf{w}^{\mathrm T}\mathbf{x} \tag{4.20}
$$
$$
m_k = \mathbf{w}^{\mathrm T}\mathbf{m}<em>k \tag{4.23}
$$
と
$$
s</em>{k}^{2}=\sum_{n \in \mathcal{C}<em>{k}}\left(y</em>{n}-m_{k}\right)^{2} \tag{4.24}
$$
を使って，フィッシャーの判別規準
$$
J(\mathbf{w})=\frac{\left(m_{2}-m_{1}\right)^{2}}{s_{1}^{2}+s_{2}^{2}} \tag{4.25}
$$
が
$$
J(\mathbf{w})=\frac{\mathbf{w}^{\mathrm{T}} \mathbf{S}<em>{\mathrm{B}} \mathbf{w}}{\mathbf{w}^{\mathrm{T}} \mathbf{S}</em>{\mathrm{W}} \mathbf{w}} \tag{4.26}
$$
の形で書けることを示せ．</p>
</div>
<p>※$\mathbf{S}<em>{\mathrm{B}}$と$\mathbf{S}</em>{\mathrm{W}}$の定義はそれぞれ$(4.27)$と$(4.28)$で与えられる。</p>
<p>$(4.27)$と$(4.28)$より
$$
\begin{aligned}
\mathbf{S}<em>{\mathrm{B}}&amp;=\left(\mathbf{m}</em>{2}-\mathbf{m}<em>{1}\right)\left(\mathbf{m}</em>{2}-\mathbf{m}<em>{1}\right)^{\mathrm{T}} \
\mathbf{S}</em>{\mathrm{W}}&amp;=\sum_{n \in \mathcal{C}<em>{1}}\left(\mathbf{x}</em>{n}-\mathbf{m}<em>{1}\right)\left(\mathbf{x}</em>{n}-\mathbf{m}<em>{1}\right)^{\mathrm{T}}+\sum</em>{n \in \mathcal{C}<em>{2}}\left(\mathbf{x}</em>{n}-\mathbf{m}<em>{2}\right)\left(\mathbf{x}</em>{n}-\mathbf{m}_{2}\right)^{\mathrm{T}}
\end{aligned}
$$
である。これを用いて$(4.25)$を変形していくと</p>
<p>$$
\begin{aligned}
J(\mathbf{w}) &amp;=\frac{\left(m_{2}-m_{1}\right)^{2}}{s_{1}^{2}+s_{2}^{2}} \
&amp;=\frac{\left(\mathbf{w}^{\mathrm{T}}\left(\mathbf{m}<em>{2}-\mathbf{m}</em>{1}\right)\right)^{2}}{\sum_{n \in \mathcal{C}<em>{1}}\left(\mathbf{w}^{\mathrm{T}}\left(\mathbf{x}</em>{n}-\mathbf{m}<em>{1}\right)\right)^{2}+\sum</em>{n \in \mathcal{C}<em>{2}}\left(\mathbf{w}^{\mathrm{T}}\left(\mathbf{x}</em>{n}-\mathbf{m}<em>{2}\right)\right)^{2}} \
&amp;=\frac{\left(\mathbf{w}^{\mathrm{T}}\left(\mathbf{m}</em>{2}-\mathbf{m}<em>{1}\right)\right)\left(\left(\mathbf{m}</em>{2}-\mathbf{m}<em>{1}\right)^{\mathrm{T}} \mathbf{w}\right)}{\sum</em>{n \in \mathcal{C}<em>{1}}\left(\mathbf{w}^{\mathrm{T}}\left(\mathbf{x}</em>{n}-\mathbf{m}<em>{1}\right)\left(\mathbf{x}</em>{n}-\mathbf{m}<em>{1}\right)^{\mathrm{T}} \mathbf{w}\right)+\sum</em>{n \in \mathcal{C}<em>{2}}\left(\mathbf{w}^{\mathrm{T}}\left(\mathbf{x}</em>{n}-\mathbf{m}<em>{2}\right)\left(\mathbf{x}</em>{n}-\mathbf{m}<em>{2}\right)^{\mathrm{T}} \mathbf{w}\right)} \
&amp;=\frac{\mathbf{w}^{\mathrm{T}} \mathbf{S}</em>{\mathrm B} \mathbf{w}}{\mathbf{w}^{\mathrm{T}}\left{\sum_{n \in \mathcal{C}<em>{1}}\left(\mathbf{x}</em>{n}-\mathbf{m}<em>{1}\right)\left(\mathbf{x}</em>{n}-\mathbf{m}<em>{1}\right)^{\mathrm{T}}+\sum</em>{n \in \mathcal{C}<em>{2}}\left(\mathbf{x}</em>{n}-\mathbf{m}<em>{2}\right)\left(\mathbf{x}</em>{n}-\mathbf{m}<em>{2}\right)^{\mathrm{T}}\right} \mathbf{w}} \
&amp;=\frac{\mathbf{w}^{\mathrm{T}} \mathbf{S}</em>{\mathrm B} \mathbf{w}}{\mathbf{w}^{\mathrm{T}} \mathbf{S}_{\mathrm W} \mathbf{w}}
\end{aligned}
$$</p>
<p>となり$(4.26)$式が得られる。</p>
<h2 id="演習-46"><a class="header" href="#演習-46">演習 4.6</a></h2>
<div class="panel-primary">
<p>$$
\mathbf{S}<em>{\mathbf{B}}=\left(\mathbf{m}</em>{2}-\mathbf{m}<em>{1}\right)\left(\mathbf{m}</em>{2}-\mathbf{m}<em>{1}\right)^{\mathrm{T}} \tag{4.27}
$$
で与えられるクラス間共分散行列と
$$
\mathbf{S}</em>{\mathbf{W}}=\sum_{n \in \mathcal{C}<em>{1}}\left(\mathbf{x}</em>{n}-\mathbf{m}<em>{1}\right)\left(\mathbf{x}</em>{n}-\mathbf{m}<em>{1}\right)^{\mathrm{T}}+\sum</em>{n \in \mathcal{C}<em>{2}}\left(\mathbf{x}</em>{n}-\mathbf{m}<em>{2}\right)\left(\mathbf{x}</em>{n}-\mathbf{m}<em>{2}\right)^{\mathrm{T}} \tag{4.28}
$$
で与えられるクラス内共分散行列のそれぞれの定義と，
$$
w_0 = -\mathbf{w}^{\mathrm T}\mathbf{m} \tag{4.34}
$$
$$
\mathbf{m}=\frac{1}{N} \sum</em>{n=1}^{N} \mathbf{x}<em>{n}=\frac{1}{N}\left(N</em>{1} \mathbf{m}<em>{1}+N</em>{2} \mathbf{m}<em>{2}\right) \tag{4.36}
$$
および4.1.5節で述べた目的値を使って，二乗和誤差関数を最小化する表現
$$
\sum</em>{n=1}^{N}\left(\mathbf{w}^{\mathrm{T}} \mathbf{x}<em>{n}+w</em>{0}-t_{n}\right) \mathbf{x}<em>{n}=0 \tag{4.33}
$$
が
$$
\left(\mathbf{S}</em>{\mathbf{W}}+\frac{N_{1} N_{2}}{N} \mathbf{S}<em>{\mathbf{B}}\right) \mathbf{w}=N\left(\mathbf{m}</em>{1}-\mathbf{m}_{2}\right) \tag{4.37}
$$
の形で書けることを示せ．</p>
</div>
<p>※ 簡単な代数演算をして……と書かれてあるが、計算量的には簡単じゃない。まず$\displaystyle \sum_{n=1}^N t_n\mathbf{x}_n = N(\mathbf{m}_1-\mathbf{m}_2)$となることを利用して、残りの部分で$(\cdot)\mathbf{w}$の式に書き直せるようにスカラー部分とベクトル部分を入れ替える技（($\mathbf{a}^{\mathrm{T}}\mathbf{b})\mathbf{c} = \mathbf{c}\mathbf{b}^{\mathrm{T}}\mathbf{a}$）を使う。</p>
<p>まず$(4.33)$の$\displaystyle \sum_{n=1}^N t_n\mathbf{x}<em>n$について変形すると
$$
\begin{aligned}
\sum</em>{n=1}^N t_n\mathbf{x}<em>n &amp;= \frac{N}{N_1}\sum</em>{n \in \mathcal{C}<em>1}\mathbf{x}<em>n - \frac{N}{N_2}\sum</em>{n \in \mathcal{C}<em>2}\mathbf{x}<em>n \
&amp;=\frac{N}{N_1}(N_1\mathbf{m}<em>1)-\frac{N}{N_2}(N_2\mathbf{m}<em>2) \
&amp;=N(\mathbf{m}<em>1 - \mathbf{m}<em>2)
\end{aligned}
$$
となる。これは$(4.37)$式の右辺になっているので、
$$
\sum</em>{n=1}^{N}\left(\mathbf{w}^{\mathrm{T}} \mathbf{x}</em>{n}+w</em>{0}\right)\mathbf{x}</em>{n} = \left(\mathbf{S}</em>{\mathrm{W}}+\frac{N</em>{1} N</em>{2}}{N} \mathbf{S}<em>{\mathrm{B}}\right) \mathbf{w}
$$
となることを示せれば良い。
$$
\begin{aligned}
\sum</em>{n=1}^{N}\left(\mathbf{w}^{\mathrm{T}} \mathbf{x}<em>{n}+w</em>{0}\right) \mathbf{x}<em>{n}&amp;=\sum</em>{n=1}^{N}\left(\mathbf{w}^{\mathrm{T}} \mathbf{x}<em>{n}-\mathbf{w}^{\mathrm{T}} \mathbf{m}\right) \mathbf{x}</em>{n} \
&amp;=\sum_{n=1}^{N}\left{\left(\mathbf{x}<em>{n} \mathbf{x}</em>{n}^{\mathrm{T}}-\mathbf{x}<em>{n} \mathbf{m}^{\mathrm{T}}\right) \mathbf{w}\right} \
&amp;= \sum</em>{n \in \mathcal{C}<em>{1}}\left{\left(\mathbf{x}</em>{n} \mathbf{x}<em>{n}^{\mathrm{T}}-\mathbf{x}</em>{n} \mathbf{m}^{\mathrm{T}}\right) \mathbf{w}\right} + \sum_{m \in \mathcal{C}<em>{2}}\left{\left(\mathbf{x}</em>{m} \mathbf{x}<em>{m}^{\mathrm{T}}-\mathbf{x}</em>{m} \mathbf{m}^{\mathrm{T}}\right) \mathbf{w}\right} \
&amp;=\left(\sum_{n \in \mathcal{C}<em>{1}} \mathbf{x}</em>{n} \mathbf{x}<em>{n}^{\mathrm{T}}-N</em>{1} \mathbf{m}<em>{1} \mathbf{m}^{\mathrm{T}}\right) \mathbf{w} + \left(\sum</em>{m \in \mathcal{C}<em>{2}} \mathbf{x}</em>{m} \mathbf{x}<em>{m}^{\mathrm{T}}-N</em>{2} \mathbf{m}<em>{2} \mathbf{m}^{\mathrm{T}}\right) \mathbf{w} \
&amp;=\left(\sum</em>{n \in \mathcal{C}<em>{1}} \mathbf{x}</em>{n} \mathbf{x}<em>{n}^{\mathrm{T}}+\sum</em>{m \in \mathcal{C}<em>{2}} \mathbf{x}</em>{m} \mathbf{x}<em>{m}^{\mathrm{T}}-\left(N</em>{1} \mathbf{m}<em>{1}+N</em>{2} \mathbf{m}_{2}\right) \mathbf{m}^{\mathrm{T}}\right) \mathbf{w}
\end{aligned}
$$
となる。</p>
<p>ここで、$(4.27)$と$(4.28)$の展開した形を用意しておく。$(4.27)$は</p>
<p>$$
\begin{aligned}
\mathbf{S}<em>{\mathrm{B}} &amp;= \left(\mathbf{m}</em>{2}-\mathbf{m}<em>{1}\right)\left(\mathbf{m}</em>{2}-\mathbf{m}_{1}\right)^{\mathrm{T}} \
&amp;=\mathbf{m}_2\mathbf{m}_2^{\mathrm{T}}-\mathbf{m}_2\mathbf{m}_1^{\mathrm{T}}-\mathbf{m}_1\mathbf{m}_2^{\mathrm{T}}+\mathbf{m}_1\mathbf{m}_1^{\mathrm{T}}
\end{aligned}
$$</p>
<p>となり、$(4.28)$は</p>
<p>\begin{aligned}
\mathbf{S}<em>{\mathrm W} = \sum</em>{k=1}^2 \sum_{i \in \mathcal{C}<em>{k}}\left(\mathbf{x}</em>{i}-\mathbf{m}<em>{k}\right)\left(\mathbf{x}</em>{i}-\mathbf{m}<em>{k}\right)^{\mathrm{T}} &amp;= \sum</em>{k=1}^2 \sum_{i \in \mathcal{C}<em>{k}}\left(\mathbf{x}</em>{i} \mathbf{x}<em>{i}^{\mathrm{T}}-\mathbf{x}</em>{i} \mathbf{m}<em>{k}^{\mathrm{T}}-\mathbf{m}</em>{k} \mathbf{x}<em>{i}^{\mathrm{T}}+\mathbf{m}</em>{k} \mathbf{m}<em>{k}^{\mathrm{T}}\right) \
&amp;=\sum</em>{k=1}^2  \left( \sum_{i \in \mathcal{C}<em>{k}} \mathbf{x}</em>{i} \mathbf{x}<em>{i}^{\mathrm{T}}-N</em>{k} \mathbf{m}<em>{k} \mathbf{m}</em>{k}^{\mathrm{T}}\right) \
&amp;=\sum_{n \in \mathcal{C}<em>{1}} \mathbf{x}</em>{n} \mathbf{x}<em>{n}^{\mathrm{T}} + \sum</em>{m \in \mathcal{C}<em>{2}} \mathbf{x}</em>{m} \mathbf{x}<em>{m}^{\mathrm{T}} - N_1\mathbf{m}</em>{1} \mathbf{m}<em>{1}^{\mathrm{T}} - N_2\mathbf{m}</em>{2} \mathbf{m}_{2}^{\mathrm{T}}
\end{aligned}</p>
<p>であるから、これらを利用して書き直すと
$$
\begin{aligned}
&amp;\quad \left(\sum_{n \in \mathcal{C}<em>{1}} \mathbf{x}</em>{n} \mathbf{x}<em>{n}^{\mathrm{T}}+\sum</em>{m \in \mathcal{C}<em>{2}} \mathbf{x}</em>{m} \mathbf{x}<em>{m}^{\mathrm{T}}-\left(N</em>{1} \mathbf{m}<em>{1}+N</em>{2} \mathbf{m}<em>{2}\right) \mathbf{m}^{\mathrm{T}}\right) \mathbf{w} \
&amp;= \left(\mathbf{S}</em>{\mathbf{w}}+N_{1} \mathbf{m}<em>{1} \mathbf{m}</em>{1}^{\mathrm{T}}+N_{2} \mathbf{m}<em>{2} \mathbf{m}</em>{2}^{\mathrm{T}}-\left(N_{1} \mathbf{m}<em>{1}+N</em>{2} \mathbf{m}<em>{2}\right) \frac{1}{N}\left(N</em>{1} \mathbf{m}<em>{1}+N</em>{2} \mathbf{m}<em>{2}\right)^{\mathrm{T}}\right) \mathbf{w} \
&amp;= \left(\mathbf{S}</em>{\mathbf{w}}+\left(N_{1}-\frac{N_{1}^{2}}{N}\right) \mathbf{m}<em>{1} \mathbf{m}</em>{1}^{\mathrm{T}}-\frac{N_{1} N_{2}}{N}\left(\mathbf{m}<em>{1} \mathbf{m}</em>{2}^{\mathrm{T}}+\mathbf{m}<em>{2} \mathbf{m}</em>{1}^{\mathrm{T}}\right) +\left(N_{2}-\frac{N_{2}^{2}}{N}\right) \mathbf{m}<em>{2} \mathbf{m}</em>{2}^{\mathrm{T}}\right) \mathbf{w} \
&amp;= \left(\mathbf{S}<em>{\mathbf{w}}+\frac{\left(N</em>{1}+N_{2}\right) N_{1}-N_{1}^{2}}{N} \mathbf{m}<em>{1} \mathbf{m}</em>{1}^{\mathrm{T}}-\frac{N_{1} N_{2}}{N}\left(\mathbf{m}<em>{1} \mathbf{m}</em>{2}^{\mathrm{T}}+\mathbf{m}<em>{2} \mathbf{m}</em>{1}^{\mathrm{T}}\right) +\frac{\left(N_{1}+N_{2}\right) N_{2}-N_{2}^{2}}{N} \mathbf{m}<em>{2} \mathbf{m}</em>{2}^{\mathrm{T}}\right) \mathbf{w} \
&amp;= \left(\mathbf{S}<em>{\mathbf{w}}+\frac{N</em>{1} N_{2}}{N}\left(\mathbf{m}<em>{1} \mathbf{m}</em>{1}^{\mathrm{T}}-\mathbf{m}<em>{1} \mathbf{m}</em>{2}^{\mathrm{T}}-\mathbf{m}<em>{2} \mathbf{m}</em>{1}^{\mathrm{T}}+\mathbf{m}<em>{2} \mathbf{m}</em>{2}^{\mathrm{T}}\right)\right) \mathbf{w} \
&amp;= \left(\mathbf{S}<em>{\mathbf{w}}+\frac{N</em>{1} N_{2}}{N} \mathbf{S}<em>{\mathrm{B}}\right) \mathbf{w}
\end{aligned}
$$
となる。
以上から、$(4.33)$式が
$$
\left(\mathbf{S}</em>{\mathbf{W}}+\frac{N_{1} N_{2}}{N} \mathbf{S}<em>{\mathbf{B}}\right) \mathbf{w}=N\left(\mathbf{m}</em>{1}-\mathbf{m}_{2}\right) \tag{4.37}
$$
と書き直せることが示された。</p>
<h2 id="演習-47"><a class="header" href="#演習-47">演習 4.7</a></h2>
<div class="panel-primary">
<p>ロジスティックシグモイド関数
$$
\sigma(a) = \frac{1}{1+ \exp(-a)} \tag{4.59}
$$
が$\sigma(-a) = 1-\sigma(a)$を満たすことを示せ．また，その逆関数が$\sigma^{-1}(y) = \ln {y/(1-y)}$で与えられることを示せ．</p>
</div>
<p>$$\begin{aligned} \sigma(-a) &amp;= \frac{1}{1+\exp(a)} \&amp;= \frac{1}{1+\left{ 1/\exp(-a) \right}} \&amp;= \frac{\exp(-a)}{\exp(-a)+1} \&amp;= 1-\frac{1}{1+\exp(-a)} \&amp;= 1-\sigma(a) \end{aligned}$$</p>
<p>逆関数について$y=\sigma(a)$としたとき</p>
<p>$$y=\frac{1}{1+\exp(-a)}$$
$$\begin{aligned} \exp(-a)&amp;=\frac{1}{y}-1 \&amp;= \frac{1-y}{y} \end{aligned}$$
$$a=\ln(\frac{y}{1-y})$$</p>
<p>よって</p>
<p>$$\sigma^{-1}(y)=\ln(\frac{y}{1-y})$$</p>
<h2 id="演習-48"><a class="header" href="#演習-48">演習 4.8</a></h2>
<div class="panel-primary">
<p>$$
\begin{aligned}
p\left(\mathcal{C}<em>{1} \mid \mathbf{x}\right) &amp;=\frac{p\left(\mathbf{x} \mid \mathcal{C}</em>{1}\right) p\left(\mathcal{C}<em>{1}\right)}{p\left(\mathbf{x} \mid \mathcal{C}</em>{1}\right) p\left(\mathcal{C}<em>{1}\right)+p\left(\mathbf{x} \mid \mathcal{C}</em>{2}\right) p\left(\mathcal{C}<em>{2}\right)} \
&amp;=\frac{1}{1+\exp (-a)}=\sigma(a)
\end{aligned} \tag{4.57}
$$
と
$$
a=\ln \frac{p\left(\mathbf{x} \mid \mathcal{C}</em>{1}\right) p\left(\mathcal{C}<em>{1}\right)}{p\left(\mathbf{x} \mid \mathcal{C}</em>{2}\right) p\left(\mathcal{C}<em>{2}\right)} \tag{4.58}
$$
を使って，ガウス確率密度分布を用いた2クラス生成モデルにおけるクラスの事後確率に対する
$$
p\left(\mathcal{C}</em>{1} \mid \mathbf{x}\right)=\sigma\left(\mathbf{w}^{\mathrm{T}} \mathbf{x}+w_{0}\right) \tag{4.65}
$$
の結果を導出せよ．また，パラメータ$\mathbf{w}$と$w_0$に対する結果
$$
\mathbf{w} =\mathbf{\Sigma}^{-1}\left(\boldsymbol{\mu}<em>{1}-\boldsymbol{\mu}</em>{2}\right) \tag{4.66}
$$
と
$$
w_{0} =-\frac{1}{2} \boldsymbol{\mu}<em>{1}^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}</em>{1}+\frac{1}{2} \boldsymbol{\mu}<em>{2}^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}</em>{2}+\ln \frac{p\left(\mathcal{C}<em>{1}\right)}{p\left(\mathcal{C}</em>{2}\right)} \tag{4.67}
$$
を検証せよ．</p>
</div>
<p>※
$(4.65)$式を$(4.57)$, $(4.58)$, それから</p>
<p>$$
p\left(\mathbf{x} \mid \mathcal{C}<em>{k}\right)=\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\mathbf{\Sigma}|^{1 / 2}} \exp \left{-\frac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}</em>{k}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\mathbf{x}-\boldsymbol{\mu}<em>{k}\right)\right} \tag{4.64}
$$
を利用して変形していく。
$$
\begin{aligned}
p(\mathcal{C}<em>1\mid \mathbf{x}) &amp;= \sigma\left(\ln \frac{p\left(\mathbf{x}\mid \mathcal{C}</em>{1}\right) p\left(\mathcal{C}</em>{1}\right)}{p\left(\mathbf{x} \mid \mathcal{C}<em>{2}\right) p\left(\mathcal{C}</em>{2}\right)}\right) \
&amp;=\sigma\left(\ln \frac{\exp \left{ -\frac{1}{2} \left(\mathbf{x}-\boldsymbol{\mu}<em>{1}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\mathbf{x}-\boldsymbol{\mu}</em>{1}\right)\right}}{\exp \left{ -\frac{1}{2}  \left(\mathbf{x}-\boldsymbol{\mu}<em>{2}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\mathbf{x}-\boldsymbol{\mu}</em>{2}\right)\right}}+\ln \frac{p\left(\mathcal{C}<em>{1}\right)}{p\left(\mathcal{C}</em>{2}\right)}\right) \
&amp;=\sigma\left(-\frac{1}{2}\left{\left(\mathbf{x}-\boldsymbol{\mu}<em>{1}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\mathbf{x}-\boldsymbol{\mu}</em>{1}\right)\right}+\frac{1}{2}\left{\left(\mathbf{x}-\boldsymbol{\mu}<em>{2}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\mathbf{x}-\boldsymbol{\mu}</em>{2}\right)\right}+\ln \frac{p\left(\mathcal{C}<em>{1}\right)}{p\left(\mathcal{C}</em>{2}\right)}\right)\
&amp;=\sigma\left(\boldsymbol{\mu}<em>{1}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \mathbf{x}-\frac{1}{2} \boldsymbol{\mu}</em>{1}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \boldsymbol{\mu}<em>{1}-\boldsymbol{\mu}</em>{2}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \mathbf{x}+\frac{1}{2} \boldsymbol{\mu}<em>{2}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \boldsymbol{\mu}</em>{2}+\ln \frac{p\left(\mathcal{C}<em>{1}\right)}{p\left(\mathcal{C}</em>{2}\right)}\right)\
&amp;=\sigma\left(\left((\mathbf{\Sigma}^{-1})^{\mathrm{T}}\left(\boldsymbol{\mu}<em>{1}-\boldsymbol{\mu}</em>{2}\right)\right)^{\mathrm{T}} \mathbf{x}-\frac{1}{2} \boldsymbol{\mu}<em>{1}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \boldsymbol{\mu}</em>{1}+\frac{1}{2} \boldsymbol{\mu}<em>{2}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \boldsymbol{\mu}</em>{2}+\ln \frac{p\left(\mathcal{C}<em>{1}\right)}{p\left(\mathcal{C}</em>{2}\right)}\right)
\end{aligned}
$$</p>
<p>$(\mathbf{\Sigma}^{-1})^{\mathrm{T}} = \mathbf{\Sigma}^{-1}$なので、$\mathbf{w} = \mathbf{\Sigma}^{-1}(\boldsymbol{\mu}<em>1 - \boldsymbol{\mu}<em>2) \quad (4.66)$と$\displaystyle w_0 = -\frac{1}{2} \boldsymbol{\mu}</em>{1}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \boldsymbol{\mu}</em>{1}+\frac{1}{2} \boldsymbol{\mu}<em>{2}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \boldsymbol{\mu}</em>{2}+\ln \frac{p\left(\mathcal{C}<em>{1}\right)}{p\left(\mathcal{C}</em>{2}\right)} \quad (4.67)$の定義を用いると,</p>
<p>$$
p(\mathcal{C}_1\mid \mathbf{x}) = \sigma(\mathbf{w}^{\mathrm T}\mathbf{x}+w_0) \tag{4.65}
$$</p>
<p>と書ける。</p>
<h2 id="演習-49"><a class="header" href="#演習-49">演習 4.9</a></h2>
<div class="panel-primary">
<p>クラスの事前確率$p(\mathcal{C}_k)=\pi_k$と一般的なクラスの条件付き確率密度$p(\boldsymbol{\phi}\mid \mathcal{C}_k)$によって定義される$K$クラス分類問題の生成モデルを考える．ここで$\boldsymbol{\phi}$は入力特徴ベクトルである．学習データ集合${ \boldsymbol{\phi}_n, \mathbf{t}_n }$が与えられたと仮定する．ここで，$n=1,\ldots,N$であり，$\mathbf{t}<em>n$は，1-of-$K$符号化法を使う長さ$K$の2値目的変数ベクトルである．つまり，パターン$n$のクラスが$\mathcal{C}<em>k$である場合，2値目的変数ベクトルは構成要素$t</em>{nj} = I</em>{jk}$を持つ．データがこのモデルから独立に抽出されると仮定すると，その事前確率に対する最尤解が以下の式で与えられることを示せ．</p>
<p>$$
\pi_k = \frac{N_k}{N} \tag{4.159}
$$</p>
<p>ここで，$N_k$はクラス$\mathcal{C}_k$に割り当てられるデータの個数である．</p>
</div>
<p>※ 「パターン$n$のクラスが$\mathcal{C}<em>k$である場合，2値目的変数ベクトルは構成要素$t</em>{nj} = I_{jk}$を持つ」というのは、$j=k$であれば$t_{nk} = I_{kk} = 1$, $j\neq k$であれば$0$ということである。</p>
<p>2クラス分類のときの尤度関数</p>
<p>$$
p(\mathbf{t},\mathbf{X}\mid \pi, \boldsymbol{\mu}_1,\boldsymbol{\mu}<em>2,\mathbf{\Sigma}) = \prod</em>{n=1}^{N}[\pi \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_1,\mathbf{\Sigma})]^{t_n}[(1-\pi)\mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_2,\mathbf{\Sigma})]^{1-t_n}  \tag{4.71}
$$</p>
<p>を多クラスに拡張したい。</p>
<p>$$
\mathbf{I}_k=\begin{pmatrix}
0 \
0 \
\vdots \
1 \
\vdots \
0 \
\end{pmatrix}
$$</p>
<p>という$k$番目の要素が$1$で他が$0$であるベクトルを用いて、多クラスの尤度関数$p({\mathbf{t}_n, \boldsymbol{\phi}_n }\mid { \pi_k } )$は以下のようになる。（$\mathbf{I}_k^\mathrm{T}\mathbf{t}<em>n = t</em>{nk}$である。）</p>
<p>$$
\begin{aligned}
p({\mathbf{t}<em>n, \boldsymbol{\phi}<em>n }\mid { \pi_k } ) = \prod</em>{n=1}^{N}\prod</em>{k=1}^{K}[\pi_k p(\boldsymbol{\phi}_n\mid \mathcal{C}_k)]^{\mathbf{I}_k^\mathrm{T}\mathbf{t}_n}
\end{aligned}
$$</p>
<p>最大化するにあたって、これの対数尤度関数をとる。</p>
<p>$$
\begin{aligned}
\ln p({\mathbf{t}<em>n, \boldsymbol{\phi}<em>n }\mid { \pi_k } ) = \sum</em>{n=1}^{N}\sum</em>{k=1}^{K}\mathbf{I}_k ^\mathrm{T}\mathbf{t}_n  (\ln\pi_k + p(\boldsymbol{\phi}_n\mid \mathcal{C}_k))
\end{aligned}
$$</p>
<p>ここで、最大化に関係する部分だけ取り出し、$\sum_{k=1}^{K} \pi_k = 1$であることを考慮してラグランジュの未定乗数法を用いる(あまり本来の使用法と違う気もしますが使えるので)と</p>
<p>$$
\begin{aligned}
L = \sum_{n=1}^{N}\sum_{k=1}^{K}\mathbf{I}_k ^\mathrm{T}\mathbf{t}<em>n \ln\pi_k -\lambda \left{ \left(\sum</em>{k=1}^{K}\pi_k \right) -1 \right} \
\end{aligned}
$$</p>
<p>すべての$k$における$\pi_k$と$\lambda$で$L$を微分し、$0$を取ると</p>
<p>$$
\begin{aligned}
\frac{\partial L}{\partial \pi_k} &amp;= \sum_{n=1}^{N}\frac{\mathbf{I}_k ^\mathrm{T} \mathbf{t}<em>n}{\pi_k}-\lambda = 0 \
\frac{\partial L}{\partial \lambda} &amp;= \left( \sum</em>{k=1}^{K}\pi_k \right) -1 = 0
\end{aligned}
$$</p>
<p>となる。上式の分母を払うと</p>
<p>$$
\lambda\pi_k = \sum_{n=1}^N \mathbf{I}_k ^\mathrm{T} \mathbf{t}_n
$$</p>
<p>ここで$\sum_{n=1}^N \mathbf{I}_k ^\mathrm{T} \mathbf{t}_n$は定義からクラス$\mathcal{C}_k$に割り当てられるデータの個数$N_k$であるから、$\pi_k = N_k/\lambda$となる。</p>
<p>一方、すべての$k$について足し合わせると</p>
<p>$$
\begin{aligned}
\sum_{k=1}^K \lambda\pi_k &amp;= \sum_{k=1}^K \sum_{n=1}^N \mathbf{I}<em>k ^\mathrm{T} \mathbf{t}<em>n \
\lambda \left( \sum</em>{k=1}^K \pi_k \right) &amp;= \sum</em>{k=1}^K N_k \
\lambda &amp;= N\quad \left(\because \frac{\partial L}{\partial \lambda} = 0から \left( \sum_{k=1}^{K}\pi_k \right) = 1 \right)
\end{aligned}
$$</p>
<p>より</p>
<p>$$
\pi_k = \frac{N_k}{N}
$$
となり、示された。</p>
<h2 id="演習-410"><a class="header" href="#演習-410">演習 4.10</a></h2>
<div class="panel-primary">
<p>演習問題4.9の分類モデルを考え，クラスの条件付き確率密度が共通の共分散行列を持つガウス分布によって与えられる，つまり，以下の式が成立すると仮定する．</p>
<p>$$
p\left(\boldsymbol{\phi} \mid \mathcal{C}<em>{k}\right)=\mathcal{N}\left(\boldsymbol{\phi} \mid \boldsymbol{\mu}</em>{k}, \mathbf{\Sigma} \right) \tag{4.160}
$$</p>
<p>クラス$\mathcal{C}_k$のガウス分布の平均に対する最尤解が以下の式で与えられることを示せ．</p>
<p>$$
\boldsymbol{\mu}<em>k=\frac{1}{N</em>{k}}\sum_{n=1}^N t_{nk}\boldsymbol{\phi}_n  \tag{4.161}
$$</p>
<p>これは，クラス$\mathcal{C}_k$に割り当てられる特徴ベクトルの平均を表す．同様に，共通の共分散に対する最尤解が，以下の式で与えられることを示せ．</p>
<p>$$
\mathbf{\Sigma}=\sum_{k=1}^{K} \frac{N_{k}}{N} \mathbf{S}_{k} \tag{4.162}
$$</p>
<p>ここで</p>
<p>$$
\mathbf{S}<em>{k}=\frac{1}{N</em>{k}} \sum_{n=1}^{N} t_{nk}\left(\boldsymbol{\phi}<em>{n}-\boldsymbol{\mu}</em>{k}\right)\left(\boldsymbol{\phi}<em>{n}-\boldsymbol{\mu}</em>{k}\right)^{\mathrm{T}} \tag{4.163}
$$</p>
<p>である．よって，$\mathbf{\Sigma}$は，各クラスのデータの共分散の重み付き平均で与えられる．重み付け係数はクラスの事前確率で与えられる．</p>
</div>
<p>※ 演習問題4.9での$p(\boldsymbol{\phi}_n\mid \mathcal{C}_k)$に正規分布$\displaystyle \mathcal{N}(\boldsymbol{\phi}_n \mid \boldsymbol{\mu}_k,\mathbf{\Sigma}) = \frac{1}{(2\pi)^{D/2}|\mathbf{\Sigma}|^{1/2}} \exp\left{ -\frac{1}{2}(\boldsymbol{\phi}_n - \boldsymbol{\mu}_k)^{\mathrm T}\mathbf{\Sigma}^{-1}(\boldsymbol{\phi}_n - \boldsymbol{\mu}_k) \right}$を仮定した場合になる。</p>
<p>演習問題4.9を利用すると尤度関数は
$$
p\left(\left{\mathbf{t}<em>{n}, \boldsymbol{\phi}</em>{n}\right} \mid\left{\pi_{k}\right}\right)=\prod_{n=1}^{N} \prod_{k=1}^{K}\left[\pi_{k} \mathcal{N}(\boldsymbol{\phi}<em>n \mid \boldsymbol{\mu}<em>k,\mathbf{\Sigma})p(\mathcal{C}<em>k)\right]^{t</em>{nk}}
$$
であり、対数尤度関数は$\boldsymbol{\mu}<em>k$と$\mathbf{\Sigma}$に依存する項を抜き出して残りを$\textrm{const.}$とすると
$$
\begin{aligned}
\ln p\left(\left{\mathbf{t}</em>{n}, \boldsymbol{\phi}</em>{n}\right} \mid\left{\pi</em>{k}\right}\right) &amp;=\sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk}\left(
\ln \pi_k +\ln \mathcal{N}(\boldsymbol{\phi}<em>n \mid \boldsymbol{\mu}<em>k,\mathbf{\Sigma})+\ln p(\mathcal{C}<em>k)\right) \
&amp;=-\frac{1}{2}\sum</em>{n=1}^{N} \sum</em>{k=1}^{K} t</em>{nk}\left(
\ln |\mathbf{\Sigma}| + (\boldsymbol{\phi}<em>n - \boldsymbol{\mu}<em>k)^{\mathrm T}\mathbf{\Sigma}^{-1}(\boldsymbol{\phi}<em>n - \boldsymbol{\mu}<em>k)
\right) + \textrm{const.}
\end{aligned}
$$
$\boldsymbol{\mu}<em>k$についての微分を$0$とすると(※ここで$k$は特定の値をとる)
$$
\begin{aligned}
\frac{\partial}{\partial \boldsymbol{\mu}</em>{k}} \ln p
&amp;=-\frac{1}{2} \sum</em>{n=1}^{N}\left{t</em>{n k} \frac{\partial}{\partial \boldsymbol{\mu}</em>{k}}\left(\boldsymbol{\phi}</em>{n}-\boldsymbol{\mu}<em>{k}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\boldsymbol{\phi}</em>{n}-\boldsymbol{\mu}<em>{k}\right)\right} \
&amp;=\sum</em>{n=1}^{N}\left{t_{n k} \mathbf{\Sigma}^{-1}\left(\boldsymbol{\phi}<em>{n}-\boldsymbol{\mu}</em>{k}\right)\right} \
&amp;=\mathbf{\Sigma}^{-1} \sum_{n=1}^{N}\left{t_{n k}\left(\boldsymbol{\phi}<em>{n}-\boldsymbol{\mu}</em>{k}\right)\right} \
&amp;=\mathbf{\Sigma}^{-1}\left(\sum_{n=1}^{N} t_{n k} \boldsymbol{\phi}<em>{n}-\sum</em>{n=1}^{N} t_{n k} \boldsymbol{\mu}<em>{k}\right) \
&amp;=\mathbf{\Sigma}^{-1}\left(\sum</em>{n=1}^{N} t_{n k} \boldsymbol{\phi}<em>{n}-N</em>{k} \boldsymbol{\mu}<em>{k}\right) = 0
\end{aligned}
$$
以上から
$$
\boldsymbol{\mu}</em>{k}=\frac{1}{N_{k}} \sum_{n=1}^{N} t_{n k} \boldsymbol{\phi}_{n} \tag{4.161}
$$
が求まる。</p>
<p>続いて$\mathbf{S}<em>k$をうまく使えるように対数尤度関数を変形すると、
$$
\begin{aligned}
\ln p &amp;=\sum</em>{n=1}^{N} \sum_{k=1}^{K}\left{t_{n k} \ln \mathcal{N}\left(\boldsymbol{\phi}<em>{n} \mid \boldsymbol{\mu}</em>{k}, \mathbf{\Sigma}\right)\right} \
&amp;=\sum_{n=1}^{N} \sum_{k=1}^{K}\left{t_{n k}\left[-\frac{1}{2} \ln |\mathbf{\Sigma}|-\frac{1}{2}\left(\boldsymbol{\phi}<em>{n}-\boldsymbol{\mu}</em>{k}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\boldsymbol{\phi}<em>{n}-\boldsymbol{\mu}</em>{k}\right)\right]\right}+\mathrm{const.} \
&amp;=-\frac{1}{2} \sum_{n=1}^{N} \sum_{k=1}^{K} t_{n k} \ln |\mathbf{\Sigma}|-\frac{1}{2} \sum_{n=1}^{N} \sum_{k=1}^{K} t_{n k}\left(\boldsymbol{\phi}<em>{n}-\boldsymbol{\mu}</em>{k}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\boldsymbol{\phi}<em>{n}-\boldsymbol{\mu}</em>{k}\right)+\mathrm{const.} \
&amp;=-\frac{N}{2} \ln |\mathbf{\Sigma}|-\frac{1}{2} \sum_{n=1}^{N} \sum_{k=1}^{K} t_{n k}\left(\boldsymbol{\phi}<em>{n}-\boldsymbol{\mu}</em>{k}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\boldsymbol{\phi}<em>{n}-\boldsymbol{\mu}</em>{k}\right)+\mathrm{const.} \
&amp;=-\frac{N}{2} \ln |\mathbf{\Sigma}|-\frac{1}{2} \sum_{n=1}^{N} \sum_{k=1}^{K} t_{n k} \operatorname{Tr}\left[\mathbf{\Sigma}^{-1}\left(\boldsymbol{\phi}<em>{n}-\boldsymbol{\mu}</em>{k}\right)\left(\boldsymbol{\phi}<em>{n}-\boldsymbol{\mu}</em>{k}\right)^{\mathrm{T}}\right]+\mathrm{const.} \
&amp;=-\frac{N}{2} \ln |\mathbf{\Sigma}|-\frac{1}{2} \sum_{k=1}^{K}\left( N_k \sum_{n=1}^{N} \frac{1}{N_k} t_{nk} \operatorname{Tr}\left[\mathbf{\Sigma}^{-1}\left(\boldsymbol{\phi}<em>{n}-\boldsymbol{\mu}</em>{k}\right)\left(\boldsymbol{\phi}<em>{n}-\boldsymbol{\mu}</em>{k}\right)^{\mathrm{T}}\right]\right) +\mathrm{const.} \
&amp;=-\frac{N}{2} \ln |\mathbf{\Sigma}|-\frac{1}{2} \sum_{k=1}^{K} N_k \left( \operatorname{Tr}\left[ \mathbf{\Sigma}^{-1} \frac{1}{N_k} \sum_{n=1}^{N} t_{nk} \left(\boldsymbol{\phi}<em>{n}-\boldsymbol{\mu}</em>{k}\right)\left(\boldsymbol{\phi}<em>{n}-\boldsymbol{\mu}</em>{k}\right)^{\mathrm{T}}\right]\right) +\mathrm{const.} \
&amp;=-\frac{N}{2} \ln |\mathbf{\Sigma}|-\frac{1}{2} \sum_{k=1}^K N_k \operatorname{Tr}\left[\mathbf{\Sigma}^{-1} \mathbf{S}_k\right]+\mathrm{const.}
\end{aligned}
$$</p>
<p>$\mathbf{\Sigma}$についての微分をとって$0$とする。<a href="../PRML%E3%81%AE%E6%BC%94%E7%BF%92%E5%95%8F%E9%A1%8C%E8%A7%A3%E7%AD%94%E9%9B%86%20%E7%AC%AC2%E7%AB%A0%202.61%E3%81%BE%E3%81%A7#%E6%BC%94%E7%BF%92%202.34">演習問題2.34</a>と同様に変形していくと、</p>
<p>$$
\begin{aligned}
\frac{\partial}{\partial \mathbf{\Sigma}}\ln p &amp;= -\frac{N}{2}(\mathbf{\Sigma}^{-1})^{\mathrm T} - \frac{1}{2}\sum_{k=1}^K N_k \frac{\partial}{\partial \mathbf{\Sigma}}\operatorname{Tr}\left[\mathbf{\Sigma}^{-1} \mathbf{S}<em>k\right] \
&amp;= -\frac{N}{2}(\mathbf{\Sigma}^{-1})^{\mathrm T} - \frac{1}{2}\sum</em>{k=1}^K N_k (\mathbf{\Sigma}^{-1} \mathbf{S}_k \mathbf{\Sigma}^{-1})^{\mathrm T} = 0
\end{aligned}
$$</p>
<p>転置をとって移項すると</p>
<p>$$
\mathbf{\Sigma}^{-1} = \sum_{k=1}^K \frac{N_k}{N} \mathbf{\Sigma}^{-1} \mathbf{S}_k \mathbf{\Sigma}^{-1}
$$</p>
<p>左と右からそれぞれ$\mathbf{\Sigma}$をかければ
$$
\mathbf{\Sigma} = \sum_{k=1}^K \frac{N_k}{N}\mathbf{S}_k \tag{4.162}
$$
を得る。</p>
<h2 id="演習-411"><a class="header" href="#演習-411">演習 4.11</a></h2>
<div class="panel-primary">
<p>各々が$L$の離散状態を取ることのできる$M$個の要素を持つ特徴ベクトル$\boldsymbol{\phi}$に対する$K$クラスの分類問題を考える．1-of-$L$符号化法によって成分の値は表現されるとする．さらに，クラス$\mathcal{C}_k$に対し$\boldsymbol{\phi}$の$M$個の成分が独立であり，クラスの条件付き確率密度は特徴ベクトルの要素に分解できると仮定する．クラスの事後確率を記述しているソフトマックス関数の引数に現れる</p>
<p>$$
a_{k}=\ln \left(p\left(\mathbf{x} \mid \mathcal{C}<em>{k}\right) p\left(\mathcal{C}</em>{k}\right)\right) \tag{4.63}
$$</p>
<p>によって与えられる量$a_k$が$\boldsymbol{\phi}$の成分の線形関数であることを示せ．これが，8.2.2節で議論されるナイーブベイズモデルの一例であることに注意せよ．</p>
</div>
<p>要素が$L$個のベクトル$\mathbf{x}$が$M$個並んだ以下のような行列$\boldsymbol{\phi}$を考える</p>
<p>$$
\boldsymbol{\phi}=\begin{pmatrix}
\mathbf{x}_1 \
\mathbf{x}_2 \
\vdots \
\mathbf{x}_M \
\end{pmatrix}
$$
また、</p>
<p>$$
\begin{aligned}
p(\boldsymbol{\phi} \mid \mathcal{C}<em>k) = \prod</em>{m=1}^{M} \prod_{l=1}^{L} \mu_{kml}^{\phi_{ml}}
\end{aligned}
$$
である。ここで、$\mu$はそのデータがクラス$\mathcal{C}_k$に属する確率を表している。</p>
<p>$$
\sum_{l} \mu_{kml} = 1
$$</p>
<p>である。
これを用いて</p>
<p>$$
\begin{aligned}
a_k &amp;= \ln \left(\prod_{m=1}^{M}\prod_{l=1}^{L}\mu_{kml}^{x_{ml}}\right) + \ln p(\mathcal{C}<em>k) \
&amp;= \sum</em>{m=1}^{M}\sum_{l=1}^{L}x_{ml} \ln \mu_{kml}+ \ln p(\mathcal{C}<em>k) \
&amp;= \sum</em>{m=1}^{M}x_m ^\mathrm{T} \begin{pmatrix}
\ln \mu_{km1} \
\ln \mu_{km2} \
\vdots \
\ln \mu_{kmL} \
\end{pmatrix}
+ \ln p(\mathcal{C}_k)
\end{aligned}
$$
以上により$a_k$が$\boldsymbol{\phi}$の成分の線形関数であることが示された。</p>
<h2 id="演習-412"><a class="header" href="#演習-412">演習 4.12</a></h2>
<div class="panel-primary">
<p>$$
\sigma(a) = \frac{1}{1+ \exp(-a)} \tag{4.59}
$$
で定義されるロジスティックシグモイド関数の微分に対する関係
$$
\frac{d\sigma}{da} = \sigma(1-\sigma) \tag{4.88}
$$
を検証せよ．</p>
</div>
<p>$$
\begin{aligned}
\frac{ \mathrm{d} }{ \mathrm{d} a}  \sigma(a) &amp;=  \frac{ \mathrm{d} }{ \mathrm{d} a} \frac{1}{1+ \exp(-a)} \
&amp;= - e^{-a} \frac{-1}{\left(1+ \exp(-a)\right)^2} \
&amp;= \frac{1}{1+ \exp(-a)} \frac{e^{-a}}{1+ \exp(-a)} \
&amp;= \sigma(1-\sigma)
\end{aligned}
$$</p>
<h2 id="演習-413"><a class="header" href="#演習-413">演習 4.13</a></h2>
<div class="panel-primary">
<p>ロジスティックシグモイドの微分に対する結果
$$
\frac{d\sigma}{da} = \sigma(1-\sigma) \tag{4.88}
$$
を使って，ロジスティック回帰モデルに対する誤差関数
$$
E(\mathbf{w})=-\ln p(\mathsf{t} \mid \mathbf{w})=-\sum_{n=1}^{N}\left{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right} \tag{4.90}
$$
の微分が，
$$
\nabla E(\mathbf{w})=\sum_{n=1}^{N}\left(y_{n}-t_{n}\right) \phi_{n} \tag{4.91}
$$
で与えられることを示せ．</p>
</div>
<p>$E$は$y_n$の関数、$y_n$は$a_n$の関数、$a_n$は$\mathbf{w}$の関数なので、連鎖律を利用する。</p>
<p>すなわち$\nabla E(\mathbf{w}) = \frac{\partial E}{\partial y_n}\frac{\partial y_n}{\partial a_n}\nabla a_n $ としてそれぞれの項を求めれば良い。</p>
<br>
<p>$E(\mathbf{w})=-\sum_{n=1}^{N}\left{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right}$ より</p>
<p>$$
\begin{aligned}
\frac{\partial E}{\partial y_n} &amp;=-\sum_{n=1}^{N}\left( \frac{t_{n}}{y_{n}}-\frac{1-t_{n}}{1-y_{n}} \right)\
&amp;=-\sum_{n=1}^{N}\left(\frac{t_{n}\left(1-y_{n}\right)-y_{n}\left(1-t_{n}\right)}{y_{n}\left(1-y_{n}\right)}\right) \
&amp;=\sum_{n=1}^{N} \frac{y_{n}-t_{n}}{y_{n}\left(1-y_{n}\right)}
\end{aligned} \tag{1}
$$
<br></p>
<p>$y_n=\sigma(a_n)$ より $\frac{d\sigma}{da} = \sigma(1-\sigma)$を利用して</p>
<p>$$
\frac{\partial y_{n}}{\partial a_{n}}=\frac{\partial \sigma\left(a_{n}\right)}{\partial a_{n}}=\sigma\left(a_{n}\right)\left(1-\sigma\left(a_{n}\right)\right)=y_{n}\left(1-y_{n}\right) \tag{2}
$$</p>
<br>
<p>$a_n = \mathbf{w}^{\mathrm T}\boldsymbol{\phi}_n$より</p>
<p>$$
\nabla a_n = \boldsymbol{\phi}_n \tag{3}
$$</p>
<br>
<p>よって(1), (2), (3)より</p>
<p>$$
\nabla E(\mathbf{w}) = \frac{\partial E}{\partial y_n}\frac{\partial y_n}{\partial a_n}\nabla a_n = \sum_{n=1}^{N}(y_n - t_n)\boldsymbol{\phi}_n
$$</p>
<p>となり、$(4.91)$式が示された。</p>
<br>
<h2 id="演習-414"><a class="header" href="#演習-414">演習 4.14</a></h2>
<div class="panel-primary">
<p>線形分離可能なデータ集合に対し，ロジスティック回帰モデルの最尤解が，クラスを分離する決定境界$\mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}(\mathbf{x})=0$を満足するベクトル$\mathbf{w}$に対し，その値を$\infty$とすることで得られることを示せ．</p>
</div>
<p>データ集合${ \boldsymbol{\phi}<em>n, t_n}$, $t_n \in {0, 1}$について、もしデータ集合が線形分離可能ならば、2つのクラスを分ける決定境界の超平面は$\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x})=0$で与えられ、
$$
\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}</em>{n} \left{\begin{aligned}
\geq 0 &amp; \text { if } t_{n}=1 \
&lt;0 &amp; \text { otherwise } (t_n = 0)
\end{aligned}\right.
$$
となる。</p>
<p>一方、最尤法で尤度の負の対数をとって誤差関数$(4.90)$を定義すると</p>
<p>$$
E(\mathbf{w})=-\ln p(\mathsf{t} \mid \mathbf{w})=-\sum_{n=1}^{N}\left{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right} \tag{4.90}
$$</p>
<p>この勾配は</p>
<p>$$
\nabla E(\mathbf{w})=\sum_{n=1}^{N}\left(y_{n}-t_{n}\right) \phi_{n} \tag{4.91}
$$</p>
<p>すなわち、すべての$n$について$y_n = \sigma(\mathbf{w}^{\mathrm T}\boldsymbol{\phi}_n)=t_n$のときに最小化される。
ここで、$t_n \in { 0, 1}$なので$y_n \in { 0, 1}$になるが、これはシグモイド関数の両端、つまり$\mathbf{w}^{\mathrm T}\boldsymbol{\phi}_n = \pm \infty$のときに相当する。$\boldsymbol{\phi}_n$は固定なので、$\mathbf{w}$の大きさが$\infty$の時に相当する。このとき$E(\mathbf{w})$は最小化されロジスティック回帰モデルの最尤解が得られることになる。</p>
<h2 id="演習-415"><a class="header" href="#演習-415">演習 4.15</a></h2>
<div class="panel-primary">
<p>$$
\mathbf{H}=\nabla \nabla E(\mathbf{w})=\sum_{n=1}^{N} y_{n}\left(1-y_{n}\right) \boldsymbol{\phi}<em>{n} \boldsymbol{\phi}</em>{n}^{\mathrm{T}}=\mathbf{\Phi}^{\mathrm{T}} \mathbf{R} \mathbf{\Phi} \tag{4.97}
$$
で与えられるロジスティック回帰モデルのヘッセ行列$\mathbf{H}$が正定値行列であることを示せ．ここで$\mathbf{R}$は，要素を$y_n(1-y_n)$とする対角行列であり，$y_n$は，入力ベクトル$\mathbf{x}_n$に対するロジスティック回帰モデルの出力である．したがって，誤差関数は$\mathbf{w}$の凸関数であり，唯一の最小解を持つことを示せ．</p>
</div>
<p>ヘッセ行列$\mathbf{H}$が正定値行列であるためには任意の$\mathbf{x}$に対して$\mathbf{x}^\mathrm{T} \mathbf{\Phi}^\mathrm{T} \mathbf{R} \mathbf{\Phi} \mathbf{x}&gt;0$が成り立つことを確認すれば良い。</p>
<p>ここで、$M×1$ベクトルである$\mathbf{z}= \mathbf{\Phi}\mathbf{x}$について考えると、
$\mathbf{\Phi}$はフルランクであるため$\mathbf{z}\neq \mathbf{0}$であることがわかる。</p>
<p>よって、$\mathbf{x}^\mathrm{T} \mathbf{\Phi}^\mathrm{T} \mathbf{R} \mathbf{\Phi} \mathbf{x} = \mathbf{z}^\mathrm{T} \mathbf{R} \mathbf{z}$となるが、$\mathbf{R}$の要素$y_n(1-y_n)$は$0&lt;y_n&lt;1$であることから全ての対角成分が正である対角行列なので常に$\mathbf{z}^\mathrm{T} \mathbf{R} \mathbf{z} &gt; 0$が成り立つ。</p>
<p>以上よりヘッセ行列$\mathbf{H}$が正定値行列であることが確認できた。
また、$\mathbf{H}=\nabla \nabla E(\mathbf{w})$であるので$\mathbf{H}$が正定値であれば誤差関数は唯一の最小解を持つ。</p>
<h2 id="演習-416"><a class="header" href="#演習-416">演習 4.16</a></h2>
<div class="panel-primary">
<p>$t=0$または$t=1$に対応する2クラスの1つに属することが知られている各観測値$\mathbf{x}_n$における2値分類問題を考える．このとき，学習データがときどき間違ったラベルを付けられるため，学習データの収集手順は完全なものではないと仮定する．すべてのデータ$\mathbf{x}_n$に対し，クラスラベルの値$t_n$を与える代わりに，$t_n = 1$となる確率を表現する値$\pi_n$を与える．確率モデル$p(t=1\mid \boldsymbol{\phi})$が与えられた場合そのようなデータ集合に適切な対数尤度関数を記述せよ．</p>
</div>
<p>※尤度関数の解釈を問う問題？</p>
<p>もし完全にラベル付け$t_n$が間違っていない場合、$p_n = p(t_n = 1 \mid \boldsymbol{\phi}(\mathbf{x}_n))$とすると、ある観測点$\mathbf{x}_n$についての尤度関数は
$$
p(t_n\mid \boldsymbol{\phi}(\mathbf{x}_n)) = p_n^{t_n}(1-p_n)^{1-t_n}
$$
すべてのデータ${\mathsf{t}, \boldsymbol{\phi}(\mathbf{x}_n)}$に対する尤度関数と対数尤度関数は
$$
p(\mathsf{t}\mid\boldsymbol{\phi}(\mathbf{x}<em>n)) = \prod</em>{n=1}^{N}p_n^{t_n}(1-p_n)^{1-t_n}
$$
$$
\ln p(\mathsf{t}\mid\boldsymbol{\phi}(\mathbf{x}<em>n)) = \sum</em>{n=1}^{N}{t_n \ln p_n + (1-t_n)\ln(1-p_n)}
$$
となる。この式を考えると、$t_n=1$のときに対数尤度関数を$\ln p_n$だけ増加させ、$t_n=0$のときには$\ln(1-p_n)$だけ増加させると解釈することができる。この解釈に基づくと、確率$\pi_n$で$t_n=1$となる（確率$1-\pi_n$で$t_n=0$となる）とき、対数尤度関数は</p>
<p>$$
\begin{aligned}
\ln p(\mathsf{t}\mid\boldsymbol{\phi}(\mathbf{x}<em>n)) &amp;= \sum</em>{n=1}^{N}{\pi_n \ln p_n + (1-\pi_n)\ln(1-p_n)} \
&amp;=\sum_{n=1}^{N}{\pi_n \ln p(t_n = 1 \mid \boldsymbol{\phi}(\mathbf{x}_n)) + (1-\pi_n)\ln(1-p(t_n = 1 \mid \boldsymbol{\phi}(\mathbf{x}_n)))}
\end{aligned}
$$</p>
<p>で与えられることになる。</p>
<h2 id="演習-417"><a class="header" href="#演習-417">演習 4.17</a></h2>
<div class="panel-primary">
<p>ソフトマックス活性化関数
$$
p\left(\mathcal{C}<em>{k} \mid \boldsymbol{\phi}\right)=y</em>{k}(\boldsymbol{\phi})=\frac{\exp \left(a_{k}\right)}{\sum_{j} \exp \left(a_{j}\right)} \tag{4.104}
$$
の微分が，
$$
\frac{\partial y_{k}}{\partial a_{j}}=y_{k}\left(I_{k j}-y_{j}\right) \tag{4.106}
$$
によって与えられることを示せ．$I_{kj}$は単位行列の要素である。ここで，$a_k$は
$$
a_k = \mathbf{w}_k^{\mathrm T}\boldsymbol{\phi} \tag{4.105}
$$
によって定義される．</p>
</div>
<p>※4.3.4 多クラスロジスティック回帰を参照。$k=j$と$k\neq j$のときに分けて考える。</p>
<p>$(i)$ $k\neq j$のとき
$$
\begin{aligned}
\frac{\partial y_k}{\partial a_j} &amp;= \frac{\partial}{\partial a_j}\left( \frac{e^{a_k}}{\sum_j \exp(a_j)} \right) \
&amp;= -\frac{e^{a_k}e^{a_j}}{\left( \sum_j \exp(a_j) \right)^2} \
&amp;=-y_k y_j
\end{aligned}
$$</p>
<p>$(ii)$ $k = j$のとき
$$
\begin{aligned}
\frac{\partial y_j}{\partial a_j} &amp;= \frac{\partial}{\partial a_j}\left( \frac{e^{a_j}}{\sum_j \exp(a_j)} \right) \
&amp;= \frac{e^{a_j}}{\sum_j \exp(a_j)} - \frac{e^{a_j}\cdot e^{a_j}}{\left(\sum_j \exp(a_j) \right)^2} \
&amp;=y_j(1-y_j)
\end{aligned}
$$</p>
<p>これらをまとめると
$$
\frac{\partial y_k}{\partial a_j} = y_k(I_{kj}-y_j) \tag{4.106}
$$</p>
<p>となる。ここで、$I_{kj}$は単位行列の$kj$成分である。</p>
<h2 id="演習-418"><a class="header" href="#演習-418">演習 4.18</a></h2>
<div class="panel-primary">
<p>ソフトマックス活性化関数の微分の結果
$$
\frac{\partial y_{k}}{\partial a_{j}}=y_{k}\left(I_{k j}-y_{j}\right) \tag{4.106}
$$
を使って，交差エントロピー誤差
$$
E\left(\mathbf{w}<em>{1}, \ldots, \mathbf{w}</em>{K}\right)=-\ln p\left(\mathbf{T} \mid \mathbf{w}<em>{1}, \ldots, \mathbf{w}</em>{K}\right)=-\sum_{n=1}^{N} \sum_{k=1}^{K} t_{n k} \ln y_{n k} \tag{4.108}
$$
の勾配が
$$
\nabla_{\mathbf{w}<em>{j}} E\left(\mathbf{w}</em>{1}, \ldots, \mathbf{w}<em>{K}\right)=\sum</em>{n=1}^{N}\left(y_{n j}-t_{n j}\right) \boldsymbol{\phi}_{n} \tag{4.109}
$$
で与えられることを示せ．</p>
</div>
<p>※演習問題4.17を利用する。</p>
<p>まず前提として行列$\mathbf{T}$は、「クラス$\mathcal{C}_k$に属する特徴ベクトル$\boldsymbol{\phi}_n$に対する目的変数ベクトル$\mathbf{t}_n$が、$k$番目の要素で$1$で残りはすべて$0$となる1-of-$K$符号化法」で記述されるような行列（教科書P.208）である。</p>
<p>$\nabla_{\mathbf{w}<em>j}E$について微分のchain ruleを使う。
$E$がすでに$y</em>{nk}(=y_k(\boldsymbol{\phi}_n))$の関数となっていることを用いると</p>
<p>$$
\begin{aligned}
\nabla_{\mathbf{w}<em>{j}} E(\mathbf{w}<em>1,\ldots,\mathbf{w}<em>K)
&amp;=\frac{\partial E}{\partial y</em>{nk}}\frac{\partial y</em>{nk}}{\partial a_j}\nabla</em>{\mathbf{w}<em>{j}} a_j \
&amp;=-\sum</em>{n=1}^{N} \sum_{k=1}^{K} \frac{t_{nk}}{y_{nk}}y_{nk}(I_{kj}-y_{nj})\boldsymbol{\phi}<em>n \
&amp;=-\sum</em>{n=1}^{N} \sum_{k=1}^{K} t_{nk}(I_{kj}-y_{nj})\boldsymbol{\phi}<em>n \
&amp;=\sum</em>{n=1}^{N} \left{ y_{nj}\left( \sum_{k=1}^{K} t_{nk} \right) - \left( \sum_{k=1}^{K} t_{nk}I_{kj} \right) \right}\boldsymbol{\phi}_n
\end{aligned}
$$</p>
<p>ここで、「前提」より$\sum_{k=1}^{K}t_{nk}=1$となり、$k=j$の場合$I_{kj}=1$で、$k\neq j$で$I_{kj}=0$であることを用いると
$$
\begin{aligned}
\nabla E_{\mathbf{w}<em>{j}}(\mathbf{w}<em>1,\ldots,\mathbf{w}<em>K)
&amp;=\sum</em>{n=1}^{N} \left{ y</em>{nj}\left( \sum</em>{k=1}^{K} t_{nk} \right) - \left( \sum_{k=1}^{K} t_{nk}I_{kj} \right) \right}\boldsymbol{\phi}<em>n \
&amp;=\sum</em>{n=1}^{N} (y_{nj}-t_{nj})\boldsymbol{\phi}_n
\end{aligned}
$$
となり、$(4.109)$式が与えられた。</p>
<h2 id="演習-419"><a class="header" href="#演習-419">演習 4.19</a></h2>
<div class="panel-primary">
<p>4.3.5節で定義したプロビット回帰モデルに対し対数尤度の勾配および，対応するヘッセ行列を求めよ．これらは，IRLSを使って，プロビット回帰モデルのようなモデルを学習するために必要とされる量である．</p>
</div>
<p>※まずプロビット回帰モデルの定義と尤度関数を確認する。微分計算は結構大変で、演習問題4.13をうまく利用して進めていく。ヘッセ行列は勾配をもう一度偏微分することで求まる。</p>
<p>プロビット回帰モデルは
$$
p(t_n=1\mid a_n) = y_n = \Phi(a_n) = \int_{-\infty}^{a_n}\mathcal{N}(\theta\mid 0,1)d\theta
$$
で表現するモデルである。ここで、$a_n = \mathbf{w}^{\mathrm T}\boldsymbol{\phi_n}$である。</p>
<p>尤度関数は</p>
<p>$$
p(\mathsf{t}\mid \mathbf{w}) = \prod_{n=1}^{N}y_n^{t_n}(1-y_n)^{1-t_n}
$$</p>
<p>なので負の対数尤度(cross entropy誤差関数)は$(4.90)$と同形で
$$
E(\mathbf{w}) = -\sum_{n=1}^N { t_n\ln y_n + (1-t_n)\ln(1-y_n) }
$$
を与える。</p>
<p>演習問題4.13と同様にして
$$
\begin{aligned}
\frac{\partial E(\mathbf{w})}{\partial y_{n}} &amp;=\sum_{n=1}^{N} \frac{y_{n}-t_{n}}{y_{n}\left(1-y_{n}\right)} \
\nabla_{\mathbf{w}} a_n &amp;= \boldsymbol{\phi}<em>n
\end{aligned}
$$
であり、
$$
\frac{\partial \Phi(a_n)}{\partial a_n}=\frac{\partial}{\partial a_n} \int</em>{-\infty}^{a_n} \mathcal{N}(\theta \mid 0,1) \mathrm{d} \theta=\mathcal{N}(a_n \mid 0,1)
$$
となることから$\nabla_{\mathbf{w}} E(\mathbf{w})$を求めると</p>
<p>$$
\begin{aligned}
\nabla_{\mathbf{w}} E(\mathbf{w})&amp;=\sum_{n=1}^{N} \frac{\partial E_{n}}{\partial y_{n}} \frac{\partial y_{n}}{\partial a_{n}} \nabla_{\mathbf{w}} a_{n} \
&amp;=\sum_{n=1}^{N} \frac{y_{n}-t_{n}}{y_{n}\left(1-y_{n}\right)} \mathcal{N}\left(a_{n} \mid 0,1\right) \boldsymbol{\phi}<em>{n} \
&amp;= \sum</em>{n=1}^{N} \frac{y_{n}-t_{n}}{y_{n}\left(1-y_{n}\right)} \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{a_n^2}{2}\right) \boldsymbol{\phi}_{n}
\end{aligned}
$$</p>
<p>次にこれを用いてヘッセ行列$\nabla_{\mathbf{w}} \nabla_{\mathbf{w}} E(\mathbf{w})$を求める。ベクトルをベクトルで微分するとき、$\boldsymbol{\phi}_n\to\boldsymbol{\phi}_n^{\mathrm T}$にしておく。</p>
<p>$$
\begin{aligned}
\nabla_{\mathbf{w}} \nabla_{\mathbf{w}} E(\mathbf{w}) &amp;=\nabla_{\mathbf{w}} \sum_{n=1}^{N} \frac{y_{n}-t_{n}}{y_{n}\left(1-y_{n}\right)} \mathcal{N}\left(a_{n} \mid 0,1\right) \boldsymbol{\phi}<em>{n}^{\mathrm{T}} \
&amp;=\sum</em>{n=1}^{N}\left{\left[\nabla_{\mathbf{w}} \frac{y_{n}-t_{n}}{y_{n}\left(1-y_{n}\right)}\right] \mathcal{N}\left(a_{n} \mid 0,1\right) + \frac{y_{n}-t_{n}}{y_{n}\left(1-y_{n}\right)} \left[\nabla_{\mathbf{w}} \mathcal{N}\left(a_{n} \mid 0,1\right)\right]\right} \boldsymbol{\phi}<em>{n}^{\mathrm{T}} \
&amp;=\sum</em>{n=1}^{N}\left{\left(\frac{\partial}{\partial y_{n}} \frac{y_{n}-t_{n}}{y_{n}\left(1-y_{n}\right)}\right) \frac{\partial y_{n}}{\partial a_{n}} \nabla_{\mathbf{w}} a_{n} \mathcal{N}\left(a_{n} \mid 0,1\right)+\frac{y_{n}-t_{n}}{y_{n}\left(1-y_{n}\right)}\left(\frac{\partial}{\partial a_{n}} \mathcal{N}\left(a_{n} \mid 0,1\right)\right) \nabla_{\mathbf{w}} a_{n}\right} \boldsymbol{\phi}<em>{n}^{\mathrm{T}} \
&amp;=\sum</em>{n=1}^{N}\left{\frac{y_{n}^{2}+t_{n}-2 y_{n} t_{n}}{y_{n}^{2}\left(1-y_{n}\right)^{2}} \mathcal{N}\left(a_{n} \mid 0,1\right)^{\mathrm{2}} \boldsymbol{\phi}<em>{n}+\frac{y</em>{n}-t_{n}}{y_{n}\left(1-y_{n}\right)}\left(-a_{n}\right) \mathcal{N}\left(a_{n} \mid 0,1\right) \boldsymbol{\phi}<em>{n}\right} \boldsymbol{\phi}</em>{n}^{\mathrm{T}} \
&amp;=\sum_{n=1}^{N}\left{\frac{y_{n}^{2}+t_{n}-2 y_{n} t_{n}}{y_{n}\left(1-y_{n}\right)}\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{a_n^2}{2}\right)-a_{n}\left(y_{n}-t_{n}\right)\right} \frac{e^{-\frac{a_n^2}{2}}}{\sqrt{2\pi}y_{n}\left(1-y_{n}\right)}\boldsymbol{\phi}<em>{n}\boldsymbol{\phi}</em>{n}^{\mathrm{T}}
\end{aligned}
$$</p>
<blockquote>
<p>公式解答例は$e$の肩に乗るべき係数が$1/2$ずつ間違えてる。</p>
</blockquote>
<h2 id="演習-420"><a class="header" href="#演習-420">演習 4.20</a></h2>
<div class="panel-primary">
<p>$$
\nabla_{\mathbf{w}<em>{k}} \nabla</em>{\mathbf{w}<em>{j}} E\left(\mathbf{w}</em>{1}, \ldots, \mathbf{w}<em>{K}\right)=\sum</em>{n=1}^{N} y_{n k}\left(I_{k j}-y_{n j}\right) \boldsymbol{\phi}<em>{n} \boldsymbol{\phi}</em>{n}^{\mathrm{T}} \tag{4.110}
$$
で定義される多クラスロジスティック回帰問題に対するヘッセ行列が半正定値行列であることを示せ．この問題における非退化ヘッセ行列のサイズは，$MK \times MK$であることに注意されたい．ここで，$M$はパラメータ数であり，$K$はクラス数である．半正定値性を証明するため，長さ$MK$の任意のベクトル$\mathbf{u}$に関して，積$\mathbf{u}^{\mathrm{T}}\mathbf{H}\mathbf{u}$を考え，イェンセンの不等式を適用する．</p>
</div>
<p>※多クラスロジスティック回帰問題でのヘッセ行列は、その$jk$成分が$(4.110)$式で与えられる$M\times M$の部分行列から構成される。つまり、
$$
\mathbf{H}=\begin{pmatrix}
\mathbf{H}<em>{11} &amp; \mathbf{H}</em>{12} &amp; \cdots &amp; \mathbf{H}<em>{1K} \
\mathbf{H}</em>{21} &amp; \mathbf{H}<em>{22} &amp; \cdots &amp; \mathbf{H}</em>{2K} \
\vdots &amp; \vdots &amp; &amp; \vdots \
\mathbf{H}<em>{K 1} &amp; \mathbf{H}</em>{K 2} &amp; \cdots &amp; \mathbf{H}<em>{KK}
\end{pmatrix}, \quad \mathbf{H}</em>{jk} = \sum_{n=1}^{N}y_{nk}(I_{kj}-y_{nj})\underbrace{\boldsymbol{\phi}_n\boldsymbol{\phi}<em>n^{\mathrm T}}</em>{M\times M}
$$
となっている。よって、$\mathbf{H}$は$MK \times MK$の行列である。</p>
<p>このため、ベクトル$\mathbf{u}$も$MK$次元である必要があり、$M$次元ベクトル$\mathbf{u}<em>j(1\le j \le K)$について
$$
\mathbf{u} = \begin{pmatrix}
\mathbf{u}</em>{1} \ \mathbf{u}<em>{2} \ \vdots \ \mathbf{u}</em>{K}
\end{pmatrix}
$$
となる。</p>
<p>演習問題4.15と同じように、任意のベクトル$\mathbf{u}$について$\mathbf{u}^{\mathrm T}\mathbf{Hu} \ge 0$であることを示せば良い（半正定値行列なので$\ge 0$を示す）</p>
<p>要素について注目し計算していくと</p>
<p>$$
\begin{aligned}
\mathbf{u}^{\mathrm T}\mathbf{Hu} &amp;= \sum_{k}\sum_{j}\mathbf{u}<em>{j}^{\mathrm T} \mathbf{H}</em>{jk}\mathbf{u}<em>{k} \
&amp;= \sum</em>{k}\sum_{j}\mathbf{u}<em>{j}^{\mathrm T} \sum_n y</em>{nk}(I_{kj}-y_{nj})\boldsymbol{\phi}<em>n\boldsymbol{\phi}<em>n^{\mathrm T} \mathbf{u}</em>{k} \
&amp;= \sum</em>{k}\sum_{j}\sum_n \mathbf{u}<em>{j}^{\mathrm T} y</em>{nk}(I_{kj}-y_{nj})\boldsymbol{\phi}<em>n\boldsymbol{\phi}<em>n^{\mathrm T} \mathbf{u}</em>{k} \
&amp;= \sum</em>{k}\sum_{j}\sum_n\left( \mathbf{u}<em>{j}^{\mathrm T} y</em>{nk}I_{kj}\boldsymbol{\phi}<em>n\boldsymbol{\phi}<em>n^{\mathrm T} \mathbf{u}</em>{k} - \mathbf{u}</em>{j}^{\mathrm T} y_{nk}y_{nj}\boldsymbol{\phi}<em>n\boldsymbol{\phi}<em>n^{\mathrm T} \mathbf{u}</em>{k} \right) \
&amp;= \sum</em>{k}\sum_n\left( \mathbf{u}<em>{k}^{\mathrm T} y</em>{nk}\boldsymbol{\phi}<em>n\boldsymbol{\phi}<em>n^{\mathrm T} \mathbf{u}</em>{k} - \sum</em>{j} \mathbf{u}<em>{j}^{\mathrm T} y</em>{nk}y_{nj}\boldsymbol{\phi}<em>n\boldsymbol{\phi}<em>n^{\mathrm T} \mathbf{u}</em>{k} \right) (\because I</em>{kj}=1\textrm{ when } k=j, \textrm{ otherwise } 0)\
&amp;= \sum_{k}\sum_n\left{ y_{nk}(\mathbf{u}<em>{k}^{\mathrm T} \boldsymbol{\phi}<em>n)^2 - y</em>{nk} \sum</em>{j} y_{nj} \mathbf{u}<em>{j}^{\mathrm T} \boldsymbol{\phi}<em>n\boldsymbol{\phi}<em>n^{\mathrm T} \mathbf{u}</em>{k} \right} \
&amp;= \sum_n\left{\sum_k y</em>{nk}(\mathbf{u}</em>{k}^{\mathrm T} \boldsymbol{\phi}<em>n)^2 - \sum_k y</em>{nk} \sum_{j} y_{nj} \sum_{k,j} \mathbf{u}<em>{j}^{\mathrm T} \boldsymbol{\phi}<em>n\boldsymbol{\phi}<em>n^{\mathrm T} \mathbf{u}</em>{k} \right} \
&amp;= \sum_n\left{\sum_k y</em>{nk}(\mathbf{u}</em>{k}^{\mathrm T} \boldsymbol{\phi}<em>n)^2 - \left(\sum_j y</em>{nj}\mathbf{u}<em>{j}^{\mathrm T} \boldsymbol{\phi}<em>n \right) \left(\sum_k y</em>{nk}\mathbf{u}</em>{k}^{\mathrm T} \boldsymbol{\phi}<em>n \right) \right} \
&amp;= \sum_n\left{\sum_k y</em>{nk}(\mathbf{u}<em>{k}^{\mathrm T} \boldsymbol{\phi}<em>n)^2 - \left( \sum_k y</em>{nk} \mathbf{u}</em>{k}^{\mathrm T} \boldsymbol{\phi}_n \right)^2
\right} \
\end{aligned}
$$</p>
<p>ここで、$0\le y_{nk} \le 1, \sum_{k}y_{nk}=1$であり、<a href="https://ja.wikipedia.org/wiki/%E3%82%A4%E3%82%A7%E3%83%B3%E3%82%BB%E3%83%B3%E3%81%AE%E4%B8%8D%E7%AD%89%E5%BC%8F">イェンセンの不等式</a>を凸関数$f(x)=x^2$に対して適用する。$x = \mathbf{u}<em>{k}^{\mathrm T} \boldsymbol{\phi}<em>n$とすれば
$$
\sum_k y</em>{nk}(\mathbf{u}</em>{k}^{\mathrm T} \boldsymbol{\phi}<em>n)^2 \ge \left( \sum_k y</em>{nk} \mathbf{u}_{k}^{\mathrm T} \boldsymbol{\phi}_n \right)^2
$$
が成立する。以上から$\mathbf{u}^{\mathrm T}\mathbf{Hu} \ge 0$となり、多クラスロジスティック回帰問題に対するヘッセ行列$\mathbf{H}$が半正定値行列であることが示された。</p>
<h2 id="演習-421"><a class="header" href="#演習-421">演習 4.21</a></h2>
<div class="panel-primary">
<p>プロビット関数の逆関数
$$
\Phi(a)=\int_{-\infty}^{a} \mathcal{N}(\theta \mid 0,1) \mathrm{d} \theta \tag{4.114}
$$
とerf関数
$$
\operatorname{erf}(a)=\frac{2}{\sqrt{\pi}} \int_{0}^{a} \exp \left(-\theta^{2}\right) \mathrm{d} \theta \tag{4.115}
$$
が
$$
\Phi(a)=\frac{1}{2}\left{1+\operatorname{erf}\left(\frac{a}{\sqrt{2}}\right)\right} \tag{4.116}
$$
によって関連付けられることを示せ．</p>
</div>
<p>$$
\begin{aligned}
\mathbf{\Phi}(a) &amp;= \int_{-\infty}^{a} \mathcal{N}(\theta \mid 0,1)d\theta\
&amp;=\int_{-\infty}^{0} \mathcal{N}(\theta \mid 0,1)d\theta + \int_{0}^{a} \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{\theta^2}{2}\right)d\theta \
&amp;= \frac{1}{2} + \int_{0}^{a} \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{\theta^2}{2}\right)d\theta \quad \left(\because\quad \int_{-\infty}^{\infty} \mathcal{N}(\theta \mid 0,1)d\theta = 1 \right)
\end{aligned}
$$
ここで$\displaystyle \frac{\theta}{\sqrt{2}} = x$とおくと
$$
\begin{aligned}
\mathbf{\Phi}(a) &amp;= \frac{1}{2}+\frac{1}{\sqrt{\pi}}\int_{0}^{\frac{a}{\sqrt{2}}} \exp(-x^2)dx \
&amp;= \frac{1}{2}\left{1+\mathrm{erf}\left(\frac{a}{\sqrt{2}}\right)\right}
\end{aligned}
$$</p>
<p>となり、$(4.116)$式が示された。</p>
<h2 id="演習-422"><a class="header" href="#演習-422">演習 4.22</a></h2>
<div class="panel-primary">
<p>$$
\begin{aligned}
Z &amp;=\int f(\mathbf{z}) \mathrm{d} \mathbf{z} \
&amp; \simeq f\left(\mathbf{z}<em>{0}\right) \int \exp \left{-\frac{1}{2}\left(\mathbf{z}-\mathbf{z}</em>{0}\right)^{\mathrm{T}} \mathbf{A}\left(\mathbf{z}-\mathbf{z}<em>{0}\right)\right} \mathrm{d} \mathbf{z} \
&amp;=f\left(\mathbf{z}</em>{0}\right) \frac{(2 \pi)^{M / 2}}{|\mathbf{A}|^{1 / 2}}
\end{aligned} \tag{4.135}
$$
を使って，ラプラス近似の下での対数モデルエビデンスに対する表現
$$
\ln p(\mathcal{D}) \simeq \ln p\left(\mathcal{D} \mid \theta_{\mathrm{MAP}}\right)+\underbrace{\ln p\left(\theta_{\mathrm{MAP}}\right)+\frac{M}{2} \ln (2 \pi)-\frac{1}{2} \ln |\mathbf{A}|}_{\text {Occam 係数 }} \tag{4.137}
$$
を導出せよ．</p>
</div>
<p>P.216参照。$(4.136)$式を$(4.135)$式に代入すると</p>
<p>$$
\begin{aligned}
p(\mathcal{D}) &amp;=\int p(\mathcal{D} \mid \boldsymbol{\theta}) p(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta} \
&amp; \simeq p\left(\mathcal{D} \mid \boldsymbol{\theta}<em>{\mathrm{MAP}}\right) p\left(\boldsymbol{\theta}</em>{\mathrm{MAP}}\right) \
&amp; \int \exp \left{-\frac{1}{2}\left(\boldsymbol{\theta}-\boldsymbol{\theta}<em>{\mathrm{MAP}}\right) \mathbf{A}\left(\boldsymbol{\theta}-\boldsymbol{\theta}</em>{\mathrm{MAP}}\right)\right} \mathrm{d} \boldsymbol{\theta} \
&amp;=p\left(\mathcal{D} \mid \boldsymbol{\theta}<em>{\mathrm{MAP}}\right) p\left(\boldsymbol{\theta}</em>{\mathrm{MAP}}\right) \frac{(2 \pi)^{M / 2}}{|\mathbf{A}|^{1 / 2}}
\end{aligned}
$$</p>
<p>これより、両辺の対数を取ると
$$
\ln p(\mathcal{D}) \simeq \ln p\left(\mathcal{D} \mid \boldsymbol{\theta}<em>{\mathrm{MAP}}\right)+\underbrace{\ln p\left(\theta</em>{\mathrm{MAP}}\right)+\frac{M}{2} \ln (2 \pi)-\frac{1}{2} \ln |\mathbf{A}|}_{\text {Occam 係数 }} \tag{4.137}
$$
が得られる。</p>
<h2 id="演習-423"><a class="header" href="#演習-423">演習 4.23</a></h2>
<div class="panel-primary">
<p>この演習問題では$(4.137)$で与えられるモデルエビデンスに対し，ラプラス近似から始めて，BICの結果
$$
\ln p(\mathcal{D}) \simeq \ln p\left(\mathcal{D} \mid \theta_{\mathrm{MAP}}\right)-\frac{1}{2} M \ln N \tag{4.139}
$$
を導出する．パラメータ上での事前確率が$p(\theta)=\mathcal{N}\left(\boldsymbol{\theta} \mid \mathbf{m}, \mathbf{V}_{0}\right)$形式のガウス分布のとき，ラプラス近似の下での対数モデルエビデンスが以下の式となることを示せ．</p>
<p>$$
\ln p(\mathcal{D}) \simeq \ln p\left(\mathcal{D} \mid \boldsymbol{\theta}<em>{\mathrm{MAP}}\right)-\frac{1}{2}\left(\boldsymbol{\theta}</em>{\mathrm{MAP}}-\mathbf{m}\right)^{\mathrm{T}} \mathbf{V}<em>{0}^{-1}\left(\boldsymbol{\theta}</em>{\mathrm{MAP}}-\mathbf{m}\right)-\frac{1}{2} \ln |\mathbf{H}|+\text {const.}
$$</p>
<p>ここで，$\mathbf{H}$は$\boldsymbol{\theta}<em>{\mathrm{MAP}}$で評価された負の対数尤度$\ln p(\mathcal{D}|\boldsymbol{\theta})$の2階微分の行列である．事前確率が広い幅を持っている，つまり，$\mathbf{V}</em>{0}^{-1}$が小さく，上式右辺第2項が無視できると仮定する．さらに，$\mathbf{H}$が各データ点に対応する項の和で書けるように独立同時分布(i.i.d)の場合を考える．その場合，対数モデルエビデンスが近似的にBIC表現$(4.139)$の式で書けることを示せ．</p>
</div>
<p>$(4.137)$式における$\mathbf{A}$は事後分布の負の対数の2階微分であるヘッセ行列であり、
$$
\mathbf{A} = -\nabla\nabla \ln p(\mathcal{D} \mid \boldsymbol{\theta}<em>{\mathrm{MAP}})p(\boldsymbol{\theta}</em>{\mathrm{MAP}}) = -\nabla\nabla\ln p(\boldsymbol{\theta}<em>{\mathrm{MAP}} \mid \mathcal{D})
$$
である。ここで、問題文から$\mathbf{H}$は$\boldsymbol{\theta}</em>{\mathrm{MAP}}$で評価された負の対数尤度$\ln p(\mathcal{D} \mid \boldsymbol{\theta}<em>{\mathrm{MAP}})$の2階微分の行列なので
$$
\mathbf{H} = -\nabla\nabla\ln p(\mathcal{D} \mid \boldsymbol{\theta}</em>{\mathrm{MAP}})
$$
であり、この2式から
$$
\mathbf{A} = \mathbf{H} -\nabla\nabla\ln p(\boldsymbol{\theta}_{\mathrm{MAP}})
$$
となる。今、$p(\boldsymbol{\theta}) = \mathcal{N}(\boldsymbol{\theta}\mid \mathbf{m},\mathbf{V}_0)$とすると</p>
<p>$$
\begin{aligned}
-\nabla\nabla\ln p(\boldsymbol{\theta}_{\mathrm{MAP}}) &amp;= -\nabla\nabla \ln \left[ \frac{1}{(2\pi)^{M/2}|\mathbf{V}<em>0|^{1/2}} \exp\left{ -\frac{1}{2} (\boldsymbol{\theta}</em>{\mathrm{MAP}}-\mathbf{m})^{\mathrm T} \mathbf{V}<em>0^{-1} (\boldsymbol{\theta}</em>{\mathrm{MAP}}-\mathbf{m}) \right} \right] \
&amp;=\mathbf{V}_0^{-1}
\end{aligned}
$$
なので、$\mathbf{A} = \mathbf{H} + \mathbf{V}_0^{-1}$となる。</p>
<p>問題文の通り、もし事前分布が十分に広い幅を持っていれば（またはデータの数$N$が多ければ）$\mathbf{V}_0^{-1}$は$\mathbf{H}$に比べ十分に小さくなるので$\mathbf{A} \simeq \mathbf{H}$と近似できる。$(4.137)$式にこれを代入すれば</p>
<p>$$
\ln p(\mathcal{D}) \simeq \ln p\left(\mathcal{D} \mid \theta_{\mathrm{MAP}}\right)-\frac{1}{2}\left(\theta_{\mathrm{MAP}}-\mathbf{m}\right)^{\mathrm{T}} \mathbf{V}<em>{0}^{-1}\left(\theta</em>{\mathrm{MAP}}-\mathbf{m}\right)-\frac{1}{2} \ln |\mathbf{H}|+\text { const. }
$$</p>
<p>が得られる。</p>
<p>再び$\boldsymbol{\theta}$の事前分布が平坦であるという仮定を用いると、上式の第1項に対して第2項を無視できる。またデータ点がi.i.d.であると仮定すると、ヘッセ行列を各データ点からの寄与$\mathbf{H}<em>n$の和で表すことができる。また、その和をデータ点の数$N$と各データ点からの寄与の平均値$\hat{\mathbf{H}}$を用いて表すこともできる。
$$
\mathbf{H} = \sum</em>{n=1}^N \mathbf{H}<em>n = N\hat{\mathbf{H}}, \quad \hat{\mathbf{H}} = \frac{1}{N}\sum</em>{n=1}^N \mathbf{H}_n
$$
すると、$\mathbf{H}$は$M \times M$（$M$は$\boldsymbol{\theta}$のパラメータ数）の行列で、非退化（フルランク）であると仮定するなら
$$
\ln |\mathbf{H}| = \ln |N\hat{\mathbf{H}}| = \ln(N^M |\hat{\mathbf{H}}|) = M\ln N + \ln |\hat{\mathbf{H}}|
$$</p>
<p>第2項は$\ln N$と比較して$O(1)$なので無視できる(ここよくわからない)ので、最終的に$(4.139)$式</p>
<p>$$
\ln p(\mathcal{D}) \simeq \ln p\left(\mathcal{D} \mid \theta_{\mathrm{MAP}}\right)-\frac{1}{2} M \ln N \tag{4.139}
$$</p>
<p>のように粗く近似することができる。これが <strong>ベイズ情報量規準 (BIC)</strong> である。</p>
<h2 id="演習-424"><a class="header" href="#演習-424">演習 4.24</a></h2>
<div class="panel-primary">
<p>2.3.2節からの結果を利用して，パラメータ$\mathbf{w}$の事後確率分布がガウス分布である場合のロジスティック回帰モデルの周辺化に対する結果
$$
p\left(\mathcal{C}<em>{1} \mid \mathbf{t}\right)=\int \sigma(a) p(a) \mathrm{d} a=\int \sigma(a) \mathcal{N}\left(a \mid \mu</em>{a}, \sigma_{a}^{2}\right) \mathrm{d} a \tag{4.151}
$$
を導け．</p>
</div>
<p>※P.218 4.5.2 予測分布のところで、「デルタ関数は$\mathbf{w}$に線形制約を科し，$\boldsymbol{\phi}$に直交するすべての方向に積分することで，同時分布$q(\mathbf{w})$から周辺分布を形成することに留意して，$p(a)$を評価することができる．」という記述がある。そこで、$M$次元空間を回転させ、$\boldsymbol{\phi}$に平行な$\mathbf{w}$の成分を$w_{\parallel}$とし、$\boldsymbol{\phi}$に直交する$M-1$個の成分をまとめて$\mathbf{w}_{\perp}$とする。すなわち</p>
<p>$$
a = \mathbf{w}^{\mathrm T}\boldsymbol{\phi} = w_{\parallel}|\boldsymbol{\phi}|,\quad \textrm{where}\ \mathbf{w}^{\mathrm T} = (w_{\parallel}, \mathbf{w}_{\perp})
$$</p>
<p>とする。これを用いて$(4.147)$式に代入すると
$$
\begin{aligned}
\int \sigma\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\right) q(\mathbf{w}) \mathrm{d} \mathbf{w}
&amp;=\iint \sigma\left(w_{\parallel}|\boldsymbol{\phi}|\right) q\left(w_{\parallel}, \mathbf{w}<em>{\perp}\right) \mathrm{d} w</em>{\parallel} \mathrm{d} \mathbf{w}<em>{\perp} \
&amp;=\int \sigma\left(w</em>{\parallel}|\boldsymbol{\phi}|\right) \int q\left(w_{\parallel}, \mathbf{w}<em>{\perp} \right) \mathrm{d}\mathbf{w}</em>{\perp} \mathrm{d} w_{\parallel} \
&amp;=\int \sigma\left(w_{\parallel}|\boldsymbol{\phi}|\right) q\left(w_{|}\right) \mathrm{d} w_{\parallel} \
&amp;=\int \sigma(a) q(w_{|}) \mathrm{d} w_{\parallel}
\end{aligned}
$$</p>
<p>ここで、同時確率分布$q(w_{\parallel}, \mathbf{w}<em>{\perp})$はガウス分布なので、2.3.2節の内容から、周辺分布$q(w</em>{\parallel})$もガウス分布である。すなわち</p>
<p>$$
q(w_{\parallel}) = \mathcal{N}(w_{\parallel}\mid \mu, \sigma^2)
$$
のような形で記述することができる。</p>
<p>上式は$w_{\parallel}$についてのスカラー量になっているので、行列形式で書くために単位ベクトル</p>
<p>$$
\mathbf{e} = \frac{\boldsymbol{\phi}}{|\boldsymbol{\phi}|}
$$
を導入すると、$\mu = \mathbf{e}^{\mathrm T}\mathbf{m}<em>{\mathrm{MAP}}, \sigma^2 = \mathbf{e}^{\mathrm T}\mathbf{S}</em>{\mathrm{MAP}}\mathbf{e}$となるような$\mathbf{m}<em>{\mathrm{MAP}}, \mathbf{S}</em>{\mathrm{MAP}}$が存在する。</p>
<p>$$
q(w_{\parallel}) = \mathcal{N}(w_{\parallel}\mid \mu, \sigma^2) = |\boldsymbol{\phi}|\mathcal{N}(a\mid \boldsymbol{\phi}^{\mathrm T}\mathbf{m}<em>{\mathrm{MAP}}, \boldsymbol{\phi}^{\mathrm T}\mathbf{S}</em>{\mathrm{MAP}}\boldsymbol{\phi})
$$</p>
<p>また、$a = w_{\parallel}|\boldsymbol{\phi}|$より$|\boldsymbol{\phi}|\mathrm{d}w_{\parallel} = \mathrm{d}a$を利用すると</p>
<p>$$
\begin{aligned}
\int \sigma(a) q(w_{|}) \mathrm{d} w_{\parallel} &amp;= |\boldsymbol{\phi}| \int \sigma(a) \mathcal{N}(a\mid \boldsymbol{\phi}^{\mathrm T}\mathbf{m}<em>{\mathrm{MAP}}, \boldsymbol{\phi}^{\mathrm T}\mathbf{S}</em>{\mathrm{MAP}}\boldsymbol{\phi})\mathrm{d}w_{\parallel} \
&amp;= \int \sigma(a) \mathcal{N}(a\mid \boldsymbol{\phi}^{\mathrm T}\mathbf{m}<em>{\mathrm{MAP}}, \boldsymbol{\phi}^{\mathrm T}\mathbf{S}</em>{\mathrm{MAP}}\boldsymbol{\phi})\mathrm{d}a
\end{aligned}
$$</p>
<p>最後に$\mu_a = \boldsymbol{\phi}^{\mathrm T}\mathbf{m}<em>{\mathrm{MAP}}, \sigma^{2}</em>{a} = \boldsymbol{\phi}^{\mathrm T}\mathbf{S}_{\mathrm{MAP}}\boldsymbol{\phi}$とすれば、$(4.151)$式が導かれる。</p>
<h2 id="演習-425"><a class="header" href="#演習-425">演習 4.25</a></h2>
<div class="panel-primary">
<p>$$
\sigma(a)=\frac{1}{1+\exp (-a)} \tag{4.59}
$$
で定義されるロジスティックシグモイド関数$\sigma(a)$をスケールパラメータを持つプロビット関数の逆関数$\mathrm{\Phi}(\lambda a)$で近似するとする．ここで，$\mathrm{\Phi}(a)$は
$$
\Phi(a)=\int_{-\infty}^{a} \mathcal{N}(\theta \mid 0,1) \mathrm{d} \theta \tag{4.114}
$$
で定義される．2つの関数の微分が$a=0$で等しいように$\lambda$を選ぶ場合，$\lambda^2 = \pi /8$となることを示せ．</p>
</div>
<p>※大学入試にありそうな問題。</p>
<p>ロジスティックシグモイド$\sigma(a)$の$a=0$での微分$\displaystyle \left. \frac{d\sigma}{da}\right|<em>{a=0}$とプロビット関数の逆関数$\Phi(\lambda a)$の$a=0$での微分$\displaystyle \left. \frac{d\Phi(\lambda a)}{da}\right|</em>{a=0}$が一致すれば良い。</p>
<p>$$
\left. \frac{d\sigma}{da} \right|<em>{a=0} = \left. \frac{e^{-a}}{(1+e^{-a})^2}\right|</em>{a=0} = \frac{1}{4} \tag{1}
$$</p>
<p>$$
\begin{aligned}
\left. \frac{d\Phi(\lambda a)}{da}\right|<em>{a=0} &amp;= \left. \frac{d(\lambda a)}{da} \frac{d}{d(\lambda a)}\int</em>{-\infty}^{\lambda a} \mathcal{N}(\theta\mid 0, 1)d\theta\right|<em>{a=0} \
&amp;= \left. \lambda \mathcal{N}(\lambda a\mid 0, 1)\right|</em>{a=0} \
&amp;= \left. \frac{\lambda}{\sqrt{2\pi}} \exp\left( -\frac{(\lambda a)^2}{2} \right)\right|_{a=0} \
&amp;= \frac{\lambda}{\sqrt{2\pi}}
\end{aligned}\tag{2}
$$</p>
<p>$(1), (2)$が一致するので、</p>
<p>$$
\lambda = \frac{\sqrt{2\pi}}{4},\quad \lambda^2 = \frac{\pi}{8}
$$</p>
<p>となる。</p>
<h2 id="演習-426"><a class="header" href="#演習-426">演習 4.26</a></h2>
<div class="panel-primary">
<p>この演習問題で，ガウス分布とプロビット関数の逆関数のたたみ込みに対する関係
$$
\int \Phi(\lambda a) \mathcal{N}\left(a \mid \mu, \sigma^{2}\right) \mathrm{d} a=\Phi\left(\frac{\mu}{\left(\lambda^{-2}+\sigma^{2}\right)^{1 / 2}}\right) \tag{4.152}
$$
を証明する．この証明を行うため，$\mu$に関する左辺の微分が右辺の微分に等しいことを示せ．また，$\mu$に関して両辺を積分して，積分定数が消えることを示せ．左辺を微分する前に，$a$での積分を$z$での積分で置き換えるように変数変換$a=\mu+\sigma z$を導入すれば便利である．関係$(4.152)$の左辺を微分すれば解析的に評価可能な$z$に関するガウス分布の積分が得られる．</p>
</div>
<p>※右辺の$\mu$についての微分は簡単なのでそちらから求める。</p>
<p>$$
\frac{d}{d \mu} \Phi\left(\frac{\mu}{\left(\lambda^{-2}+\sigma^{2}\right)^{1 / 2}}\right)=\frac{1}{\sqrt{2 \pi}} \frac{1}{\left(\lambda^{-2}+\sigma^{2}\right)^{1 / 2}} \exp \left{-\frac{\mu^{2}}{2\left(\lambda^{-2}+\sigma^{2}\right)}\right} \tag{1}
$$</p>
<p>次に左辺について、左辺の式を$f(a)$とおき、誘導に従って$a = \mu + \sigma\lambda$を導入すると</p>
<p>$$
\begin{aligned}
f(a)&amp;=\int \Phi(\lambda a) \mathcal{N}\left(a \mid \mu, \sigma^{2}\right) d a \
&amp;=\int \Phi(\lambda(\mu+\sigma z)) \mathcal{N}\left(\mu+\sigma z \mid \mu, \sigma^{2}\right) \sigma d z \quad (\because da = \sigma dz)\
&amp;=\int \Phi(\lambda \mu+\lambda \sigma z) \frac{1}{\left(2 \pi \sigma^{2}\right)^{1/2}} \exp \left{-\frac{1}{2} z^{2}\right} \sigma d z
\end{aligned}
$$</p>
<p>$f(a)$に対し$\mu$についての微分を実行する。ここで</p>
<p>$$
\frac{\partial}{\partial \mu}\Phi(\lambda(\mu + \sigma z)) = \frac{\partial (\lambda(\mu + \sigma z))}{\partial\mu}\frac{\partial \Phi(\lambda(\mu + \sigma z))}{\partial (\lambda(\mu + \sigma z))} = \lambda \mathcal{N}(\lambda(\mu + \sigma z))
$$</p>
<p>であることに注意すると</p>
<p>$$
\begin{aligned}
\frac{\partial f}{\partial \mu} &amp;=\int \lambda \mathcal{N}(\lambda \mu+\lambda \sigma z) \frac{1}{\sqrt{2 \pi}} \exp \left{-\frac{1}{2} z^{2}\right} d z \
&amp;=\frac{\lambda}{2 \pi} \int \exp \left{-\frac{1}{2} z^{2}-\frac{\lambda^{2}}{2}(\mu+\sigma z)^{2}\right} d z
\end{aligned}
$$</p>
<p>ここで、積分記号の中がガウス分布の形になるように$z$について平方完成を行うと、積分記号の中は</p>
<p>$$
\begin{aligned}
-\frac{1}{2}\left(z^{2}+\lambda^{2}(\mu+\sigma z)^{2}\right) &amp;=-\frac{1}{2}\left{\left(1+\lambda^{2} \sigma^{2}\right) z^{2}+2 \lambda^{2} \mu \sigma z+\lambda^{2} \mu^{2}\right} \
&amp;=-\frac{1+\lambda^{2} \sigma^{2}}{2}\left{\left(z+\frac{\lambda^{2} u \sigma}{1+\lambda^{2} \sigma^{2}}\right)^{2}-\frac{\lambda^{4} \mu^{2} \sigma^{2}}{\left(1+\lambda^{2} \sigma^{2}\right)^{2}}+\frac{\lambda^{2} \mu^{2}}{1+\lambda^{2} \sigma^{2}}\right} \
&amp;=-\frac{1+\lambda^{2} \sigma^{2}}{2}\left(z+\frac{\lambda^{2} \mu \sigma}{1+\lambda^{2} \sigma^{2}}\right)^{2}-\frac{\lambda^{2} u^{2}}{2\left(1+\lambda^{2} \sigma^{2}\right)}
\end{aligned}
$$</p>
<p>となる。さらに途中でガウス分布の正規化定数$\displaystyle \int \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right) dz = \sqrt{2\pi \sigma^2}$を利用することで</p>
<p>$$
\begin{aligned}
\frac{\partial f}{\partial \mu} &amp;=\frac{\lambda}{2 \pi} \int \exp \left{-\frac{1+\lambda^{2} \sigma^{2}}{2}\left(z+\frac{\lambda^{2} \mu \sigma}{1+\lambda^{2} \sigma^{2}}\right)^{2}-\frac{\lambda^{2} u^{2}}{2\left(1+\lambda^{2} \sigma^{2}\right)}\right} d z \
&amp;=\frac{\lambda}{2 \pi} \exp \left(-\frac{\lambda^{2} u^{2}}{2\left(1+\lambda^{2} \sigma^{2}\right)}\right) \underbrace{\int \exp \left{-\frac{1+\lambda^{2} \sigma^{2}}{2}\left(z+\frac{\lambda^{2} \mu \sigma}{1+\lambda^{2} \sigma^{2}}\right)^{2}\right}}_{\textrm{Gaussian integral }} d z \
&amp;=\frac{\lambda}{2 \pi} \exp \left(-\frac{\lambda^{2} u^{2}}{2\left(1+\lambda^{2} \sigma^{2}\right)}\right) \sqrt{\frac{2 \pi}{1+\lambda^{2} \sigma^{2}}} \
&amp;=\frac{\lambda}{\sqrt{2 \pi}} \frac{1}{\left(1+\lambda^{2} \sigma^{2}\right)^{1 / 2}} \exp \left(\frac{-\lambda^{2} \mu^{2}}{2\left(1+\lambda^{2} \sigma^{2}\right)}\right) \
&amp;=\frac{1}{\sqrt{2 \pi}} \frac{1}{\left(\lambda^{-2}+\sigma^{2}\right)^{1 / 2}} \exp \left{-\frac{\mu^{2}}{2\left(\lambda^{-2}+\sigma^{2}\right)}\right}
\end{aligned} \tag{2}
$$</p>
<p>$(1)$, $(2)$の結果から両辺の$\mu$に関する微分が等しいことが示された。</p>
<p>また、$(4.152)$の両辺について$\mu \to -\infty$を考えると両辺とも$0$となる。これは積分定数が$0$であることを示し、微分形が一致することと合わせると、$(4.152)$式の等号が成立することが示された。</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="prml第5章演習問題解答"><a class="header" href="#prml第5章演習問題解答">PRML第5章演習問題解答</a></h1>
<head>
<style>
  div.panel-primary {
	border: 1px solid #000;
    margin: 10px 5px;
    padding: 16px 10px 0px;
  }
</style>
</head>
<h2 id="演習-51"><a class="header" href="#演習-51">演習 5.1</a></h2>
<div class="panel-primary">
<p>$$
y_{k}(\mathbf{x}, \mathbf{w})=\sigma\left(\sum_{j=1}^{M} w_{k j}^{(2)} h\left(\sum_{i=1}^{D} w_{j i}^{(1)} x_{i}+w_{j 0}^{(1)}\right)+w_{k 0}^{(2)}\right) \tag{5.7}
$$
の形の2層ネットワーク関数で隠れユニットの非線形活性化関数がロジスティックシグモイド関数</p>
<p>$$
\sigma(a) = {1+ \exp(-a)}^{-1} \tag{5.191}
$$</p>
<p>で与えられるものを考える．これと等価なネットワーク，すなわち全く同じ関数を計算するが，隠れユニットの活性化関数が$\tanh(a)$で与えられるものが存在することを示せ．ただし，$\tanh$関数は
$$
\tanh (a)=\frac{e^{a}-e^{-a}}{e^{a}+e^{-a}} \tag{5.59}
$$
で定義される．</p>
<p>ヒント：まず始めに$\sigma(a)$と$\tanh(a)$の関係を求め，次に2つのネットワークのパラメータの違いは線形変換であることを示す．</p>
</div>
<p>演習問題3.1と同様。まず$\tanh(a)$関数の定義から、シグモイド関数$\sigma(a)$との関係式は
$$
\begin{aligned}
\tanh (a) &amp;=\frac{e^{a}-e^{-a}}{e^{a}+e^{-a}} \
&amp;=-1+\frac{2 e^{a}}{e^{a}+e^{-a}} \
&amp;=-1+2 \frac{1}{1+e^{-2 a}} \
&amp;=2 \sigma(2 a)-1
\end{aligned}
$$
で表される（$(3.100)$式）。</p>
<p>ここで、$w_{ji}^{(1s)}$と$w_{j0}^{(1s)}, w_{kj}^{(2s)}$と$w_{k0}^{(2s)}$をネットワークのシグモイド活性化関数の重みパラメータ、$w_{ji}^{(1t)}$と$w_{j0}^{(1t)}, w_{kj}^{(2t)}$と$w_{k0}^{(2t)}$を$\tanh(a)$活性化関数を使う場合の重みパラメータとする。テキストの$(5.4)$式$\displaystyle a_{k}=\sum_{j=1}^{M} w_{k j}^{(2)} z_{j}+w_{k 0}^{(2)}$を使うと以下のように書き下せる。</p>
<p>$$
\begin{aligned}
a_{k}^{(t)} &amp;=\sum_{j=1}^{M} w_{k j}^{(2 t)} \tanh \left(a_{j}^{(t)}\right)+w_{k 0}^{(2 t)} \
&amp;=\sum_{j=1}^{M} w_{k j}^{(2 t)}\left[2 \sigma\left(2 a_{j}^{(t)}\right)-1\right]+w_{k 0}^{(2 t)} \
&amp;=\sum_{j=1}^{M} 2 w_{k j}^{(2 t)} \sigma\left(2 a_{j}^{(t)}\right)+\left[-\sum_{j=1}^{M} w_{k j}^{(2 t)}+w_{k 0}^{(2 t)}\right]
\end{aligned}
$$</p>
<p>一方、シグモイド活性化関数を使う場合だと</p>
<p>$$
a_{k}^{(s)}=\sum_{j=1}^{M} w_{k j}^{(2 s)} \sigma\left(a_{j}^{(s)}\right)+w_{k 0}^{(2 s)}
$$
のように書ける。</p>
<p>これらのネットワークが等価になるとき、すなわち$a_{k}^{(t)} = a_{k}^{(s)}$であるならば、これら2つの式を比較して</p>
<p>$$
\left{\begin{array}{l}
a_{j}^{(s)}=2 a_{j}^{(t)} \
w_{k j}^{(2 s)}=2 w_{k j}^{(2 t)} \
w_{k 0}^{(2 s)}=-\sum_{j=1}^{M} w_{k j}^{(2 t)}+w_{k 0}^{(2 t)}
\end{array}\right.
$$</p>
<p>と変換すれば等価になることがわかる。ここで、1つ目の条件は以下のように設定することで達成される。
$$
w_{j i}^{(1 s)}=2 w_{j i}^{(1 t)}, \quad \text { and } \quad w_{j 0}^{(1 s)}=2 w_{j 0}^{(1 t)}
$$</p>
<p>以上から、シグモイド活性化関数と$\tanh$活性化関数の2つのネットワークは、線形変換において等価であることが示された。</p>
<h2 id="演習-52"><a class="header" href="#演習-52">演習 5.2</a></h2>
<div class="panel-primary">
<p>複数の出力を持つニューラルネットワークについて，条件付き分布
$$
p(\mathbf{t} \mid \mathbf{x}, \mathbf{w})=\mathcal{N}\left(\mathbf{t} \mid \mathbf{y}(\mathbf{x}, \mathbf{w}), \beta^{-1} \mathbf{I}\right) \tag{5.16}
$$
の尤度関数最大化は，二乗和誤差関数
$$
E(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N}\left|\mathbf{y}\left(\mathbf{x}<em>{n}, \mathbf{w}\right)-\mathbf{t}</em>{n}\right|^{2} \tag{5.11}
$$
の<s>最大化</s>最小化 <strong>（※おそらく誤植）</strong> と等価であることを示せ．</p>
</div>
<p>尤度関数は
$$
\prod_{n=1}^N \mathcal{N}\left(\mathbf{t} \mid \mathbf{y}(\mathbf{x}, \mathbf{w}), \beta^{-1} \mathbf{I}\right)
$$
と書けるから、負の対数尤度は、$K$を目標変数$\mathbf{t}$の数として</p>
<p>$$
\begin{aligned}
&amp;-\sum_{n=1}^{N} \ln \mathcal{N}\left(\mathbf{t} \mid \mathbf{y}(\mathbf{x}, \mathbf{w}), \beta^{-1} \mathbf{I}\right) \
=&amp;-\sum_{n=1}^{N} \ln \left{\frac{1}{(2 \pi)^{K / 2}} \frac{1}{\left(\beta^{-1}\right)^{K / 2}} \exp \left(-\frac{\beta}{2}(\mathbf{t}_n-\mathbf{y}(\mathbf{x}_n, \mathbf{w}))^{\mathrm T} \mathbf{I} (\mathbf{t}_n-\mathbf{y}(\mathbf{x}<em>n, \mathbf{w}))\right)\right} \
=&amp;-\sum</em>{n=1}^{N}\left{\frac{K}{2} \ln \frac{\beta}{2 \pi}-\frac{\beta}{2}|\mathbf{t}_n-\mathbf{y}(\mathbf{x}<em>n, \mathbf{w})|^{2}\right} \
=&amp; -\frac{N K}{2} \ln \frac{\beta}{2 \pi}+\frac{\beta}{2} \sum</em>{n=1}^{N}|\mathbf{y}(\mathbf{x}_n, \mathbf{w})-\mathbf{t}_n|^{2}
\end{aligned}
$$</p>
<p>となる。$(5.11)$は上式の第2項の定数$\beta$倍であることがわかる。また、第1項は定数である。
尤度関数の最大化は負の対数尤度の最小化と等価であるから、二乗和誤差関数$(5.11)$式の最小化に等しいことが示された。</p>
<h2 id="演習-53"><a class="header" href="#演習-53">演習 5.3</a></h2>
<div class="panel-primary">
<p>複数の目標変数を持ち，入力ベクトル$\mathbf{x}$を固定したときの目標変数の分布が</p>
<p>$$
p(\mathbf{t} \mid \mathbf{x}, \mathbf{w})=\mathcal{N}(\mathbf{t} \mid \mathbf{y}(\mathbf{x}, \mathbf{w}), \mathbf{\Sigma}) \tag{5.192}
$$</p>
<p>という形のガウス関数であるような回帰問題を考える．ここで，$\mathbf{y}(\mathbf{x}, \mathbf{w})$は入力べクトル$\mathbf{x}$，重みベクトル$\mathbf{w}$を持つニューラルネットワークの出力であり，$\mathbf{\Sigma}$は目標値の想定されたガウスノイズの共分散である．$\mathbf{x}$と$\mathbf{t}$の独立な観測値の集合が与えられたとき，$\mathbf{\Sigma}$は固定で既知と仮定して，$\mathbf{w}$に関する最尤推定解を見つけるための最小化すべき誤差関数を書き下せ．さらに，$\mathbf{\Sigma}$もまたデータから決定すべきと仮定し，$\mathbf{\Sigma}$の最尤推定解の式を書き下せ．ここでは5.2節で議論した独立な目標変数の場合と異なり，$\mathbf{w}$と$\mathbf{\Sigma}$の最適化が連結されている点に注意せよ．</p>
</div>
<p>※後半の$\mathbf{\Sigma}$の最尤推定解を求める部分は演習問題2.34とほぼ同じなのでこちらを参照。</p>
<p>この場合の尤度関数は</p>
<p>$$
L=\prod_{n=1}^{N} \mathcal{N}\left(\mathbf{t}_{n} \mid \mathbf{y}(\mathbf{x}_n, \mathbf{w})), \mathbf{\Sigma}\right)
$$</p>
<p>で与えられるので、対数尤度関数は</p>
<p>$$
\begin{aligned}
\ln L &amp;=\sum_{n=1}^{N} \ln \mathcal{N}\left(\mathbf{t}<em>{n} \mid \mathbf{y}(\mathbf{x}<em>n, \mathbf{w}), \mathbf{\Sigma}\right) \
&amp;=-\frac{NK}{2}\ln(2\pi)-\frac{N}{2} \ln |\mathbf{\Sigma}|-\frac{1}{2} \sum</em>{n=1}^{N}\left(\mathbf{t}</em>{n}-\mathbf{y}(\mathbf{x}<em>n, \mathbf{w})\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\mathbf{t}</em>{n}-\mathbf{y}(\mathbf{x}_n, \mathbf{w})\right)
\end{aligned}
$$</p>
<p>となる。</p>
<p>今、$\mathbf{\Sigma}$は固定で既知と仮定すると、上式より最小化すべき誤差関数は
$$
E(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N}\left(\mathbf{t}_{n}-\mathbf{y}(\mathbf{x}<em>n, \mathbf{w})\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\mathbf{t}</em>{n}-\mathbf{y}(\mathbf{x}_n, \mathbf{w})\right)
$$</p>
<p>である。</p>
<p>次に$\mathbf{\Sigma}$の最尤推定解の式を求めるために$\mathbf{\Sigma}$について偏微分すると、$\mathbf{y}_n = \mathbf{y}(\mathbf{x}_n, \mathbf{w})$として</p>
<p>$$
\begin{aligned}
\frac{\partial}{\partial \mathbf{\Sigma}} \ln L &amp;=-\frac{N}{2} \frac{\partial}{\partial \mathbf{\Sigma}} \ln |\mathbf{\Sigma}|-\frac{1}{2} \frac{\partial}{\partial \mathbf{\Sigma}} \sum_{n=1}^{N}\left(\mathbf{t}<em>{n}-\mathbf{y}</em>{n}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\mathbf{t}<em>{n}-\mathbf{y}</em>{n}\right) \
&amp;=-\frac{N}{2} \mathbf{\Sigma}^{-1}-\frac{1}{2} \frac{\partial}{\partial \mathbf{\Sigma}} \sum_{n=1}^{N} \operatorname{Tr}\left{\mathbf{\Sigma}^{-1}\left(\mathbf{t}<em>{n}-\mathbf{y}</em>{n}\right)\left(\mathbf{t}<em>{n}-\mathbf{y}</em>{n}\right)^{\mathrm{T}}\right} \
&amp;=-\frac{N}{2} \mathbf{\Sigma}^{-1}-\frac{1}{2} \frac{\partial}{\partial \mathbf{\Sigma}} \operatorname{Tr}\left{\mathbf{\Sigma}^{-1} \sum_{n=1}^{N}\left(\mathbf{t}<em>{n}-\mathbf{y}</em>{n}\right)\left(\mathbf{t}<em>{n}-\mathbf{y}</em>{n}\right)^{\mathrm{T}}\right} \
&amp;=-\frac{N}{2} \mathbf{\Sigma}^{-1}-\frac{1}{2} \mathbf{\Sigma}^{-1}\left{\sum_{n=1}^{N}\left(\mathbf{t}<em>{n}-\mathbf{y}</em>{n}\right)\left(\mathbf{t}<em>{n}-\mathbf{y}</em>{n}\right)^{\mathrm{T}}\right} \mathbf{\Sigma}^{-1}
\end{aligned}
$$</p>
<p>$\frac{\partial}{\partial \mathbf{\Sigma}} \ln L = 0$を求めると、最尤推定解は</p>
<p>$$
\mathbf{\Sigma}<em>{\mathrm{ML}}=\frac{1}{N} \sum</em>{n=1}^{N}\left(\mathbf{t}_{n}-\mathbf{y}(\mathbf{x}<em>n, \mathbf{w})\right)\left(\mathbf{t}</em>{n}-\mathbf{y}(\mathbf{x}_n, \mathbf{w})\right)^{\mathrm{T}}
$$</p>
<p>となる。</p>
<p>ちなみに$\mathbf{w}$と$\mathbf{\Sigma}$は相互に依存しているので、これらの解を求めるときの1つの方法としては、$\mathbf{w}$と$\mathbf{\Sigma}$の解を交互にある一定の収束値以下の誤差になるまで繰り返し求める、というものがある。</p>
<h2 id="演習-54"><a class="header" href="#演習-54">演習 5.4</a></h2>
<div class="panel-primary">
<p>目標値が$t \in { 0,1}$であり，ネットワークの出力$y(\mathbf{x}, \mathbf{w})$が$p(t=1\mid \mathbf{x})$を表すような2クラス分類問題を考え，訓練データ点のクラスラベルが誤っている確率が$\epsilon$であるとする．独立同分布のデータを仮定して，負の対数尤度に相当する誤差関数を書き下せ．また，$\epsilon=0$のときは誤差関数
$$
E(\mathbf{w})=-\sum_{n=1}^{N}\left{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right} \tag{5.21}
$$
が得られることを確かめよ．通常の誤差関数と比べ，この誤差関数を用いると，誤ってラベル付けされたデータに対してモデルが頑健になることに注意せよ．</p>
</div>
<p>※P.235の流れと演習問題4.16を参照。</p>
<p>出力の$y(\mathbf{x}, \mathbf{w})$が$p(t=1\mid \mathbf{x})$となるので
$$
p(t=1\mid \mathbf{x}) = y(\mathbf{x}, \mathbf{w})
$$
で表される。ここで目標値$t$のデータラベルが確率$\varepsilon$で間違っているとすると、$k$を真のクラスラベルとして
$$
\begin{aligned}
P(t=1 \mid \mathbf{x}) &amp;=\sum_{k=0}^1 p(t=1 \mid k) p(k \mid \mathbf{x}) \
&amp;=\varepsilon(1-y(\mathbf{x}, \mathbf{w}))+(1-\varepsilon) y(\mathbf{x}, \mathbf{w})
\end{aligned}
$$</p>
<p>P.235の議論から、入力が与えられたときの目標の条件付き分布は
$$
p(t \mid \mathbf{x}, \mathbf{w})=p(t=1 \mid \mathbf{x})^{t}(1-p(t=1 \mid \mathbf{x}))^{1-t}
$$
となる。独立同分布から$N$個のデータ点を取得して負の対数尤度を取り、これを誤差関数として定義すると
$$
\begin{aligned}
E(\mathbf{w}) &amp;=-\ln \prod_{n=1}^{N} p\left(t_{n} \mid \mathbf{x}<em>{n}, \mathbf{w}\right) \
&amp;=-\sum</em>{n=1}^{N}\left{t_{n} \ln p(t_{n}=1 \mid \mathbf{x}<em>n)+\left(1-t</em>{n}\right) \ln {1-p(t=1 \mid \mathbf{x}<em>n)} \right} \
&amp;=-\sum</em>{n=1}^{N}\left{t_{n} \ln \left[\varepsilon\left(1-y\left(\mathbf{x}<em>{n}, \mathbf{w}\right)\right)+(1-\varepsilon) y\left(\mathbf{x}</em>{n}, \mathbf{w}\right)\right] + \left(1-t_{n}\right) \ln \left[1-\varepsilon\left(1-y\left(\mathbf{x}<em>{n}, \mathbf{w}\right)\right)-(1-\varepsilon) y\left(\mathbf{x}</em>{n}, \mathbf{w}\right)\right]\right}
\end{aligned}
$$
となる。これは$\varepsilon = 0$（ラベルミスがない）ならば
$$
E(\mathbf{w})=-\sum_{n=1}^{N}\left{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right} \tag{5.21}
$$
と同型になる。</p>
<h2 id="演習-55"><a class="header" href="#演習-55">演習 5.5</a></h2>
<div class="panel-primary">
<p>出力が$y_k(\mathbf{x}, \mathbf{w}) = p(t_k =1\mid \mathbf{x})$と解釈される多クラスニューラルネットワークモデルについて，尤度を最適化することは，交差エントロピー誤差関数
$$
E(\mathbf{w})=-\sum_{n=1}^{N} \sum_{k=1}^{K} t_{n k} \ln y_{k}\left(\mathbf{x}_{n}, \mathbf{w}\right) \tag{5.24}
$$
を最小化することと等価であることを示せ．</p>
</div>
<p>$K$クラスにおける条件付き確率分布は
$$
p(\mathbf{t} \mid \mathbf{x}, \mathbf{w})=\prod_{k=1}^{K} p\left(t_{k}=1 \mid \mathbf{x}, \mathbf{w}\right)^{t_{k}}=\prod_{k=1}^{K} y_{k}^{t_{k}}
$$
とかけるので尤度関数は$\mathbf{T}=\left{\mathbf{t}_1\dots\mathbf{t}_N\right}, \mathbf{X}=\left{\mathbf{x}_1\dots\mathbf{x}_N\right}$とおいて</p>
<p>$$
p\left(\mathbf{T} \mid \mathbf{X} , \mathbf{w} \right)=\prod_{n=1}^{N} p(\mathbf{t}<em>n \mid \mathbf{x}<em>n, \mathbf{w})=\prod</em>{n=1}^{N} \prod</em>{k=1}^{K} y_{n k}^{t_{n k}}
$$
とかける。尤度関数の負の対数は
$$
-\ln p\left(\mathbf{T} \mid \mathbf{X}, \mathbf{w} \right)=-\sum_{n=1}^{N} \sum_{k=1}^{K} t_{n k} \ln y_{n k}
$$
となるので、$K$クラスの尤度関数の最適化は$(5.24)$の最小化と等価である。</p>
<h2 id="演習-56"><a class="header" href="#演習-56">演習 5.6</a></h2>
<div class="panel-primary">
<p>ロジスティックシグモイド活性化関数を持つ出力ユニットの活性$a_k$に関する誤差関数
$$
E(\mathbf{w})=-\sum_{k=1}^{N}\left{t_{k} \ln y_{k}+\left(1-t_{k}\right) \ln \left(1-y_{k}\right)\right} \tag{5.21}
$$
の微分は，
$$
\frac{\partial E}{\partial a_{k}}=y_{k}-t_{k} \tag{5.18}
$$
を満たすことを示せ．</p>
</div>
<p>ロジスティックシグモイド$\displaystyle y_k = \sigma(a_{k}) = \frac{1}{1+e^{-a_k}}$を$a_k$で微分すると（これは演習問題4.12や$(4.88)$式と同じ）</p>
<p>$$
\begin{aligned}
\frac{\partial y_k}{\partial a_k} &amp;= \frac{e^{-a_k}}{(1+e^{-a_k})^2} \
&amp;=\frac{1}{1+e^{-a_k}}\left( 1-\frac{1}{1+e^{-a_k}} \right) \
&amp;= y_k(1-y_k)
\end{aligned}
$$
なので、ある$k$のときの$a_k$についての$E$の偏微分は
$$
\begin{aligned}
\frac{\partial E}{\partial a_{k}} &amp;=-\left{t_{k} \frac{\partial}{\partial a_{k}} \ln y_{k}+\left(1-t_{k}\right) \frac{\partial}{\partial a_{k}} \ln \left(1-y_{k}\right)\right} \
&amp;=-\left{\frac{t_{k}}{y_{k}} \frac{\partial y_{k}}{\partial a_{k}}-\frac{1-t_{k}}{1-y_{k}} \frac{\partial y_{k}}{\partial a_{k}}\right} \
&amp;=-\left(\frac{t_{k}-y_{k}}{y_{k}\left(1-y_{k}\right)}\right) y_k (1- y_k ) \
&amp;=y_{k}-t_{k}
\end{aligned}
$$
となる。よって$(5.18)$式が得られた。</p>
<h2 id="演習-57"><a class="header" href="#演習-57">演習 5.7</a></h2>
<div class="panel-primary">
<p>ソフトマックス活性化関数を持つ出力ユニットの活性$a_k$に関する誤差関数
$$
E(\mathbf{w})=-\sum_{n=1}^{N} \sum_{k=1}^{K} t_{n k} \ln y_{k}\left(\mathbf{x}<em>{n}, \mathbf{w}\right) \tag{5.24}
$$
の微分は
$$
\frac{\partial E}{\partial a</em>{k}}=y_{k}-t_{k} \tag{5.18}
$$
を満たすことを示せ．</p>
</div>
<p>演習問題4.17とほとんど同じ。簡略化のため$y_{nk}=y_{k}\left(\mathbf{x}<em>{n}, \mathbf{w}\right)$と書くことにする。実際は$\displaystyle \frac{\partial E}{\partial a</em>{nk}}=y_{nk}-t_{nk}$を示すことになる。</p>
<p>$y_{nk}$はソフトマックス関数を出力の活性化関数に持っているので
$$
y_{n k}=y_{k}\left(\mathbf{x}<em>{n}, \mathbf{w}\right)=\frac{\exp a</em>{nk}}{\sum_{j} \exp a_{nj}}
$$
微分のchain ruleを使うと
$$
\frac{\partial E}{\partial a_{n k}}=\sum_{i=1}^{N} \sum_{j=1}^{K} \frac{\partial E}{\partial y_{ij}} \frac{\partial y_{i j}}{\partial a_{n k}}
$$
となる。$\displaystyle \frac{\partial E}{\partial y_{ij}}=-\frac{t_{ij}}{y_{ij}}$は簡単に求まる。</p>
<p>$\displaystyle \frac{\partial y_{i j}}{\partial a_{n k}}$は演習問題4.17の結果から
$$
\frac{\partial y_{i j}}{\partial a_{n k}}=\left{\begin{array}{cc}
0 &amp; (\textrm{if}\ \  i \neq n) \
y_{nj}\left(\delta_{kj}-y_{nk}\right) &amp; (\textrm{if}\ \  i = n)
\end{array}\right.
$$
となる（$\delta_{kj}$はクロネッカーのデルタ）。よって
$$
\begin{aligned}
\frac{\partial E}{\partial a_{n k}} &amp;=\sum_{j=1}^{K} t_{n j}\left(y_{n k}-\delta_{kj}\right) \
&amp;=y_{n k}\left(\sum_{j=1}^{K} t_{nj}\right)-\sum_{j=1}^{K} t_{nj} \delta_{kj} \
&amp;=y_{nk}-t_{nk} \left( \because \sum_{j=1}^{K} t_{nj}=1\right)
\end{aligned}
$$
以上から$(5.18)$式が得られた。最後の計算部分は演習問題4.18も参照。</p>
<h2 id="演習-58"><a class="header" href="#演習-58">演習 5.8</a></h2>
<div class="panel-primary">
<p>$$
\frac{d \sigma}{d a}=\sigma(1-\sigma) \tag{4.88}
$$
ではロジスティックシグモイド活性化関数の微分は，関数の値そのもので表されることがわかった．活性化関数が
$$
\tanh (a)=\frac{e^{a}-e^{-a}}{e^{a}+e^{-a}} \tag{5.59}
$$
で定義される$\tanh$である場合について，対応する結果を導け．</p>
</div>
<p>※単純に微分するだけ</p>
<p>$$
\begin{aligned}
\frac{d}{da}\tanh(a) &amp;= \frac{(e^{a}+e^{-a})(e^{a}+e^{-a})-(e^{a}-e^{-a})(e^{a}-e^{-a})}{(e^{a}+e^{-a})^2} \
&amp;= 1-\left( \frac{e^{a}-e^{-a}}{e^{a}+e^{-a}} \right)^2\
&amp;= 1-\tanh^2 (a)\
\end{aligned}
$$</p>
<h2 id="演習-59"><a class="header" href="#演習-59">演習 5.9</a></h2>
<div class="panel-primary">
<p>$0\leq y(\mathbf{x}, \mathbf{w})\leq 1$となるロジスティックシグモイド活性化関数を出力に持ち，データが目標値$t \in {0,1}$を持つようなネットワークについて，2クラス分類問題における誤差関数
$$
E(\mathbf{w})=-\sum_{n=1}^{N}\left{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right} \tag{5.21}
$$
が導かれた．ネットワークの出力が$-1\leq y(\mathbf{x}, \mathbf{w})\leq 1$であり，目標値がクラス$\mathcal{C}_1$については$t=1$，クラス$\mathcal{C}_2$については$t=-1$であるようなネットワークを考えたとき，対応する誤差関数を導け．出力ユニットの活性化関数はどのように選ぶのが適切か．</p>
</div>
<p>指定された値域になるような変換を考える．</p>
<p>$$
\begin {aligned}
t \in {0,1} &amp;\longrightarrow \hat{t} \in {-1,1} \
0\leqslant y \leqslant 1 &amp;\longrightarrow -1 \leqslant \hat{y} \leqslant 1
\end {aligned}
$$</p>
<p>となる$\hat{t}, \hat{y}$は</p>
<p>$$
\begin {aligned}
\hat{t} =  2t-1 &amp;\Leftrightarrow t = \frac{\hat{t}+1}{2} \
\hat{y} =  2y-1 &amp;\Leftrightarrow y = \frac{\hat{y}+1}{2}
\end {aligned}
$$</p>
<p>で与えられる．</p>
<p>これを$5.21$に代入すると，</p>
<p>$$
\begin {aligned}
E(\mathbf{w}) &amp;= -\sum_{n=1}^{N}\left{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right} \
&amp;= -\sum_{n=1}^{N}\left{\frac{\hat{t}<em>{n}+1}{2} \ln \frac{\hat{y}</em>{n}+1}{2}+\left(1-\frac{\hat{t}<em>{n}+1}{2}\right) \ln \left(1-\frac{\hat{y}</em>{n}+1}{2}\right)\right} \
&amp;= -\frac{1}{2} \sum_{n=1}^{N}\left{(1+\hat{t}<em>{n}) \ln (1+\hat{y}</em>{n})+\left(1-\hat{t}<em>{n}\right) \ln \left(1-\hat{y}</em>{n}\right)\right} + N \ln 2
\end {aligned}
$$</p>
<p>となり，対応する誤差関数が導かれる．</p>
<p>また，出力ユニットの活性化関数$h(a)$については$y=\sigma (a)$の値域が$0 \leqslant y \leqslant 1$であったので</p>
<p>$$
\begin {aligned}
h(a) &amp;= 2\sigma (a) - 1 \
&amp;= \frac{2}{1+e^{-a}} - 1 \
&amp;= \frac{1-e^{-a}}{1+e^{-a}} \
&amp;= \frac{e^{a/2}-e^{a/2}}{e^{a/2}+e^{a/2}} \
&amp;= \tanh (a/2)
\end {aligned}
$$</p>
<p>を選べば良い．</p>
<h2 id="演習-510"><a class="header" href="#演習-510">演習 5.10</a></h2>
<div class="panel-primary">
<p>固有方程式
$$
\mathbf{H}\mathbf{u}<em>i = \lambda_i \mathbf{u}<em>i \tag{5.33}
$$
を持つヘッセ行列$\mathbf{H}$を考える．
$$
\mathbf{v}^{\mathrm{T}} \mathbf{H} \mathbf{v}=\sum</em>{i} c</em>{i}^{2} \lambda_{i} \tag{5.39}
$$
におけるベクトル$\mathbf{v}$を順々に固有ベクトル$\mathbf{u}_i$のそれぞれと等しくすることにより，すべての固有値が正であるときそのときに限り$\mathbf{H}$は正定値であることを示せ．</p>
</div>
<p>(5.33)と(5.34)より</p>
<p>$$\mathbf{u}_i^T\mathbf{H}\mathbf{u}_i=\mathbf{u}_i^T\mathbf{\lambda}_i\mathbf{u}_i=\mathbf{\lambda}_i$$</p>
<p>$\mathbf{H}$が正定値のとき、(5.37)が成り立つ。</p>
<p>ここで$\mathbf{v}=\mathbf{u}_i$とすると、</p>
<p>$$\mathbf{\lambda}_i=\mathbf{u}_i^T\mathbf{H}\mathbf{u}_i&gt;0$$</p>
<p>よって$\mathbf{H}$が正定値のとき、すべての固有値は正となる。</p>
<p>またすべての固有値が正であるとき、任意のベクトル$\mathbf{v}$に対して、(5.39)より、</p>
<p>$$\mathbf{v}^T\mathbf{H}\mathbf{v}=\sum_{i}\mathbf{c}_i^2\mathbf{\lambda}_i&gt;0$$</p>
<p>ゆえに、すべての固有値が正であるとき$\mathbf{H}$は正定値となる。</p>
<h2 id="演習-511"><a class="header" href="#演習-511">演習 5.11</a></h2>
<div class="panel-primary">
<p>$$
E(\mathbf{w}) \simeq E\left(\mathbf{w}^{<em>}\right)+\frac{1}{2}\left(\mathbf{w}-\mathbf{w}^{</em>}\right)^{\mathrm{T}} \mathbf{H}\left(\mathbf{w}-\mathbf{w}^{*}\right) \tag{5.32}
$$
で定義される二次誤差関数を考える．ここで，ヘッセ行列$\mathbf{H}$が
$$
\mathbf{H}\mathbf{u}_i = \lambda_i \mathbf{u}_i \tag{5.33}
$$
で与えられる固有方程式を持つとする．このとき，誤差一定の等高線は，方向が固有ベクトル$\mathbf{u}_j$であり，長さが対応する固有値$\lambda_i$の平方根の逆数であるような軸を持つ楕円であることを示せ．</p>
</div>
<p>$(5.36)$より、$(5.32)$の式は</p>
<p>$$
E(\mathbf{w})=E(\mathbf{w}^{\star})+\frac{1}{2}\sum_{i}\mathbf{\lambda}_i\mathbf{\alpha}_i^2
$$</p>
<p>の形で書くことができる。</p>
<p>誤差一定の等高線を考えたとき、$E(\mathbf{w})$を定数$C$で表すと、</p>
<p>$$C=E(\mathbf{w}^{\star})+\frac{1}{2}\sum_{i}\mathbf{\lambda}_i\mathbf{\alpha}_i^2$$</p>
<p>このとき、</p>
<p>$$
\begin{aligned}
\frac{1}{2}\sum_{i}\mathbf{\lambda}_i\mathbf{\alpha}_i^2&amp;=\mathbf{C}-E(\mathbf{w}^{\star})\&amp;=\tilde{C}
\end{aligned}
$$</p>
<p>と表すことができる。なお、$\tilde{C}$は定数である。</p>
<p>$\mathbf{\alpha}_i$は$\mathbf{u}_i$方向の長さを表すため、この式は$\mathbf{u}_i$によって記述された座標に表現される楕円の方程式である。</p>
<p>また、すべての$i \neq j$について$\mathbf{\alpha}_i=0$とすることで軸$j$の長さが決定される。</p>
<p>このとき、</p>
<p>$$\mathbf{\alpha}_j=\left(\frac{\tilde{C}}{\mathbf{\lambda}_j}\right)^{\frac{1}{2}}$$</p>
<p>軸の長さが、対応する固有値$\mathbf{\lambda}_j$の平方根の逆数であるような軸を持つことが示された。</p>
<h2 id="演習-512"><a class="header" href="#演習-512">演習 5.12</a></h2>
<div class="panel-primary">
<p>停留点$\mathbf{w}^{<em>}$のまわりでの誤差関数のテイラー展開
$$
E(\mathbf{w}) \simeq E\left(\mathbf{w}^{</em>}\right)+\frac{1}{2}\left(\mathbf{w}-\mathbf{w}^{<em>}\right)^{\mathrm{T}} \mathbf{H}\left(\mathbf{w}-\mathbf{w}^{</em>}\right) \tag{5.32}
$$
を考えることで，停留点が誤差関数の局所的極小点であることの必要十分条件は，
$$
\left.(\mathbf{H})<em>{i j} \equiv \frac{\partial E}{\partial w</em>{i} \partial w_{j}}\right|_{\mathbf{w}=\widehat{\mathbf{w}}} \tag{5.30}
$$
で定義されるヘッセ行列$\mathbf{H}$が正定値であることを示せ．ただし$\hat{\mathbf{w}} = \mathbf{w}^{*}$である．</p>
</div>
<p>まず十分条件の証明を行う。$\mathbf{w}^{<em>}$が誤差関数$E(\mathbf{w})$の局所的極小点であるならば、$\mathbf{w}^{</em>}$周辺の任意のベクトル$\mathbf{w}$について$E(\mathbf{w}) &gt; E(\mathbf{w}^{*})$が成立することになる。同様に、$(5.32)$のテイラー展開の式から任意のベクトル$\mathbf{w}$について</p>
<p>$$
\frac{1}{2}\left(\mathbf{w}-\mathbf{w}^{<em>}\right)^{\mathrm{T}} \mathbf{H}\left(\mathbf{w}-\mathbf{w}^{</em>}\right) &gt; 0
$$</p>
<p>となる必要がある。任意の$0$ベクトルでない$\left(\mathbf{w}-\mathbf{w}^{*}\right)$について上式が成り立つので、演習問題2.20でも証明されたように、ヘッセ行列$\mathbf{H}$は正定値行列となる。</p>
<p>反対に必要条件の証明として、ヘッセ行列$\mathbf{H}$が正定値行列ならば任意の$0$ベクトルでない$\left(\mathbf{w}-\mathbf{w}^{<em>}\right)^{\mathrm{T}} \mathbf{H}\left(\mathbf{w}-\mathbf{w}^{</em>}\right) &gt; 0$なので$E(\mathbf{w}) &gt; E(\mathbf{w}^{<em>})$となる。すなわち、（$\mathbf{w}^{</em>}$の近傍で）$E(\mathbf{w})$のとりうる最小の値は$E(\mathbf{w}^{<em>})$であるため、$\mathbf{w}^{</em>}$において極小値を取る。</p>
<p>以上から題意が示された。</p>
<p>※一般に多変数関数の極値判定において、「<strong>ある点で偏導関数の値が全て$0$かつヘッセ行列が正定値ならば、その点は極小である</strong>」という定理が存在する（反対にヘッセ行列が負定値ならば、その点は極大である）。ヘッセ行列$\mathbf{H}$が正定値行列であるかどうかは、以下の3つの条件のうちいずれかを満たすことを調べればよい（いずれも数学的には等価）。参考：https://mathtrain.jp/hessian</p>
<ol>
<li>全ての（$0$ベクトルではない）$n$次元ベクトル$\mathbf{v}$に対して$\mathbf{v}^{\mathrm{T}} \mathbf{H} \mathbf{v} &gt; 0$であること</li>
<li>$\mathbf{H}$の固有値が全て正</li>
<li>首座小行列の行列式が全て正。</li>
</ol>
<h2 id="演習-513"><a class="header" href="#演習-513">演習 5.13</a></h2>
<div class="panel-primary">
<p>ヘッセ行列$\mathbf{H}$の対称性により，二次誤差関数
$$
E(\mathbf{w}) \simeq E(\widehat{\mathbf{w}})+(\mathbf{w}-\widehat{\mathbf{w}})^{\mathrm{T}} \mathbf{b}+\frac{1}{2}(\mathbf{w}-\widehat{\mathbf{w}})^{\mathrm{T}} \mathbf{H}(\mathbf{w}-\widehat{\mathbf{w}}) \tag{5.28}
$$
の独立成分の数は$W(W+3)/2$で与えられることを示せ．</p>
</div>
<p>この式の各項は</p>
<p>$$
\begin{aligned}
E(\mathbf{w}) \simeq E(\widehat{\mathbf{w}})+(\mathbf{w}-\widehat{\mathbf{w}})^{\mathrm{T}} \underbrace{\mathbf{b}}<em>{W個}+\frac{1}{2}(\mathbf{w}-\widehat{\mathbf{w}})^{\mathrm{T}} \underbrace{\mathbf{H}}</em>{\frac{W(W+1)}{2}個}(\mathbf{w}-\widehat{\mathbf{w}})
\end{aligned}
$$</p>
<p>の独立変数があり他は全て定数である。これらの線形和なので全部で独立成分の数は$W(W+3)/2$である。</p>
<h2 id="演習-514"><a class="header" href="#演習-514">演習 5.14</a></h2>
<div class="panel-primary">
<p>テイラー展開をすることにより，
$$
\frac{\partial E_{n}}{\partial w_{j i}}=\frac{E_{n}\left(w_{j i}+\epsilon\right)-E_{n}\left(w_{j i}-\epsilon\right)}{2 \epsilon}+O\left(\epsilon^{2}\right) \tag{5.69}
$$
の右辺において$O(\epsilon)$である項は消えることを確かめよ．</p>
</div>
<p>$\displaystyle E_n^{\prime} = \frac{\partial E_n}{\partial w_{ji}}$としてテイラー展開を用いると
$$
\begin{aligned}
E_{n}\left(w_{j i}+\varepsilon\right) &amp;= E_{n}\left(w_{j i}\right)+\varepsilon E_{n}^{\prime}\left(w_{j i}\right)+\frac{\varepsilon^{2}}{2} E_{n}^{\prime \prime}\left(w_{j i}\right)+\mathcal{O}\left(\varepsilon^{3}\right) \
E_{n}\left(w_{j i}-\varepsilon\right) &amp;= E_{n}\left(w_{j i}\right)-\varepsilon E_{n}^{\prime}\left(w_{j i}\right)+\frac{\varepsilon^{2}}{2} E_{n}^{\prime \prime}\left(w_{i i}\right)+\mathcal{O}\left(\varepsilon^{3}\right)
\end{aligned}
$$
よって
$$
E_{n}\left(w_{j i}+\varepsilon\right)-E\left(w_{j i}-\varepsilon\right)=2 \varepsilon E_{n}^{\prime}\left(w_{j i}\right)+\mathcal{O}\left(\varepsilon^{3}\right)
$$
これを変形すれば
$$
\begin{aligned}
\frac{\partial E_{n}}{\partial w_{j i}} &amp;= \frac{E_{n}\left(w_{j i}+\epsilon\right)-E_{n}\left(w_{j i}-\epsilon\right)+\mathcal{O}\left(\varepsilon^{3}\right)}{2 \epsilon} \
&amp;= E_{n}^{\prime}\left(w_{j i}\right) +\mathcal{O}\left(\varepsilon^{2}\right)
\end{aligned}
$$
が得られる。</p>
<h2 id="演習-515"><a class="header" href="#演習-515">演習 5.15</a></h2>
<div class="panel-primary">
<p>5.3.4節において．逆伝播手続きを利用してニューラルネットワークのヤコビ行列を評価する手続きを導いた．順伝播方程式に基づき，ヤコビ行列を見つけるための他の定式化を導出せよ．</p>
</div>
<p>※たぶん5.3.4節で行っていた逆伝播の手順と逆向きに連鎖律を適用することでヤコビ行列を導け、という意味。</p>
<p>イメージとしては以下のような図になる。</p>
<img src="/attachment/617b8a2ad44de45e6ef74f72" width="600px">
<p>$(5.73)$式から、ユニット$j$を出力層と結合する全てのユニットとすると</p>
<p>$$
J_{k i}=\frac{\partial y_{k}}{\partial x_{i}}=\sum_{j} \frac{\partial y_{k}}{\partial a_{j}} \frac{\partial a_{j}}{\partial x_{i}}
$$
であり、出力ユニット活性に関わる項$\displaystyle \frac{\partial y_{k}}{\partial a_{j}}$は$(5.75)$, $(5.76)$式で示されているように</p>
<p>$$
\frac{\partial y_{k}}{\partial a_{j}}=\delta_{k j} \sigma^{\prime}\left(a_{j}\right)
$$
などで表されるので、</p>
<p>$$
\sum_{j} \frac{\partial y_{k}}{\partial a_{j}} \frac{\partial a_{j}}{\partial x_{i}} = \sum_{j} \delta_{k j} \sigma^{\prime}\left(a_{j}\right) \frac{\partial a_j}{\partial x_i}
$$</p>
<p>となる。</p>
<p>また、ユニット$j$に結合している全てのユニットを$l$(層状構造なら$j$の一つ手前の層に属するユニット)とすると</p>
<p>$a_j = \sum_{l} w_{jl}z_l$と$z_l = h(a_l)$を利用して</p>
<p>$$
\begin{aligned}
\frac{\partial a_{j}}{\partial x_{i}} &amp;= \sum_l \frac{\partial a_j}{\partial z_l}\frac{\partial z_l}{\partial a_l}\frac{\partial a_l}{\partial x_i} \
&amp;= \sum_l w_{jl}h^{\prime}(a_l)\frac{\partial a_l}{\partial x_i}
\end{aligned}
$$
で評価できる。もし$a_l$がインプット層から見て最初の隠れ層ならば</p>
<p>$$
\frac{\partial a_{l}}{\partial x_{i}} = \frac{\partial}{\partial x_i}\sum_i w_{li}x_i = w_{li}
$$
となっている。
これを再帰的に計算することによってヤコビ行列を計算することができる。</p>
<h2 id="演習-516"><a class="header" href="#演習-516">演習 5.16</a></h2>
<div class="panel-primary">
<p>二乗和誤差関数を用いたニューラルネットワークのヘッセ行列の外積による近似は
$$
\mathbf{H} \simeq \sum_{n=1}^{N} \mathbf{b}<em>{n} \mathbf{b}</em>{n}^{\mathrm{T}} \tag{5.84}
$$
で与えられる．この結果を複数の出力を持つ場合に拡張せよ．</p>
</div>
<p>※5.4.2 外積による近似を参照。</p>
<p>$(5.82)$式から二乗和誤差関数
$$
E=\frac{1}{2} \sum_{n=1}^{N}\left(y_{n}-t_{n}\right)^{2}
$$</p>
<p>の出力$y_n, t_n$を$K$次元に拡張すると</p>
<p>$$
\begin{aligned}
E &amp;=\frac{1}{2} \sum_{n=1}^{N}\left|\mathbf{y}_n-\mathbf{t}<em>n\right|^{2} \
&amp;=\frac{1}{2} \sum</em>{n=1}^{N}\left(\mathbf{y}<em>n-\mathbf{t}<em>n\right)^{\mathrm T}\left(\mathbf{y}<em>n-\mathbf{t}<em>n\right) \
&amp;=\frac{1}{2} \sum</em>{n=1}^{N} \sum</em>{k=1}^{K}\left(y</em>{n k}-t</em>{nk}\right)^{2}
\end{aligned}
$$</p>
<p>となる。$(5.83)$にならってヘッセ行列を求めるために$w_i, w_j$で偏微分すると</p>
<p>$$
\frac{\partial E}{\partial w_{i}}=\sum_{n=1}^{N} \sum_{k=1}^{K}\left(y_{n k}-t_{n k}\right) \frac{\partial y_{n k}}{\partial w_{i}}
$$
$$
\frac{\partial}{\partial w_{j}}\left(\frac{\partial E}{\partial w_{i}}\right)=\sum_{n=1}^{N} \sum_{k=1}^{K}\left{\frac{\partial y_{nk}}{\partial w_{j}} \frac{\partial y_{n k}}{\partial w_{i}}+\left(y_{n k}-t_{n k}\right) \frac{\partial}{\partial w_{j}} \frac{\partial y_{n k}}{\partial w_{i}}\right}
$$
P.252の記述のように、もし訓練によって出力値$\mathbf{y}<em>n$が目標値$\mathbf{t}<em>n$に十分近づいているとするならば、$\sum$の中の第2項は無視できるほど小さくなる。すなわち、
$$
\frac{\partial}{\partial w</em>{j}}\left(\frac{\partial E}{\partial w</em>{i}}\right) \simeq \sum_{n=1}^{N} \sum_{k=1}^{K}\left( \frac{\partial y_{nk}}{\partial w_{j}} \frac{\partial y_{n k}}{\partial w_{i}}\right)
$$
となる。これをもとにヘッセ行列を考えると</p>
<p>$$
\begin{aligned}
\mathbf{H} = \nabla\nabla E
&amp;= \sum_{n=1}^{N} \sum_{k=1}^{K}\begin{pmatrix}
\frac{\partial y_{n k}}{\partial w_{0}}\frac{\partial y_{n k}}{\partial w_{0}} &amp; \cdots &amp; \frac{\partial y_{n k}}{\partial w_{0}}\frac{\partial y_{n k}}{\partial w_{M-1}} \
\vdots&amp; \ddots &amp; \vdots \
\frac{\partial y_{n k}}{\partial w_{M-1}}\frac{\partial y_{n k}}{\partial w_{0}} &amp; \cdots &amp; \frac{\partial y_{n k}}{\partial w_{M-1}}\frac{\partial y_{n k}}{\partial w_{M-1}}
\end{pmatrix}\
&amp;= \sum_{n=1}^{N} \sum_{k=1}^{K}\begin{pmatrix}
\frac{\partial y_{n k}}{\partial w_{0}} \
\vdots \
\frac{\partial y_{n k}}{\partial w_{M-1}}
\end{pmatrix}\begin{pmatrix}\frac{\partial y_{n k}}{\partial w_{0}}, \cdots, \frac{\partial y_{n k}}{\partial w_{M-1}}\end{pmatrix} \
&amp;= \sum_{n=1}^{N} \sum_{k=1}^{K}\mathbf{b}<em>{n, k} \mathbf{b}</em>{n, k}^{\mathrm T}
\end{aligned}
$$</p>
<p>と書ける。ここで、$\mathbf{b}<em>{n,k}=\nabla y</em>{nk}=\begin{pmatrix}\frac{\partial y_{n k}}{\partial w_{0}} \ \vdots \ \frac{\partial y_{n k}}{\partial w_{M-1}} \end{pmatrix}$とした。</p>
<p>または$\sum_{n=1}^{N} \sum_{k=1}^{K}\mathbf{b}<em>{n, k} \mathbf{b}</em>{n, k}^{\mathrm T}$の式を$k$について拡張すると
$$
\begin{aligned}
\sum_{n=1}^{N} \sum_{k=1}^{K}\mathbf{b}<em>{n, k} \mathbf{b}</em>{n, k}^{\mathrm T}
&amp;= \sum_{n=1}^{N}
\begin{pmatrix}
\frac{\partial y_{n 1}}{\partial w_{0}} &amp; \cdots &amp; \frac{\partial y_{nK}}{\partial w_{0}} \
\vdots &amp; \ddots &amp; \vdots \
\frac{\partial y_{n_{1}}}{\partial w_{M-1}} &amp; \cdots &amp; \frac{\partial y_{nK}}{\partial w_{M-1}}
\end{pmatrix}
\begin{pmatrix}
\frac{\partial y_{n 1}}{\partial w_{0}} &amp; \cdots &amp; \frac{\partial y_{n 1}}{\partial w_{M-1}} \
\vdots &amp; \ddots &amp; \vdots \
\frac{\partial y_{nK}}{\partial w_{0}} &amp; \cdots &amp; \frac{\partial y_{nK}}{\partial w_{M-1}}
\end{pmatrix} \
&amp;=\sum_{n=1}^{N}
\begin{pmatrix}
\mathbf{b}<em>{n1},\cdots,\mathbf{b}</em>{nK}
\end{pmatrix}
\begin{pmatrix}
\mathbf{b}<em>{n1}^{\mathrm T} \ \vdots \ \mathbf{b}</em>{nK}^{\mathrm T}
\end{pmatrix} \
&amp;=\sum_{n=1}^{N}\mathbf{B}<em>n\mathbf{B}<em>n^{\mathrm T}
\end{aligned}
$$
と書くこともできる。$\mathbf{B}<em>n$は$\mathbf{B}<em>n = \begin{pmatrix}
\mathbf{b}</em>{n1},\cdots,\mathbf{b}</em>{nK}
\end{pmatrix}$で表される$M\times K$行列を用いた。$\mathbf{B}<em>n$の$l$行$k$列成分は$\displaystyle \left(\mathbf{B}</em>{n}\right)</em>{l k}=\frac{\partial y</em>{n k}}{\partial w_{l-1}}$となっている。</p>
<blockquote>
<p>結局このヘッセ行列は3階のテンソルになっているらしい。</p>
</blockquote>
<h2 id="演習-517"><a class="header" href="#演習-517">演習 5.17</a></h2>
<div class="panel-primary">
<p>二乗誤差関数</p>
<p>$$
E=\frac{1}{2} \iint{y(\mathbf{x}, \mathbf{w})-t}^{2} p(\mathbf{x}, t) \mathrm{d} \mathbf{x} \mathrm{d} t \tag{5.193}
$$</p>
<p>を考える．ここで，$y(\mathbf{x},\mathbf{w})$はニューラルネットワークのようなパラメトリックな関数である．
$$
y(\mathbf{x})=\frac{\int t p(\mathbf{x}, t) \mathrm{d} t}{p(\mathbf{x})}=\int t p(t \mid \mathbf{x}) \mathrm{d} t=\mathbb{E}_{t}[t \mid \mathbf{x}] \tag{1.89}
$$
の結果はこの誤差を最小化する関数$y(\mathbf{x},\mathbf{w})$が$\mathbf{x}$が与えられたときの$t$の条件付き期待値で与えられることを示している．この結果を使って，ベクトル$\mathbf{w}$の2つの要素$w_r$と$w_s$に関する$E$の2階微分が</p>
<p>$$
\frac{\partial^{2} E}{\partial w_{r} \partial w_{s}}=\int \frac{\partial y}{\partial w_{r}} \frac{\partial y}{\partial w_{s}} p(\mathbf{x}) \mathrm{d} \mathbf{x} \tag{5.194}
$$</p>
<p>で与えられることを示せ．ここで$p(\mathbf{x})$から有限個サンプリングする場合には，
$$
\mathbf{H} \simeq \sum_{n=1}^{N} \mathbf{b}<em>{n} \mathbf{b}</em>{n}^{\mathrm{T}} \tag{5.84}
$$
が得られることに注意せよ．</p>
</div>
<p>※まず要素$w_r$と$w_s$についての微分を行う。</p>
<p>$$
\begin{aligned}
\frac{\partial^{2} E}{\partial w_{r} \partial w_{s}} &amp;=\frac{\partial}{\partial w_{r}}\left{\frac{1}{2} \frac{\partial}{\partial w_{s}} \iint{y(\mathbf{x}, \mathbf{w})-t}^{2} p(\mathbf{x}, t) d\mathbf{x} dt\right} \
&amp;=\frac{\partial}{\partial w_r}\left{\frac{1}{2} \iint 2{y(\mathbf{x}, \mathbf{w})-t} \frac{\partial y}{\partial w_{s}} p(\mathbf{x},\mathbf{t}) d\mathbf{x} dt \right} \
&amp;=\iint\left{\frac{\partial y}{\partial w_{r}} \frac{\partial y}{\partial w_{s}}+{y(\mathbf{x}, \mathbf{w})-t} \frac{\partial^{2} y}{\partial w_{r} \partial w_{s}}\right} p(\mathbf{x}, t) d\mathbf{x} dt
\end{aligned}
$$</p>
<p>$(5.194)$式と比べると${y(\mathbf{x}, \mathbf{w})-t}\frac{\partial^{2} y}{\partial w_{r} \partial w_{s}}$が消えているので、この項について計算してみると</p>
<p>$$
\begin{aligned}
&amp; \iint(y(\mathbf{x}, \mathbf{w})-t) \frac{\partial^{2} y}{\partial w_{r} \partial w_{s}} p(\mathbf{x}, t) d\mathbf{x} d t \
=&amp; \iint y(\mathbf{x}, \mathbf{w}) \frac{\partial^{2} y}{\partial w_{r} \partial w_{s}} p(\mathbf{x}, t) d\mathbf{x} d t-\int \frac{\partial^{2} y}{\partial w_{r} \partial w_{s}}\left{\int t p(\mathbf{x}, t) d t\right} d\mathbf{x} \
=&amp; \int y(\mathbf{x}, \mathbf{w}) \frac{\partial^{2} y}{\partial w_{r} \partial w_{s}} p(\mathbf{x}) d\mathbf{x}-\int \frac{\partial^{2} y}{\partial w_{r} \partial w_{s}} y(\mathbf{x},\mathbf{w}) p(\mathbf{x}) d\mathbf{x} \
=&amp;\ 0
\end{aligned}
$$
となるので、</p>
<p>$$
\begin{aligned}
\frac{\partial^{2} E}{\partial w_{r} \partial w_{s}} &amp;=\iint \frac{\partial y}{\partial w_{r}} \frac{\partial y}{\partial w_{s}} p(\mathbf{x}, t) d\mathbf{x} d t \
&amp;=\int \frac{\partial y}{\partial w_{r}} \frac{\partial y}{\partial w_{s}} p(\mathbf{x}) d\mathbf{x}
\end{aligned}
$$</p>
<p>これより$(5.194)$式が求められた。</p>
<p>ちなみにこの式は有限のデータセット$N$個であれば$p(\mathbf{x})$からサンプリングしたときに
$$
\begin{aligned}
\frac{\partial^{2} E}{\partial w_{r} \partial w_{s}} &amp;=\frac{1}{N}\sum_{n=1}^N \frac{\partial y_n}{\partial w_{r}} \frac{\partial y_n}{\partial w_{s}}
\end{aligned}
$$
となるので、$(5.84)$式のヘッセ行列と同型になることがわかる。</p>
<h2 id="演習-518"><a class="header" href="#演習-518">演習 5.18</a></h2>
<div class="panel-primary">
<p><img src="/attachment/617b8a46d44de45e6ef74fd9" alt="fig5.1.png" /></p>
<p>図5.1に示す形の2層ネットワークで，入力から出力へ直接つながる，層を飛び越えた結合に相当するパラメータを追加したものを考える．5.3.2節での議論を拡張して，誤差関数の追加されたパラメータに関する微分の方程式を書き下せ．</p>
</div>
<p>入力から出力層へ追加されたパラメータを考慮すると、</p>
<p>$$
y_{k} = a_{k} = \sum_{j=1}^{M} w_{k j} z_{j} + \sum_{i=1}^{D} w_{k i} x_{i}
$$</p>
<p>ここでは、Eは二乗和誤差関数、出力には恒等関数を使用しているので、追加されたパラメータに対して偏微分を取ってあげると、</p>
<p>$$
\begin{aligned}
\frac{\partial E}{\partial w_{k i}} &amp;= \frac{\partial E}{\partial y_{k}} \frac{\partial y_{k}}{\partial a_{k}} \frac{\partial a_{k}}{\partial w_{k i}} \
&amp;= (y_{k} - t_{k}) x_{i} \
\end{aligned}
$$</p>
<p>※</p>
<h2 id="演習-519"><a class="header" href="#演習-519">演習 5.19</a></h2>
<div class="panel-primary">
<p>出力ユニットは1つでその活性化関数はロジスティックシグモイド関数，誤差関数は交差エントロピーであるネットワークについて，二乗和誤差関数の場合の結果
$$
\mathbf{H} \simeq \sum_{n=1}^{N} \mathbf{b}<em>{n} \mathbf{b}</em>{n}^{\mathrm{T}} \tag{5.84}
$$
に対応する，ネットワークのヘッセ行列の外積による近似式
$$
\mathbf{H} \simeq \sum_{n=1}^{N} y_{n}\left(1-y_{n}\right) \mathbf{b}<em>{n} \mathbf{b}</em>{n}^{\mathrm{T}} \tag{5.85}
$$
を導け．</p>
</div>
<p>ロジスティックシグモイド関数$\displaystyle y_n = \sigma(a_n) = \frac{1}{1+e^{-a_n}}$の微分形は$\displaystyle \frac{\partial y_n}{\partial a_n} = y_n(1-y_n)$になる演習問題4.12。</p>
<p>誤差関数は交差エントロピー
$$
E(\mathbf{w})=-\sum_{n=1}^{N}\left{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right} \tag{5.21}
$$
である場合、演習問題4.13の結果から
$$
\begin{aligned}
\nabla E(\mathbf{w}) &amp;=\frac{\partial E}{\partial y_{n}} \frac{\partial y_{n}}{\partial a_{n}} \nabla a_{n} \
&amp;=\sum_{n=1}^{N} \frac{y_{n}-t_{n}}{y_{n}\left(1-y_{n}\right)} \cdot y_{n}\left(1-y_{n}\right) \nabla a_{n} \
&amp;=\sum_{n=1}^{N}\left(y_{n}-t_{n}\right) \nabla a_{n}
\end{aligned}
$$
となる。ヘッセ行列$\mathbf{H}$を計算するためにもう一度$\nabla$をとると
$$
\begin{aligned}
\mathbf{H}=\nabla \nabla E(\mathbf{w}) &amp;=\sum_{n=1}^{N}\left{\nabla\left(y_{n}-t_{n}\right) \nabla a_{n}^{\mathrm T}+\left(y_{n}-t_{n}\right) \nabla \nabla a_{n}\right} \
&amp;=\sum_{n=1}^{N}\left{\frac{\partial y_{n}}{\partial a_{n}} \nabla a_{n} \nabla a_{n}^{\mathrm T}+\left(y_{n}-t_{n}\right) \nabla \nabla a_{n}\right} \
&amp;=\sum_{n=1}^{N}\left{y_{n}\left(1-y_{n}\right) \nabla a_{n} \nabla a_{n}^{\mathrm T}+\left(y_{n}-t_{n}\right) \nabla \nabla a_{n}\right}
\end{aligned}
$$
P.252の記述のように、ネットワークをデータ集合で訓練し，その出力$y_n$が目標値$t_n$に非常に近づいたとしたら，第2項は小さくなり無視できるので
$$
\begin{aligned}
\mathbf{H} &amp; \simeq \sum_{n=1}^{N} y_{n}\left(1-y_{n}\right) \nabla a_{n} \nabla a_{n}^{\mathrm T} \
&amp;=\sum_{n=1}^{N} y_{n}\left(1-y_{n}\right) \mathbf{b}<em>{n} \mathbf{b}</em>{n}^{\mathrm T}
\end{aligned}\tag{5.85}
$$
が得られる。</p>
<h2 id="演習-520"><a class="header" href="#演習-520">演習 5.20</a></h2>
<div class="panel-primary">
<p>出力ユニットは$K$個でその活性化関数はソフトマックス関数，誤差関数は交差エントロピーであるネットワークについて，二乗和誤差関数の場合の結果
$$
\mathbf{H} \simeq \sum_{n=1}^{N} \mathbf{b}<em>{n} \mathbf{b}</em>{n}^{\mathrm{T}} \tag{5.84}
$$
に対応する，ネットワークのヘッセ行列の外積による近似式を導け．</p>
</div>
<p>※重み$w$の添字に$i,j$を、クラスの添字に$k,l$を使う。</p>
<p>多クラスの場合の交差エントロピー誤差関数は
$$
\begin{aligned}
E(\mathbf{w}) &amp;=-\sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk} \ln y_{k}\left(\mathbf{x}<em>{n}, \mathbf{w}\right) \
&amp;=-\sum</em>{n=1}^{N} \sum_{k=1}^{K} t_{nk} \ln y_{n k}
\end{aligned} \tag{5.24, 4.108}
$$
であり、出力ユニットのソフトマックス活性化関数
$$
y_{k}\left(\mathbf{x}<em>{n}, \mathbf{w}\right)=\frac{\exp \left(a</em>{k}\left(\mathbf{x}<em>{n}, \mathbf{w}\right)\right)}{\sum</em>{l=1}^{K} \exp \left(a_{l}\left(\mathbf{x}<em>{n}, \mathbf{w}\right)\right)}
$$
の偏微分は、演習問題5.7の結果から
$$
\frac{\partial E}{\partial a</em>{nk}}=y_{n k}-t_{n k}
$$
である。</p>
<p>重み$w_i$についての$E$の偏微分を行うと
$$
\begin{aligned}
\nabla_{w_{i}} E &amp;=\sum_{n=1}^{N} \sum_{k=1}^{K} \frac{\partial E}{\partial a_{n k}} \nabla_{w_{i}} a_{n k} \
&amp;=\sum_{n=1}^{N} \sum_{k=1}^{K}\left(y_{n k}-t_{n k}\right) \nabla_{w_{i}} a_{n k}
\end{aligned}
$$
となる。さらに重み$w_j$で偏微分すると
$$
\mathbf{H}=\nabla_{w_{j}} \nabla_{w_{i}} E=\sum_{n=1}^{N} \sum_{k=1}^{K}\left{
(\nabla_{w_{j}}\left(y_{nk}-t_{nk}\right)) \nabla_{w_{i}} a_{n k}+\left(y_{nk}-t_{nk}\right) \nabla_{w_{j}} \nabla_{w_{i}} a_{nk}
\right}
$$
ここで$\displaystyle \nabla_{w_{j}}\left(y_{nk}-t_{nk}\right)$についての計算は</p>
<p>$$
\begin{aligned}
\nabla_{w_{j}}\left(y_{nk}-t_{nk}\right) &amp;=\sum_{l=1}^{K} \frac{\partial y_{n k}}{\partial a_{nl}} \nabla_{w_{j}} a_{n l} \
&amp;=\sum_{l=1}^{K} y_{n k}\left(\delta_{k l}-y_{n l}\right) \nabla_{w_{j}} a_{n l}
\end{aligned}
$$</p>
<p>となるので、ヘッセ行列は</p>
<p>$$
\begin{aligned}
\mathbf{H}&amp;=\nabla_{w_{j}} \nabla_{w_{i}} E \
&amp;=\sum_{n=1}^{N} \sum_{k=1}^{K} \left{ \sum_{l=1}^{K} y_{nk}\left(\delta_{kl}-y_{nl}\right) \nabla_{w_{j}} a_{nl} \nabla_{w_{i}} a_{nk} +\left(y_{nk}-t_{nk}\right) \nabla_{w_{j}} \nabla_{w_{i}} a_{nk}\right}
\end{aligned}
$$
例によって第2項が無視できる状況ならば
$$
\begin{aligned}
\mathbf{H} &amp;\simeq \sum_{n=1}^{N} \sum_{k=1}^{K} \sum_{l=1}^{K} y_{n k}\left(\delta_{k l}-y_{nl}\right) \nabla_{w_{j}} a_{nl} \nabla_{w_{i}} a_{n k} \
&amp;= \sum_{n=1}^{N} \sum_{k=1}^{K} \sum_{l=1}^{K} y_{n k}\left(\delta_{k l}-y_{n l}\right)\begin{pmatrix}
\frac{\partial a_{nl}}{\partial w_{0}} \
\vdots \
\frac{\partial a_{nl}}{\partial w_{M-1}}
\end{pmatrix}
\left(\frac{\partial a_{n k}}{\partial w_{0}}, \cdots, \frac{\partial a_{n k}}{\partial w_{M-1}}\right) \
&amp;= \sum_{n=1}^{N} \sum_{k=1}^{K} \sum_{l=1}^{K} y_{n k}\left(\delta_{k l}-y_{n l}\right)\begin{pmatrix}
\frac{\partial a_{nl} \partial a_{nk}}{\partial w_{0}^{2}} &amp; \cdots &amp; \frac{\partial a_{nl} \partial a_{nk}}{\partial w_{0} \partial w_{M-1}} \
\vdots &amp; \ddots &amp;\vdots \
\frac{\partial a_{nl} \partial a_{n k}}{\partial w_{M-1} \partial w_{0}} &amp; \cdots &amp; \frac{\partial a_{nl} \partial a_{nk}}{\partial w_{M-1} \partial w_{M-1}}
\end{pmatrix}
\end{aligned}
$$
を得る。</p>
<h2 id="演習-521"><a class="header" href="#演習-521">演習 5.21</a></h2>
<div class="panel-primary">
<p>（難問）ヘッセ行列の外積による近似式
$$
\mathbf{H}<em>{N}=\sum</em>{n=1}^{N} \mathbf{b}<em>{n} \mathbf{b}</em>{n}^{\mathrm{T}} \tag{5.86}
$$
を出力ユニットが$K&gt;1$個ある場合に拡張せよ．すなわち，パターンの寄与だけではなく出力の寄与も逐次的に受ける形の
$$
\mathbf{H}<em>{L+1}=\mathbf{H}</em>{L}+\mathbf{b}<em>{L+1} \mathbf{b}</em>{L+1}^{\mathrm{T}} \tag{5.87}
$$
を導け．この式と
$$
\left(\mathbf{M}+\mathbf{vv}^{\mathrm{T}}\right)^{-1}=\mathbf{M}^{-1}-\frac{\left(\mathbf{M}^{-1} \mathbf{v}\right)\left(\mathbf{v}^{\mathbf{T}} \mathbf{M}^{-1}\right)}{1+\mathbf{v}^{\mathbf{T}} \mathbf{M}^{-1} \mathbf{v}} \tag{5.88}
$$
により，
$$
\mathbf{H}<em>{L+1}^{-1}=\mathbf{H}</em>{L}^{-1}-\frac{\mathbf{H}<em>{L}^{-1} \mathbf{b}</em>{L+1} \mathbf{b}<em>{L+1}^{\mathrm{T}} \mathbf{H}</em>{L}^{-1}}{1+\mathbf{b}<em>{L+1}^{\mathrm{T}} \mathbf{H}</em>{L}^{-1} \mathbf{b}_{L+1}} \tag{5.89}
$$
を利用して個々のパターンと出力からの寄与を逐次的に扱うことでヘッセ行列の逆行列を求めることができるようになる．</p>
</div>
<p>※ 演習問題5.16とほぼ同じ……？</p>
<p>演習5.16の結果から、$K$次元の複数出力を持つ場合のヘッセ行列の外積による近似式は
$$
\mathbf{H}<em>{N,K}=\sum</em>{n=1}^{N} \sum_{k=1}^{K} \mathbf{b}<em>{n,k} \mathbf{b}</em>{n,k}^{\mathrm{T}}
$$
である。ここで$\mathbf{b}<em>{n,k} = \nabla y</em>{nk}$である。</p>
<p>これより単純に$N \to N+1$とすれば
$$
\begin{aligned}
\mathbf{H}<em>{N+1,K} &amp;= \mathbf{H}</em>{N,K} + \sum_{k=1}^{K}\mathbf{b}<em>{N+1,k}\mathbf{b}</em>{N+1,k}^{\mathrm T} \
&amp;= \mathbf{H}<em>{N,K} + \mathbf{B}</em>{N+1}\mathbf{B}<em>{N+1}^{\mathrm T}
\end{aligned}
$$
の式が成り立つ。ここで$\mathbf{B}</em>{N+1}$は$\left( \mathbf{b}<em>{N+1,1}, \ldots , \mathbf{b}</em>{N+1,K} \right)$で構成される$W\times K$の行列である。</p>
<p>$(5.88)$を使えば</p>
<p>$$
\mathbf{H}<em>{N+1, K}^{-1}=\mathbf{H}</em>{N, K}^{-1}-\frac{\mathbf{H}<em>{N, K}^{-1} \mathbf{B}</em>{N+1} \mathbf{B}<em>{N+1}^{\mathrm T} \mathbf{H}</em>{N, K}^{-1}}{1+\mathbf{B}<em>{N+1}^{\mathrm T} \mathbf{H}</em>{N, K}^{-1} \mathbf{B}_{N+1}}
$$</p>
<p>と書ける。</p>
<h2 id="演習-522"><a class="header" href="#演習-522">演習 5.22</a></h2>
<div class="panel-primary">
<p>微分のチェーンルールを応用して，2層フィードフォワードネットワークのヘッセ行列の要素について
$$
\frac{\partial^{2} E_{n}}{\partial w_{k j}^{(2)} \partial w_{k^{\prime} j^{\prime}}^{(2)}}=z_{j} z_{j^{\prime}} M_{k k^{\prime}} \tag{5.93}
$$
$$
\frac{\partial^{2} E_{n}}{\partial w_{j i}^{(1)} \partial w_{j^{\prime} i^{\prime}}^{(1)}}=x_{i} x_{i^{\prime}} h^{\prime \prime}\left(a_{j^{\prime}}\right) I_{j j^{\prime}} \sum_{k} w_{k j^{\prime}}^{(2)} \delta_{k} +x_{i} x_{i^{\prime}} h^{\prime}\left(a_{j^{\prime}}\right) h^{\prime}\left(a_{j}\right) \sum_{k} \sum_{k^{\prime}} w_{k^{\prime} j^{\prime}}^{(2)} w_{k j}^{(2)} M_{k k^{\prime}} \tag{5.94}
$$
および
$$
\frac{\partial^{2} E_{n}}{\partial w_{j i}^{(1)} \partial w_{k j^{\prime}}^{(2)}}=x_{i} h^{\prime}\left(a_{j}\right)\left{\delta_{k} I_{j j^{\prime}}+z_{j^{\prime}} \sum_{k^{\prime}} w_{k^{\prime} j}^{(2)} M_{k k^{\prime}}\right} \tag{5.95}
$$
の結果を導け．</p>
</div>
<p>$(5.93)$について
$$
\left{\begin{array}{l}
a_{j}=\sum_{i} w_{j i} x_{i} \
z_{j}=h\left(a_{j}\right) \
y_{k}=g\left(a_{k}\right) \
\delta_{k}=\frac{\partial E_{n}}{\partial a_{k}} \
M_{k k^{\prime}}=\frac{\partial^{2} E_{n}}{\partial a_{k} \partial a_{k}^{\prime}}
\end{array}\right.
$$
とする。
$$
\begin{aligned}
\frac{\partial E_{n}}{\partial w_{kj}^{(2)}} &amp;=\frac{\partial E_{n}}{\partial a_{k}} \frac{\partial a_{k}}{\partial w_{kj}^{(2)}} \
&amp;=\frac{\partial E_{n}}{\partial a_{k}} \cdot \frac{\partial} {\partial w_{k j}^{(2)}}\sum_{j} w_{kj}^{(2)} z_{j} \
&amp;=\delta_{k} z_{j}
\end{aligned}
$$
これより
$$
\begin{aligned}
\frac{\partial^{2} E_{n}}{\partial w_{k j}^{(2)} \partial w_{k^{\prime} j^{\prime}}^{(2)}} &amp;= \frac{\partial}{\partial w_{k^{\prime} j^{\prime}}^{(2)}}\left( \frac{\partial E_{n}}{\partial w_{kj}^{(2)}} \right) \
&amp;= \frac{\partial}{\partial w_{k^{\prime} j^{\prime}}^{(2)}}(\delta_k z_j) \
&amp;= z_{j} \frac{\partial \delta_{k}}{\partial w_{k^{\prime} j^{\prime}}^{(2)}} \
&amp;= z_{j} \frac{\partial^{2} E_{n}}{\partial a_{k} \partial a_{k^{\prime}}} \frac{\partial a_{k}}{\partial w_{k^{\prime} j^{\prime}}^{(2)}} \
&amp;= z_{j} z_{j^{\prime}} M_{k k^{\prime}}
\end{aligned}
$$
となる。</p>
<h2 id="演習-523"><a class="header" href="#演習-523">演習 5.23</a></h2>
<div class="panel-primary">
<p>2層ネットワークの正確なヘッセ行列に関する5.4.5節の結果を，入力から出力へ直接つながる，層を飛び越えた結合を含むように拡張せよ．</p>
</div>
<p>※</p>
<h2 id="演習-524"><a class="header" href="#演習-524">演習 5.24</a></h2>
<div class="panel-primary">
<p>入力への変換
$$
x_{i} \rightarrow \widetilde{x}<em>{i}=a x</em>{i}+b \tag{5.115}
$$
の下で，重みとバイアスが
$$
w_{j i} \rightarrow \widetilde{w}<em>{j i}=\frac{1}{a} w</em>{j i} \tag{5.116}
$$
と
$$
w_{j 0} \rightarrow \widetilde{w}<em>{j 0}=w</em>{j 0}-\frac{b}{a} \sum_{i} w_{j i} \tag{5.117}
$$
を用いて同時に変換されれば，
$$
z_{j}=h\left(\sum_{i} w_{j i} x_{i}+w_{j 0}\right) \tag{5.113}
$$
と
$$
y_{k}=\sum_{j} w_{k j} z_{j}+w_{k 0} \tag{5.114}
$$
で定義されたネットワーク関数は不変であることを確かめよ．同様に，ネットワーク出力は
$$
w_{k j} \rightarrow \widetilde{w}<em>{k j}=c w</em>{k j} \tag{5.119}
$$
と
$$
w_{k 0} \rightarrow \widetilde{w}<em>{k 0}=c w</em>{k 0}+d \tag{5.120}
$$
の変換を第2層の重みとバイアスに施すことにより，
$$
y_{k} \rightarrow \widetilde{y}<em>{k}=c y</em>{k}+d \tag{5.118}
$$
に従って変換できることを示せ．</p>
</div>
<p>（前半）$x_i \to \tilde{x}<em>i$, $w</em>{ji} \to \tilde{w}<em>{ji}$, $w</em>{j0} \to \tilde{w}_{j0}$が同時に満たされれば、$z_j$と$y_k$が不変であることを示せばよい。</p>
<p>$$
\begin{aligned}
h\left(\sum_{i} \tilde{w}<em>{j i} \tilde{x</em>{i}}+\tilde{w}<em>{j 0}\right) &amp;= h\left(\sum</em>{i}\left(\frac{1}{a} w_{j i}\right)\left(a x_{i}+b\right)+\left(w_{j0}-\frac{b}{a} \sum_{i} w_{j i}\right)\right) \
&amp;= h\left(\sum_{i} w_{j i} x_{i}+w_{j 0}\right) \
&amp;= z_{j}
\end{aligned}
$$</p>
<p>$\sum_{j}\tilde{w}<em>{kj} \tilde{z</em>{j}}+\tilde{w}<em>{k 0}$について、入力の変換$\tilde{z}<em>j = az</em>{j}+b$を行う。
$$
\begin{aligned}
\sum</em>{j}\tilde{w}<em>{kj} \tilde{z</em>{j}}+\tilde{w}<em>{k 0} &amp;=\sum</em>{j}\left(\frac{1}{a} w_{k j}\left(a z_{j}+b\right)\right)+\left(w_{k 0}-\frac{b}{a} \sum_{j} w_{k j}\right) \
&amp;=\sum_{j} w_{k j}+w_{k 0} \
&amp;=y_{k}
\end{aligned}
$$
以上から$z_j$と$y_k$が不変であることが示された。</p>
<p>（後半）
$(5.114)$式の右辺について$w_{k j} \rightarrow \widetilde{w}<em>{k j}, w</em>{k 0} \rightarrow \widetilde{w}<em>{k 0}$とすると
$$
\begin{aligned}
\sum</em>{j} \tilde{w}<em>{k j} z</em>{j}+\tilde{w}<em>{k 0} &amp;= \sum</em>{j}\left(c w_{k j}\right) z_{j}+c w_{k 0}+d \
&amp;= c\left(\sum_{j} w_{k j} z_{j}+w_{k 0}\right)+d \
&amp;= c y_{k}+d
\end{aligned}
$$
より、$(5.118)$式の変換が成立することが示された。</p>
<h2 id="演習-525"><a class="header" href="#演習-525">演習 5.25</a></h2>
<div class="panel-primary">
<p>（難問）二次誤差関数</p>
<p>$$
E=E_{0}+\frac{1}{2}\left(\mathbf{w}-\mathbf{w}^{<em>}\right)^{\mathrm{T}} \mathbf{H}\left(\mathbf{w}-\mathbf{w}^{</em>}\right) \tag{5.195}
$$</p>
<p>を考える．ただし，$\mathbf{w}^{\star}$は最小値を表し，ヘッセ行列$\mathbf{H}$は正定値で定数とする．重みベクトルは初期値$\mathbf{w}^{(0)}$が原点であり，単純な勾配降下法
$$
\mathbf{w}^{(\tau)}=\mathbf{w}^{(\tau-1)}-\rho \nabla E \tag{5.196}
$$
によって更新されるとしよう．ただし，$\tau$はステップ数，$\rho$は学習率（小さいと仮定）を表す．$\tau$ステップ後に，$\mathbf{H}$の固有ベクトルに平行な重みベクトルの成分は
$$
w_{j}^{(\tau)}=\left{1-\left(1-\rho \eta_{j}\right)^{\tau}\right} w_{j}^{\star} \tag{5.197}
$$
と書けることを示せ．ただし，$w_j = \mathbf{w}^{\mathrm T}\mathbf{u}_j$,$\mathbf{u}_j$と$\eta_j$はそれぞれ$\mathbf{H}$の固有ベクトルと固有値で，</p>
<p>$$
\mathbf{H}\mathbf{u}<em>j = \eta</em>{j}\mathbf{u}_j \tag{5.198}
$$</p>
<p>とする．もし$|1-\rho\eta_j|&lt;1$ならば，$\tau \to \infty$において期待通り$\mathbf{w}^{(\tau)}\to \mathbf{w}^{\star}$が与えられることを示せ．もし訓練が有限ステップ数$\tau$で止まったなら，ヘッセ行列の固有ベクトルに平行な重みベクトルの成分は
$$
w_{j}^{(\tau)} \simeq w_{j}^{\star} \left(\eta_{j} \gg(\rho \tau)^{-1}\right) \tag{5.199}
$$
$$
\left|w_{j}^{(r)}\right| \ll\left|w_{j}^{\star}\right| \left(\eta_{j} \ll(\rho \tau)^{-1}\right) \tag{5.200}
$$
を満たすことを示せ．この結果を，3.5.3節での単純な荷重減衰による正則化の議論と比較し，$(\rho\tau)^{-1}$が正則化パラメータ$\require{enclose}\enclose{horizontalstrike}{\lambda}\alpha$(←誤植)に相当することを示せ．上の結果は
$$
\tau=\sum_{i} \frac{\lambda_{i}}{\alpha+\lambda_{i}} \tag{3.91}
$$
で定義されるネットワークの有効パラメータ数が，訓練が進むにつれて増加することも示している．</p>
</div>
<p>誤差関数の式$(5.195)$からヘッセ行列の計算を行う。微分すると</p>
<p>$$
\nabla E=\mathbf{H}\left(\mathbf{w}-\mathbf{w}^{\star}\right)
$$</p>
<p>なので、$(5.196)$式に代入すれば
$$
\mathbf{w}^{(\tau)}=\mathbf{w}^{(\tau-1)}-\rho \mathbf{H}\left(\mathbf{w}^{(\tau-1)}-\mathbf{w}^{\star}\right)
$$</p>
<p>$\mathbf{H}$の固有ベクトル$\mathbf{u}_j$を使い、$w_j^{(\tau)} = \mathbf{u}_j^{\mathrm{T}}\mathbf{w}^{(\tau)}$を用いると</p>
<p>$$
\begin{aligned}
w_{j}^{(\tau)} &amp;=\mathbf{u}<em>{j}^{\mathrm{T}} \mathbf{w}^{(\tau)} \
&amp;=\mathbf{u}</em>{j}^{\mathrm{T}} \mathbf{w}^{(\tau-1)}-\rho \mathbf{u}<em>{j}^{\mathrm{T}} \mathbf{H}\left(\mathbf{w}^{(\tau-1)}-\mathbf{w}^{\star}\right) \
&amp;=w</em>{j}^{(r-1)}-\rho \eta_{j} \mathbf{u}<em>{j}^{\mathrm{T}}\left(\mathbf{w}-\mathbf{w}^{\star}\right) \
&amp;=w</em>{j}^{(\tau-1)}-\rho \eta_{j}\left(w_{j}^{(\tau-1)}-w_{j}^{\star}\right)
\end{aligned} \tag{*}
$$
となる。
この式をもとに、数学的帰納法を用いてすべての整数$\tau$について$(5.197)$式が成立することを示す。</p>
<p>まず$\tau=0$について
$$
w_j^{(0)} = { 1- (1-\rho\eta_j)^0}w_j^{\star} = 0
$$
初期座標が$\mathbf{0}$なので成立している。次に$\tau=1$について$(<em>)$の結果を用いると
$$
\begin{aligned}
w_{j}^{(1)} &amp;=w_{j}^{(0)}-\rho \eta_{j}\left(w_{j}^{(0)}-w_{j}^{\star}\right) \
&amp;=\rho \eta_{j} w_{j}^{</em>} \
&amp;=\left{1-\left(1-\rho \eta_{j}\right)\right} w_{j}^{\star}
\end{aligned}
$$
これは$(5.197)$式に$\tau=1$を代入したものと同じになるので$\tau=1$のときにも成立することが示された。</p>
<p>次に$\tau= N-1$で$(5.197)$式が成立すると仮定したとき、$\tau=N$において
$$
\begin{aligned}
w_{j}^{(N)} &amp;=w_{j}^{(N-1)}-\rho \eta_{j}\left(w_{j}^{(N-1)}-w_{j}^{\star}\right) \
&amp;=w_{j}^{(N-1)}\left(1-\rho \eta_{j}\right)+\rho \eta_{j} w_{j}^{\star} \
&amp;=\left{1-\left(1-\rho \eta_{j}\right)^{N-1}\right} w_{j}^{<em>}\left(1-\rho \eta_{j}\right)+\rho \eta_{j} w_{j}^{</em>} \
&amp;=\left{\left(1-\rho \eta_{j}\right)-\left(1-\rho \eta_{j}\right)^{N}\right} w_{j}^{\star}+\rho \eta_{j} w_{j}^{*} \
&amp;=\left{1-\left(1-\rho \eta_{j}\right)^{N}\right} w_{j}^{\star}
\end{aligned}
$$
となり、$\tau=N$のときにも成立することが示された。</p>
<p>上式から$|1-\rho \eta_j| &lt; 1$ならば$(1-\rho \eta_j)^N \to 0$となるため、$\tau \to \infty$にて$w_j^{(\tau)} = w^{(\star)}$つまり$\mathbf{w}^{(\tau)} = \mathbf{w}^{(\star)}$が与えられる。</p>
<p>$\tau$が有限で$\eta_j \gg (\rho\tau)^{-1}$というのは$\eta_j \rho \tau \gg 1$を満たすので、$\tau$がとても大きい状態に相当する。これは上の議論から$w_j^{(\tau)} \simeq w^{(\star)}$となる。</p>
<p>$\eta_j \ll (\rho\tau)^{-1}$すなわち$\eta_j\rho\tau \ll 1$ならば、$\tau$が整数であることから$\rho\eta_j \ll 1$の状況であることが求められる。これより
$$
(1-\rho\eta_j)^{\tau} = 1-\tau\rho\eta_j + O(\rho^2\eta_j^2)
$$
とテイラー展開してみると
$$
\begin{aligned}
\left|w_{j}^{(\tau)}\right| &amp;=\left|\left{1-\left(1-\rho \eta_{j}\right)^{\tau}\right} w_{j}^{\star}\right| \
&amp;=\left|\left{1-\left(1-\tau \rho \eta_{j}+O\left(\rho^{2} \eta_{j}^{2}\right)\right)\right} w_{j}^{\star}\right| \
&amp; \simeq \tau \rho \eta_{j}\left|w_{j}^{\star}\right|
\end{aligned}
$$
となり、これは$|w_j^{(\tau)}|\ll\left|w_{j}^{\star}\right|$となる。</p>
<p>3.5.3節の議論から、この節で$\alpha$とされていた正則化パラメータが固有ベクトルの1つ$\lambda_i$よりもとても大きな値（$\lambda_i \ll \alpha$）のとき、対応する$w_i$の値は$0$に近くなる。反対に$\lambda_i \gg \alpha$ならば$w_i$は最尤推定値に最も近くなる。このことから、$\alpha$は$(\rho\tau)^{-1}$の役割ととても似ていることがわかる。</p>
<h2 id="演習-526"><a class="header" href="#演習-526">演習 5.26</a></h2>
<div class="panel-primary">
<p>任意のフィードフォワード構造を持つ多層パーセプトロンを考える．ここで訓練には，正則化関数として
$$
\Omega=\frac{1}{2} \sum_{n} \sum_{k}\left(\left.\frac{\partial y_{n k}}{\partial \xi}\right|<em>{\xi=0}\right)^{2}=\frac{1}{2} \sum</em>{n} \sum_{k}\left(\sum_{i=1}^{D} J_{n k i} \tau_{n i}\right)^{2} \tag{5.128}
$$
を持つ接線伝播誤差関数
$$
\widetilde{E} = E + \lambda\Omega \tag{5.127}
$$
の最小化を用いる．正則化項$\Omega$が，全パターンについて
$$
\Omega_{n}=\left. \frac{1}{2} \sum_{k}\left(\mathcal{G} y_{k}\right)^{2}\right|_{\mathbf{x}_n} \tag{5.201}
$$
という形の項を足し合わせたもので書けることを示せ．ここで$\mathcal{G}$は</p>
<p>$$
\mathcal{G} \equiv \sum_{i} \tau_{i} \frac{\partial}{\partial x_{i}} \tag{5.202}
$$</p>
<p>で定義される微分演算子である．演算子$\mathcal{G}$と順伝播方程式
$$
z_{j}=h\left(a_{j}\right), \quad a_{j}=\sum_{i} w_{j i} z_{i} \tag{5.203}
$$
を利用し，$\Omega_n$が
$$
\alpha_{j}=h^{\prime}\left(a_{j}\right) \beta_{j}, \quad \beta_{j}=\sum_{i} w_{j i} \alpha_{i} \tag{5.204}
$$
という方程式を用いた順伝播によって評価できることを示せ．ただし
$$
\alpha_{j} \equiv \mathcal{G} z_{j}, \quad \beta_{j} \equiv \mathcal{G} a_{j} \tag{5.205}
$$
と定義した．上の結果を用いて，$\Omega_n$のネットワーク内の重み$w_{rs}$に関する微分が
$$
\frac{\partial \Omega_{n}}{\partial w_{r s}}=\sum_{k} \alpha_{k}\left{\phi_{k r} z_{s}+\delta_{k r} \alpha_{s}\right} \tag{5.206}
$$
という形で書けることを示せ．ただし
$$
\delta_{k r} \equiv \frac{\partial y_{k}}{\partial a_{r}}, \quad \phi_{k r} \equiv \mathcal{G} \delta_{k r} \tag{5.207}
$$
と定義した．$\delta_{kr}$についての逆伝播方程式を書き下し，$\phi_{kr}$を評価するための逆伝播方程式系を導け．</p>
</div>
<p>※ この問題では、厳密にはある1つの入力$\mathbf{x}<em>n$に依存する正則化項$\Omega</em>{n}$を考える必要があるので、$(5.201)$など必要に応じて下付き文字$n$をつけて考えることにする（が、実際に問題を解く上ではあまり影響はない）</p>
<p>$(5.201)$式について$(5.202)$式を用いて書き表すと
$$
\begin{aligned}
\Omega_{n} &amp;=\left.\frac{1}{2} \sum_{k}\left(\sum_{i} \tau_{n i} \frac{\partial y_{nk}}{\partial x_{n i}}\right)^{2}\right|<em>{\mathbf{x}</em>{n}} \
&amp;=\left.\frac{1}{2} \sum_{k}\left(\sum_{i=1}^{D} J_{nki} \tau_{n i}\right)^{2}\right|<em>{\mathbf{x}</em>{n}}
\end{aligned}
$$
すべての$n$について足し合わせると
$$
\sum_{n} \Omega_{n}=\frac{1}{2} \sum_{n} \sum_{k}\left(\sum_{i=1}^{D} J_{n k i} \tau_{n i}\right)^{2}
$$
となり、$(5.128)$式を得ることができる。</p>
<p>$(5.204)$式について
$$
\begin{aligned}
\alpha_{j}=\mathcal{G} z_{j} &amp;=\sum_{i} \tau_{i} \frac{\partial}{\partial x_{i}} h\left(a_{j}\right) \
&amp;=\sum_{i} \tau_{i} \frac{\partial h\left(a_{i}\right)}{\partial a_{j}} \frac{\partial}{\partial x_{i}} a_{i} \
&amp;=h^{\prime}\left(a_{j}\right) \mathcal{G} a_{j} \
&amp;=h^{\prime}\left(a_{j}\right) \beta_{j} \
\beta_{j}=\mathcal{G} a_{j} &amp;=\sum_{i} \tau_{i} \frac{\partial}{\partial x_{i}} \sum_{l} w_{j l} z_{l} \
&amp;=\sum_{l} w_{jl}\left(\sum_{i} r_{i} \frac{\partial}{\partial x_{i}} z_{l}\right) \
&amp;=\sum_{l} w_{jl} \mathcal{G}z_{l} \
&amp;=\sum_{l} w_{jl} \alpha_{l}
\end{aligned}
$$
より、$(5.204)$式が示された。また、インプット層について計算をさらに進めると
$$
\begin{aligned}
\beta_{n j} &amp;=\sum_{l} w_{j l} \alpha_{n l} \
&amp;=\sum_{l} w_{jl} \mathcal{G} x_{n l} \
&amp;=\sum_{l} w_{jl} \sum_{l^{\prime}} \tau_{nl^{\prime}} \frac{\partial x_{nl}}{\partial x_{n l^{\prime}}} \
&amp;=\sum_{l} w_{jl} \tau_{nl}
\end{aligned}
$$
となり$\tau_{n}$が$(5.204)$式によって順伝播していることが示された。</p>
<p>$(5.206)$式について
$$
\begin{aligned}
\frac{\partial \Omega_{n}}{\partial w_{r s}} &amp;=\frac{1}{2} \frac{\partial}{\partial w_{rs}} \sum_{k}\left(\mathcal{G}y_{n k}\right)^{2} \
&amp;=\sum_{k}\left(\mathcal{G}y_{n k}\right) \frac{\partial}{\partial w_{r s}} \mathcal{G} y_{n k} \
&amp;=\sum_{k} \alpha_{n k} \frac{\partial}{\partial w_{r s}} \sum_{i} \tau_{i} \frac{\partial}{\partial x_{n i}} y_{n k} \
&amp;=\sum_{k} \alpha_{n k} \sum_{i} \tau_{i} \frac{\partial}{\partial x_{n i}}\left(\frac{\partial}{\partial w_{r s}} y_{n k}\right) \
&amp;=\sum_{k} \alpha_{n k}\left( \mathcal{G} \left(\delta_{nkr} z_{n s}\right)\right) \quad \left( \because \frac{\partial y_{n k}}{\partial w_{r s}}=\frac{\partial y_{n k}}{\partial a_{n r}} \frac{\partial a_{n r}}{\partial w_{r s}}=\delta_{nkr} z_{ns} \quad (\textrm{eq}\ 5.52)\right)\
&amp;=\sum_{k} \alpha_{n k}\left( (\mathcal{G} \delta_{n k r}) z_{n s}+\delta_{n k r}\left(\mathcal{G} z_{n s}\right)\right) \
&amp;=\sum_{k} \alpha_{n k}\left{\phi_{n k r} z_{n s}+\delta_{nkr} \alpha_{n s}\right}
\end{aligned}
$$</p>
<p>$\delta_{nkr}$についての逆伝播方程式は</p>
<p>$$
\begin{aligned}
\delta_{nkr} \equiv \frac{\partial y_{nk}}{\partial a_{n r}} &amp;=\sum_{l} \frac{\partial y_{n k}}{\partial a_{n l}} \frac{\partial a_{n l}}{\partial a_{n r}} \
&amp;=\sum_{l} \frac{\partial y_{n k}}{\partial a_{n l}} \frac{\partial}{\partial a_{n r}}\left(\sum_{r} w_{lr} h\left(a_{n r}\right)\right) \
&amp;=h^{\prime}(a_{nr})\sum_{l}w_{lr}\frac{\partial y_{nk}}{\partial a_{nl}} \
&amp;=h^{\prime}(a_{nr})\sum_{l}w_{lr}\delta_{nkl}
\end{aligned}
$$</p>
<p>となり、これを用いた$\phi_{nkr}$を評価する逆伝播方程式は</p>
<p>$$
\begin{aligned}
\phi_{n k r} \equiv \mathcal{G} \delta_{n k r} &amp;=\sum_{i} r_{i} \frac{\partial}{\partial x_{i}}\left(h^{\prime}\left(a_{n r}\right) \sum_{l} w_{lr} \delta_{n k l}\right) \
&amp;=\sum_{i} \tau_{i}\left{\left(\frac{\partial}{\partial x_{i}} h^{\prime}\left(a_{n r}\right)\right) \sum_{l} w_{lr} \delta_{nkl}+h^{\prime}\left(a_{n r}\right)\left(\frac{\partial}{\partial x_{i}} \sum_{l} w_{lr} \delta_{n k l}\right)\right} \
&amp;=\sum_{i} \tau_{i}\left(h^{\prime \prime}\left(a_{n r}\right) \frac{\partial a_{n r}}{\partial x_{i}}\right) \sum_{l} w_{lr} \delta_{nkl}+h^{\prime}\left(a_{n r}\right) \sum_{l} w_{lr} \mathcal{G} \delta_{nkl} \
&amp;=h^{\prime \prime}\left(a_{n r}\right) \mathcal{G} a_{n r} \sum_{l} w_{lr} \delta_{nkl}+h^{\prime}\left(a_{n r}\right) \sum_{l} w_{lr} \mathcal{G} \delta_{nkl} \
&amp;=h^{\prime \prime}\left(a_{n r}\right) \beta_{n r} \sum_{l} w_{lr} \delta_{nkl}+h^{\prime}\left(a_{n r}\right) \sum_{l} w_{lr} \mathcal{G} \delta_{nkl}
\end{aligned}
$$</p>
<p>と書き下せる。</p>
<h2 id="演習-527"><a class="header" href="#演習-527">演習 5.27</a></h2>
<div class="panel-primary">
<p>変換がランダムノイズの加算$\mathbf{x}\to\mathbf{x}+\boldsymbol{\xi}$のみであるという特別な場合について，変換されたデータを訓練する枠組みを考える．ただし，$\boldsymbol{\xi}$は平均がゼロ，分散が単位行列のガウス分布を持つとする．5.5.5節での議論と類似の議論に従って，結果として得られる正則化項はTikhonov正則化項
$$
\Omega=\frac{1}{2} \int|\nabla y(\mathbf{x})|^{2} p(\mathbf{x}) \mathrm{d} \mathbf{x} \tag{5.135}
$$
に帰着することを示せ．</p>
</div>
<p>※ $\mathbf{s}(\mathbf{x},\boldsymbol{\xi}) = \mathbf{x} + \boldsymbol{\xi}$である特別な場合において$5.5.5$節の議論を展開する。$(5.130)$式に導入して$\Omega$を計算すれば良いのだが、ベクトルの場合のテイラー展開などを丁寧に考える必要があるので計算は大変……というか難しすぎる？</p>
<p>まず$y(\mathbf{s}(\mathbf{x},\boldsymbol{\xi})) = y(\mathbf{x}+\boldsymbol{\xi})$を$\boldsymbol{\xi}$でテイラー展開すると
$$
y(\mathbf{x}+\boldsymbol{\xi}) = y(\mathbf{x}) + \nabla y(\boldsymbol{x})\boldsymbol{\xi} + \frac{1}{2}\boldsymbol{\xi}^{\mathrm{T}}\nabla \nabla y(\mathbf{x}) \boldsymbol{\xi}+O(\boldsymbol{\xi}^3)
$$
ここで、$\nabla y(\mathbf{x})$は$\frac{\partial y}{\partial \xi_i}$を成分とする行ベクトル（なので$\nabla y(\boldsymbol{x})\boldsymbol{\xi}$はスカラー値）である。これより
$$
\begin{aligned}
{y(\mathbf{x}+\boldsymbol{\xi})-t}^{2} &amp;=\left{(y(\mathbf{x})-t)+\nabla y(\mathbf{x}) \boldsymbol{\xi}+\frac{1}{2} \boldsymbol{\xi}^{\mathrm{T}} \nabla \nabla y(\mathbf{x}) \boldsymbol{\xi}+O\left(\boldsymbol{\xi}^{3}\right)\right}^{2} \
&amp;=(y(\mathbf{x})-t)^{2}+(\nabla y(\mathbf{x}) \boldsymbol{\xi})^{2}+2 \nabla y(\mathbf{x}) \boldsymbol{\xi}(y(\mathbf{x})-t) +\boldsymbol{\xi}^{\mathrm{T}} \nabla \nabla y(\mathbf{x}) \boldsymbol{\xi}(y(\mathbf{x})-t)+O(\boldsymbol{\xi}^{3})
\end{aligned}
$$
となる。
この式と、以下の計算
$$
\begin{aligned}
(\nabla y(\mathbf{x}) \boldsymbol{\xi})^{2} &amp;=\boldsymbol{\xi}^{\mathrm{T}} \nabla y(\mathbf{x})^{\mathrm{T}} \nabla y(\mathbf{x}) \boldsymbol{\xi}\
\int \boldsymbol{\xi} p(\boldsymbol{\xi}) d \boldsymbol{\xi} &amp;=\mathbb{E}[\boldsymbol{\xi}]=0, \int p(\boldsymbol{\xi}) d \boldsymbol{\xi}=1
\end{aligned}
$$
を用いて$(5.130)$式に代入すると</p>
<p>$$
\begin{aligned}
\tilde{E} &amp;= \frac{1}{2} \iint{y(\mathbf{x})-t}^{2} p(t \mid \mathbf{x}) p(\mathbf{x}) \int p(\boldsymbol{\xi}) d \boldsymbol{\xi} d \mathbf{x} d t + \iiint \nabla y(\mathbf{x})\boldsymbol{\xi} p(\boldsymbol{\xi}) d \boldsymbol{\xi} (y(\mathbf{x})-t) p(t \mid \mathbf{x}) p(\mathbf{x}) d \mathbf{x} d t \
&amp;+\frac{1}{2} \iiint \boldsymbol{\xi}^{\mathrm{T}} \nabla \nabla y(\mathbf{x})(y(\mathbf{x})-t) \boldsymbol{\xi} p(t \mid \mathbf{x})p(\mathbf{x}) d \mathbf{x} d t d \boldsymbol{\xi} \
&amp;+\frac{1}{2} \iiint \boldsymbol{\xi}^{\mathrm{T}} \nabla y(\mathbf{x})^{\mathrm{T}} \nabla y(\mathbf{x}) \boldsymbol{\xi} p(t \mid \mathbf{x}) p(\mathbf{x}) d \mathbf{x} d t d \boldsymbol{\xi} \
&amp;= \frac{1}{2} \iint{y(\mathbf{x})-t}^{2} p(t \mid \mathbf{x}) p(\mathbf{x}) d \mathbf{x} d t + \mathbb{E}[\boldsymbol{\xi}] \iiint \nabla y(\mathbf{x})(y(\mathbf{x})-t) p(t \mid \mathbf{x}) p(\mathbf{x}) d \mathbf{x} d t \
&amp;+ \frac{1}{2}\iiint \boldsymbol{\xi}^{\mathrm{T}} \left[ (y(\mathbf{x})-t)\nabla \nabla y(\mathbf{x}) + \nabla y(\mathbf{x})^{\mathrm{T}} \nabla y(\mathbf{x})\right] \boldsymbol{\xi} p(\boldsymbol{\xi}) p(t \mid \mathbf{x}) p(\mathbf{x}) p(\boldsymbol{\xi}) d \mathbf{x} dt d \boldsymbol{\xi} \
&amp;\equiv E + \Omega
\end{aligned}
$$
となる（これ正則化係数$\lambda$がないけれどいいんですかね？）。ここで、$E$はもとの二乗和誤差関数であり、$\Omega$は
$$
\begin{aligned}
\Omega &amp;= \frac{1}{2}\iiint \boldsymbol{\xi}^{\mathrm{T}} \left[ (y(\mathbf{x})-t)\nabla \nabla y(\mathbf{x}) + \nabla y(\mathbf{x})^{\mathrm{T}} \nabla y(\mathbf{x})\right] \boldsymbol{\xi} p(\boldsymbol{\xi}) p(t \mid \mathbf{x}) p(\mathbf{x})  d \mathbf{x} dt d \boldsymbol{\xi} \
&amp;= \frac{1}{2}\iint \boldsymbol{\xi}^{\mathrm{T}} \left[ \left{ y(\mathbf{x})-\mathbb{E}[t\mid \mathbf{x}]\right}\nabla \nabla y(\mathbf{x}) + \nabla y(\mathbf{x})^{\mathrm{T}} \nabla y(\mathbf{x})\right] \boldsymbol{\xi} p(\boldsymbol{\xi}) p(\mathbf{x}) d \mathbf{x} d \boldsymbol{\xi}\end{aligned}
$$
で与えられる関数である。</p>
<p>ここで、1.5.5節の議論と$(5.133)$の式から正則化項$\Omega$の括弧内の第1項は
$$
y(\mathbf{x})-\mathbb{E}[t\mid \mathbf{x}] = O(\boldsymbol{\xi})
$$
となるのに対し、$\Omega$は$O(\boldsymbol{\xi}^3)$の項を無視していることから、上の$\Omega$で残るのは
$$
\Omega \simeq \frac{1}{2}\iint \boldsymbol{\xi}^{\mathrm{T}} \left[ \nabla y(\mathbf{x})^{\mathrm{T}} \nabla y(\mathbf{x})\right] \boldsymbol{\xi} p(\boldsymbol{\xi}) p(\mathbf{x}) d \mathbf{x} d \boldsymbol{\xi}
$$
となり、これは
$$
\begin{aligned}
\Omega &amp; \simeq \frac{1}{2} \iint \boldsymbol{\xi}^{\mathrm{T}}\left(\nabla y(\mathbf{x})^{\mathrm{T}} \nabla y(\mathbf{x})\right) \boldsymbol{\xi} p(\boldsymbol{\xi}) p(\mathbf{x}) \mathrm{d} \boldsymbol{\xi} \mathrm{d} \mathbf{x} \
&amp;=\frac{1}{2} \iint \operatorname{Tr}\left[\left(\boldsymbol{\xi} \boldsymbol{\xi}^{\mathrm{T}}\right)\left(\nabla y(\mathbf{x})^{\mathrm{T}} \nabla y(\mathbf{x})\right)\right] p(\boldsymbol{\xi}) p(\mathbf{x}) \mathrm{d} \boldsymbol{\xi} \mathrm{d} \mathbf{x} \
&amp;=\frac{1}{2} \int \operatorname{Tr}\left[\mathbf{I}\left(\nabla y(\mathbf{x})^{\mathrm{T}} \nabla y(\mathbf{x})\right)\right] p(\mathbf{x}) \mathrm{d} \mathbf{x} \
&amp;=\frac{1}{2} \int \nabla y(\mathbf{x})^{\mathrm{T}} \nabla y(\mathbf{x}) p(\mathbf{x}) \mathrm{d} \mathbf{x}=\frac{1}{2} \int|\nabla y(\mathbf{x})|^{2} p(\mathbf{x}) \mathrm{d} \mathbf{x}
\end{aligned}
$$</p>
<p>となる。ここで、$\boldsymbol{\xi}$が平均$\mathbf{0}$，分散が単位行列のガウス分布なので$(2.62)$式から$\mathbb{E}(\boldsymbol{\xi} \boldsymbol{\xi}^{\mathrm{T}}) = \mathbf{I}$となることを用いた。</p>
<h2 id="演習-528"><a class="header" href="#演習-528">演習 5.28</a></h2>
<div class="panel-primary">
<p>5.5.6節で議論したたたみ込みニューラルネットワークのような，複数の重みが同じ値を持つように制約されているニューラルネットワークを考える．そのような制約条件を満たすためには，ネットワーク内の調整可能なパラメータに関する誤差関数の微分を評価する際に，標準的な逆伝播アルゴリズムをどのように変更しなければならないかを議論せよ．</p>
</div>
<p>※</p>
<h2 id="演習-529"><a class="header" href="#演習-529">演習 5.29</a></h2>
<div class="panel-primary">
<p>$$
\frac{\partial \widetilde{E}}{\partial w_{i}}=\frac{\partial E}{\partial w_{i}}+\lambda\sum_{j} \gamma_{j}\left(w_{i}\right) \frac{\left(w_{i}-\mu_{j}\right)}{\sigma_{j}^{2}} \tag{5.141}
$$
の結果を確かめよ．</p>
</div>
<p>※<strong>テキストの$(5.141)$式では$\lambda$が抜けている誤植がある</strong>。$(5.142)$,$(5.143)$も同様。</p>
<p>$(5.139)$を用いるが、$(5.139)$式は$(5.138)$式に依存しているので先に$(5.138)$式の$w_i$についての微分を取る。このとき、$(1.46)$式の微分を先に計算しておく。</p>
<p>$$
\mathcal{N}\left(x \mid \mu, \sigma^{2}\right)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right} \tag{1.46}
$$
の微分形は
$$
\frac{\partial \mathcal{N}}{\partial x}=-\frac{(x-\mu)}{\sigma^{2}} \mathcal{N}\left(x \mid \mu, \sigma^{2}\right)
$$
である。</p>
<p>$$
\Omega(\mathbf{w})=-\sum_{i} \ln \left(\sum_{j=1}^{M} \pi_{j} \mathcal{N}\left(w_{i} \mid \mu_{j}, \sigma_{j}^{2}\right)\right) \tag{5.138}
$$
の$w_i$についての微分を取ると
$$
\begin{aligned}
\frac{\partial \Omega}{\partial w_{i}} &amp;= \frac{-1}{\sum_{k=1}^{M} \pi_{k} \mathcal{N}\left(w_{i} \mid u_{k}, \sigma_{k}^{2}\right)} \sum_{j=1}^{M} \pi_{j} \left(\frac{\partial \mathcal{N}\left(w_{i} \mid \mu_{i}, \sigma_{j}^{2}\right)}{\partial w_{i}}\right) \
&amp;= \frac{1}{\sum_{k=1}^{M} \pi_{k} \mathcal{N}\left(w_{i} \mid \mu_{k}, \sigma_{k}^{2}\right)} \sum_{j=1}^{M} \pi_{j} \mathcal{N}\left(w_{i} \mid \mu_{j}, \sigma_{j}^{2}\right) \frac{\left(w_{i}-\mu_{j}\right)}{\sigma^{2}}
\end{aligned}
$$
これを$\tilde{E}(\mathbf{w})=E(\mathbf{w})+\lambda \Omega(\mathbf{w})\hspace{1em}(5.139)$式の微分形
$$
\frac{\partial \widetilde{E}}{\partial w_{i}}=\frac{\partial E}{\partial w_{i}}+\lambda \frac{\partial \Omega}{\partial w_{i}}
$$
に代入すると
$$
\frac{\partial \widetilde{E}}{\partial w_{i}}=\frac{\partial E}{\partial w_{i}}+\lambda\sum_{j} \gamma_{j}\left(w_{i}\right) \frac{\left(w_{i}-\mu_{j}\right)}{\sigma_{j}^{2}},\ \textrm{where}\ \gamma_{j}(w_{i})=\frac{\pi_{j} \mathcal{N}\left(w_{i} \mid \mu_{j}, \sigma_{j}^{2}\right)}{\sum_{k} \pi_{k} \mathcal{N}\left(w_{i} \mid \mu_{k}, \sigma_{k}^{2}\right)} \tag{5.141}
$$
が得られる。</p>
<h2 id="演習-530"><a class="header" href="#演習-530">演習 5.30</a></h2>
<div class="panel-primary">
<p>$$
\frac{\partial \widetilde{E}}{\partial \mu_{j}}=\lambda \sum_{i} \gamma_{j}\left(w_{i}\right) \frac{\left(\mu_{j}-w_{i}\right)}{\sigma_{j}^{2}} \tag{5.142}
$$
の結果を確かめよ．</p>
</div>
<p>※テキストの$(5.142)$でも$\lambda$が抜けている誤植があるので注意。</p>
<p>$\mu_j$は$E$の項には現れず$\Omega(\mathbf{w})$の項にのみ現れるので、$(5.139)$式の微分は</p>
<p>$$
\frac{\partial \widetilde{E}}{\partial \mu_j}=\lambda \frac{\partial \Omega}{\partial \mu_j}
$$
となる。$(5.138)$式の$\mu_j$についての微分は
$$
\begin{aligned}
\frac{\partial \Omega}{\partial \mu_{j}} &amp;=-\sum_{i} \frac{1}{\sum_{k=1}^{M} \pi_{k} \mathcal{N}\left(w_{i} \mid \mu_{k} \sigma_{k}^{2}\right)} \pi_{j} \mathcal{N}\left(w_{i} \mid \mu_{j}, \sigma_{j}^{2}\right) \frac{w_{i}-\mu_{j}}{\sigma_{j}^{2}} \
&amp;=-\sum_{i} \gamma_{j}\left(w_{i}\right) \frac{w_{i}-\mu_{j}}{\sigma_{j}^{2}}
\end{aligned}
$$
よって
$$
\frac{\partial \widetilde{E}}{\partial \mu_{j}}=\lambda \sum_{i} \gamma_{j}\left(w_{i}\right) \frac{\left(\mu_{j}-w_{i}\right)}{\sigma_{j}^{2}} \tag{5.142}
$$
を得る。</p>
<h2 id="演習-531"><a class="header" href="#演習-531">演習 5.31</a></h2>
<div class="panel-primary">
<p>$$
\frac{\partial \tilde{E}}{\partial \sigma_{j}}=\lambda \sum_{i} \gamma_{j}\left(w_{i}\right)\left(\frac{1}{\sigma_{j}}-\frac{\left(w_{i}-\mu_{j}\right)^{2}}{\sigma_{j}^{3}}\right) \tag{5.143}
$$
の結果を確かめよ．</p>
</div>
<p>※ 演習問題 5.29, 5.30と同様。テキストの$(5.143)$でも$\lambda$が抜けている誤植があるので注意。</p>
<p>(5.139)より $\tilde{E}(\mathbf{w}) = E (\mathbf{w}) + \lambda \Omega(\mathbf{w})$
(5.138)より $\Omega(\mathbf{w}) = - \Sigma_i \ln (\Sigma_{j=1}^M \pi_j \mathcal{N} (w_j|µ_j,\sigma_j^2))$</p>
<br>
<p>$\frac{\partial E(\mathbf{w})}{\partial \sigma_j}=0 $ なので</p>
<p>$$
\begin{align}
\frac{\partial \tilde E(\mathbf{w})}{\partial \sigma_j} &amp;= \lambda\frac{\partial \Omega }{\partial \sigma_j}\
&amp;= \lambda\frac{\partial}{\partial \sigma_j} (- \Sigma_i \ln (\Sigma_{j=1}^M \pi_j \mathcal{N} (w_j|µ_j,\sigma_j^2))) \
\end{align}
$$</p>
<br>
<p>ガウス分布について</p>
<p>$$\mathcal{N}(x|µ, \sigma) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp -\frac{(x-µ)^2}{2\sigma^2}$$</p>
<p>であるので</p>
<p>$$
\begin{aligned}
\frac{\partial \mathcal{N}}{\partial \sigma_{j}} &amp;=(2 \pi)^{-\frac{1}{2}}\left[-\sigma_{j}^{-2} \exp \left{-\frac{\left(w_{i}-\mu_{j}\right)^{2}}{2 \sigma_{j}^{2}}\right}+\sigma_{j}^{-1} \sigma_{j}^{-3}\left(w_{i}-\mu_{j}\right)^{2}\exp \left{-\frac{\left(w_{i}-\mu_{j}\right)^{2}}{2 \sigma_{j}^{2}}\right} \right]\
&amp;=(2 \pi)^{-\frac{1}{2}}\left(-\sigma_{j}^{-2}+\frac{\left(w_{i}-\mu_{j}\right)^{2}}{\sigma_{j}^{-4}}\right) \exp \left{-\frac{\left(w_{i}-\mu_{j}\right)^{2}}{2 \sigma_{j}^{2}}\right} \
&amp;=-\left(\frac{1}{\sigma_{j}}-\frac{\left(w_{i}-\mu_{j}\right)^{2}}{\sigma_{j}^{3}}\right) \mathcal{N}\left(w_{i} \mid \mu_{j}, \sigma_{j}^{2}\right)
\end{aligned}
$$
より、
$$
\begin{aligned}
\frac{\partial \Omega}{\partial \sigma_{j}} &amp;=-\sum_{i} \frac{1}{\sum_{k=1}^{M} \pi_{k} \mathcal{N}\left(w_{i} \mid \mu_{k}, \sigma_{k}^{2}\right)} \pi_{j} \frac{\partial \mathcal{N}}{\partial \sigma_{j}} \
&amp;= -\sum_{i} \frac{\pi_j{-\frac{1}{\sigma_j}+\frac{(w_j-µ_j)^2}{\sigma^3_j}}{\mathcal{N}(w_j|µ_j,\sigma_j^2)}}{\Sigma^M_{j=1}\pi_k\mathcal{N}(w_i|µ_k,\sigma_k^2) }
\end{aligned}
$$</p>
<br>
<p>(5.140)より $\gamma_j(w)= \frac{\pi_j\mathcal{N}(w|µ_j,\sigma_j^2)}{\Sigma_k \pi_k \mathcal{N}(w|µ_k,\sigma_k^2)}$ を利用して</p>
<p>$$
\frac{\partial \Omega}{\partial \sigma_{j}} =\sum_{i} \gamma_{j}\left(w_{i}\right)\left(\frac{1}{\sigma_{j}}-\frac{\left(w_{i}-\mu_{j}\right)^{2}}{\sigma_{j}^{3}}\right)
$$</p>
<br>
よって
<p>$$
\frac{\partial \tilde{E}}{\partial \sigma_{j}}=\lambda \sum_{i} \gamma_{j}\left(w_{i}\right)\left(\frac{1}{\sigma_{j}}-\frac{\left(w_{i}-\mu_{j}\right)^{2}}{\sigma_{j}^{3}}\right) \tag{5.143}
$$
を得る。</p>
<h2 id="演習-532"><a class="header" href="#演習-532">演習 5.32</a></h2>
<div class="panel-primary">
<p>$$
\pi_{j}=\frac{\exp \left(\eta_{j}\right)}{\sum_{k=1}^{M} \exp \left(\eta_{k}\right)} \tag{5.146}
$$
で定義される混合係数${\pi_k }$の補助パラメータ${ \eta_j }$に関する微分が
$$
\frac{\partial \pi_{k}}{\partial \eta_{j}}=\delta_{j k} \pi_{j}-\pi_{j} \pi_{k} \tag{5.208}
$$
で与えられることを示せ．また，すべての$i$について$\sum_k \gamma_k (w_i)=1$という制約条件を利用して，
$$
\frac{\partial \widetilde{E}}{\partial \eta_{j}}=\lambda \sum_{i}\left{\pi_{j}-\gamma_{j}\left(w_{i}\right)\right} \tag{5.147}
$$
の結果を導け．</p>
</div>
<p>※<strong>テキストの$(5.147)$式でも$\lambda$が抜けている誤植がある</strong>。</p>
<p>前半部分は演習問題4.17と同じなので省略。ただ、文字$k$,$j$がややこしいので
$$
\pi_{k}=\frac{\exp \left(\eta_{k}\right)}{\sum_{j=1}^{M} \exp \left(\eta_{j}\right)} \tag{5.146}
$$
をもとに偏微分し、$j=k$と$j\neq k$の場合に分けて考える。結果的に$(5.208)$式が求まる。</p>
<p>後半は演習問題5.31までと同様に解いていく。</p>
<p>$$
\begin{aligned}
\frac{\partial \widetilde{E}}{\partial \eta_{j}} &amp;=\lambda \frac{\partial \Omega(\mathbf{w})}{\partial \eta_{j}} \
&amp;=-\lambda \frac{\partial}{\partial \eta_{j}}\left{\sum_{i} \ln \left(\sum_{j=1}^{M} \pi_{j} \mathcal{N}\left(w_{i} \mid \mu_{j}, \sigma_{j}^{2}\right)\right)\right} \
&amp;=-\lambda \sum_{i} \frac{\partial}{\partial \eta_{j}}\left{\ln \left(\sum_{j=1}^{M} \pi_{j} \mathcal{N}\left(w_{i} \mid \mu_{j}, \sigma_{j}^{2}\right)\right)\right} \
&amp;=-\lambda \sum_{i} \frac{1}{\sum_{j=1}^{M} \pi_{j} \mathcal{N}\left(w_{i} \mid \mu_{j}, \sigma_{j}^{2}\right)} \sum_{k=1}^{M} \frac{\partial}{\partial \eta_{j}}\left{\pi_{k} \mathcal{N}\left(w_{i} \mid \mu_{k}, \sigma_{k}^{2}\right)\right} \
&amp;=-\lambda \sum_{i} \frac{1}{\sum_{j=1}^{M} \pi_{j} \mathcal{N}\left(w_{i} \mid \mu_{j}, \sigma_{j}^{2}\right)} \sum_{k=1}^{M} \frac{\partial}{\partial \pi_{k}}\left{\pi_{k} \mathcal{N}\left(w_{i} \mid \mu_{k}, \sigma_{k}^{2}\right)\right} \frac{\partial \pi_{k}}{\partial \eta_{j}} \
&amp;=-\lambda \sum_{i} \frac{1}{\sum_{j=1}^{M} \pi_{j} \mathcal{N}\left(w_{i} \mid \mu_{j}, \sigma_{j}^{2}\right)} \sum_{k=1}^{M} \mathcal{N}\left(w_{i} \mid \mu_{k}, \sigma_{k}^{2}\right)\left(\delta_{kj} \pi_{j}-\pi_{j} \pi_{k}\right) \
&amp;=-\lambda \sum_{i} \frac{1}{\sum_{j=1}^{M} \pi_{j} \mathcal{N}\left(w_{i} \mid \mu_{j}, \sigma_{j}^{2}\right)}\left{\pi_{j} \mathcal{N}\left(w_{i} \mid \mu_{j}, \sigma_{j}^{2}\right)- \pi_{j} \sum_{k=1}^{M} \pi_{k} \mathcal{N}\left(w_{i} \mid \mu_{k}, \sigma_{k}^{2}\right)\right} \
&amp;=-\lambda \sum_{i}\left{\frac{\pi_{j} \mathcal{N}\left(w_{i} \mid \mu_{j}, \sigma_{j}^{2}\right)}{\sum_{j=1}^{M} \pi_{j} \mathcal{N}\left(w_{i} \mid \mu_{j}, \sigma_{j}^{2}\right)}-\frac{\left.\pi_{j} \sum_{k=1}^{M} \pi_{k} \mathcal{N}\left(w_{i} \mid \mu_{k}, \sigma_{k}^{2}\right)\right)}{\sum_{j=1}^{M} \pi_{j} \mathcal{N}\left(w_{i} \mid \mu_{j}, \sigma_{j}^{2}\right)}\right} \
&amp;=-\lambda \sum_{i}\left{\gamma_{j}\left(w_{i}\right)-\pi_{j}\right} \
&amp;=\lambda \sum_{i}\left{\pi_{j}-\gamma_{j}\left(w_{i}\right)\right}
\end{aligned}
$$</p>
<p>ちなみに後半部分の別解がある。</p>
<blockquote>
<p>Just as in Solutions 5.30 and 5.31, $j$ only affect $\widetilde{E}$ through $\Omega$. However, $j$ will affect $k$ for all values of $k$ (not just $j = k$). Thus we have
$$
\frac{\partial \Omega}{\partial \eta_{j}}=\sum_{k} \frac{\partial \Omega}{\partial \pi_{k}} \frac{\partial \pi_{k}}{\partial \eta_{j}} \tag{192}
$$
From $(5.138)$ and $(5.140)$, we get
$$
\frac{\partial \Omega}{\partial \pi_{k}}=-\sum_{i} \frac{\gamma_{k}\left(w_{i}\right)}{\pi_{k}}
$$
Substituting this and $(5.208)$ into $(192)$ yields
$$
\begin{aligned}
\frac{\partial \Omega}{\partial \eta_{j}} &amp;=\frac{\partial \widetilde{E}}{\partial \eta_{j}}=-\sum_{k} \sum_{i} \frac{\gamma_{k}\left(w_{i}\right)}{\pi_{k}}\left{\delta_{j k} \pi_{j}-\pi_{j} \pi_{k}\right} \
&amp;=\sum_{i}\left{\pi_{j}-\gamma_{j}\left(w_{i}\right)\right}
\end{aligned}
$$
where we have used the fact that $\sum_{k} \gamma_{k}\left(w_{i}\right)=1$ for all $i$.</p>
</blockquote>
<h2 id="演習-533"><a class="header" href="#演習-533">演習 5.33</a></h2>
<div class="panel-primary">
<img src="/attachment/617ba640d44de45e6ef750a8" width="600px">
<p>図5.18に示すロボットアームのデカルト座標$(x_1, x_2)$を表す2つの方程式を関節角$\theta_1, \theta_2$とリンクの長さ$L_1, L_2$で書き下せ．ここで，座標系の原点は下側のアームの接続点で与えられるとせよ．これらの方程式は，ロボットアームの「順運動学」を定義する．</p>
</div>
<p>$$
\begin{aligned}
x_1 &amp;= L_1\cos{\theta_1} + L_2\cos{(\theta_1+\theta_2-\pi)} \&amp;= L_1\cos{\theta_1} - L_2\cos{(\theta_1+\theta_2)}
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
x_2 &amp;= L_1\sin{\theta_1} + L_2\sin{(\theta_1+\theta_2-\pi)} \&amp;= L_1\sin{\theta_1} - L_2\sin{(\theta_1+\theta_2)}
\end{aligned}
$$</p>
<h2 id="演習-534"><a class="header" href="#演習-534">演習 5.34</a></h2>
<div class="panel-primary">
<p>混合密度ネットワーク内の混合係数を制御するネットワークの出力活性に関する誤差関数の微分について，
$$
\frac{\partial E_{n}}{\partial a_{k}^{\pi}}=\pi_{k}-\gamma_{n k} \tag{5.155}
$$
の結果を導け．</p>
</div>
<p>※ソフトマックス関数の$\pi_k$は$k=1, \cdots, K$に依存しているので</p>
<p>微分のchain ruleから（ソフトマックス関数の$\pi_k$は$k=1, \cdots, K$に依存しているので$\sum$を使う）
$$
\frac{\partial E_{n}}{\partial a_{k}^{\pi}}=\sum_{j=1}^{K} \frac{\partial E_{n}}{\partial \pi_{j}} \frac{\partial \pi_{j}}{\partial a_{k}^{\pi}}
$$</p>
<p>この第1項について
$$
\frac{\partial E_{n}}{\partial \pi_{j}}=-\frac{\mathcal{N}<em>{n j}}{\sum</em>{l=1}^{K} \pi_{l} \mathcal{N}<em>{n l}}=-\frac{\gamma</em>{nj}}{\pi_{j}}\quad (\because (5.154))
$$
第2項について（演習問題4.17を参照）
$$
\begin{aligned}
\frac{\partial \pi_{j}}{\partial a_{k}^{\pi}} &amp;=\frac{\partial}{\partial a_{k}^{\pi}}\left(\frac{e^{a_{j}^{\pi}}}{\sum_{l=1}^{K} e^{a_{l}^{\pi}}}\right) \
&amp;=\pi_{j}\left(\delta_{k j}-\pi_{k}\right)
\end{aligned}
$$</p>
<p>よって、この二式を結合させると
$$
\begin{aligned}
\frac{\partial E_{n}}{\partial a_{k}^{\pi}} &amp;=\sum_{j=1}^{K}\left(-\frac{\gamma_{nj}}{\pi_{j}}\right) \pi_{j}\left(\delta_{k j}-\pi_{k}\right) \
&amp;=\sum_{j=1}^{K} \gamma_{nj}\left(\pi_{k}-\delta_{k j}\right) \
&amp;=-\gamma_{n_{k}}+\sum_{j=1}^{K} \gamma_{n j} \pi_{k} \
&amp;=\pi_{k}-\gamma_{n k}\left(\because \sum_{j=1}^{K} \gamma_{nj}=1\right)
\end{aligned}
$$
以上から$(5.155)$式が示された。</p>
<h2 id="演習-535"><a class="header" href="#演習-535">演習 5.35</a></h2>
<div class="panel-primary">
<p>混合密度ネットワーク内の各要素の平均を制御するネットワークの出力活性に関する誤差関数の微分について，
$$
\frac{\partial E_{n}}{\partial a_{k l}^{\mu}}=\gamma_{n k}\left{\frac{\mu_{k l}-t_{n l}}{\sigma_{k}^{2}}\right} \tag{5.156}
$$
の結果を導け．</p>
</div>
<p>$$
a_{k l}^{\mu}=\mu_{k l}\tag{5.152}
$$
より
$$
\frac{\partial E_{n}}{\partial a_{k l}^{\mu}}=\frac{\partial E_{n}}{\partial \mu_{k l}}
$$
が得られる。
$$
\partial E_{n}=-{\sum_{n=1}^N}\ln \bigg({\sum_{k=1}} \pi_k \mathcal{N}<em>{n k}\bigg)\tag{5.153}
$$
$$
\gamma</em>{n k}=\frac{\pi_k \mathcal{N}<em>{n k}}{\sum</em>{l=1}^K\pi_l \mathcal{N}<em>{n l}}\tag{5.154}
$$
これらと(2.43)のガウス分布の式を用いると以下のように導ける。
$$
\begin{aligned}
\frac{\partial E</em>{n}}{\partial \mu_{k l}} &amp;=-\frac{\pi_k}{{\sum_{k=1}} \pi_k \mathcal{N}<em>{n k}} \cdot \mathcal{N}</em>{n k} \cdot \frac{t_{n l}-\mu_{k l}}{\sigma^2} \
&amp;=\gamma_{n k} \frac{\mu_{k l}-t_{n l}}{\sigma_{k}^2}
\end{aligned}
$$</p>
<h2 id="演習-536"><a class="header" href="#演習-536">演習 5.36</a></h2>
<div class="panel-primary">
<p>混合密度ネットワーク内の各要素の分散を制御するネットワークの出力活性に関する誤差関数の微分について，
$$
\frac{\partial E_{n}}{\partial a_{k}^{\sigma}}=\gamma_{n k}\left(L-\frac{\left|\mathbf{t}<em>{n}-\boldsymbol{\mu}</em>{k}\right|^{2}}{\sigma_{k}^{2}}\right) \tag{5.157}
$$
の結果を導け．</p>
</div>
<p>微分のchain-ruleより
$$
\frac{\partial E_{n}}{\partial a_{k}^{\sigma}}=\frac{\partial E_{n}}{\partial \sigma_{k}}\frac{\partial \sigma_{k}}{\partial a_{k}^{\sigma}}
$$
第二項について
$$
\sigma_{k}=\exp({a_{k}^{\sigma}})\tag{5.151}
$$
より
$$
\frac{\partial \sigma_{k}}{\partial a_{k}^{\sigma}}=\exp({a_{k}^{\sigma}})=\sigma_{k}
$$
(2.43)のガウス分布の式は以下のように変形できる。
$$
\begin{aligned}
\mathcal{N}<em>{n k}&amp;=\frac{1}{{2 \pi}^{D/2}}\frac{1}{\vert{\sigma</em>{k^2} I}\vert}\exp\bigg({-\frac{1}{2}(\mathbf{t}<em>{n}-\boldsymbol{\mu}</em>{k})^T \frac{1}{\sigma_k^2}(\mathbf{t}<em>{n}-\boldsymbol{\mu}</em>{k})}\bigg)\
&amp;=\bigg(\frac{1}{2 \pi \sigma_k^2}\bigg)^{\frac{D}{2}}\exp\bigg({-\frac{1}{2}(\mathbf{t}<em>{n}-\boldsymbol{\mu}</em>{k})^T \frac{1}{\sigma_k^2}(\mathbf{t}<em>{n}-\boldsymbol{\mu}</em>{k})}\bigg)\
&amp;=\bigg(\frac{1}{2 \pi \sigma_k^2}\bigg)^{\frac{D}{2}}\exp\bigg(-{\frac{\Vert{\mathbf{t}<em>{n}-\boldsymbol{\mu}</em>{k}\Vert}^2}{\sigma_k^2}}\bigg)
\end{aligned}
$$
第一項について変形したガウス分布の指揮,(1.153),(1.154)を用いて以下のように導ける。
$$
\begin{aligned}
\frac{\partial E_{n}}{\partial \sigma_{k}}&amp;=\frac{\pi_k}{-{\sum_{k=1}^K}\pi_k \mathcal{N}<em>{n k}} \bigg(\frac{1}{2 \pi}\bigg)^{\frac{D}{2}}\bigg({-\frac{L}{\sigma^{L+1}}exp\bigg(-{\frac{\Vert{\mathbf{t}</em>{n}-\boldsymbol{\mu}<em>{k}\Vert}^2}{\sigma_k^2}}\bigg)+\frac{1}{\sigma_k^2}exp\bigg(-{\frac{\Vert{\mathbf{t}</em>{n}-\boldsymbol{\mu}<em>{k}\Vert}^2}{\sigma_k^2}}\bigg)\frac{\Vert{\mathbf{t}</em>{n}-\boldsymbol{\mu}<em>{k}\Vert}^2}{\sigma_k^3}}\bigg)\
&amp;=\frac{\mathcal{N}</em>{n k}\pi_k}{-{\sum_{k=1}^K}\pi_k \mathcal{N}<em>{n k}} \bigg({-\frac{L}{\sigma_k}+\frac{\Vert{\mathbf{t}</em>{n}-\boldsymbol{\mu}<em>{k}\Vert}^2}{\sigma_k^3}}\bigg)\
&amp;=\gamma</em>{n k}\bigg({\frac{L}{\sigma_k}-\frac{\Vert{\mathbf{t}<em>{n}-\boldsymbol{\mu}</em>{k}\Vert}^2}{\sigma_k^3}}\bigg)
\end{aligned}
$$
最後に求めた第一項と第二項を掛け合わせて(5.157)の結果を得られる。
$$
\begin{aligned}
\frac{\partial E_{n}}{\partial a_{k}^{\sigma}}&amp;=\frac{\partial E_{n}}{\partial \sigma_{k}}\frac{\partial \sigma_{k}}{\partial a_{k}^{\sigma}}\
&amp;=\gamma_{n k}\bigg({\frac{L}{\sigma_k}-\frac{\Vert{\mathbf{t}<em>{n}-\boldsymbol{\mu}</em>{k}\Vert}^2}{\sigma_k^3}}\bigg)\cdot \sigma_k \
&amp;=\gamma_{n k}\bigg({L-\frac{\Vert{\mathbf{t}<em>{n}-\boldsymbol{\mu}</em>{k}\Vert}^2}{\sigma_k^2}}\bigg)
\end{aligned}
$$</p>
<h2 id="演習-537"><a class="header" href="#演習-537">演習 5.37</a></h2>
<div class="panel-primary">
<p>混合密度ネットワークモデルの条件付き平均と分散について，
$$
\mathbb{E}[\mathbf{t} \mid \mathbf{x}]=\int \mathbf{t} p(\mathbf{t} \mid \mathbf{x}) \mathrm{d} \mathbf{t}=\sum_{k=1}^{K} \pi_{k}(\mathbf{x}) \boldsymbol{\mu}<em>{k}(\mathbf{x}) \tag{5.158}
$$
および
$$
s^2(\mathbf{x})=\sum</em>{k=1}^{K} \pi_{k}(\mathbf{x})\left{L\sigma_{k}^{2}(\mathbf{x})+\left|\mu_{k}(\mathbf{x})-\sum_{l=1}^{K} \pi_{l}(\mathbf{x}) \mu_{l}(\mathbf{x})\right|^{2}\right} \tag{5.160}
$$
の結果を確かめよ．</p>
</div>
<p>※<strong>テキストの$(5.160)$式は間違っており、実際には$\sigma_{k}^{2}(\mathbf{x})$に係数$L$がつくはずである。</strong></p>
<p>$$
p(\mathbf{t} \mid \mathbf{x})=\sum_{k=1}^{K} \pi_{k}(\mathbf{x}) \mathcal{N}\left(\mathbf{t} \mid \boldsymbol{\mu}<em>{k}(\mathbf{x}), \sigma</em>{k}^{2}(\mathbf{x}) \mathbf{I}\right) \tag{5.148}
$$
を利用してまず平均の$\mathbb{E}[\mathbf{t}\mid \mathbf{x}]$を計算すると
$$
\begin{aligned}
\mathbb{E}[\mathbf{t} \mid \mathbf{x}] &amp;=\int \mathbf{t} p(\mathbf{t} \mid \mathbf{x}) d \mathbf{t} \
&amp;=\int \mathbf{t} \sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\mathbf{t} \mid \boldsymbol{\mu}<em>{k}, \sigma</em>{k}^{2}\mathbf{I}\right) d \mathbf{t} \
&amp;=\sum_{k=1}^{K} \pi_{k} \int \mathbf{t} \mathcal{N}\left(\mathbf{t} \mid \boldsymbol{\mu}<em>{k}, \sigma</em>{k}^{2}\mathbf{I}\right) d \mathbf{t} \
&amp;=\sum_{k=1}^{K} \pi_{k} \boldsymbol{\mu}<em>{k}
\end{aligned}
$$
となる。次に分散は$s^{2}(x)=\mathbb{E}\left[\mathbf{t}^{2} \mid \mathbf{x}\right]-{\mathbb{E}[\mathbf{t} \mid \mathbf{x}]}^{2}$で求められるため、$\mathbb{E}\left[\mathbf{t}^{2} \mid \mathbf{x}\right]$を計算すると
$$
\begin{aligned}
\mathbb{E}\left[\mathbf{t}^{2} \mid \mathbf{x}\right] &amp;=\mathbb{E}\left[\mathbf{t}^{\mathrm{T}} \mathbf{t} \mid \mathbf{x}\right] \
&amp;=\mathbb{E}\left[\operatorname{Tr}\left[\mathbf{t}^{\mathrm{T}} \mathbf{t}\right] \mid \mathbf{x}\right] \
&amp;=\mathbb{E}\left[\operatorname{Tr}\left[\mathbf{t}\mathbf{t}^{\mathrm{T}}\right] \mid \mathbf{x}\right] \
&amp;=\operatorname{Tr}\left[\int \mathbf{t}\mathbf{t}^{\mathrm{T}} \sum</em>{k=1}^{K} \pi_k \mathcal{N}\left(\mathbf{t} \mid \boldsymbol{\mu}<em>{k}, \sigma</em>{k}^{2} \mathbf{I}\right) d \mathbf{t}\right] \
&amp;=\sum_{k=1}^{K}\pi_k \operatorname{Tr}\left[\boldsymbol{\mu}<em>{k} \boldsymbol{\mu}</em>{k}^{\mathrm{T}}+\sigma_{k}^{2} \mathbf{I}\right] \
&amp;=\sum_{k=1}^{K}\pi_k \left(\left|\boldsymbol{\mu}<em>{k}\right|^{2}+L \sigma</em>{k}^{2}\right)
\end{aligned}
$$
ここで、$L$は$\mathbf{t}$の次元数である（この計算本当に合ってるのか疑問）。途中の式変形では
$$
\mathbb{E}\left[\mathbf{xx}^{\mathrm{T}}\right]=\boldsymbol{\mu \mu}^{\mathrm{T}}+\mathbf{\Sigma} \tag{2.62}
$$
を用いた。</p>
<p>以上を用いて計算すると
$$
\begin{aligned}
s^{2}(\mathbf{x}) &amp;= \sum_{k=1}^{K} \pi_{k}\left(L \sigma_{k}^{2}+\left|\boldsymbol{\mu}<em>{k}\right|^{2}\right)-\left|\sum</em>{l=1}^{K} \pi_{l} \boldsymbol{\mu}<em>{l}\right|^{2} \
&amp;=L \sum</em>{k=1}^{K} \pi_{k} \sigma_{k}^{2}+\sum_{k=1}^{K} \pi_{k}\left|\boldsymbol{\mu}<em>{k}\right|^{2}-\left|\sum</em>{l=1}^{K} \pi_{l} \boldsymbol{\mu}<em>{l}\right|^{2} \
&amp;=L \sum</em>{k=1}^{K} \pi_{k} \sigma_{k}^{2}+\sum_{k=1}^{K} \pi_{k}\left|\boldsymbol{\mu}<em>{k}\right|^{2}-2 \times\left|\sum</em>{l=1}^{K} \pi_{l} \boldsymbol{\mu}<em>{l}\right|^{2}+1 \times\left|\sum</em>{l=1}^{K} \pi_{l} \boldsymbol{\mu}<em>{l}\right|^{2} \
&amp;=L \sum</em>{k=1}^{K} \pi_{k} \sigma_{k}^{2}+\sum_{k=1}^{K} \pi_{k}\left|\boldsymbol{\mu}<em>{k}\right|^{2}-2\left(\sum</em>{l=1}^{K} \pi_{l} \boldsymbol{\mu}<em>{l}\right)\left(\sum</em>{k=1}^{K} \pi_{k} \boldsymbol{\mu}<em>{k}\right)+\left(\sum</em>{k=1}^{K} \pi_{k}\right)\left|\sum_{l=1}^{K} \pi_{l} \boldsymbol{\mu}<em>{l}\right|^{2} \
&amp;=L \sum</em>{k=1}^{K} \pi_{k} \sigma_{k}^{2}+\sum_{k=1}^{K} \pi_{k}\left|\boldsymbol{\mu}<em>{k}\right|^{2}-2\left(\sum</em>{l=1}^{K} \pi_{l} \boldsymbol{\mu}<em>{l}\right)\left(\sum</em>{k=1}^{K} \pi_{k} \boldsymbol{\mu}<em>{k}\right)+\sum</em>{k=1}^{K} \pi_{k}\left|\sum_{l=1}^{K} \pi_{l} \boldsymbol{\mu}<em>{l}\right|^{2} \
&amp;=L \sum</em>{k=1}^{K} \pi_{k} \sigma_{k}^{2}+\sum_{k=1}^{K} \pi_{k}\left|\boldsymbol{\mu}<em>{k}-\sum</em>{l=1}^{K} \pi_{l} \boldsymbol{\mu}<em>{l}\right|^{2} \
&amp;=\sum</em>{k=1}^{K} \pi_{k}\left(L \sigma_{k}^{2}+\left|\boldsymbol{\mu}<em>{k}-\sum</em>{l=1}^{K} \pi_{l} \boldsymbol{\mu}_{l}\right|^{2}\right)
\end{aligned}
$$</p>
<p>以上から$(5.160)$式が導出された。</p>
<h2 id="演習-538"><a class="header" href="#演習-538">演習 5.38</a></h2>
<div class="panel-primary">
<p>一般的な結果
$$
p(\mathbf{y})=\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \boldsymbol{\mu}+\mathbf{b}, \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}\right) \tag{2.115}
$$
を用いて，ベイズニューラルネットワークモデルのラプラス近似による予測分布
$$
p(t \mid \mathbf{x}, \mathcal{D}, \alpha, \beta)=\mathcal{N}\left(t \mid y\left(\mathbf{x}, \mathbf{w}_{\mathrm{MAP}}\right), \sigma^{2}(\mathbf{x})\right) \tag{5.172}
$$
を導け．</p>
</div>
<p>※$(2.115)$と$(5.172)$式が一致するように変数の値を変換できれば題意を満たせる。</p>
<p>$(5.173)$式から、$\sigma^2(\mathbf{x}) = \beta^{-1}+\mathbf{g}^{\mathrm T}\mathbf{A}^{-1}\mathbf{g}$で定義されている。</p>
<p>$(2.115)$式と$(5.172)$式の比較から
$$
\mathbf{y} \Rightarrow t,\quad \mathbf{L}^{-1} \Rightarrow \beta^{-1},\quad \mathbf{A} \Rightarrow \mathbf{g}^{\mathrm{T}},\quad\mathbf{\Lambda}^{-1} \Rightarrow \mathbf{A}^{-1}
$$
となることはわかる。</p>
<p>$\mathbf{A} \boldsymbol{\mu}+\mathbf{b} = y(\mathbf{x},\mathbf{w}<em>{\mathrm{MAP}})$の関係は、ラプラス近似ではMAP（最大事後確率）解付近での近似なので$\boldsymbol{\mu} \Rightarrow \mathbf{w}</em>{\mathrm{MAP}}$とすればよく、このとき$\mathbf{g}^{\mathrm T}\mathbf{w}<em>{\mathrm{MAP}} + \mathbf{b} = y(\mathbf{x},\mathbf{w}</em>{\mathrm{MAP}})$なので、
$$
\mathbf{b} \Rightarrow y\left(\mathbf{x}, \mathbf{w}<em>{\mathrm{MAP}}\right)-\mathbf{g}^{\mathrm{T}} \mathbf{w}</em>{\mathrm{MAP}}
$$
とすればよい。</p>
<p>以上の変数変換を適用すると、
$$
\begin{aligned}
p(t) &amp;=\mathcal{N}\left(t \mid \mathbf{g}^{\mathrm{T}} \mathbf{w}<em>{\mathrm{MAP}}+y\left(\mathbf{x}, \mathbf{w}</em>{\mathrm{MAP}}\right)-\mathbf{g}^{\mathrm{T}} \mathbf{w}<em>{\mathrm{MAP}}, \beta^{-1}+\mathbf{g}^{\mathrm{T}} \mathbf{A}^{-1} \mathbf{g}\right) \
&amp;=\mathcal{N}\left(t \mid y\left(\mathbf{x}, \mathbf{w}</em>{\mathrm{MAP}}\right), \sigma^{2}\right)
\end{aligned}
$$
となる。</p>
<h2 id="演習-539"><a class="header" href="#演習-539">演習 5.39</a></h2>
<div class="panel-primary">
<p>ラプラス近似の結果
$$
\begin{aligned}
Z &amp;=\int f(\mathbf{z}) \mathrm{d} \mathbf{z} \
&amp; \simeq f\left(\mathbf{z}<em>{0}\right) \int \exp \left{-\frac{1}{2}\left(\mathbf{z}-\mathbf{z}</em>{0}\right)^{\mathrm{T}} \mathbf{A}\left(\mathbf{z}-\mathbf{z}<em>{0}\right)\right} \mathrm{d} \mathbf{z} \
&amp;=f\left(\mathbf{z}</em>{0}\right) \frac{(2 \pi)^{M / 2}}{|\mathbf{A}|^{1 / 2}}
\end{aligned} \tag{4.135}
$$
を用いて，ベイズニューラルネットワークモデルにおける超パラメータ$\alpha, \beta$のエビデンス関数が
$$
\ln p(\mathcal{D} \mid \alpha, \beta) \simeq-E\left(\mathbf{w}<em>{\mathrm{MAP}}\right)-\frac{1}{2} \ln |\mathbf{A}|+\frac{W}{2} \ln \alpha+\frac{N}{2} \ln \beta-\frac{N}{2} \ln (2 \pi) \tag{5.175}
$$
で近似できることを示せ．ただし
$$
E\left(\mathbf{w}</em>{\mathrm{MAP}}\right)=\frac{\beta}{2} \sum_{n=1}^{N}\left{y\left(\mathbf{x}<em>{n}, \mathbf{w}</em>{\mathrm{MAP}}\right)-t_{n}\right}^{2}+\frac{\alpha}{2} \mathbf{w}<em>{\mathrm{MAP}}^{\mathrm{T}} \mathbf{w}</em>{\mathrm{MAP}} \tag{5.176}
$$
である。</p>
</div>
<p>※ やや計算量が多いが、ラプラス近似とガウス分布の計算さえしっかりすれば大丈夫。</p>
<p>まず$(5.174)$式
$$
p(\mathcal{D} \mid \alpha, \beta)=\int p(\mathcal{D} \mid \mathbf{w}, \beta) p(\mathbf{w} \mid \alpha) \mathrm{d} \mathbf{w}
$$
について$f(\mathbf{w})=p(\mathcal{D} \mid \mathbf{w}, \beta)p(\mathbf{w} \mid \alpha),\ Z=p(\mathcal{D} \mid \alpha, \beta)$として$(4.135)$式のラプラス近似の式に代入すると</p>
<p>$$
\begin{aligned}
p(\mathcal{D} \mid \alpha, \beta) &amp;\simeq p\left(\mathcal{D} \mid \mathbf{w}<em>{\mathrm{MAP}}, \beta\right) p\left(\mathbf{w}</em>{\mathrm{MAP}} \mid \alpha\right) \int \exp \left{-\frac{1}{2}\left(\mathbf{w}-\mathbf{w}<em>{\mathrm{MAP}}\right)^{\mathrm{T}} \mathbf{A}\left(\mathbf{w}-\mathbf{w}</em>{\mathrm{MAP}}\right)\right} \mathrm{d} \mathbf{w} \
&amp;=f\left(\mathbf{w}_{\mathrm {MAP}}\right) \frac{(2 \pi)^{W/2}}{|\mathbf{A}|^{1/2}}
\end{aligned}
$$
とおくことができる。ここで、$W$は$\mathbf{w}$の次元数である。</p>
<p>$f(\mathbf{w}<em>{\mathrm{MAP}})$について展開すると, $(5.162)$, $(5.163)$式を用いて
$$
\begin{aligned}
f\left(\mathbf{w}</em>{\mathrm {MAP}}\right)=&amp; p\left(\mathcal{D} \mid \mathbf{w}<em>{\mathrm {MAP}}, \beta\right) p\left(\mathbf{w}</em>{\mathrm {MAP}} \mid \alpha\right) \
=&amp; \prod_{n=1}^{N} \mathcal{N}\left(t_{n} \mid y\left(\mathbf{x}<em>{n}, \mathbf{w}</em>{\mathrm {MAP}}\right), \beta^{-1}\right) \mathcal{N}\left(\mathbf{w}<em>{\mathrm {MAP}} \mid \mathbf{0}, \alpha^{-1} \mathrm{I}\right) \
=&amp; \prod</em>{n=1}^{N} \left(\frac{\beta}{2 \pi}\right)^{1 / 2} \exp \left[-\frac{\beta}{2}\left{t_{n}-y\left(\mathbf{x}<em>{n}, \mathbf{w}</em>{\mathrm {MAP}}\right)\right}^{2}\right] \
&amp; \frac{1}{(2 \pi)^{W / 2}} \frac{1}{\left|\alpha^{-1} \mathbf{I}\right|^{1 / 2}} \exp \left{-\frac{1}{2} \mathbf{w}<em>{\mathrm {MAP}}^{\mathrm{T}}\left(\alpha^{-1} \mathbf{I}\right)^{-1} \mathbf{w}</em>{\mathrm {MAP}}\right} \
=&amp; \prod_{n=1}^{N}\left(\frac{\beta}{2 \pi}\right)^{1 / 2} \exp \left[-\frac{\beta}{2}\left{t_{n}-y\left(\mathbf{x}<em>{n}, \mathbf{w}</em>{\mathrm {MAP}}\right)\right}^{2}\right] \left(\frac{\alpha}{2 \pi}\right)^{W/2} \exp \left(-\frac{\alpha}{2} \mathbf{w}<em>{\mathrm {MAP}}^{\mathrm{T}} \mathbf{w}</em>{\mathrm {MAP}}\right)
\end{aligned}
$$
これの対数を取ると
$$
\begin{aligned}
\ln p(\mathcal{D} \mid \alpha, \beta) &amp; \simeq \ln f\left(\mathbf{w}<em>{\mathrm{MAP}}\right)+\frac{W}{2} \ln (2 \pi)-\frac{1}{2} \ln |\mathbf{A}| \
&amp;=\sum</em>{n=1}^{N}\left[\frac{1}{2}{\ln \beta-\ln (2 \pi)}-\frac{\beta}{2}\left{t_{n}-y\left(\mathbf{x}<em>{n}, \mathbf{w}</em>{\mathrm{MAP}}\right)\right}^{2}\right] \
&amp;+\frac{W}{2}{\ln \alpha-\ln (2 \pi)}-\frac{\alpha}{2} \mathbf{w}<em>{\mathrm{MAP}}^{\mathrm{T}} \mathbf{w}</em>{\mathrm{MAP}}+\frac{W}{2} \ln (2 \pi)-\frac{1}{2} \ln |\mathbf{A}| \
&amp;=-\left[\frac{\beta}{2} \sum_{n=1}^{N}\left{t_{n}-y\left(\mathbf{x}<em>{n}, \mathbf{w}</em>{\mathrm{MAP}}\right)\right}^{2}+\frac{\alpha}{2} \mathbf{w}<em>{\mathrm{MAP}}^{\mathrm{T}} \mathbf{w}</em>{\mathrm{MAP}}\right] -\frac{1}{2} \ln |\mathbf{A}|+\frac{N}{2} \ln \beta-\frac{N}{2} \ln (2 \pi)+\frac{W}{2} \ln \alpha \
&amp;=-E\left(\mathbf{w}_{\mathrm{MAP}}\right)-\frac{1}{2} \ln |\mathbf{A}|+\frac{W}{2} \ln \alpha+\frac{N}{2} \ln \beta-\frac{N}{2} \ln (2 \pi)
\end{aligned}
$$
以上から$(5.175)$式を得た。</p>
<h2 id="演習-540"><a class="header" href="#演習-540">演習 5.40</a></h2>
<div class="panel-primary">
<p>5.7.3節で議論されたベイズニューラルネットワークの枠組みを，ソフトマックス活性化関数を出力ユニットに持つネットワークを用いて多クラス問題を扱えるようにするために必要な変更について．概略を述べよ．</p>
</div>
<p>※5.7.3節の議論をソフトマックス関数・多クラス問題の置き換えるだけ</p>
<p>まずソフトマックス関数$(5.25)$の式から
$$
y_{k}(\mathbf{x}, \mathbf{w})=\frac{\exp \left(a_{k}(\mathbf{x}, \mathbf{w})\right)}{\sum_{j} \exp \left(a_{j}(\mathbf{x}, \mathbf{w})\right)}
$$
となり、目標変数$\mathbf{t}$の条件付き分布を多項分布にとって
$$
p(\mathbf{t} \mid \mathbf{x}, \mathbf{w})=\prod_{k=1}^{K} y_{k}(\mathbf{x}, \mathbf{w})^{t_{k}}
$$
これをもとに尤度を計算すると、$\mathcal{D}=\left{\mathbf{t}<em>{1}, \mathbf{t}</em>{2}, \ldots\right}, \mathbf{X}=\left{\mathbf{x}<em>{1}, \mathbf{x}</em>{2}, \ldots\right}, y_{n k}=y_{k}\left(\mathbf{x}<em>{n}, \mathbf{w}\right)$として、
$$
\begin{aligned}
p(\mathcal{D} \mid \mathbf{X}, \mathbf{w}) &amp;=\prod</em>{n=1}^{N} p\left(\mathbf{t}<em>{n} \mid \mathbf{x}</em>{n}, \mathbf{w}\right) \
&amp;=\prod_{n=1}^{N} \prod_{k=1}^{K} y_{n k}^{t_{n k}}
\end{aligned}
$$
となる。これより対数尤度は
$$
\ln p(\mathcal{D} \mid \mathbf{X}, \mathbf{w})=\sum_{n=1}^{N} \sum_{k=1}^{K} t_{n k} \ln y_{n k}
$$
となる。</p>
<p>対数尤度関数が求まったので、ラプラス近似理論を適用するために超パラメータ$\alpha$を初期化する。$\mathbf{w}$の事後分布は
$$
p(\mathbf{w} \mid \mathcal{D}, \mathbf{X})=\frac{p(\mathcal{D}, \mathbf{w} \mid \mathbf{X})}{p(\mathcal{D} \mid \mathbf{X})}=\frac{p(\mathcal{D} \mid \mathbf{w}, \mathbf{X}) p(\mathbf{w})}{p(\mathcal{D} \mid \mathbf{X})} \simeq p(\mathcal{D} \mid \mathbf{w}, \mathbf{X}) p(\mathbf{w})
$$
なので、対数事後分布は
$$
\ln p(\mathbf{w} \mid \mathcal{D}, \mathbf{X}) = \sum_{n=1}^{N} \sum_{k=1}^{K} t_{n k} \ln y_{n k}+\ln p(\mathbf{w})+ \textrm{const.}
$$
となる。重みの事前分布$p(\mathbf{w})$を$(5.162)$のように
$$
p(\mathbf{w} \mid \alpha)=\mathcal{N}\left(\mathbf{w} \mid \mathbf{0}, \alpha^{-1} \mathbf{I}\right)
$$
とすると、
$$
\ln p(\mathbf{w} \mid \mathcal{D}, \mathbf{X})=\sum_{n=1}^{N} \sum_{k=1}^{K} t_{n k} \ln y_{n k}-\frac{\alpha}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w}+\textrm{const.}
$$
となるので、対数事後分布の最大化は正則化誤差関数
$$
\begin{aligned}
E(\mathrm{w}) &amp;=-\ln p(\mathcal{D} \mid \mathrm{w}, \mathrm{X})+\frac{\alpha}{2} \mathrm{w}^{\mathrm{T}} \mathrm{w} \
&amp;=-\sum_{n=1}^{N} \sum_{k=1}^{K} t_{n k} \ln y_{n k}+\frac{\alpha}{2} \mathrm{w}^{\mathrm{T}} \mathrm{w}
\end{aligned}
$$
の最小化と等価になることがわかる。</p>
<p>$E(\mathbf{w})$を最小にする$\mathbf{w}_{\mathrm{MAP}}$を$\frac{\partial E}{\partial \mathbf{w}} = 0$から求める。</p>
<p>$\mathbf{w}<em>{\mathrm{MAP}}$を求めたらラプラス近似を使って
$$
\begin{aligned}
p(\mathcal{D} \mid \alpha, \mathbf{X}) &amp;=\int p(\mathcal{D} \mid \mathbf{w}, \mathbf{X}) p(\mathbf{w} \mid \alpha) d \mathbf{w} \
&amp;\simeq p(\mathcal{D} \mid \mathbf{w}</em>{\mathrm{MAP}}, \mathbf{X}) p(\mathbf{w}<em>{\mathrm{MAP}} \mid \alpha)\frac{(2\pi)^{W/2}}{|\mathbf{A}|^{1/2}}
\end{aligned}
$$
となる。これを使って対数をとっていくと
$$
\ln p(\mathcal{D} \mid \alpha, \mathbf{X})=-E\left(\mathbf{w}</em>{\mathrm{MAP}}\right)-\frac{1}{2} \ln |\mathbf{A}|+\frac{W}{2} \ln \alpha
$$
となる。</p>
<p>あとはP.284と同様に$\ln p(\mathcal{D}\mid \alpha)$を最大化して$\alpha$の点推定を行う。結果は$(5.178)$のように
$$
\alpha=\frac{\gamma}{\mathrm{w}<em>{\mathrm{MAP}}^{\mathrm{T}} \mathrm{w}</em>{\mathrm{MAP}}}, \quad \gamma=\sum_{i=1}^{W} \frac{\lambda_{i}}{\alpha+\lambda_{i}}
$$
となる。</p>
<h2 id="演習-541"><a class="header" href="#演習-541">演習 5.41</a></h2>
<div class="panel-primary">
<p>回帰ネットワークに関する5.7.1節および5.7.2節と類似のステップに従って，交差エントロピー誤差関数とロジスティックシグモイド活性化関数の出力ユニットを持つネットワークの場合の周辺化尤度の結果</p>
<p>$$
\ln p(\mathcal{D} \mid \alpha) \simeq-E\left(\mathbf{w}_{\mathrm{MAP}}\right)-\frac{1}{2} \ln |\mathbf{A}|+\frac{W}{2} \ln \alpha \tag{5.183}
$$</p>
<p>を導け．ただし</p>
<p>$$
E\left(\mathbf{w}<em>{\mathrm{MAP}}\right)=-\sum</em>{n=1}^{N}\left{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right}+\frac{\alpha}{2} \mathbf{w}<em>{\mathrm{MAP}}^{\mathrm{T}} \mathbf{w}</em>{\mathrm{MAP}} \tag{5.184}
$$</p>
<p>である．</p>
</div>
<p>※演習問題5.39と流れはほとんど同じ</p>
<p>ラプラス近似を用いると
$$
\begin{aligned}
p(\mathcal{D} \mid \alpha) &amp;=\int p\left(\mathcal{D} \mid \mathbf{w}<em>{\mathrm{MAP}}\right) p\left(\mathbf{w}</em>{\mathrm{MAP}} \mid \alpha\right) d \mathbf{w} \
&amp; \simeq p\left(\mathcal{D} \mid \mathbf{w}<em>{\mathrm{MAP}}\right) p\left(\mathbf{w}</em>{\mathrm{MAP}}\mid \alpha\right) \frac{(2 \pi)^{W / 2}}{|\mathbf{A}|^{1 / 2}}
\end{aligned}
$$
と書ける。ここで、今$p\left(\mathcal{D} \mid \mathbf{w}<em>{\mathrm{MAP}}\right)$はロジスティック回帰を出力に持つ活性化関数となっているので、対数形は
$$
\ln p(\mathcal{D} \mid \mathbf{w}</em>{\mathrm{MAP}})=\sum_{n=1}^{N}\left{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right}
$$
で示される（$y_{n} \equiv y\left(\mathbf{x}<em>{n}, \mathbf{w}</em>{\mathrm{MAP}}\right)$である）。$p\left(\mathbf{w}<em>{\mathrm{MAP}}\mid \alpha\right)$は引き続きガウス分布
$$
p(\mathbf{w}</em>{\mathrm{MAP}} \mid \alpha)=\mathcal{N}\left(\mathbf{w}_{\mathrm{MAP}} \mid \mathbf{0}, \alpha^{-1} \mathbf{I}\right) \tag{5.162}
$$
を用いる。</p>
<p>以上から$\ln p(\mathcal{D}\mid \alpha)$を計算すると
$$
\begin{aligned}
\ln P\left(\mathcal{D} \mid \alpha\right) &amp; \simeq \sum_{n=1}^{N}\left{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right} +\ln \mathcal{N}\left(\mathbf{w}<em>{\mathrm{MAP}} \mid \mathbf{0}, \alpha^{-1} \mathbf{I}\right)+\frac{W}{2} \ln (2 \pi)-\frac{1}{2} \ln |\mathbf{A}| \
&amp; = \sum</em>{n=1}^{N}\left{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right} +\ln \left{ \left(\frac{\alpha}{2 \pi}\right)^{W / 2} \exp \left(-\frac{\alpha}{2} \mathbf{w}<em>{\mathrm{MAP}}^{\mathrm{T}} \mathbf{w}</em>{\mathrm{MAP}}\right) \right}+\frac{W}{2} \ln (2 \pi)-\frac{1}{2} \ln |\mathbf{A}| \
&amp; = \sum_{n=1}^{N}\left{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right} -\frac{\alpha}{2} \mathbf{w}<em>{\mathrm{MAP}}^{\mathrm{T}} \mathbf{w}</em>{\mathrm{MAP}}+\frac{W}{2}\ln \alpha-\frac{W}{2}\ln (2\pi)+\frac{W}{2} \ln (2 \pi)-\frac{1}{2} \ln |\mathbf{A}| \
&amp; = \sum_{n=1}^{N}\left{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right} -\frac{\alpha}{2} \mathbf{w}<em>{\mathrm{MAP}}^{\mathrm{T}} \mathbf{w}</em>{\mathrm{MAP}}+\frac{W}{2}\ln \alpha-\frac{W}{2}\ln (2\pi)+\frac{W}{2} \ln (2 \pi)-\frac{1}{2} \ln |\mathbf{A}| \
&amp;= -E\left(\mathbf{w}_{\mathrm{MAP}}\right) -\frac{1}{2} \ln |\mathbf{A}| +\frac{W}{2}\ln \alpha
\end{aligned}
$$
となる。以上から$(5.183)$が導けた。</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="prml第6章演習問題解答"><a class="header" href="#prml第6章演習問題解答">PRML第6章演習問題解答</a></h1>
<head>
<style>
  div.panel-primary {
	border: 1px solid #000;
    margin: 10px 5px;
    padding: 16px 10px 0px;
  }
</style>
</head>
<h2 id="演習-61"><a class="header" href="#演習-61">演習 6.1</a></h2>
<div class="panel-primary">
<p>6.1節で紹介した最小二乗法線形回帰問題の双対表現を示せ．また解のべクトル$\mathbf{a}$の要素$a_n$がベクトル$\boldsymbol{\phi}(\mathbf{x}_n)$の要素の線形結合で表されることを示せ．それらの係数をベクトル$\mathbf{w}$として双対表現の双対表現がもともとの表現に戻ることを，$\mathbf{w}$をパラメータベクトルとして示せ．</p>
</div>
<p>$(6.2)$から始めて双対表現を得るところまでを復習する。</p>
<p>$$
J(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N}\left{\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}<em>{n}\right)-t</em>{n}\right}^{2}+\frac{\lambda}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w} \tag{6.2}
$$</p>
<p>行列形式では</p>
<p>$$
J(\mathbf{w})=\frac{1}{2} \left( \mathbf{\Phi}\mathbf{w} - \mathbf{t} \right)^{\mathrm T}\left( \mathbf{\Phi}\mathbf{w} - \mathbf{t} \right) + \frac{\lambda}{2}\mathbf{w}^{\mathrm T}\mathbf{w}
$$</p>
<p>P.3にならって$\displaystyle \frac{\partial J}{\partial \mathbf{w}} = 0$をとると</p>
<p>$$
\begin{aligned}
\frac{\partial J}{\partial \mathbf{w}} &amp;=\mathbf{\Phi}^{\mathrm T}(\mathbf{\Phi} \mathbf{w}-\mathbf{t})+\lambda \mathbf{w} \hspace{1em} \left(\because \frac{\partial}{\partial s}(\mathbf{A} \mathbf{s}-\mathbf{b})^{\mathrm T}(\mathbf{A} \mathbf{s}-\mathbf{b})=2 \mathbf{A}^{\mathrm T}(\mathbf{A} \mathbf{s}-\mathbf{b})\right) \
&amp;=\mathbf{\Phi}^{\mathrm T} \mathbf{\Phi} \mathbf{w}-\mathbf{\Phi}^{\mathrm T} \mathbf{t}+\lambda \mathbf{w}=0
\end{aligned}
$$</p>
<p>これを解いて</p>
<p>$$
\begin{aligned}
\mathbf{w} &amp;= -\frac{1}{\lambda} \mathbf{\Phi}^{\mathrm T}\left( \mathbf{\Phi} \mathbf{w} - \mathbf{t} \right) \
&amp;= \mathbf{\Phi}^{\mathrm T}\mathbf{a}
\end{aligned}
$$</p>
<p>これは$(6.3)$と同じで、$\displaystyle \mathbf{a} = -\frac{1}{\lambda} \left( \mathbf{\Phi} \mathbf{w} - \mathbf{t} \right)$と置いた。</p>
<p>これを$(6.2)$に代入し直すと</p>
<p>$$
\begin{aligned}
J(\mathbf{a}) &amp;=\frac{1}{2}\left(\mathbf{\Phi} \mathbf{\Phi}^{\mathrm T} \mathbf{a}-\mathbf{t}\right)^{\mathrm T}\left(\mathbf{\Phi} \mathbf{\Phi}^{\mathrm T} \mathbf{a}-\mathbf{t}\right)+\frac{\lambda}{2} \mathbf{a}^{\mathrm T} \mathbf{\Phi} \mathbf{\Phi}^{\mathrm T} \mathbf{a} \
&amp;=\frac{1}{2}\left(\mathbf{a}^{\mathrm T} \mathbf{\Phi} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi} \mathbf{\Phi}^{\mathrm T} \mathbf{a}-2 \mathbf{a}^{\mathrm T} \mathbf{\Phi} \mathbf{\Phi}^{\mathrm T} \mathbf{t}+\mathbf{t}^{\mathrm T} \mathbf{t}\right)+\frac{\lambda}{2} \mathbf{a}^{\mathrm T} \mathbf{\Phi} \mathbf{\Phi}^{\mathrm T} \mathbf{a} \
&amp;=\frac{1}{2} \mathbf{a}^{\mathrm T} \mathbf{\Phi} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi} \mathbf{\Phi}^{\mathrm T} \mathbf{a}-\mathbf{a}^{\mathrm T} \mathbf{\Phi} \mathbf{\Phi}^{\mathrm T} \mathbf{t}+\frac{1}{2} \mathbf{t}^{\mathrm T} \mathbf{t}+\frac{1}{2} \mathbf{a}^{\mathrm T} \mathbf{\Phi} \mathbf{\Phi}^{\mathrm T} \mathbf{a}
\end{aligned} \tag{6.5}
$$</p>
<p>この式を$\mathbf{K} = \mathbf{\Phi}\mathbf{\Phi}^{\mathrm T}$で定義されるグラム行列を用いて書くと</p>
<p>$$
J(\mathbf{a})=\frac{1}{2} \mathbf{a}^{\mathrm{T}} \mathbf{K} \mathbf{K} \mathbf{a}-\mathbf{a}^{\mathrm{T}} \mathbf{K} \mathbf{t}+\frac{1}{2} \mathbf{t}^{\mathrm{T}} \mathbf{t}+\frac{\lambda}{2} \mathbf{a}^{\mathrm{T}} \mathbf{K} \mathbf{a} \tag{6.7}
$$</p>
<p>となる。さらにこれは$\mathbf{K}^{\mathrm T} = \mathbf{K}$であるから</p>
<p>$$
J(\mathbf{a})=\frac{1}{2}(\mathbf{K} \mathbf{a}-\mathbf{t})^{\mathrm T}(\mathbf{K} \mathbf{a}-\mathbf{t})+\frac{\lambda}{2} \mathbf{a}^{\mathrm T} \mathbf{Ka}
$$</p>
<p>となる。これが最小二乗法の線形回帰問題の双対表現である。</p>
<p>以上が教科書3ページの復習。次に、(6.7)式から出発して(6.2)式を再現する。
$\mathbf{K}=\mathbf{\Phi\Phi^T}$であり、$\mathbf{\Phi}$は$N\times M$行列なので、$N\times N$行列である$\mathbf{K}$は$M$次までランク落ちしている。（{$\phi_1(\mathbf{x}), \phi_2(\mathbf{x}),\dots ,\phi_M(\mathbf{x})$}が線型独立なので、$M$次元未満にはならない。）
そこで、(6.7)式の$\mathbf{a}$を$\mathbf{K}$の像空間(image space)と$\mathbf{K}$の核空間(kernel space)に分解して、不定性の残る$\mathbf{K}$の核空間の成分は$\mathbf{0}$（または十分小さい）とする。（$\mathbf{a}$は、(6.7)式において$\mathbf{Ka}$の形でしか登場しないので、$\mathbf{a}$の核空間の成分を$\mathbf{0}$としても$\mathbf{J(a)}$の一般性を失わない。）
$M$個のベクトル{$\phi_1(\mathbf{x}), \phi_2(\mathbf{x}),\dots ,\phi_M(\mathbf{x})$}が（互いに線型独立なので）像空間の基底を成すことから、係数ベクトル$\mathbf{u}$を用いて$\mathbf{a=\Phi u}$と表せる。これを(6.7)式に代入して、
$$
J(\mathbf{u})=\frac{1}{2}\mathbf{(\Phi\Phi^T\Phi u-t)^T(\Phi\Phi^T\Phi u-t)}+\frac{\lambda}{2}\mathbf{u^T\Phi^T\Phi\Phi^T\Phi u}
$$
ここで、$\mathbf{\Phi^T\Phi}$は（$\mathbf{\Phi\Phi^T}$とは異なり）フルランクなので、$\mathbf{u}$の代わりに$\mathbf{\Phi^T\Phi u}$を係数ベクトルとしても等価である。この$\mathbf{\Phi^T\Phi u}$を改めて$\mathbf{w}$と置くと、(6.2)式が再現される。</p>
<h2 id="演習-62"><a class="header" href="#演習-62">演習 6.2</a></h2>
<div class="panel-primary">
<p>この演習問題では，パーセプトロンの学習アルゴリズムの双対表現を導く．パーセプトロンでの更新則
$$
\mathbf{w}^{(\tau+1)}=\mathbf{w}^{(\tau)}-\eta \nabla E_{\mathrm{P}}(\mathbf{w})=\mathbf{w}^{(\tau)}+\eta \boldsymbol{\phi}<em>{n} t</em>{n} \tag{4.55}
$$
を用いて，訓練後の重みベクトル$\mathbf{w}$が，ベクトル$t_n\boldsymbol{\phi}(\mathbf{x}<em>n)$(ただし$t</em>{n} \in{-1,+1}$)の線形結合で表されることを示せ．この線形結合の係数を$\alpha_n$として，パーセプトロンの学習アルゴリズムを導き，また，$\alpha_n$を用いてパーセプトロンの予測関数を示せ．また，特徴ベクトル$\boldsymbol{\phi}(\mathbf{x})$はカーネル関数$k(\mathbf{x},\mathbf{x}^{\prime}) = \boldsymbol{\phi}({\mathbf{x}})^{\mathrm T}\boldsymbol{\phi}({\mathbf{x}^{\prime}})$の形でのみ現れることを示せ．</p>
</div>
<p>上巻190ページを参照する。パーセプトロン規準では、ある入力ベクトル$\mathbf{x}_n$を変換して特徴ベクトル$\boldsymbol{\phi}(\mathbf{x}_n)$を得て、以下の式で表される一般化線形モデルを構成する</p>
<p>$$
y(\mathbf{x}) = f(\mathbf{w}^{\mathrm T}\boldsymbol{\phi}(\mathbf{x}))
$$</p>
<p>ここで$f(\cdot)$はステップ関数</p>
<p>$$
f(a)=\left{\begin{array}{ll}
+1 &amp; (a&gt;0) \
-1 &amp; (a&lt;0)
\end{array}\right.
$$</p>
<p>で与えられる。</p>
<p>パーセプトロンではクラス$\mathcal{C}_1$については$\mathbf{w}^{\mathrm T}{\boldsymbol{\phi}(\mathbf{x}_n)} \gt 0$, クラス$\mathcal{C}_2$については$\mathbf{w}^{\mathrm T}{\boldsymbol{\phi}(\mathbf{x}_n)} \le 0$となるような重みベクトルを求めることが目的となる。さらに目的変数の値$t_n \in {-1, 1}$を使うとすべてのパターンは$\mathbf{w}^{\mathrm T}\boldsymbol{\phi}(\mathbf{x}_n)t_n \gt 0$を満たす。そして正しく分類された$\mathbf{x}_n$についての誤差は$0$、誤分類された$\mathbf{x}_n$についての誤差は$\mathbf{w}^{\mathrm T}{\boldsymbol{\phi}(\mathbf{x}_n)}$となる。</p>
<p>すなわち誤差関数$E_{\mathrm{P}}(\mathbf{\mathbf{w}})$は</p>
<p>$$
E_{\mathrm{P}}(\mathbf{w})=-\sum_{n \in \mathcal{M}} \mathrm{w}^{\mathrm{T}} \boldsymbol{\phi}<em>{n} t</em>{n} \tag{4.54}
$$</p>
<p>となる($\mathcal{M}$は誤分類されたすべてのパターンを表す)。この誤差関数の最小化に確率的最急降下法を用いると</p>
<p>$$
\mathrm{w}^{(\tau+1)}=\mathrm{w}^{(\tau)}-\eta \nabla E_{\mathrm{P}}(\mathrm{w})=\mathrm{w}^{(\tau)}+\eta \boldsymbol{\phi}<em>{n} t</em>{n} \tag{4.55}
$$</p>
<p>が得られる。</p>
<p>重みベクトルの初期値を$\mathbf{w}^{(0)}$として（$\mathbf{w}^{(0)} = \mathbf{0}$としても良い）、訓練後の重みベクトル$\mathbf{w}$はベクトル$t_n\boldsymbol{\phi}(\mathbf{x}_n)$の線形結合の形になっていることがわかる。</p>
<p>この線形結合の係数を$\alpha_n$とすると</p>
<p>$$
\mathbf{w} = \sum_{n=1}^N \alpha_n t_n \mathbf{x}_n
$$</p>
<p>の形で更新後の重みベクトルを表すことができる。この重みを用いたクラスラベル予測関数は</p>
<p>$$
\begin{aligned}
y(\mathbf{x}) &amp;=\operatorname{sgn}\left(\mathbf{w}^{\mathrm T} \boldsymbol{\phi}(\mathbf{x})\right) \
&amp;=\operatorname{sgn}\left(\sum_{n=1}^{N} \alpha_{n} t_n \boldsymbol{\phi}\left(\mathbf{x}<em>{n}\right)^{\mathrm T} \boldsymbol{\phi}(\mathbf{x})\right) \
&amp;=\operatorname{sgn}\left(\sum</em>{n=1}^{N} \alpha_{n} t_n k\left(\mathbf{x}_{n}, \mathbf{x}\right)\right)
\end{aligned}
$$</p>
<p>の形で書き表せる。これより、パーセプトロンの予測関数がカーネル関数でのみ表せていることがわかる。</p>
<p>また$E_P$の最小化（＝パーセプトロンの学習アルゴリズム）はすべてのパターンが$\mathbf{w}^{\mathrm T}\boldsymbol{\phi}(\mathbf{x}_n)t_n \gt 0$を満たしているので</p>
<p>$$
\mathbf{w}^{\mathrm T}\boldsymbol{\phi}(\mathbf{x}<em>n)t_n = \alpha</em>{n}t_{n}^2\left(
\sum_{m=1}^{M}k(\mathbf{x}_m, \mathbf{x}_n)
\right) \gt 0
$$</p>
<p>となり、グラム行列のみの双対表現で書き表すことができる。</p>
<h2 id="演習-63"><a class="header" href="#演習-63">演習 6.3</a></h2>
<div class="panel-primary">
<p>最近傍法(2.5.2節)は，新しい入力ベクトル$\mathbf{x}$を訓練集合の中でこれに最も近い入力ベクトル$\mathbf{x}_n$を持つものと同じクラスに分類する．最も単純な場合では，距離はユークリッド距離$| \mathbf{x} - \mathbf{x}_n|^2$が用いられる．これをスカラー積で表すことでカーネル置換を用いて，一般的な非線形カーネルを用いた最近傍法を導け．</p>
</div>
<p>ユークリッド距離の2乗$|\mathbf{x} - \mathbf{x}_n|^2$をスカラー積（内積）$(\mathbf{x} - \mathbf{x}_n)^{\mathrm T}(\mathbf{x} - \mathbf{x}_n)$と考える。</p>
<p>$$
\begin{aligned}
D\left(\mathbf{x}, \mathbf{x}<em>{n}\right) &amp;=\left|\mathbf{x}-\mathbf{x}</em>{n}\right|^{2} \
&amp;=\left(\mathbf{x}-\mathbf{x}<em>{n}\right)^{\mathrm T}\left(\mathbf{x}-\mathbf{x}</em>{n}\right) \
&amp;=\mathbf{x}^{\mathrm T} \mathbf{x}-2 \mathbf{x}^{\mathrm T} \mathbf{x}<em>{n}+\mathbf{x}</em>{n}^{2} \
&amp;=k(\mathbf{x}, \mathbf{x})+k\left(\mathbf{x}<em>{n}, \mathbf{x}</em>{n}\right)-2k\left(\mathbf{x}, \mathbf{x}_{n}\right)
\end{aligned}
$$</p>
<p>ここで$k(\mathbf{x}, \mathbf{x}_n) = \mathbf{x}^{\mathrm T}\mathbf{x}_n$をカーネル関数として用いた。この結果は非線形写像$|\mathbf{x} - \mathbf{x}_n|^2$は他の（有効な）カーネル関数で置換して表現できることを示している。</p>
<h2 id="演習-64"><a class="header" href="#演習-64">演習 6.4</a></h2>
<div class="panel-primary">
<p>付録Cでは，要素がすべて正であるが，負の固有値をもつために，正定値ではない行列の例を紹介している．逆に，$2\times 2$行列で，すべての固有値が正であるが，少なくとも1つの負の要素をもつような行列を挙げよ．</p>
</div>
<p>行列の全ての固有値が正⇔正定値行列⇔任意のべクトル $\mathbf{x} \neq \mathbf{0}$に対して$\mathbf{x}^{\mathrm T} \mathbf{A} \mathbf{x}&gt;0$
が成り立つ。</p>
<p>ここで負の要素を持つ行列$\left[\begin{array}{rr}2 &amp; -1 \ -1 &amp; 2\end{array}\right]$の正定値性を確認する。</p>
<p>$$
\mathbf{x}^{\mathrm T} \mathbf{A} \mathbf{x}=\left[\begin{array}{ll}
x_{1} &amp; x_{2}
\end{array}\right]\left[\begin{array}{rr}
2 &amp; -1 \
-1 &amp; 2
\end{array}\right]\left[\begin{array}{l}
x_{1} \
x_{2}
\end{array}\right]=2 x_{1}{ }^{2}+2 x_{2}{ }^{2}-2 x_{1} x_{2}=x_{1}{ }^{2}+x_{2}{ }^{2}+\left(x_{1}-x_{2}\right)^{2} \geqq 0
$$
さらに、$\mathbf{x}^{\mathrm T} \mathbf{A} \mathbf{x}=0$となるのは、$x_{1}=x_{2}=0$に限られるのでこの行列は正定値で、全ての固有値は正である。</p>
<h2 id="演習-65"><a class="header" href="#演習-65">演習 6.5</a></h2>
<div class="panel-primary">
<p>有効なカーネル関数を構成するために利用できる等式
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=c k_{1}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)\tag{6.13}
$$
と
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=f(\mathbf{x}) k_{1}\left(\mathbf{x}, \mathbf{x}^{\prime}\right) f\left(\mathbf{x}^{\prime}\right)
\tag{6.14}
$$
を確かめよ．</p>
</div>
<p>$(6.1)$の定義から、$k_1(\mathbf{x}, \mathbf{x}^{\prime})$が有効なカーネル関数であるならば、何らかの特徴空間への写像$\boldsymbol{\phi}(\mathbf{x})$が存在して</p>
<p>$$
k_1(\mathbf{x}, \mathbf{x}^{\prime}) = \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\boldsymbol{\phi}(\boldsymbol{\mathbf{x}^{\prime}})
$$</p>
<p>と表すことができる。そこで$c k_{1}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)$について考えると</p>
<p>$$
\begin{aligned}
c k_{1}\left(\mathbf{x}_{1}, \mathbf{x}^{\mathrm T}\right) &amp;=c \boldsymbol{\phi}(\mathbf{x})^{\mathrm T} \boldsymbol{\phi}\left(\mathbf{x}^{\prime}\right) \
&amp;=(\sqrt{c} \boldsymbol{\phi}(\mathbf{x}))^{\mathrm T}(\sqrt{c} \boldsymbol{\phi}(\mathbf{x}))
\end{aligned}
$$</p>
<p>ここで新たに$\mathbf{u}(\mathbf{x}) \equiv \sqrt{c}\boldsymbol{\phi}(\mathbf{x})$と定義すれば</p>
<p>$$
c k_1(\mathbf{x}, \mathbf{x}^{\prime}) = \mathbf{u}(\mathbf{x})^{\mathrm T}\mathbf{u}(\mathbf{x})
$$</p>
<p>と書けるので、$(6.1)$の定義から$ck_1(\mathbf{x}, \mathbf{x}^{\prime})$も有効なカーネル関数である。</p>
<p>次に任意の関数$f(\cdot)$について（この$f(\cdot)$はスカラー値となる）、$\mathbf{v}(\mathbf{x}) \equiv f(\mathbf{x})\boldsymbol{\phi}(\mathbf{x})$を定義すると</p>
<p>$$
\begin{aligned}
f(\mathbf{x})k_1(\mathbf{x}, \mathbf{x}^{\prime})f(\mathbf{x}) &amp;= f(\mathbf{x})\boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\boldsymbol{\phi}(\boldsymbol{\mathbf{x}^{\prime}})f(\mathbf{x}^{\prime}) \
&amp;= \left( f(\mathbf{x})\boldsymbol{\phi}(\mathbf{x}) \right)^{\mathrm T} \left( f(\mathbf{x}^{\prime})\boldsymbol{\phi}(\mathbf{x}^{\prime}) \right) \
&amp;= \mathbf{v}(\mathbf{x})^{\mathrm T}\mathbf{v}(\mathbf{x})
\end{aligned}
$$</p>
<p>となり、カーネル関数の定義から、これも有効なカーネル関数であることが示された。</p>
<h2 id="演習-66"><a class="header" href="#演習-66">演習 6.6</a></h2>
<div class="panel-primary">
<p>有効なカーネル関数を構成するために利用できる等式
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=q\left(k_{1}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)\right) \tag{6.15}
$$
と
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\exp \left(k_{1}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)\right) \tag{6.16}
$$
を確かめよ．</p>
</div>
<p>まず、$q(k_1(x, x^{\prime}))$カーネル関数として有効であることを示す。</p>
<p>$q(\cdot)$は、非負の係数を持つ多項式なので、</p>
<p>$$
q(x) = \sum_j^n a_j x^j
$$</p>
<p>と置くことができる。よって、</p>
<p>$$
q(k_1(x, x^{\prime})) = \sum_j^n a_j k_1(x, x^{\prime})^j
$$</p>
<p>が成り立つ。</p>
<p>今、$(6.18)$より、$k_1(x, x^{\prime})^j$ ($0 \leq j \leq n$)は、全てカーネル関数として有効である。そして、$(6.13)$より、$a_j k_1(x, x^{\prime})^j$も、全てカーネル関数として有効である。そして、$(6.17)$より、$\sum_j^n a_j k_1(x, x^{\prime})^j$はカーネル関数として有効である。</p>
<p>よって、$q(k_1(x, x^{\prime}))$もまたカーネル関数として有効である。</p>
<p>次に、$\exp(k(x, x^{\prime}))$がカーネル関数として有効であることを示す。</p>
<p>テイラー展開により、$\exp(x) = \sum \frac{x^j}{j!}$である。よって、</p>
<p>$$
\exp(k(x, x^{\prime}))= \sum \frac{k_1(x, x^{\prime})^j}{j!}
$$</p>
<p>いま、$(6.18)$より、$k_1(x, x^{\prime})^j$ ($0 \leq j \leq n$)は、全てカーネル関数として有効である。そして、$(6.13)$より、$\frac{k_1(x, x^{\prime})^j}{j!}$も、全てカーネル関数として有効である。そして、$(6.17)$より、$\sum \frac{k_1(x, x^{\prime})^j}{j!}$はカーネル関数として有効である。</p>
<p>よって、$\exp(k(x, x^{\prime}))$もまたカーネル関数として有効である。</p>
<h2 id="演習-67"><a class="header" href="#演習-67">演習 6.7</a></h2>
<div class="panel-primary">
<p>有効なカーネル関数を構成するために利用できる等式
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=k_{1}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)+k_{2}\left(\mathbf{x}, \mathbf{x}^{\prime}\right) \tag{6.17}
$$
と
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=k_{1}\left(\mathbf{x}, \mathbf{x}^{\prime}\right) k_{2}\left(\mathbf{x}, \mathbf{x}^{\prime}\right) \tag{6.18}
$$
を確かめよ．</p>
</div>
<p>$(6.17)$を示す。$(6.1)$の定義から、$k_1(\mathbf{x}, \mathbf{x}^{\prime})$, $k_2(\mathbf{x}, \mathbf{x}^{\prime})$が有効なカーネル関数であることから、何らかの特徴空間への写像$\boldsymbol{\phi}(\mathbf{x})$, $\boldsymbol{\psi}(\mathbf{x})$が存在して</p>
<p>$$
\begin{aligned}
k_1(\mathbf{x}, \mathbf{x}^{\prime}) &amp;= \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\boldsymbol{\phi}(\boldsymbol{\mathbf{x}^{\prime}})\
k_2(\mathbf{x}, \mathbf{x}^{\prime}) &amp;= \boldsymbol{\psi}(\mathbf{x})^{\mathrm T}\boldsymbol{\psi}(\boldsymbol{\mathbf{x}^{\prime}})
\end{aligned}
$$</p>
<p>と表すことができる。したがって</p>
<p>$$
\begin{aligned}
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right) &amp;= k_{1}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)+k_{2}\left(\mathbf{x}, \mathbf{x}^{\prime}\right) \
&amp;= \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\boldsymbol{\phi}(\boldsymbol{\mathbf{x}^{\prime}})+\boldsymbol{\psi}(\mathbf{x})^{\mathrm T}\boldsymbol{\psi}(\boldsymbol{\mathbf{x}^{\prime}})\
&amp;=\boldsymbol{\varphi}(\mathbf{x})^{\mathrm T}\boldsymbol{\varphi}(\boldsymbol{\mathbf{x}^{\prime}})
\end{aligned}
$$</p>
<p>ただし，$\boldsymbol\varphi(\mathbf{x})$は</p>
<p>$$
\boldsymbol\varphi(\mathbf{x}) = \left( \begin{array}{cc} \boldsymbol{\phi}(\mathbf{x})\ \boldsymbol{\psi}(\mathbf{x}) \ \end{array} \right)
$$</p>
<p>とする．以上により$(6.1)$の定義から$k(\mathbf{x}, \mathbf{x}^{\prime})$も有効なカーネル関数である。</p>
<p>次に$(6.18)$を示す。上記の方法と同様の考え方で，$k_1(\mathbf{x}, \mathbf{x}^{\prime})$, $k_2(\mathbf{x}, \mathbf{x}^{\prime})$が有効なカーネル関数であることから、何らかの特徴空間への写像$\boldsymbol{\phi}(\mathbf{x})$, $\boldsymbol{\psi}(\mathbf{x})$が存在して</p>
<p>$$
\begin{aligned}
k_1(\mathbf{x}, \mathbf{x}^{\prime}) &amp;= \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\boldsymbol{\phi}(\boldsymbol{\mathbf{x}^{\prime}})\
k_2(\mathbf{x}, \mathbf{x}^{\prime}) &amp;= \boldsymbol{\psi}(\mathbf{x})^{\mathrm T}\boldsymbol{\psi}(\boldsymbol{\mathbf{x}^{\prime}})
\end{aligned}
$$</p>
<p>と表すことができる。したがって
$$
\begin{aligned}
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right) &amp;= k_{1}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)k_{2}\left(\mathbf{x}, \mathbf{x}^{\prime}\right) \
&amp;= \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\boldsymbol{\phi}(\boldsymbol{\mathbf{x}^{\prime}})\boldsymbol{\psi}(\mathbf{x})^{\mathrm T}\boldsymbol{\psi}(\boldsymbol{\mathbf{x}^{\prime}})\
&amp;=\sum_{m=1}^{M}{\phi_{m}}\left(\mathbf{x}\right){\phi_{m}}\left(\mathbf{x}^{\prime}\right)\sum_{n=1}^{N}{\psi_{n}}\left(\mathbf{x}\right){\psi_{n}}\left(\mathbf{x}^{\prime}\right)\
&amp;=\sum_{m=1}^{M}\sum_{n=1}^{N}{\phi_{m}}\left(\mathbf{x}\right){\psi_{n}}\left(\mathbf{x}\right){\phi_{m}}\left(\mathbf{x}^{\prime}\right){\psi_{n}}\left(\mathbf{x}^{\prime}\right)\
&amp;=\sum_{k=1}^{K}\varphi_k(\mathbf{x}) \varphi_k(\mathbf{x}^{\prime})\
&amp;=\boldsymbol{\varphi}(\mathbf{x})^{\mathrm T}\boldsymbol{\varphi}(\boldsymbol{\mathbf{x}^{\prime}})
\end{aligned}
$$
ただし$K=MN$で，$\boldsymbol\varphi(\mathbf{x})$はテンソル積
$$
\boldsymbol\varphi(\mathbf{x}) = \boldsymbol{\phi}(\mathbf{x})\otimes\boldsymbol{\psi}(\mathbf{x})
$$
である。以上によりカーネル関数の定義から、これも有効なカーネル関数であることが示された。</p>
<h2 id="演習-68"><a class="header" href="#演習-68">演習 6.8</a></h2>
<div class="panel-primary">
<p>有効なカーネル関数を構成するために利用できる等式
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=k_{3}\left(\boldsymbol{\phi}(\mathbf{x}), \boldsymbol{\phi}\left(\mathbf{x}^{\prime}\right)\right) \tag{6.19}
$$
と
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\mathbf{x}^{\mathrm{T}} \mathbf{A} \mathbf{x}^{\prime} \tag{6.20}
$$
を確かめよ．</p>
</div>
<p>$(6.19)$を示す。
$k_{3}(\cdot, \cdot)$は$\mathbb{R}^{M}$で定義された有効なカーネルであり、
$(6.1)$と同様に、$\mathbf{y}, \mathbf{y}^{\prime} \in \mathbb{R}^{M}$を用いて
$$
k_{3}\left(\mathbf{y}, \mathbf{y}^{\prime}\right)=\boldsymbol{\varphi}(\mathbf{y})^{\mathrm T} \boldsymbol{\varphi}\left(\mathbf{y}^{\prime}\right)
$$
と表せる。
$$
\mathbf{y}=\boldsymbol{\phi}(\mathbf{x}), \quad \mathbf{y}^{\prime}=\boldsymbol{\phi}\left(\mathbf{x}^{\prime}\right)
$$
としたとき、
$$
\begin{aligned}
k_{3}\left(\boldsymbol{\phi}(\mathbf{x}), \boldsymbol{\phi}\left(\mathbf{x}^{\prime}\right)\right) &amp;=\boldsymbol{\varphi}(\boldsymbol{\phi}(\mathbf{x}))^{\mathrm T} \boldsymbol{\varphi}\left(\boldsymbol{\phi}\left(\mathbf{x}^{\prime}\right)\right) \
&amp;=\boldsymbol{\psi}(\mathbf{x})^{\mathrm T} \boldsymbol{\psi}\left(\mathbf{x}^{\prime}\right)
\end{aligned}
$$
と表せることから$(6.19)$が示される。ただし、
$$
\boldsymbol{\psi}(\mathbf{x})=\boldsymbol{\varphi}(\boldsymbol{\phi}(\mathbf{x}))
$$
とした。</p>
<p>$(6.20)$を示す。
一般に、$n \times n$の実対称行列$\mathbf{A}$について、</p>
<p>\begin{eqnarray}
&amp;&amp;\mathbf{A}が半正定値行列\
\overset{\text{def}}\iff&amp;&amp;\forall \mathbf{x} \in \mathbb{R}^{n}, \quad \mathbf{x}^{\mathrm T} \mathbf{A} \mathbf{x} \geq 0 \tag{6.20a}\
\iff&amp;&amp;\mathbf{A}の固有値が全て非負 \tag{6.20b}\
\iff&amp;&amp;ある実正方行列\mathbf{U}により\mathbf{A}=\mathbf{U}^{\mathrm T} \mathbf{U}と表せる \tag{6.20c}
\end{eqnarray}</p>
<p>が成り立つ。
（検索すればかなり引っかかってくるためこの同値の証明は省略します）
この$(6.20c)$を用いて、</p>
<p>$$
\begin{aligned}
\mathbf{x}^{\mathrm T} \mathbf{A} \mathbf{x}^{\prime} &amp;=\mathbf{x}^{\mathrm T} \mathbf{U}^{\mathrm T} \mathbf{U} \mathbf{x}^{\prime} \
&amp;=(\mathbf{U} \mathbf{x})^{\mathrm T} \mathbf{U} \mathbf{x}^{\prime}\
&amp;=\boldsymbol{\psi}(\mathbf{x})^{\mathrm T} \boldsymbol{\psi}\left(\mathbf{x}^{\prime}\right)
\end{aligned}
$$</p>
<p>により$(6.20)$が示される。ただし、
$$
\boldsymbol{\psi}(\mathbf{x})=\mathbf{Ux}
$$</p>
<p>とした。</p>
<h2 id="演習-69"><a class="header" href="#演習-69">演習 6.9</a></h2>
<div class="panel-primary">
<p>有効なカーネル関数を構成するために利用できる等式
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=k_{a}\left(\mathbf{x}<em>{a}, \mathbf{x}</em>{a}^{\prime}\right)+k_{b}\left(\mathbf{x}<em>{b}, \mathbf{x}</em>{b}^{\prime}\right) \tag{6.21}
$$
と
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=k_{a}\left(\mathbf{x}<em>{a}, \mathbf{x}</em>{a}^{\prime}\right) k_{b}\left(\mathbf{x}<em>{b}, \mathbf{x}</em>{b}^{\prime}\right) \tag{6.22}
$$
を確かめよ．($k_a$と$k_b$はそれぞれの特徴空間において有効なカーネル関数であるとする．)</p>
</div>
<p>$(6.21)$について、$(6.1)$と同様に、ある特徴空間への写像$\boldsymbol{\phi}(\mathbf{x})$と$\boldsymbol{\psi}(\mathbf{x})$を用いて</p>
<p>$$
\begin{aligned}
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right) &amp;=k_{a}\left(\mathbf{x}<em>{a}, \mathbf{x}</em>{a}^{\prime}\right)+k_{b}\left(\mathbf{x}<em>{b}, \mathbf{x}</em>{b}^{\prime}\right) \
&amp;=\boldsymbol{\phi}\left(\mathbf{x}<em>{a}\right)^{\mathrm T} \boldsymbol{\phi}\left(\mathbf{x}</em>{a}^{\prime}\right)+\boldsymbol{\psi}\left(\mathbf{x}<em>{b}\right)^{\mathrm T} \boldsymbol{\psi}\left(\mathbf{x}</em>{b}\right) \
&amp;=\begin{pmatrix}\boldsymbol{\phi}\left(\mathbf{x}<em>{a}\right) \ \boldsymbol{\psi}\left(\mathbf{x}</em>{b}\right)\end{pmatrix}^{\mathrm T}\begin{pmatrix}\boldsymbol{\phi}\left(\mathbf{x}<em>{a}^{\prime}\right) \ \boldsymbol{\psi}\left(\mathbf{x}</em>{b}^{\prime}\right)\end{pmatrix} \
&amp;=\boldsymbol{\varphi}(\mathbf{x})^{\mathrm T} \boldsymbol{\varphi}\left(\mathbf{x}^{\prime}\right)
\end{aligned}
$$</p>
<p>と書くことができる。ここで$\displaystyle \boldsymbol{\varphi}(\mathbf{x})=\begin{pmatrix}\boldsymbol{\phi}\left(\mathbf{x}<em>{a}\right) \ \boldsymbol{\psi}\left(\mathbf{x}</em>{b}\right)\end{pmatrix}, \boldsymbol{\varphi}\left(\mathbf{x}^{\prime}\right)=\begin{pmatrix}\boldsymbol{\phi}\left(\mathbf{x}<em>{a}^{\prime}\right) \ \boldsymbol{\psi}\left(\mathbf{x}</em>{b}^{\prime}\right)\end{pmatrix}$と定義した。$(6.1)$よりこれも有効なカーネルである。</p>
<p>$(6.22)$について、上と同様に</p>
<p>$$
\begin{aligned}
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right) &amp;=k_{a}\left(\mathbf{x}<em>{a}, \mathbf{x}</em>{a}^{\prime}\right) k_{b}\left(\mathbf{x}<em>{b}, \mathbf{x}</em>{b}^{\prime}\right) \
&amp;=\boldsymbol{\phi}\left(\mathbf{x}<em>{a}\right)^{\mathrm T} \boldsymbol{\phi}\left(\mathbf{x}</em>{a}^{\prime}\right) \boldsymbol{\psi}\left(\mathbf{x}<em>{b}\right)^{\mathrm T} \boldsymbol{\psi}\left(\mathbf{x}</em>{0}^{\prime}\right) \
&amp;=\sum_{m=1}^{M} \phi_{m}\left(\mathbf{x}<em>{a}\right) \phi</em>{m}\left(\mathbf{x}<em>{a}^{\prime}\right) \sum</em>{n=1}^{N} \psi_{n}\left(\mathbf{x}<em>{b}\right) \psi</em>{n}\left(\mathbf{x}<em>{b}^{\prime}\right) \
&amp;=\sum</em>{m=1}^{M} \sum_{n=1}^{N} \phi_{m}\left(\mathbf{x}<em>{a}\right) \psi</em>{n}\left(\mathbf{x}<em>{b}\right) \phi</em>{m}\left(\mathbf{x}<em>{a}^{\prime}\right) \psi</em>{n}\left(\mathbf{x}<em>{b}^{\prime}\right) \
&amp;=\sum</em>{k=1}^{K}\varphi_k({\mathbf{x}_a})\varphi_k({\mathbf{x}_b}) \
&amp;=\boldsymbol{\varphi}(\mathbf{x}_a)^{\mathrm T}\boldsymbol{\varphi}(\mathbf{x}_b)
\end{aligned}
$$</p>
<p>ただし$K=MN$で，$\boldsymbol\varphi(\mathbf{x})$はテンソル積
$$
\boldsymbol\varphi(\mathbf{x}) = \boldsymbol{\phi}(\mathbf{x})\otimes\boldsymbol{\psi}(\mathbf{x})
$$
である。以上によりカーネル関数の定義から，これも有効なカーネル関数であることが示された。</p>
<h2 id="演習-610"><a class="header" href="#演習-610">演習 6.10</a></h2>
<div class="panel-primary">
<p>関数$f(\mathbf{x})$を学習するためのカーネルとして$k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=f(\mathbf{x}) f\left(\mathbf{x}^{\prime}\right)$が理想的であることを，このカーネルに基づく線形の学習器は，常に$f(\mathbf{x})$に比例する解を見つけることを示すことで示せ．</p>
</div>
<p>※「このカーネルに基づく線形の学習器は……」の部分がよくわからないですけど、線形回帰モデルで重み$\mathbf{w}$を学習するのにカーネル$k(\mathbf{x},\mathbf{x^{\prime}}) = f(\mathbf{x})f(\mathbf{x}^{\prime})$が最適で、常に$f(\mathbf{x})$に比例する解を求めることができることを示せれば良いのだろうか？</p>
<p>線形の学習なので、$(6.2)$の$J(\mathbf{x})$について学習済みの重みは$(6.3)$で与えられ、これを用いて新しい入力$\mathbf{x}$に対する予測$y(\mathbf{x})$は$(6.9)$になる。</p>
<p>$$
\begin{aligned}
y(\mathbf{x}) &amp;=\mathbf{k}(\mathbf{x})^{\mathrm T}\left(\mathbf{K}+\lambda \mathbf{I}<em>{N}\right)^{-1} \mathbf{t} \
&amp;=\mathbf{k}(\mathbf{x})^{\mathrm T} \mathbf{a} \
&amp;=\sum</em>{n=1}^{N} a_n k\left(\mathbf{x}, \mathbf{x}_{n}\right)
\end{aligned}
$$</p>
<p>$k\left(\mathbf{x}, \mathbf{x}_{n}\right)$が問題文のように$f(\mathbf{x})f(\mathbf{x}_n)$で与えられるならば</p>
<p>$$
\begin{aligned} y(\mathbf{x})
&amp;=\sum_{n=1}^{N} a_{n} f(\mathbf{x}) f\left(\mathbf{x}<em>{n}\right) \
&amp;=\left(\sum</em>{n=1}^{N} a_{n} f\left(\mathbf{x}_{n}\right)\right) f(\mathbf{x})
\end{aligned}
$$</p>
<p>となる。（）内はスカラー値なので、$y(\mathbf{x})$は常に$f(\mathbf{x})$に比例する解となる。</p>
<h2 id="演習-611"><a class="header" href="#演習-611">演習 6.11</a></h2>
<div class="panel-primary">
<p>$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\exp \left(-\mathbf{x}^{\mathrm{T}} \mathbf{x} / 2 \sigma^{2}\right) \exp \left(\mathbf{x}^{\mathrm{T}} \mathbf{x}^{\prime} / \sigma^{2}\right) \exp \left(-\left(\mathbf{x}^{\prime}\right)^{\mathrm{T}} \mathbf{x}^{\prime} / 2 \sigma^{2}\right) \tag{6.25}
$$
の展開の中央の要素を，べき級数展開することによって，ガウスカーネル
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\exp \left(-\left|\mathbf{x}-\mathbf{x}^{\prime}\right|^{2} / 2 \sigma^{2}\right) \tag{6.23}
$$
は，無限次元の特徴ベクトルの内積で表されることを示せ．</p>
</div>
<p>まずは、(6.25)の展開の中央の要素を、べき級数展開する。</p>
<p>\begin{eqnarray}
\exp(\mathbf{x}^T \mathbf{x'}/\sigma^2) &amp;=&amp; \sum_{n=0}^\infty  \frac{1}{n!}
\left(\frac{\mathbf{x}^T \mathbf{x'}}{\sigma^2}\right)^n \
&amp;=&amp; \sum_{n=0}^\infty  \frac{1}{n!}
\frac{(\mathbf{x}^T)^{\otimes n} \mathbf{(x')}^{\otimes n}}{\sigma^{2n} }\
&amp;=&amp; \sum_{n=0}^\infty \boldsymbol{\phi}_n ( \mathbf{x})^{\mathrm T}\boldsymbol{\phi}_n(\mathbf{x'})\
&amp;=&amp; \boldsymbol{\psi} ( \mathbf{x})^{\mathrm T}\boldsymbol{\psi}(\mathbf{x'})
\end{eqnarray}
ここで、$\boldsymbol{\phi}_n(\mathbf{x})$と$\boldsymbol{\phi}_n(\mathbf{x})$は以下のように定義した。</p>
<p>\begin{eqnarray}
\boldsymbol{\phi}_n(\mathbf{x}):&amp;=&amp;\frac{1}{\sqrt{n!}}\frac{\mathbf{x}^{\otimes n}}{\sigma ^n}\
\boldsymbol{\psi}(\mathbf{x}):&amp;=&amp; \left{ \boldsymbol{\phi}_0(\mathbf{x}), \boldsymbol{\phi}_1(\mathbf{x}), \dots \right}\
\end{eqnarray}</p>
<p>よって、元のカーネルは、$\boldsymbol{\varphi}(\mathbf{x}):=\exp (-\mathbf{x}^\mathrm{T} \mathbf{x}/2\sigma^2)\boldsymbol{\psi}(\mathbf{x})$を用いて$k(\mathbf{x},\mathbf{x}')=\boldsymbol{\varphi}(\mathbf{x})^\mathrm{T}\boldsymbol{\varphi}(\mathbf{x'})$と書ける。</p>
<h2 id="演習-612"><a class="header" href="#演習-612">演習 6.12</a></h2>
<div class="panel-primary">
<p>あらかじめ固定された集合$D$のすべての部分集合$A$の空間を考え，カーネル関数
$$
k\left(A_{1}, A_{2}\right)=2^{\left| A_{1} \cap A_{2} \right|} \tag{6.27}
$$
は，写像$\boldsymbol{\phi}(A)$によって定義される$2^{|D|}$次元の特徴空間における内積であることを示せ．なお，$A$は$D$の部分集合であり，部分集合$U$で指定される$\boldsymbol{\phi}(A)$の各要素$\phi_U(A)$は，以下で与えられるとする．
$$
\phi_{U}(A)=\left{\begin{array}{ll}
1, &amp; U \subseteq A \text { のとき } \
0, &amp; \text { それ以外. }
\end{array}\right. \tag{6.95}
$$
ここで，$U \subseteq A$は，$U$は$A$の部分集合であるか，$A$そのものであることを表すとする．</p>
</div>
<p>$(6.1)$のカーネル関数の定義から、何らかの非線形の特徴空間への写像である$\boldsymbol{\phi}(\mathbf{x})$を用いて、この$\mathbf{x}$が集合であっても</p>
<p>$$
k(A_1, A_2) = 2^{|A_1 \cap A_2 |} = \boldsymbol{\phi}(A_1)^{\mathrm T} \boldsymbol{\phi}(A_2) \tag{1}
$$</p>
<p>という形で表すことができればよい。</p>
<p>まず一般に固定された集合$D$に含まれる要素の数を$|D|$として、$D$の部分集合全体の集合は要素数$2^{|D|}$となる（<strong>べき集合</strong>）。部分集合$A$の特徴空間への写像$\boldsymbol{\phi}(A)$は$\phi_U(A)$を構成要素としており、この部分集合$U$は同じく$2^{|D|}$個存在する。このうち、もし$\phi_U(A_1)$はもし$U$が$A_1$の部分集合であれば$1$、そうでなければ$0$となっている。</p>
<hr>
<p>例として$D={1,2,3}, A_1={1,2}, A_2 = {1,2,3}$を考える。これについての$(6.27)$式は</p>
<p>$$
k\left(A_{1}, A_{2}\right)=2^{\left|A_{1} \cap A_{2}\right|}=2^{|{1,2}|}=2^{2}=4
$$</p>
<p>となる。また、部分集合$A$の特徴空間への写像$\boldsymbol{\phi}(A)$は$2^{|D|} = 8$次元であり</p>
<p>$$
\boldsymbol{\phi}(A)=\left(\begin{array}{c}
\phi_{\phi}(A) \
\phi_{{1}}(A) \
\phi_{{2}}(A) \
\phi_{{3}}(A) \
\phi_{{1,2}}(A) \
\phi_{{2,3}}(A) \
\phi_{{1,3}}(A) \
\phi_{{1,2,3}}(A)
\end{array}\right)
$$</p>
<p>のように構成される。$(6.95)$の定義を用いると</p>
<p>$$
\begin{aligned}
\boldsymbol{\phi}\left(A_{1}\right)^{\mathrm T} &amp;= (1,1,1,0,1,0,0,0) \
\boldsymbol{\phi}\left(A_{2}\right)^{\mathrm T} &amp;= (1,1,1,1,1,1,1,1)
\end{aligned}
$$</p>
<p>である。この例では$\boldsymbol{\phi}(A_1)^{\mathrm T}\boldsymbol{\phi}(A_2) = 4$となる。</p>
<hr>
<p>上の例で見たように、$(1)$式について、$2^{|A_1 \cap A_2 |}$は$A_1$と$A_2$の共通の部分集合族の要素数を表している。</p>
<p>一方で内積$\boldsymbol{\phi}(A_1)^{\mathrm T}\boldsymbol{\phi}(A_2)$は$A_1$の部分集合であり、かつ$A_2$の部分集合となっている部分集合$U$の要素数を表していることになる。</p>
<p>この両者は同じ集合を表しているので</p>
<p>$$
k(A_1, A_2) = 2^{|A_1 \cap A_2 |}
$$</p>
<p>は写像$\boldsymbol{\phi}(A)$で定義される$2^{|D|}$次元の特徴空間における内積であることが示された。</p>
<p>※※※※※※※※※※※※
（参考）
$A$の張る線型空間においては、通常の意味での内積は定義できない。内積の定義は以下だが、そもそも$A$の集合は体ではない。（例えば、和集合$\cup$を和、積集合$\cap$を積と定義したとしても、乗法の逆元（減法）が定義されないので群をなさない。）従って、線型ベクトル空間（計量線型空間）の定義を満たさない。
\begin{eqnarray}
(A,B+C)&amp;=&amp;(A,B)+(A,C)\
(A+B,C)&amp;=&amp;(A,C)+(B,C)\
(kA,B)&amp;=&amp;k(A,B)\
(A,kB)&amp;=&amp;\bar{k}(A,B)\
(A,B)&amp;=&amp;\overline{(B,A)}\
(A,A)&amp;\geq&amp;0\
(A,A)&amp;=&amp;0となるのはA=0の時に限る。
\end{eqnarray}
上で議論したのは、$A$の非線形写像$\phi(A)$同士の内積であって、$A$同士の内積ではない。
※※※※※※※※※※※※</p>
<h2 id="演習-613"><a class="header" href="#演習-613">演習 6.13</a></h2>
<div class="panel-primary">
<p>$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\mathbf{g}(\boldsymbol{\theta}, \mathbf{x})^{\mathrm{T}} \mathbf{F}^{-1} \mathbf{g}\left(\boldsymbol{\theta}, \mathbf{x}^{\prime}\right) \tag{6.33}
$$
で定義されるフィッシャーカーネルはパラメータベクトル$\boldsymbol{\theta}$に非線形の変換$\boldsymbol{\theta} \rightarrow \boldsymbol{\psi}(\boldsymbol{\theta})$を行っても不変であることを示せ．なお，$\boldsymbol{\psi}(\cdot)$は可逆で，かつ，微分可能であるとする．</p>
</div>
<p>$(6.32)$の定義から</p>
<p>$$
\mathbf{g}(\boldsymbol{\theta}, \mathbf{x})=\nabla_{\boldsymbol{\theta}} \ln p(\mathbf{x} \mid \boldsymbol{\theta}) \tag{6.32}
$$</p>
<p>そして$\mathbf{F}$はフィッシャー情報量行列</p>
<p>$$
\mathbf{F}=\mathbb{E}_{\mathbf{x}}\left[\mathbf{g}(\boldsymbol{\theta}, \mathbf{x}) \mathbf{g}(\boldsymbol{\theta}, \mathbf{x})^{\mathrm{T}}\right] \tag{6.34}
$$</p>
<p>である。</p>
<p>$\boldsymbol{\theta}\to \boldsymbol{\psi}(\boldsymbol{\theta})$の変換に伴い（$\boldsymbol{\psi}(\cdot)$は微分可能なので）</p>
<p>$$
\begin{aligned}
\mathbf{g}(\boldsymbol{\theta}, \mathbf{x}) &amp;=\nabla_{\boldsymbol{\theta}} \ln p(\mathbf{x} \mid \boldsymbol{\theta}) \
&amp;=\begin{pmatrix}\frac{\partial}{\partial \theta_{1}} \ \vdots \\frac{\partial}{\partial \theta_{n}}\end{pmatrix} \ln p(\mathbf{x} \mid \boldsymbol{\theta}) \
&amp;=\begin{pmatrix}\frac{\partial \psi_{1}}{\partial \theta_{1}} &amp; \cdots &amp; \frac{\partial \psi_{n}}{\partial \theta_{1}} \ \vdots &amp; \ddots &amp; \vdots \ \frac{\partial \psi_{1}}{\partial \theta_{m}} &amp; \cdots &amp; \frac{\partial \psi_{n}}{\partial \theta_{m}}\end{pmatrix}\begin{pmatrix}\frac{\partial}{\partial \psi_{1}} \ \vdots \ \frac{\partial}{\partial \psi_{n}}\end{pmatrix} \ln p(\mathbf{x} \mid \boldsymbol{\psi}(\boldsymbol{\theta})) \
&amp;=\mathbf{M}\nabla_{\boldsymbol{\psi}}\ln p(\mathbf{x}\mid \boldsymbol{\psi}(\boldsymbol{\theta}))
\end{aligned}
$$</p>
<p>となる。ここで$\mathbf{M}$は$M_{ij} = \frac{\partial \psi_j}{\partial \theta_i}$を成分とするヤコビ行列である。また、簡略化のために</p>
<p>$$
\mathbf{g}<em>{\boldsymbol{\psi}} = \nabla</em>{\boldsymbol{\psi}}\ln p(\mathbf{x}\mid \boldsymbol{\psi}({\boldsymbol{\theta}}))
$$</p>
<p>とおく。これよりフィッシャー情報量行列も以下のように書き換えられる。</p>
<p>$$
\begin{aligned}
\mathbf{F}<em>{\boldsymbol{\psi}} &amp;= \mathbb{E}</em>{\mathbf{x}}\left[ \mathbf{g}(\boldsymbol{\theta}, \mathbf{x}) \mathbf{g}(\boldsymbol{\theta}, \mathbf{x})^{\mathrm T} \right]\
&amp;=\mathbb{E}<em>{\mathbf{x}}\left[\mathbf{M g}</em>{\boldsymbol{\psi}} \mathbf{g}<em>{\boldsymbol{\psi}}^{\mathrm{T}} \mathbf{M}^{\mathrm{T}}\right] \ &amp;=\mathbf{M} \mathbb{E}</em>{\mathbf{x}}\left[\mathbf{g}<em>{\boldsymbol{\psi}} \mathbf{g}</em>{\boldsymbol{\psi}}^{\mathrm{T}}\right] \mathbf{M}^{\mathrm{T}}
\end{aligned}
$$</p>
<p>以上からこの$\mathbf{g}<em>{\boldsymbol{\psi}}$と$\mathbf{F}</em>{\boldsymbol{\psi}}$を用いて$\boldsymbol{\theta}\to \boldsymbol{\psi}(\boldsymbol{\theta})$での変換後の$(6.33)$のフィッシャーカーネルの右辺を計算すると</p>
<p>$$
\begin{aligned}
\mathbf{g}<em>{\boldsymbol{\psi}}^{\mathrm T} \mathbb{E}</em>{\mathbf{x}}\left[\mathbf{g}<em>{\boldsymbol{\psi}} \mathbf{g}</em>{\boldsymbol{\psi}}^{\mathrm T}\right]^{-1} \mathbf{g}<em>{\boldsymbol{\psi}} &amp;= \mathbf{g}^{\mathrm T}\left(\mathbf{M}^{-1}\right)^{\mathrm T}\left(\mathbb{E}</em>{\mathbf{x}}\left[ \mathbf{M}^{-1}\mathbf{g}\mathbf{g}^{\mathrm T}(\mathbf{M}^{-1})^{\mathrm T} \right]\right)^{-1}\mathbf{M}^{-1}\mathbf{g} \
&amp;=\mathbf{g}^{\mathrm T}\left( \mathbf{M}^{-1} \right)^{\mathrm T} \left( \left( \mathbf{M}^{-1} \right)^{\mathrm T} \right)^{-1}\mathbb{E}<em>{\mathbf{x}}\left[\mathbf{gg}^{\mathrm T}\right]^{-1} \left( \mathbf{M}^{-1} \right)^{-1}\mathbf{M}^{-1}\mathbf{g} \
&amp;=\mathbf{g}^{\mathrm T}\mathbb{E}</em>{\mathbf{x}}\left[\mathbf{gg}^{\mathrm T}\right]^{-1}\mathbf{g} \
&amp;=\mathbf{g}^{\mathrm T}\mathbf{F}\mathbf{g}
\end{aligned}
$$</p>
<p>となる。以上から、$\boldsymbol{\theta} \rightarrow \boldsymbol{\psi}(\boldsymbol{\theta})$の変換を行っても$(6.33)$式で定義されるフィッシャーカーネルは不変であることが示された。</p>
<h2 id="演習-614"><a class="header" href="#演習-614">演習 6.14</a></h2>
<div class="panel-primary">
<p>平均$\boldsymbol{\mu}$と共分散$\mathbf{S}$をもつガウス分布$p(\mathbf{x} \mid \boldsymbol{\mu})=\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{S})$に対して，
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\mathbf{g}(\boldsymbol{\theta}, \mathbf{x})^{\mathrm{T}} \mathbf{F}^{-1} \mathbf{g}\left(\boldsymbol{\theta}, \mathbf{x}^{\prime}\right) \tag{6.33}
$$
で定義されるフィッシャーカーネルの具体的な形式を導け．</p>
</div>
<p>まずフィッシャースコア$(6.32)$について計算すると</p>
<p>$$
\begin{aligned}
\mathbf{g}(\boldsymbol{\mu}, \mathbf{x}) &amp;=\nabla_{\boldsymbol{\mu}} \ln p(\mathbf{x} \mid \boldsymbol{\mu}) \ &amp;=\nabla_{\boldsymbol{\mu}} \ln \left(\exp \left{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\mathrm T} \mathbf{S}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right}\right) \
&amp;=\left(-\frac{1}{2}\right)(-2) \mathbf{S}^{-1}(\mathbf{x}-\boldsymbol{\mu}) \hspace{1em} \left( \because \frac{\partial}{\partial \mathbf{s}}(\mathbf{x}-\mathbf{s})^{\mathrm T} \mathbf{W}(\mathbf{x}-\mathbf{s})=-2 \mathbf{W}(\mathbf{x}-\mathbf{s})\right)\
&amp;=\mathbf{S}^{-1}(\mathbf{x}-\boldsymbol{\mu})
\end{aligned}
$$</p>
<p>次にフィッシャー情報量行列$\mathbf{F}$は、$\mathbf{S}$が共分散行列なので対称行列であることを利用すると</p>
<p>$$
\begin{aligned}
\mathbf{F} &amp;=\mathbb{E}<em>{\mathbf{x}}\left[\mathbf{S}^{-1}(\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^{\mathrm T}\left(\mathbf{S}^{-1}\right)^{\mathrm T}\right] \
&amp;=\mathbf{S}^{-1} \mathbb{E}</em>{\mathbf{x}}\left[(\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^{\mathrm T}\right] \mathbf{S}^{-1}\left(\because\left(\mathbf{S}^{-1}\right)^{\mathrm T}=\mathbf{S}^{-1}\right) \
&amp;=\mathbf{S}^{-1} \mathbf{S} \mathbf{S}^{-1}\left(\because \mathbb{E}_{\mathbf{x}}\left[(\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^{\mathrm T}\right]=\mathbf{S}\right) \
&amp;=\mathbf{S}^{-1}
\end{aligned}
$$</p>
<p>以上からフィッシャーカーネルは</p>
<p>$$
\begin{aligned}
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)
&amp;= \mathbf{g}(\boldsymbol{\mu}, \mathbf{x})^{\mathrm{T}} \mathbf{F}^{-1} \mathbf{g}\left(\boldsymbol{\mu}, \mathbf{x}^{\prime}\right)\
&amp;=\left(\mathbf{S}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)^{\mathrm T}\left(\mathbf{S}^{-1}\right)^{-1} \mathbf{S}^{-1}\left(\mathbf{x}^{\prime}-\boldsymbol{\mu}\right) \
&amp;=(\mathbf{x}-\boldsymbol{\mu})^{\mathrm T} \mathbf{S}^{-1}\left(\mathbf{x}^{\prime}-\boldsymbol{\mu}\right)
\end{aligned}
$$</p>
<p>となる。結果的にこの値は$(2.44)$で示された<strong>マハラノビス距離</strong>になっている。</p>
<h2 id="演習-615"><a class="header" href="#演習-615">演習 6.15</a></h2>
<div class="panel-primary">
<p>$2\times 2$のグラム行列の行列式を考えて，正定値であるカーネル関数$k(x, x^{\prime})$はコーシーシュワルツの不等式</p>
<p>$$
k\left(x_{1}, x_{2}\right)^{2} \leqslant k\left(x_{1}, x_{1}\right) k\left(x_{2}, x_{2}\right) \tag{6.96}
$$</p>
<p>を満たすことを示せ．</p>
</div>
<p>2X2のグラム行列は次のように与えられる。
$$
\left(\begin{array}{ll}k\left(x_{1}, x_{1}\right) &amp; k\left(x_{1}, x_{2}\right) \ k\left(x_{2}, x_{1}\right) &amp; k\left(x_{2}, x_{2}\right)\end{array}\right)
$$</p>
<p>有効なカーネル関数の場合グラム行列は半正定値なので行列式は0以上となる。(固有値
が非負のため)。上記グラム行列の行列式は
$$
k\left(x_{1}, x_{1}\right) \cdot k\left(x_{2}, x_{2}\right)-k\left(x_{1}, x_{2}\right) \cdot k\left(x_{2}, x_{1}\right) \geqq 0
$$</p>
<p>$$
\Leftrightarrow k\left(x_{1}, x_{2}\right)^{2} \leqq k\left(x_{1}, x_{1}\right) k\left(x_{2}, x_{2}\right)
$$</p>
<p>となり、(6.96)を満たすことが示された。</p>
<h2 id="演習-616"><a class="header" href="#演習-616">演習 6.16</a></h2>
<div class="panel-primary">
<p>パラメータベクトル$\mathbf{w}$と入力のデータ集合$\mathbf{x}<em>1, \ldots, \mathbf{x}<em>N$および非線形の特徴空間への写像$\boldsymbol{\phi}(\mathbf{x})$を持つパラメトリックモデルに対し，誤差関数が$\mathbf{w}$の関数として次のように与えられるとする．
$$
J(\mathbf{w})=f\left(\mathbf{w}^{\mathbf{T}} \boldsymbol{\phi}\left(\mathbf{x}</em>{1}\right), \ldots, \mathbf{w}^{\mathbf{T}} \boldsymbol{\phi}\left(\mathbf{x}</em>{N}\right)\right)+g\left(\mathbf{w}^{\mathbf{T}} \mathbf{w}\right)
$$
ここで，$g(\cdot)$は単調増加関数であるとする．$\mathbf{w}$を，
$$
\mathbf{w}=\sum_{n=1}^{N} \alpha_{n} \boldsymbol{\phi}\left(\mathbf{x}<em>{n}\right)+\mathbf{w}</em>{\perp} \tag{6.98}
$$
という形式で書くことによって$J(\mathbf{w})$を最小化する$\mathbf{w}$の値は$n=1,\ldots,N$についての基底関数$\boldsymbol{\phi}(\mathbf{x}<em>n)$の線形結合で表されることを示せ．ただしすべての$n$について$\mathbf{w}</em>{\perp}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)=0$であるとする．</p>
</div>
<p>リプレゼンター定理の証明。</p>
<p>(6.98)式は以下のように書き換えることができる。</p>
<p>\begin{equation}
\mathbf{w}=\mathbf{w}<em>{|}+\mathbf{w}</em>{\perp}
\end{equation}</p>
<p>\begin{equation}
\mathbf{w}<em>{|}=\sum</em>{n=1}^{N} \alpha_{n} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)
\end{equation}</p>
<p>これは、$\mathbf{w}$を$\mathbf{w}<em>{|}$とその直交補空間の$\mathbf{w}</em>{\perp}$の直和分解していることになるので、全てのnに対して$\mathbf{w}<em>{\perp}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}</em>{n}\right)=0$が成立する。</p>
<p>$\mathbf{w}$を損失関数$J(\mathbf{w})$に代入すると、以下の式になる。</p>
<p>$$
\begin{aligned}
J(\mathbf{w})=&amp; f\left(\left(\mathbf{w}<em>{|}+\mathbf{w}</em>{\perp}\right)^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}<em>{1}\right), \ldots,\left(\mathbf{w}</em>{|}+\mathbf{w}<em>{\perp}\right)^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}</em>{N}\right)\right) \
&amp;+g\left(\left(\mathbf{w}<em>{|}+\mathbf{w}</em>{\perp}\right)^{\mathrm{T}}\left(\mathbf{w}<em>{|}+\mathbf{w}</em>{\perp}\right)\right) \
=&amp; f\left(\mathbf{w}<em>{|}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}</em>{1}\right), \ldots, \mathbf{w}<em>{|}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}</em>{N}\right)\right)+g\left(\mathbf{w}<em>{|}^{\mathrm{T}} \mathbf{w}</em>{|}+\mathbf{w}<em>{\perp}^{\mathrm{T}} \mathbf{w}</em>{\perp}\right)
\end{aligned}
$$</p>
<p>損失関数の最小化を考えた場合、$\mathbf{w}<em>{\perp}$は正則化項の$g(\cdot)$の中にのみに存在し、定義より$g(\cdot)$は単調増加関数なので$\mathbf{w}</em>{\perp}$が0となる。よって</p>
<p>$$
\mathbf{w}=\mathbf{w}<em>{|}=\sum</em>{n=1}^{N} \alpha_{n} \phi\left(\mathbf{x}_{n}\right)
$$</p>
<p>が示された。</p>
<h2 id="演習-617"><a class="header" href="#演習-617">演習 6.17</a></h2>
<div class="panel-primary">
<p>入力に，分布$\nu(\boldsymbol{\xi})$を持つノイズがある場合の二乗和誤差関数</p>
<p>$$
E=\frac{1}{2} \sum_{n=1}^{N} \int\left{y\left(\mathbf{x}<em>{n}+\boldsymbol{\xi}\right)-t</em>{n}\right}^{2} \nu(\boldsymbol{\xi}) \mathrm{d} \boldsymbol{\xi} \tag{6.39}
$$</p>
<p>を考える．変分法を用いて，この誤差関数を関数$y(\mathbf{x})$について最小化し，最適な解は，基底関数として</p>
<p>$$
h\left(\mathbf{x}-\mathbf{x}<em>{n}\right)=\frac{\nu\left(\mathbf{x}-\mathbf{x}</em>{n}\right)}{\sum_{n=1}^{N} \nu\left(\mathbf{x}-\mathbf{x}_{n}\right)} \tag{6.41}
$$</p>
<p>を用いた展開</p>
<p>$$
y(\mathbf{x})=\sum_{n=1}^{N} t_{n} h\left(\mathbf{x}-\mathbf{x}_{n}\right) \tag{6.40}
$$</p>
<p>の形で与えられることを示せ．</p>
</div>
<p>※ 変分法（上巻の付録D）によれば、$y(\mathbf{x})$についての汎関数$E[y]$の最小化を考える。ある微小な定数$\varepsilon$と任意の関数$\eta(x)$を用いて、汎関数$E[y]$の$y(x)$に対する変分$\delta E/\delta y(x)$を次の式で定義する。</p>
<p>$$
E[y(x)+\varepsilon \eta(x)]=E[y(x)]+\varepsilon \int \frac{\delta E}{\delta y} \eta(x) d x+O\left(\varepsilon^{2}\right)
$$</p>
<p>汎関数が最大もしくは最小となる場合には関数$y(x)$の微小な変化に対して汎関数が変化しない（停留する）ことが必要条件となる。すなわち</p>
<p>$$
\varepsilon \int \frac{\delta E}{\delta y} \eta(x) d x = 0
$$
となる。</p>
<hr>
<p>以上の変分法の考えから、まず$y(\mathbf{x})\to y(\mathbf{x})+\epsilon\eta(\mathbf{x})$としたときの汎関数$E[y]$の変分を考える。</p>
<p>$$
\begin{align}
E[y+\varepsilon \eta]
&amp;=\frac{1}{2} \sum_{n=1}^{N} \int\left{y\left(\mathbf{x}<em>n+\boldsymbol{\xi}\right)+\varepsilon \eta\left(\mathbf{x}<em>n+\boldsymbol{\xi}\right)-t</em>{n}\right}^{2} \nu(\boldsymbol{\xi}) d \boldsymbol{\xi} \
&amp;=\frac{1}{2} \sum</em>{n=1}^{N} \int\left[\left{y\left(\mathbf{x}<em>n+\boldsymbol{\xi}\right)-t</em>{n}\right}^{2}+2 \varepsilon \eta\left(\mathbf{x}<em>n+\boldsymbol{\xi}\right)\left{y\left(\mathbf{x}<em>n+\boldsymbol{\xi}\right)-t</em>{n}\right}+\varepsilon^{2} \eta^{2}\right] \nu(\boldsymbol{\xi}) d\boldsymbol{\xi} \
&amp;= \frac{1}{2} \sum</em>{n=1}^{N} \int\left[\left{y\left(\mathbf{x}<em>n+\boldsymbol{\xi}\right)-t</em>{n}\right}^{2} \right]\nu(\boldsymbol{\xi})d\boldsymbol{\xi} + \varepsilon \sum_{n=1}^{N}\int \eta\left(\mathbf{x}<em>n+\boldsymbol{\xi}\right)\left{y\left(\mathbf{x}<em>n+\boldsymbol{\xi}\right)-t</em>{n}\right} \nu(\boldsymbol{\xi})d\boldsymbol{\xi} +  \frac{\varepsilon^{2}}{2} \sum</em>{n=1}^{N} \int\eta^{2} \nu(\boldsymbol{\xi}) d\boldsymbol{\xi} \
&amp;=E[y] + \varepsilon\sum_{n=1}^{N}\int \eta(\mathbf{x}_n+\boldsymbol{\xi})\left{ y(\mathbf{x}_n+\boldsymbol{\xi}) -t_n\right}\nu(\boldsymbol{\xi})d\boldsymbol{\xi} + O(\varepsilon^2)
\end{align}
$$</p>
<p>以上から変分が0になる停留条件は</p>
<p>$$
\sum_{n=1}^{N}\int \left{ y(\mathbf{x}_n+\boldsymbol{\xi}) -t_n\right}\eta(\mathbf{x}_n+\boldsymbol{\xi})\nu(\boldsymbol{\xi})d\boldsymbol{\xi} = 0
$$</p>
<p>である。</p>
<p>$\eta(\mathbf{x}_n+\boldsymbol{\xi})$は任意の関数である。そのため、ディラックのデルタ関数として、積分記号を外すことを目指す。</p>
<p>$$
\eta(\mathbf{x}_n+\boldsymbol{\xi}) = \delta((\mathbf{x}_n+\boldsymbol{\xi})-\mathbf{z})
$$</p>
<p>とすると、ディラックのデルタ関数の性質より、</p>
<p>$$
\int f(x)\delta(x - x_0)dx = f(x_0)
$$</p>
<p>が成り立つ。よって、$g(x) =  {y(\mathbf{x}_n) -t_n} \nu(\mathbf{\xi})$として、</p>
<p>$$
\begin{align}
\sum_{n=1}^{N} \int\left{y\left(\mathbf{x}<em>{n}+\boldsymbol{\xi}\right)-t</em>{n}\right}  \delta\left(\left(
\mathbf{x}<em>{n}+\boldsymbol{\xi}\right)-\mathbf{z}\right) \nu(\boldsymbol{\xi}) d \boldsymbol{\xi}
&amp;= \sum</em>{n=1}^{N} \int\left{y\left(\mathbf{x}<em>{n}+\boldsymbol{\xi}\right)-t</em>{n}\right}  \nu(\mathbf{\xi}) \delta\left(\left(\mathbf{x}<em>{n}+\boldsymbol{\xi}\right)-\mathbf{z}\right)d \boldsymbol{\xi} \
&amp;= \sum</em>{n=1}^{N} \int g(\mathbf{x}<em>n+ \boldsymbol{\xi}) \delta\left(\left(\mathbf{x}</em>{n}+\boldsymbol{\xi}\right)-\mathbf{z}\right)d \boldsymbol{\xi} \
&amp;= \sum_{n=1}^{N} g(\mathbf{z}) \
&amp;= \sum_{n=1}^{N}\left{y(\mathbf{z})-t_{n}\right} \nu\left(\mathbf{z}-\mathbf{x}_{n}\right) \
&amp;= 0
\end{align}
$$</p>
<p>なお、ディラックのデルタ関数の性質における$x = \mathbf{x}_{n}+\boldsymbol{\xi}$、$x_0 = \mathbf{z}$とした。また、最後の0は、停留条件である。</p>
<p>これを変形して</p>
<p>$$
\begin{aligned}
&amp;y(\mathbf{z}) \sum_{n=1}^{N}\nu(\mathbf{z}-\mathbf{x}<em>n) = \sum</em>{n=1}^{N}t_n\nu(\mathbf{z}-\mathbf{x}<em>n) \
&amp;y(\mathbf{z}) = \sum</em>{n=1}^{N}t_n\frac{\nu(\mathbf{z}-\mathbf{x}<em>n)}{\sum</em>{n=1}^N\nu(\mathbf{z}-\mathbf{x}_n)}
\end{aligned}
$$</p>
<p>最後に$\mathbf{z}\to\mathbf{x}$とすれば、題意が成立する。</p>
<h2 id="演習-618"><a class="header" href="#演習-618">演習 6.18</a></h2>
<div class="panel-primary">
<p>等方共分散をもつ，つまり，共分散行列が$\sigma^2 \mathbf{I}$ ($\mathbf{I}$は単位行列)で与えられるようなガウス基底を持つようなNadaraya-Watsonモデルを考える．入力変数$x$と，目標変数$t$はそれぞれ1次元であるとする．このとき，条件付き密度$p(t\mid x)$，条件付き期待値$\mathbb{E}[t\mid x]$，および条件付き分散$\operatorname{var}[t\mid x]$をそれぞれカーネル関数$k(x, x_n)$を用いて書け．</p>
</div>
<p>教科書下巻P.18の設定通り、$f(x,t)$が平均$\mathbf{0}$,分散$\sigma ^2 \mathbf{I}$の等方的なガウス分布$\mathbf{z}=(x,t)$で与えられる場合を考える。すなわち</p>
<p>$$
f(x, t)=\mathcal{N}\left(\mathbf{z} \mid \mathbf{0}, \sigma^{2} \mathbf{I}\right)
$$</p>
<p>これを用いてParzen推定法から同時分布を求める式$(6.42)$</p>
<p>$$
p(x, t)=\frac{1}{N} \sum_{n=1}^{N} f\left(x-x_{n}, t-t_{n}\right)
$$</p>
<p>とする。</p>
<p>条件付き確率 $\displaystyle p(t\mid x) = \frac{p(t,x)}{p(x)} = \frac{p(t,x)}{\int p(t,x)dt}$より</p>
<p>$$
p(t \mid x)=\frac{\sum_{n=1}^{N} \mathcal{N}\left(\left[x-x_{n}, t-t_{n}\right]^{\mathrm T} \mid \mathbf{0}, \sigma^{2} \mathbf{I}\right)}{\int \sum_{m=1}^{N} \mathcal{N}\left(\left[x-x_{m}, t-t_{m}\right]^{\mathrm T} \mid \mathbf{0}, \sigma^{2} \mathbf{I}\right) d t}
$$</p>
<p>となる。</p>
<p>分子は</p>
<p>$$
\begin{aligned} &amp; \sum_{n=1}^{N} \mathcal{N}\left(\begin{pmatrix}x-x_{n} \ t-t_{n}\end{pmatrix} \mid \mathbf{0}, \sigma^{2} \mathbf{I}\right) \
=&amp; \sum_{n=1}^{N}\left[\frac{1}{(2 \pi)^{2 / 2}} \frac{1}{\sqrt{\mid \sigma^{2} \mathbf{I}} \mid } \exp \left{-\frac{1}{2}\begin{pmatrix}x-x_{n} \ t-t_{n}\end{pmatrix}^{\mathrm T}\left(\sigma^{2} \mathbf{I}\right)^{-1}\begin{pmatrix}x-x_{n} \ t-t_{n}\end{pmatrix}\right}\right] \
=&amp; \sum_{n=1}^{N}\left[\frac{1}{2 \pi \sigma^{2}} \exp \left{-\frac{1}{2 \sigma^{2}}\left(x-x_{n}\right)^{2}\right} \exp \left{-\frac{1}{2 \sigma^{2}}\left(t-t_{n}\right)^{2}\right}\right] \
=&amp; \sum_{n=1}^{N}\left[\mathcal{N}\left(x-x_{n} \mid 0, \sigma^{2}\right) \cdot \mathcal{N}\left(t-t_{n} \mid 0, \sigma^{2}\right)\right]
\end{aligned}
$$</p>
<p>となる。途中では$\mathbf{I}$が$2\times 2$の単位行列であることから$|\sigma^2 \mathbf{I}| = (\sigma^2)^2$となることを用いた。一方分母は$t$についての周辺化を行うので</p>
<p>$$
\begin{aligned}
&amp;\int \sum_{m=1}^{N} \mathcal{N}\left(\begin{pmatrix}x-x_{n} \ t-t_{n}\end{pmatrix} \mid \mathbf{0}, \sigma^{2} \mathbf{I}\right)dt \
=&amp;\int \sum_{m=1}^{N}\left[\mathcal{N}\left(x-x_{n} \mid 0, \sigma^{2}\right) \cdot \mathcal{N}\left(t-t_{n} \mid 0, \sigma^{2}\right)\right] dt \
=&amp;\sum_{m=1}^{N}\mathcal{N}\left(x-x_{n} \mid 0, \sigma^{2}\right)\int \mathcal{N}\left(t-t_{n} \mid 0, \sigma^{2}\right) dt \
=&amp;\sum_{m=1}^{N}\mathcal{N}\left(x-x_{n} \mid 0, \sigma^{2}\right)
\end{aligned}
$$</p>
<p>となる。以上をまとめると</p>
<p>$$
\begin{aligned}
p(t \mid x) &amp;=\frac{\sum_{n=1}^{N}\left[\mathcal{N}\left(x-x_{n} \mid 0, \sigma^{2}\right) \cdot \mathcal{N}\left(t-t_{n} \mid 0, \sigma^{2}\right)\right]}{\sum_{m=1}^{N} \mathcal{N}\left(x-x_{m} \mid 0, \sigma^{2}\right)} \
&amp;=\sum_{n=1}^{N} \frac{\mathcal{N}\left(x-x_{n} \mid 0, \sigma^{2}\right)}{\sum_{m=1}^{N} \mathcal{N}\left(x-x_{m} \mid 0, \sigma^{2}\right)} \mathcal{N}\left(t-t_{n} \mid 0, \sigma^{2}\right)
\end{aligned}
$$</p>
<p>ここで</p>
<p>$$
\begin{aligned}
g(x) &amp;= \int_{-\infty}^{\infty} \mathcal{N}\left(\begin{pmatrix}x \ t\end{pmatrix} \mid \mathbf{0}, \sigma^{2} \mathbf{I}\right) d t \
&amp;= \mathcal{N}\left( x \mid 0 , \sigma^2 \right)
\end{aligned}
$$</p>
<p>とすると</p>
<p>$$
\begin{aligned}
p(t \mid x) &amp;=\sum_{n=1}^{N} \frac{g\left(x-x_{n}\right)}{\sum_{m=1}^{N} g\left(x-x_{m}\right)} \mathcal{N}\left(t-t_n \mid 0, \sigma^{2}\right) \
&amp;=\sum_{n=1}^{N} \frac{g\left(x-x_{n}\right)}{\sum_{m=1}^{N} g\left(x-x_{m}\right)} \mathcal{N}\left(t \mid t_{n}, \sigma^{2}\right) \ &amp; \equiv \sum_{n=1}^{N} k\left(x, x_{n}\right) \mathcal{N}\left(t \mid t_{n}, \sigma^{2}\right)
\end{aligned}
$$</p>
<p>となる。ここで定義したカーネル$k(x,x_n)$は和の制約</p>
<p>$$
\sum_{n=1}^N k(x,x_n) = 1
$$</p>
<p>を満たしている。</p>
<p>次に、条件付き期待値は定義から</p>
<p>$$
\begin{aligned}
\mathbb{E}[t \mid x]
&amp;=\int t p(t \mid x) d t \
&amp;=\int t \sum_{n=1}^{N} k\left(x, x_{n}\right) \mathcal{N}\left(t \mid t_{n}, \sigma^{2}\right) d t \
&amp;=\sum_{n=1}^{N} k\left(x, x_{n}\right) \int t \mathcal{N}\left(t \mid t_{n}, \sigma^{2}\right) d t \
&amp;=\sum_{n=1}^{N} k(x,x_n) t_n \left( \because (1.49)式, ガウス分布の性質\right)
\end{aligned}
$$</p>
<p>条件付き分散は、まず$\mathbb{E}[t^2\mid x]$について</p>
<p>$$
\begin{aligned} \mathbb{E}\left[t^{2} \mid x\right] &amp;=\int t^{2} p(t \mid x) d x \ &amp;=\int t^{2} \sum_{n=1}^{N} k\left(x, x_{n}\right) \mathcal{N}\left(t \mid t_{n}, \sigma^{2}\right) d t \ &amp;=\sum_{n=1}^{N} k\left(x, x_{n}\right) \int t^{2} \mathcal{N}\left(t \mid t_{n}, \sigma^{2}\right) d t \ &amp;=\sum_{n=1}^{N} k\left(x, x_{n}\right)\left(t_{n}^{2}+\sigma^{2}\right) \left( \because (1.50)式, ガウス分布の性質\right)
\end{aligned}
$$</p>
<p>これより</p>
<p>$$
\begin{aligned}
\operatorname{var}[t\mid x] &amp;= \mathbb{E}[t^2\mid x] - \mathbb{E}[t\mid x]^2 \
&amp;= \sum_{n=1}^{N} k\left(x, x_{n}\right)\left(t_{n}^{2}+\sigma^{2}\right)-\left{\sum_{n=1}^{N} k\left(x, x_{n}\right) t_{n}\right}^{2}
\end{aligned}
$$</p>
<h2 id="演習-619"><a class="header" href="#演習-619">演習 6.19</a></h2>
<div class="panel-primary">
<p>カーネル回婦の問題を別の視点から見ると，入力変数と目標変数が加法的なノイズによって影響されていると考えることができる．通常通り，各目標変数$t_n$を，点$\mathbf{z}<em>n$において評価された関数$y(\mathbf{z}<em>n)$に，ガウスノイズが加わったものとする．$\mathbf{z}<em>n$は直接観測されることはなく，ノイズが加わった$\mathbf{x}</em>{n}=\mathbf{z}</em>{n}+\boldsymbol{\xi}</em>{n}$が観測される．ここで，確率変数$\boldsymbol{\xi}$は，ある分布$g(\boldsymbol{\xi})$に従うとする．観測された集合$\left{\mathbf{x}<em>{n}, t</em>{n}\right}(n=1, \ldots, N)$に対して入力変数に加えられたノイズの分布で期待値を取った二乗和誤差関数
$$
E=\frac{1}{2} \sum_{n=1}^{N} \int\left{y\left(\mathbf{x}<em>{n}-\boldsymbol{\xi}</em>{n}\right)-t_{n}\right}^{2} g\left(\boldsymbol{\xi}<em>{n}\right) \mathrm{d} \boldsymbol{\xi}</em>{n} \tag{6.99}
$$
を考える．変分法(付録D)を用いて．$E$を関数$y(\mathbf{z})$について最小化することで，最適な$y(\mathbf{x})$は，カーネル
$$
k\left(\mathbf{x}, \mathbf{x}<em>{n}\right)=\frac{g\left(\mathbf{x}-\mathbf{x}</em>{n}\right)}{\sum_{m} g\left(\mathbf{x}-\mathbf{x}<em>{m}\right)} \tag{6.46}
$$
を持った，Nadaraya-Watsonカーネル回帰
$$
\begin{aligned} y(\mathbf{x})=&amp; \frac{\sum</em>{n} g\left(\mathbf{x}-\mathbf{x}<em>{n}\right) t</em>{n}}{\sum_{m} g\left(\mathbf{x}-\mathbf{x}<em>{m}\right)} \=&amp; \sum</em>{n} k\left(\mathbf{x}, \mathbf{x}<em>{n}\right) t</em>{n} \end{aligned} \tag{6.45}
$$
の形になることを示せ．</p>
</div>
<p>※演習$6.17$とほぼ同じ</p>
<p>関数$y(\mathbf{z}_n)$について、微小な定数$\varepsilon$と任意の関数$\eta(\mathbf{z}_n)$を用いた変分法を$(6.99)$式に適用する。</p>
<p>$y(\mathbf{z}_n) \to y(\mathbf{z}_n) + \varepsilon\eta(\mathbf{z}_n)$の変分を考えて,</p>
<p>$$
\begin{aligned}
E[y+\varepsilon \eta] &amp;=-\frac{1}{2} \sum_{n=1}^{N} \int\left{y\left(\mathbf{x}<em>{n}-\mathbf{\xi}<em>n\right)+\varepsilon \eta\left(\mathbf{x}</em>{n}-\mathbf{\xi}\right)-t</em>{n}\right}^{2} g\left(\mathbf{\xi}\right) d \mathbf{\xi}<em>n \
&amp;=-\frac{1}{2} \sum</em>{n=1}^{N} \int\left[\left{y\left(\mathbf{x}<em>{n}-\mathbf{\xi}<em>n\right)-t</em>{n}\right}^{2} +2 \varepsilon \eta\left(\mathbf{x}</em>{n}-\mathbf{\xi}<em>n\right)\left(y\left(\mathbf{x}</em>{n}-\mathbf{\xi}<em>n\right)-t</em>{n}\right)+\varepsilon^2\eta\left(\mathbf{x}_{n}-\mathbf{\xi}<em>n\right)^2\right]g(\mathbf{\xi})d\mathbf{z}<em>n \
&amp;=E[y]-\varepsilon \sum</em>{n=1}^{N} \int \eta\left(\mathbf{x}</em>{n}-\mathbf{\xi}<em>n \right)\left(y\left(\mathbf{x}</em>{n}-\mathbf{\xi}<em>n \right)-t</em>{n}\right) g\left(\mathbf{\xi}_n\right) d \mathbf{\xi}_n+O\left(\varepsilon^{2}\right)
\end{aligned}
$$</p>
<p>これより停留条件は</p>
<p>$$
\sum_{n=1}^{N} \int \eta\left(\mathbf{x}_{n}-\mathbf{\xi}<em>n \right)\left(y\left(\mathbf{x}</em>{n}-\mathbf{\xi}<em>n \right)-t</em>{n}\right) g\left(\mathbf{\xi}_n\right) d \mathbf{\xi}_n = 0
$$</p>
<p>となる。</p>
<p>ここで$\eta(\mathbf{x}_n - \mathbf{\xi}_n) = \delta(\mathbf{x}_n - \mathbf{\xi}_n- \mathbf{z})$とおくと</p>
<p>$$
\begin{align}
\sum_{n=1}^{N} \int \delta\left(\mathbf{x}<em>n-\boldsymbol{\xi}</em>{n}-\mathbf{z}\right)\left(y\left(\mathbf{x}<em>n-\boldsymbol{\xi}</em>{n}\right)-t_{n}\right) g\left(\mathbf{x}<em>n - \left( \mathbf{x}<em>n - \mathbf{\xi}<em>n \right) \right) d \mathbf{\xi}</em>{n}
&amp;= \sum</em>{n=1}^{N} \left(y\left(\mathbf{z}\right)-t</em>{n}\right) g\left(\mathbf{x}_n-\mathbf{z}\right) \
&amp;= 0
\end{align}
$$</p>
<p>$y(\mathbf{z})$について解くと</p>
<p>$$
\begin{aligned}
&amp;y(\mathbf{z}) \sum_{n=1}^{N} g\left(\mathbf{x}<em>{n}-\mathbf{z}\right)=\sum</em>{n=1}^{N} t_n g\left(\mathbf{x}<em>{n}-\mathbf{z}\right) \
&amp;\Leftrightarrow y(\mathbf{z}) = \sum</em>{n=1}^{N} t_n \frac{g\left(\mathbf{x}<em>{n}-\mathbf{z}\right)}{\sum</em>{m=1}^{N} g\left(\mathbf{x}<em>{m}-\mathbf{z}\right)}
\end{aligned}
$$
となる。そして、
$$
\begin{align}
y(\mathbf{z}) &amp;= \sum</em>{n=1}^{N} t_n \frac{g\left(\mathbf{x}<em>{n}-\mathbf{z}\right)}{\sum</em>{m=1}^{N} g\left(\mathbf{x}<em>{m}-\mathbf{z}\right)} \
&amp;= \sum</em>{n=1}^{N} t_n  k(z, x_n)
\end{align}
$$</p>
<p>となり、$z = x$を代入すれば、題意は満たされる。</p>
<p>これは式$(6.46)$のカーネルを持ったNadaraya-Watsonモデルとなっており、さらにこのカーネルは和の制約</p>
<p>$$
\sum_{n=1}^N k(\mathbf{x},\mathbf{x}<em>n) = \frac{\sum</em>{n=1}^N g\left(\mathbf{x}<em>{n}-\mathbf{x}\right)}{\sum</em>{m=1}^{N} g\left(\mathbf{x}_{m}-\mathbf{x}\right)} = 1
$$</p>
<p>を満たしている。</p>
<h2 id="演習-620"><a class="header" href="#演習-620">演習 6.20</a></h2>
<div class="panel-primary">
<p>$$
m\left(\mathbf{x}<em>{N+1}\right)=\mathbf{k}^{\mathrm{T}} \mathbf{C}</em>{N}^{-1} \mathbf{t} \tag{6.66}
$$
と
$$
\sigma^{2}\left(\mathbf{x}<em>{N+1}\right)=c-\mathbf{k}^{\mathrm{T}} \mathbf{C}</em>{N}^{-1} \mathbf{k} \tag{6.67}
$$
の結果を確認せよ．</p>
</div>
<p>上巻P.82の2.3.1 条件付きガウス分布を参照。多変数ガウス分布の重要な特性で、2つの変数集合の同時分布がガウス分布に従うならば、一方の変数集合が与えられたときの、もう一方の集合の条件付き分布もガウス分布となる。さらに、どちらの変数集合の周辺分布も同様にガウス分布になる。</p>
<p>今、訓練集合として入力$\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{N}$と、対応する$\mathbf{t}<em>N = (t_1, \ldots, t_N)^{\mathrm T}$が与えられているときに、新しい入力ベクトルに対する目標変数$t</em>{N+1}$を予測する。$t_1,\ldots,t_{N+1}$の同時分布は</p>
<p>$$
p(\mathbf{t}<em>{N+1}) = \mathcal{N}\left(\mathbf{t}</em>{N+1} \mid \mathbf{0}, \mathbf{C}_{N+1}\right) \tag{6.64}
$$</p>
<p>のガウス分布で、$t_1,\ldots,t_{N}$の同時分布は</p>
<p>$$
p(\mathbf{t})=\int p(\mathbf{t} \mid \mathbf{y}) p(\mathbf{y}) \mathrm{d} \mathbf{y}=\mathcal{N}(\mathbf{t} \mid \mathbf{0}, \mathbf{C}) \tag{6.61}
$$</p>
<p>のガウス分布で与えられている。</p>
<p>条件付き分布$p(t_{N+1}\mid \mathbf{t})$を求めたいので、2.3.1.条件付きガウス分布の定理$(2.81), (2.82)$に書かれている、条件付き分布$p(\mathbf{x}_a\mid \mathbf{x}_b)$の平均と共分散が</p>
<p>$$
\begin{aligned}
\boldsymbol{\boldsymbol{\mu}}<em>{a \mid b}=\boldsymbol{\mu}</em>{a}+\mathbf{\Sigma}<em>{a b} \mathbf{\Sigma}</em>{b b}^{-1}\left(\mathbf{x}<em>{b}-\boldsymbol{\mu}</em>{b}\right) \ \mathbf{\Sigma}<em>{a \mid b}=\mathbf{\Sigma}</em>{a a}-\mathbf{\Sigma}<em>{a b} \mathbf{\Sigma}</em>{b b}^{-1} \mathbf{\Sigma}_{b a}
\end{aligned}
$$</p>
<p>で与えられることを利用するために、$\mathbf{x}<em>a \to t</em>{N+1}$, $\mathbf{x}_b \to \mathbf{t}$とする。この設定より$\boldsymbol{\mu}_a \to 0$, $\boldsymbol{\mu}<em>b \to \mathbf{0}$とする。ここで、共分散行列$\mathbf{C}</em>{N+1}$はこのように設定した影響で</p>
<p>$$
\mathbf{C}<em>{N+1}=\begin{pmatrix}c &amp; \mathbf{k}^{\mathrm{T}} \ \mathbf{k} &amp; \mathbf{C}</em>{N}\end{pmatrix}
$$</p>
<p>と書き表されることに注意する。これから</p>
<p>$$
\left(\begin{array}{ll}\mathbf{\Sigma}<em>{a a} &amp; \mathbf{\Sigma}</em>{a b} \ \mathbf{\Sigma}<em>{b a} &amp; \mathbf{\Sigma}</em>{b b}\end{array}\right)=
\begin{pmatrix}c &amp; \mathbf{k}^{\mathrm{T}} \ \mathbf{k} &amp; \mathbf{C}_{N}\end{pmatrix}
$$</p>
<p>となる。</p>
<p>以上から、</p>
<p>$$
m(\mathbf{x}_{N+1}) = \mathbf{0} + \mathbf{k}^{\mathrm T}\mathbf{C}_N^{-1}(\mathbf{t}-\mathbf{0}) = \mathbf{k}^{\mathrm T}\mathbf{C}_N^{-1}\mathbf{t} \tag{6.66}
$$</p>
<p>$$
\sigma^{2}(\mathbf{x}_{N+1}) = c - \mathbf{k}^{\mathrm T}\mathbf{C}_N^{-1}\mathbf{k} \tag{6.67}
$$</p>
<p>を得る。</p>
<h2 id="演習-621"><a class="header" href="#演習-621">演習 6.21</a></h2>
<div class="panel-primary">
<p>固定された非線形の基底関数を使ってカーネル関数が定義されたガウス過程による回帰モデルを考え，その予測分布が3.3.2節で得られたベイス線形回帰モデルに対する結果
$$
p(t \mid \mathbf{x}, \mathbf{t}, \alpha, \beta)=\mathcal{N}\left(t \mid \mathbf{m}<em>{N}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x}), \sigma</em>{N}^{2}(\mathbf{x})\right) \tag{3.58}
$$
と同じになることを示せ．両方のモデルがガウス予測分布を持つことに注意する，つまり条件付き期待値と条件付き分散がそれぞれ等しくなることを示せばよい．条件付き期待値については，行列に関する等式(C.6)を条件付き分散については(C.7)を用いよ．</p>
</div>
<p>(3.58)の$m_{N}$,$\mathbf{S}<em>{N}$はそれぞれ
$$
\mathbf{m}</em>{N}=\beta\mathbf{S}<em>{N}\mathbf{\Phi}^{T}\mathbf{t}\tag{3.53}
$$
$$
\mathbf{S}</em>{N}^{-1}=\alpha\mathbf{I}+\beta\mathbf\Phi\mathbf{\Phi}^{T}\tag{3.54}
$$
と表される.
(C.6), (C.7)はそれぞれ</p>
<p>$$
(\mathbf{I}+\mathbf{A}\mathbf{B})^{-1}\mathbf{A}=\mathbf{A}(\mathbf{I}+\mathbf{B}\mathbf{A})^{-1}\tag{C.6}
$$
$$
(\mathbf{A}+\mathbf{B}\mathbf{D}^{-1}\mathbf{C})^{-1}=\mathbf{A}^{-1}-\mathbf{A}^{-1}\mathbf{B}(\mathbf{D}+\mathbf{C}\mathbf{A}^{-1}\mathbf{B})^{-1}\mathbf{C}\mathbf{A}^{-1}\tag{C.7}
$$</p>
<p>である．
固定された非線形の基底関数を$\boldsymbol{\phi}$とおくと(6.62)より
$$
\begin{aligned}
\mathbf{C}<em>{N}&amp;=\mathbf{K}+\beta^{-1}\mathbf{I}</em>{N}\
&amp;=\alpha^{-1}\mathbf{\Phi\Phi^T}+\beta^{-1}\mathbf{I}<em>{N}
\end{aligned}
$$
$$
\begin{aligned}
\mathbf{k}&amp;=(k(x</em>{1}, x_{N+1}),k(x_{2},x_{N+1}), ...,k(x_{N},x_{N+1}))^{T}\
&amp;=((\phi(x_{1})^{T}\phi(x_{N+1}),(\phi(x_{2})^{T}\phi(x_{N+1}),...,(\phi(x_{N})^{T}\phi(x_{N+1})))^{T}\
&amp;=\alpha^{-1}\mathbf{\Phi}\mathbf{\phi}(x_{N+1})
\end{aligned}
$$
$$
c = k(\mathbf{x}<em>{N+1},\mathbf{x}</em>{N+1})+\beta^{-1}
$$
これらを(6.66)に代入して
$$
\begin{aligned}
m(\mathbf{x}<em>{N+1})&amp;=\mathbf{k}^{T}\mathbf{C}</em>{N}^{-1}\mathbf{t}\
&amp;=\left{\alpha^{-1}\mathbf{\Phi}\mathbf{\phi}(\mathbf{x}<em>{N+1})\right}^{T}(\alpha^{-1}\mathbf{\Phi\Phi^T}+\beta^{-1}\mathbf{I}</em>{N})^{-1}\mathbf{t}\
&amp;=\mathbf{\phi}(\mathbf{x}<em>{N+1})^{T}\alpha^{-1}\mathbf{\Phi}^{T}\alpha\beta(\beta\mathbf{\Phi\Phi^T}+\alpha\mathbf{I}</em>{N})^{-1}\mathbf{t}\
&amp;=\mathbf{\phi}(\mathbf{x}<em>{N+1})^{T}\beta(\beta\mathbf{\Phi^T\Phi}+\alpha\mathbf{I}</em>{N})^{-1}\mathbf{\Phi}^{T}\mathbf{t}\
&amp;=\mathbf{\phi}(\mathbf{x}<em>{N+1})^{T}(\beta\mathbf{S}</em>{N}\mathbf{\Phi}^{T}\mathbf{t})\
&amp;=\mathbf{\phi}(\mathbf{x}<em>{N+1})^{T}\mathbf{m}</em>{N}
\end{aligned}
$$
となり(3.58)の結果と一致する．3行目から4行目にかけての式変形において(C.6)の右辺から左辺を導く操作を行なった．次に共分散行列について(6.67)に代入して
$$
\begin{aligned}
\sigma^{2}(\mathbf{x}<em>{N+1})&amp;=c-\mathbf{k}^{T}\mathbf{C}</em>{N}^{-1}\mathbf{k}\
&amp;=\left{\mathbf\phi(x_{N+1})^{T}\mathbf\phi(x_{N+1})+\beta^{-1}\right}-\left{\alpha^{-1}\mathbf{\Phi}\mathbf{\phi}(x_{N+1})\right}^{T}(\alpha^{-1}\mathbf{\Phi\Phi^T}+\beta^{-1}\mathbf{I}<em>{N})^{-1}\alpha^{-1}\mathbf{\Phi}\mathbf{\phi}(x</em>{N+1})\
&amp;=\beta^{-1}+\mathbf\phi(x_{N+1})^{T}\mathbf\phi(x_{N+1})-\mathbf{\phi}(\mathbf{x}<em>{N+1})^{T}\alpha^{-1}\mathbf{\Phi}^{T}\left{\alpha\beta（\beta\mathbf{\Phi\Phi^T}+\alpha\mathbf{I}</em>{N})^{-1}\right}\alpha^{-1}\mathbf{\Phi}\mathbf{\phi}(x_{N+1})\
&amp;=\beta^{-1}+\mathbf{\phi}(\mathbf{x}<em>{N+1})^{T}\left{\mathbf{I}-\alpha^{-1}\beta\mathbf{\Phi}^{T}（\beta\mathbf{\Phi\Phi^T}+\alpha\mathbf{I}</em>{N})^{-1}\mathbf{\Phi}\right}\mathbf{\phi}(\mathbf{x}<em>{N+1})\
&amp;=\beta^{-1}+\mathbf{\phi}(\mathbf{x}</em>{N+1})^{T}\left{\mathbf{I}-\alpha^{-1}\beta（\beta\mathbf{\Phi^T\Phi}+\alpha\mathbf{I}<em>{N})^{-1}\mathbf{\Phi}^{T}\mathbf{\Phi}\right}\mathbf{\phi}(\mathbf{x}</em>{N+1})\
&amp;=\beta^{-1}+\mathbf{\phi}(\mathbf{x}<em>{N+1})^{T}\left{\alpha^{-1}(\mathbf{I+\alpha^{-1}\beta\mathbf{\Phi}^T}\mathbf{\Phi})^{-1}\right}\mathbf{\phi}(\mathbf{x}</em>{N+1})\
&amp;=\beta^{-1}+\mathbf{\phi}(\mathbf{x}<em>{N+1})^{T}(\alpha\mathbf{I}+\beta\mathbf{\Phi}^T\mathbf{\Phi})^{-1}\mathbf{\phi}(\mathbf{x}</em>{N+1})\
&amp;=\beta^{-1}+\mathbf{\phi}(\mathbf{x}<em>{N+1})^{T}\mathbf{S}</em>{N}\mathbf{\phi}(\mathbf{x}<em>{N+1})\
&amp;=\sigma^{2}</em>{N}(\mathbf{x})
\end{aligned}
$$
となり(3.58)    の結果と一致している．なお4行目から5行目の式変形には(C.6)を右辺から左辺を導く形で用いた．また5行目から6行目の式変形には(C.7)を右辺から左辺を導く形で用いた．8行目から9行目は(3.59)を用いた．</p>
<h2 id="演習-622"><a class="header" href="#演習-622">演習 6.22</a></h2>
<div class="panel-primary">
<p>$N$個の入力ベクトル$\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{N}$を持つ訓練集合と，$L$個の入力ベクトル$\mathbf{x}<em>{N+1}, \ldots, \mathbf{x}</em>{N+L}$を持つテスト集合があるような回帰問題を考える．また, 関数$t(\mathbf{x})$上の事前分布としてガウス過程を考える．$t(\mathbf{x}<em>1),\ldots, t(\mathbf{x}<em>N)$が与えられたときの，$t(\mathbf{x}</em>{N+1}),\ldots, t(\mathbf{x}</em>{N+L})$の同時予測分布を導け．この分布の，ある$t_j$($N+1 \leq j \leq N+L$とする)についての周辺分布を考えたとき，それは通常のガウス過程による回帰の結果
$$
m\left(\mathbf{x}<em>{N+1}\right)=\mathbf{k}^{\mathrm{T}} \mathbf{C}</em>{N}^{-1} \mathbf{t} \tag{6.66}
$$
と
$$
\sigma^{2}\left(\mathbf{x}<em>{N+1}\right)=c-\mathbf{k}^{\mathrm{T}} \mathbf{C}</em>{N}^{-1} \mathbf{k} \tag{6.67}
$$
に一致することを示せ．</p>
</div>
<p>求めたいのは$t(\mathbf{x}<em>1),\ldots, t(\mathbf{x}<em>N)$が与えられたときの$t(\mathbf{x}</em>{N+1}),\ldots, t(\mathbf{x}</em>{N+L})$の同時予測分布なので、$p\left(\mathbf{t}<em>{N+1 \ldots N+L} \mid \mathbf{t}</em>{1 \ldots N}\right)$の形である。ここで</p>
<p>$$
\begin{aligned}
\mathbf{t}<em>{1 \ldots N}=\mathbf{t}</em>{N}=\left(t_{1}, \ldots, t_{N}\right)^{\mathrm T} \
\mathbf{t}<em>{N+1 \ldots {N+L}}=\left(t</em>{N+1}, \cdots, t_{N+L}\right)^{\mathrm T}
\end{aligned}
$$</p>
<p>と記述する。</p>
<p>$(6.61)$からすべての$n=1, \ldots, N+L$について</p>
<p>$$
p(\mathbf{t}) = \mathcal{N}(\mathbf{t}\mid \mathbf{0}, \mathbf{C})
$$</p>
<p>であり、これを2.3.1節にしたがって$\mathbf{t}$を$\mathbf{t}<em>{1\ldots N}$と$\mathbf{t}</em>{N+1\ldots N+L}$に分割する。すなわち</p>
<p>$$
\mathbf{t}=\begin{pmatrix}
\mathbf{t}<em>{N+1 ,\cdots, N+L} \
\mathbf{t}</em>{1, \ldots, N}
\end{pmatrix}
$$</p>
<p>の形にする。これに対応する共分散行列$\mathbf{C}$も</p>
<p>$$
\mathbf{C}=\begin{pmatrix}
\mathbf{C}<em>{a a} &amp; \mathbf{C}</em>{a b} \
\mathbf{C}<em>{b a} &amp; \mathbf{C}</em>{b b}
\end{pmatrix}=\begin{pmatrix}
\mathbf{C}<em>{N+1, \ldots, N+L} &amp; \mathbf{K}^{\mathrm T} \
\mathbf{K} &amp; \mathbf{C}</em>{1,\ldots,N}
\end{pmatrix}
$$</p>
<p>のように分割する。ここで$\mathbf{K}$は$N\times L$行列で</p>
<p>$$
\mathbf{K}=\begin{pmatrix}
k\left(\mathbf{x}<em>{1}, \mathbf{x}</em>{N+1}\right) &amp; \cdots &amp; k\left(\mathbf{x}<em>{1}, \mathbf{x}</em>{N+L}\right) \
\vdots &amp; \ddots &amp; \vdots \
k\left(\mathbf{x}<em>{N}, \mathbf{x}</em>{N+1}\right) &amp; \cdots &amp; k\left(\mathbf{x}<em>{N}, \mathbf{x}</em>{N+L}\right)
\end{pmatrix}
$$</p>
<p>で定義される。</p>
<p>これらを用いて$(2.81)$と$(2.82)$を用いて条件付き分布$p\left(\mathbf{t}<em>{N+1,\ldots,N+L} \mid \mathbf{t}</em>{N}\right)$の平均と共分散を求めると</p>
<p>$$
\begin{array}{l}
\boldsymbol{\mu}<em>{a b}=\mathbf{0}+\mathbf{K}^{\mathrm T} \mathbf{C}</em>{N}^{-1} \mathbf{t}<em>{N} \
\mathbf{\Sigma}</em>{a \mid b}=\mathbf{C}<em>{N+1,\ldots,N+L}-\mathbf{K}^{\mathrm T} \mathbf{C}</em>{N}^{-1} \mathbf{K}
\end{array}
$$</p>
<p>となるので、以上から</p>
<p>$$
\begin{aligned}
p\left(\mathbf{t}<em>{N+1,\ldots,N+L} \mid \mathbf{t}</em>{N}\right) &amp;= \mathcal{N}\left(\mathbf{t}<em>{N+1,\ldots,N+L} \mid \boldsymbol{\mu}</em>{a \mid b}, \mathbf{\Sigma}<em>{a \mid b}\right) \
&amp;= \mathcal{N}\left(\mathbf{t}</em>{N+1,\ldots,N+L} \mid \mathbf{K}^{\mathrm T} \mathbf{C}<em>{N}^{-1} \mathbf{t}</em>{N}, \mathbf{C}<em>{N+1,\ldots,N+L}-\mathbf{K}^{\mathrm T} \mathbf{C}</em>{N}^{-1} \mathbf{K}\right)
\end{aligned}
$$</p>
<p>となる。</p>
<p>また、この分布のある$t_j\ (N+1 \le j \le N+L)$についての周辺分布は$p(t_j \mid \mathbf{t}_N)$となり、これはP.19の議論と同様に</p>
<p>$$
\mathbf{C} = \begin{pmatrix}
c &amp; \mathbf{k}^{\mathrm T} \
\mathbf{k} &amp; \mathbf{C}_N
\end{pmatrix}
$$</p>
<p>となる($c = k(\mathbf{x}<em>j, \mathbf{x}</em>{N+1}) + \beta^{-1}$)。</p>
<p>これは結局$(6.66)$と$(6.67)$と同様に</p>
<p>$$
p\left(t_{j} \mid \mathbf{t}<em>{N}\right)=\mathcal{N}\left(t</em>{j} \mid m\left(\mathbf{x}<em>{N+1}\right), \sigma^{2}\left(\mathbf{x}</em>{N+1}\right)\right)
$$
となり、ここで</p>
<p>$$
\begin{aligned}
m\left(\mathbf{x}<em>{N+1}\right) &amp;= \mathbf{k}^{\mathrm{T}} \mathbf{C}</em>{N}^{-1} \mathbf{t}<em>N \
\sigma^{2}\left(\mathbf{x}</em>{N+1}\right) &amp;= c-\mathbf{k}^{\mathrm{T}} \mathbf{C}_{N}^{-1} \mathbf{k}
\end{aligned}
$$</p>
<p>と一致する。</p>
<h2 id="演習-623"><a class="header" href="#演習-623">演習 6.23</a></h2>
<div class="panel-primary">
<p>ガウス過程による回帰モデルで目標変数$\mathbf{t}$の次元が$D$であるようなものを考える．入力ベクトル$\mathbf{x}_1, \ldots, \mathbf{x}_N$を持つ訓練集合と，対応する目標変数の値の集合$\mathbf{t}<em>1,\ldots,\mathbf{t}<em>N$が与えられたときの，テストデータ$\mathbf{x}</em>{N+1}$に対する$\mathbf{t}</em>{N+1}$の条件付き分布を導け．</p>
</div>
<p>6.4.2節で議論されていた条件付き分布を$\mathbf t_{N}=\left(\begin{array}{c}t_{1} \
\vdots \
t_{N}
\end{array}\right)$から$\mathbf T_{N}=\left(\begin{array}{cccc}t_{11} &amp; \cdots &amp; t_{1D} \ \vdots &amp; &amp; \vdots \ t_{N 1} &amp; \cdots &amp; t_{N D}\end{array}\right)$に拡張する。</p>
<p>(6.61)式と同様に、
$$
p\left(\mathbf T_{N}\right)=\mathcal{N}\left(\mathbf T_{N} \mathbf \mid \mathbf{O}, \mathbf{C}<em>{N}\right)
$$
とあらわせる。
ただし、$\mathbf{C}</em>{N}$は$C\left(x_{n}, x_{m}\right)=k\left(x_{n}, x_{m}\right)+\beta^{-1} \delta_{n m}$を要素として持つ共分散行列。
(2.81)式、(2.82)式を用いて、(6.66)式,(6.67)式と同様に、
$$
\mathbf m\left(x_{N+1}\right)=\left(\mathbf k^{\mathrm T} \mathbf C_{N}^{-1} \mathbf T_{N}\right)^{\mathrm T}\
\sigma^{2}\left(x_{N+1}\right)=c-\mathbf{k}^{\mathrm T} \mathbf{C}<em>{N}^{-1} \mathbf k
$$
と算出されるような条件付き分布
$$
p\left(\mathbf t</em>{N+1} \mid \mathbf T_{N}\right)=\mathcal{N}\left(\mathbf t_{N+1} \mid \mathbf m\left(x_{N+1}\right), \sigma\left(x_{N+1}\right) \mathbf{I}_{N}\right)
$$
を導ける。</p>
<h2 id="演習-624"><a class="header" href="#演習-624">演習 6.24</a></h2>
<div class="panel-primary">
<p>対角行列$\mathbf{W}$で，その要素が$0 \lt W_{ii} \lt 1$を満たすものは，正定値であることを示せ．また，2つの正定値行列の和は，やはり正定値になることを示せ．</p>
</div>
<p>正定値であることの定義から、任意のベクトル$\mathbf{x}$（$\mathbf{x} \neq \mathbf{0}$）に対して$\mathbf{x}^{\mathrm T}\mathbf{Wx} \gt 0$が成立していることを示せば良い。また$\mathbf{W}$は対角行列なので</p>
<p>$$
\begin{aligned}
\mathbf{x}^{\mathrm T}\mathbf{Wx} &amp;= \sum_{i} \sum_{j} x_{i} W_{i j} x_{j} \
&amp;= \sum_{i}x_i^2 W_{ii} \hspace{1em} (\because W_{ij}=0\ \textrm{ if }\ i \neq j )
\end{aligned}
$$</p>
<p>である。ここで問題設定から$0 \lt W_{ii} \lt 1$であり、全ての$i$について$x_i^2 W_{ii} \geq 0$が成り立つ。また、$\mathbf{x} \neq \mathbf{0}$より少なくとも1つの$i$について$x_i^2 \neq 0$であり、$\mathbf{x}^{\mathrm T}\mathbf{Wx} \gt 0$が常に成立する。したがって、$\mathbf{W}$は正定値であることが示された。</p>
<p>また任意の正定値行列$\mathbf{A}_1$と$\mathbf{A}_2$と、この2つの和である行列$\mathbf{A} = \mathbf{A}<em>1+\mathbf{A}<em>2$について、これらは定義から$\mathbf{x}^{\mathrm T}\mathbf{A}</em>{1}\mathbf{x} \gt 0$, $\mathbf{x}^{\mathrm T}\mathbf{A}</em>{2}\mathbf{x} \gt 0$となっていることから、$\mathbf{x}^{\mathrm T}\mathbf{A}\mathbf{x}$について計算すると</p>
<p>$$
\mathbf{x}^{\mathrm T}\mathbf{A}\mathbf{x} = \mathbf{x}^{\mathrm T}(\mathbf{A}<em>{1} + \mathbf{A}</em>{2})\mathbf{x} = \mathbf{x}^{\mathrm T}\mathbf{A}<em>{1}\mathbf{x} + \mathbf{x}^{\mathrm T}\mathbf{A}</em>{2}\mathbf{x} \gt 0
$$</p>
<p>となる。以上から、任意の2つの正定置行列の和は正定値行列であることが示された。</p>
<h2 id="演習-625"><a class="header" href="#演習-625">演習 6.25</a></h2>
<div class="panel-primary">
<p>ニュートンーラフソン法の公式
$$
\mathbf{w}^{(\text {new})}=\mathbf{w}^{(\text {old})}-\mathbf{H}^{-1} \nabla E(\mathbf{w}) \tag{4.92}
$$
を用いて，ガウス過程による分類モデルに対する，事後分布のモード$\mathbf{a}<em>N^{\star}$を求めるための逐次更新の公式
$$
\mathbf{a}</em>{N}^{\text {new }}=\mathbf{C}<em>{N}\left(\mathbf{I}+\mathbf{W}</em>{N} \mathbf{C}<em>{N}\right)^{-1}\left{\mathbf{t}</em>{N}-\boldsymbol{\sigma}<em>{N}+\mathbf{W}</em>{N} \mathbf{a}_{N}\right} \tag{6.83}
$$
を導け．</p>
</div>
<p>$(6.81)$と$(6.82)$式をニュートンーラフソン法に当てはめる。</p>
<p>$$
\nabla \Psi\left(\mathbf{a}<em>{N}\right)=\mathbf{t}</em>{N}-\boldsymbol{\sigma}<em>{N}-\mathbf{C}</em>{N}^{-1} \mathbf{a}_{N} \tag{6.81}
$$</p>
<p>ここで$\boldsymbol{\sigma}_N$は要素$\sigma(a_n)=\frac{1}{1+e^{-a_n}}$を持つベクトルである。これの$\mathbf{a}_n$での2階微分が$(6.82)$式で</p>
<p>$$
\nabla \nabla \Psi\left(\mathbf{a}<em>{N}\right)=-\mathbf{W}</em>{N}-\mathbf{C}_{N}^{-1} \tag{6.82}
$$</p>
<p>である。ここで$\mathbf{W}_N$は$\sigma(a_n)(1-\sigma(a_n))$を要素に持つ対角行列である。</p>
<p>ニュートンーラフソン法に当てはめると、</p>
<p>$$
\mathbf{a}^{\text {new}}=\mathbf{a}<em>{N}^{\text {old}}-\left(\nabla \nabla \Psi\left(\mathbf{a}</em>{N}^{\text {old}}\right)\right)^{-1}\left(\nabla \Psi\left(\mathbf{a}_{N}^{\text {old}}\right)\right)
$$</p>
<p>となるので、これを展開していく。</p>
<p>$$
\begin{aligned}
\mathbf{a}<em>{N}^{\text{new}} &amp;=\mathbf{a}</em>{N}^{\text{old}}+\left(\mathbf{W}<em>{N}+\mathbf{C}</em>{N}^{-1}\right)^{-1}\left(\mathbf{t}<em>{N}-\boldsymbol{\sigma}</em>{N}-\mathbf{C}<em>{N}^{-1} \mathbf{a}</em>{N}^{\text{old}}\right) \
&amp;=\left(\mathbf{W}<em>{N}+\mathbf{C}</em>{N}^{-1}\right)^{-1}\left{\left(\mathbf{W}<em>{N}+\mathbf{C}</em>{N}^{-1}\right) \mathbf{a}<em>{N}^{\text{old}}+\mathbf{t}</em>{N}-\boldsymbol{\sigma}<em>{N}-\mathbf{C}</em>{N}^{-1} \mathbf{a}<em>{N}^{\text{old}}\right} \
&amp;=\left(\mathbf{W}</em>{N}+\mathbf{C}<em>{N}^{-1}\right)^{-1}\left(\mathbf{W}</em>{N} \mathbf{a}<em>{N}^{\text{old}}+\mathbf{t}</em>{N}-\boldsymbol{\sigma}<em>{N}\right) \
&amp;=\mathbf{C}</em>{N} \mathbf{C}<em>{N}^{-1}\left(\mathbf{W}</em>{N}+\mathbf{C}<em>{N}^{-1}\right)^{-1}\left(\mathbf{t}</em>{N}-\boldsymbol{\sigma}<em>{N}+\mathbf{W}</em>{N} \mathbf{a}<em>{N}^{\text{old}}\right) \
&amp;=\mathbf{C}</em>{N}\left(\mathbf{W}<em>{N} \mathbf{C}</em>{N}+\mathbf{I}\right)^{-1}\left(\mathbf{t}<em>{N}-\boldsymbol{\sigma}</em>{N}+\mathbf{W}<em>{N} \mathbf{a}</em>{N}^{\text{old}}\right) \quad \left(\because \mathbf{A}^{-1} \mathbf{B}^{-1}=\mathbf{BA}^{-1}\right) \
&amp;=\mathbf{C}<em>{N}\left(\mathbf{I}+\mathbf{W}</em>{N} \mathbf{C}<em>{N}\right)^{-1}\left(\mathbf{t}</em>{N}-\boldsymbol{\sigma}<em>{N}+\mathbf{W}</em>{N} \mathbf{a}_{N}^{\text{old}}\right)
\end{aligned}
$$</p>
<p>以上より、逐次更新式$(6.83)$が求まった。</p>
<h2 id="演習-626"><a class="header" href="#演習-626">演習 6.26</a></h2>
<div class="panel-primary">
<p>$$
p(\mathbf{y})=\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \boldsymbol{\mu}+\mathbf{b}, \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}\right) \tag{2.115}
$$
の結果を用いて，ガウス過程による分類モデルに対する事後分布$p(a_{N+1}\mid \mathbf{t}<em>N)$の平均
$$
\mathbb{E}\left[a</em>{N+1} \mid \mathbf{t}<em>{N}\right] =\mathbf{k}^{\mathrm{T}}\left(\mathbf{t}</em>{N}-\boldsymbol{\sigma}<em>{N}\right) \tag{6.87}
$$
と分散
$$
\operatorname{var}\left[a</em>{N+1} \mid \mathbf{t}<em>{N}\right] =c-\mathbf{k}^{\mathrm{T}}\left(\mathbf{W}</em>{N}^{-1}+\mathbf{C}_{N}\right)^{-1} \mathbf{k} \tag{6.88}
$$
を導け．</p>
</div>
<p>(2.113)式$\Leftrightarrow$(6.86)式、(2.114)式$\Leftrightarrow$(6.78)式の対応関係は、
$$
\begin{aligned}
\mathbf{x} &amp;= \mathbf{a}<em>N\
\mathbf{y} &amp;= \mathbf{a}</em>{N+1}\
\mathbf{A} &amp;= \mathbf{k}^{\mathrm T} \mathbf{C}_N^{-1}\
\boldsymbol{\mu} &amp;= \mathbf{a}_N^\star = \mathbf{C}_N (\mathbf{t}_N -\boldsymbol{\sigma}_N)\
\mathbf{b} &amp;= \mathbf{0} \
\mathbf{L}^{-1} &amp;= c - \mathbf{k}^{\mathrm T} \mathbf{C}_N^{-1}\mathbf{k} \
\mathbf{\Lambda} ^{-1} &amp;= \mathbf{H}^{-1} =
\left( \mathbf{W}_N + \mathbf{C}_N^{-1} \right) ^{-1}
\end{aligned}
$$</p>
<p>となる。（ニュートン-ラフソン法の下で、$\mathbf{a}_N$は$\mathbf{a}^\star_N$で置き換えられた。）</p>
<p>従って、(2.115)式における平均値$\mathbf{A} \boldsymbol{\mu} + \mathbf{b}$は、</p>
<p>$$
\begin{aligned}
&amp;\mathbf{A} \boldsymbol{\mu} + \mathbf{b} \
=&amp;\ (\mathbf{k}^{\mathrm T} \mathbf{C}_N^{-1})
{\mathbf{C}_N (\mathbf{t}_N -\boldsymbol{\sigma}_N)}+ \mathbf{0} \
=&amp;\ \mathbf{k}^{\mathrm T} (\mathbf{t}_N -\boldsymbol{\sigma}_N)
\end{aligned}
$$</p>
<p>次に、(2.115)式における分散$\mathbf{L}^{-1}+\mathbf{A}\mathbf{\Lambda}^{-1}\mathbf{A}^{\mathrm{T}}$は、</p>
<p>$$
\begin{aligned}
&amp;\mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1}\mathbf{A}^{\mathrm{T}} \
=&amp;\ (c - \mathbf{k}^{\mathrm T} \mathbf{C}_N^{-1}\mathbf{k})
+
\left( \mathbf{k}^{\mathrm T} \mathbf{C}_N ^{-1}\right)
\left( \mathbf{W}_N + \mathbf{C}_N^{-1} \right)^{-1}
\left( \mathbf{k}^{\mathrm T} \mathbf{C}_N ^{-1}\right)^{\mathrm T} \
=&amp;\ (c - \mathbf{k}^{\mathrm T} \mathbf{C}_N^{-1}\mathbf{k})
+
\mathbf{k}^{\mathrm T} \mathbf{C}_N ^{-1}
\left( \mathbf{W}_N + \mathbf{C}_N^{-1} \right)^{-1}
\mathbf{C}_N^{-1}  \mathbf{k}
\end{aligned}
$$</p>
<p>となる。なお、転置の逆行列が逆行列の転置に一致すること、および$\mathbf{C}_N$が対称行列であることを用いた。ここで、</p>
<p>$$
\begin{aligned}
&amp;\mathbf{C}_N ^{-1}
\left( \mathbf{W}_N + \mathbf{C}_N^{-1} \right)^{-1} \
=&amp;\left[ ( \mathbf{W}_N + \mathbf{C}_N^{-1} ) \mathbf{C}_N \right]^{-1} \
=&amp;\left( \mathbf{W}_N \mathbf{C}_N + \mathbf{I} \right)^{-1}
\end{aligned}
$$</p>
<p>なので、分散は、</p>
<p>$$
\begin{aligned}
&amp; \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1}\mathbf{A}^{\mathrm{T}} \
=&amp;\ (c - \mathbf{k}^{\mathrm T} \mathbf{C}_N^{-1}\mathbf{k})
+
\mathbf{k}^{\mathrm T} \left( \mathbf{W}_N \mathbf{C}_N +\mathbf{I}\right)^{-1}
\mathbf{C}_N^{-1}  \mathbf{k}\
=&amp;\ c + \mathbf{k}^{\mathrm T} \left{
-\mathbf{I}+ \left( \mathbf{W}_N \mathbf{C}_N +\mathbf{I}\right)^{-1}
\right}
\mathbf{C}_N^{-1}  \mathbf{k}\
\end{aligned}
$$
となる。ここで、</p>
<p>$$
\begin{aligned}
&amp; \left{ -\mathbf{I}+\left( \mathbf{W}_N \mathbf{C}_N +\mathbf{I}\right)^{-1} \right} \mathbf{C}_N^{-1}\
=&amp; \left{\left[ -( \mathbf{W}_N \mathbf{C}_N +\mathbf{I}) +\mathbf{I}\right]
\left( \mathbf{W}_N \mathbf{C}_N +\mathbf{I}\right)^{-1} \right} \mathbf{C}_N^{-1}\
=&amp; \left{-\mathbf{W}_N \mathbf{C}_N
\left( \mathbf{W}_N \mathbf{C}_N +\mathbf{I}\right)^{-1} \right} \mathbf{C}_N^{-1}\
=&amp; -\mathbf{W}_N \mathbf{C}_N
\left{\left( \mathbf{W}_N \mathbf{C}_N +\mathbf{I}\right)^{-1}  \mathbf{C}_N^{-1}\right}\
=&amp; -\mathbf{W}_N \mathbf{C}_N
\left{ \mathbf{C}_N \left( \mathbf{W}_N \mathbf{C}_N +\mathbf{I}\right)\right} ^{-1}\
=&amp; - {(\mathbf{W}_N \mathbf{C}_N )^{-1}}^{-1}
\left{ \mathbf{C}_N \left( \mathbf{W}_N \mathbf{C}_N +\mathbf{I}\right)\right} ^{-1}\
=&amp; -\left{ \mathbf{C}_N \left( \mathbf{W}_N \mathbf{C}_N +\mathbf{I}\right) (\mathbf{W}_N \mathbf{C}_N )^{-1}
\right} ^{-1}\
=&amp; - \left{ \mathbf{C}_N (\mathbf{I}+  (\mathbf{W}_N \mathbf{C}_N )^{-1} )
\right} ^{-1}\
=&amp;- \left( \mathbf{C}_N + \mathbf{C}_N \mathbf{C}_N ^{-1} \mathbf{W}_N ^{-1}
\right) ^{-1}\
=&amp;- \left( \mathbf{C}_N + \mathbf{W}_N ^{-1}
\right) ^{-1}\
\end{aligned}
$$</p>
<p>なので、分散は、</p>
<p>$$
\begin{aligned}
&amp;\mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1}\mathbf{A}^{\mathrm{T}} \
=\ &amp;c - \mathbf{k}^{\mathrm T} \left( \mathbf{C}_N + \mathbf{W}_N ^{-1}
\right) ^{-1}
\mathbf{k}\
\end{aligned}
$$</p>
<h2 id="演習-627"><a class="header" href="#演習-627">演習 6.27</a></h2>
<div class="panel-primary">
<p>(難問)ガウス過程による分類モデルのラプラス近似による対数尤度関数
$$
\ln p\left(\mathbf{t}<em>{N} \mid \theta\right)=\Psi\left(\mathbf{a}</em>{N}^{\star}\right)-\frac{1}{2} \ln \left|\mathbf{W}<em>{N}+\mathbf{C}</em>{N}^{-1}\right|+\frac{N}{2} \ln (2 \pi) \tag{6.90}
$$
を導け．また
$$
\frac{\partial \ln p\left(\mathbf{t}<em>{N} \mid \theta\right)}{\partial \theta</em>{j}}= \frac{1}{2} \mathbf{a}<em>{N}^{\star \mathrm{T}} \mathbf{C}</em>{N}^{-1} \frac{\partial \mathbf{C}<em>{N}}{\partial \theta</em>{j}} \mathbf{C}<em>{N}^{-1} \mathbf{a}</em>{N}^{\star} -\frac{1}{2} \operatorname{Tr}\left[\left(\mathbf{I}+\mathbf{C}<em>{N} \mathbf{W}</em>{N}\right)^{-1} \mathbf{W}<em>{N} \frac{\partial \mathbf{C}</em>{N}}{\partial \theta_{j}}\right] \tag{6.91}
$$
と
$$
\begin{aligned}
-&amp;\frac{1}{2} \sum_{n=1}^{N} \frac{\partial \ln \left|\mathbf{W}<em>{N}+\mathbf{C}</em>{N}^{-1}\right|}{\partial a_{n}^{\star}} \frac{\partial a_{n}^{\star}}{\partial \theta_{j}} \
=-&amp;\frac{1}{2} \sum_{n=1}^{N}\left[\left(\mathbf{I}+\mathbf{C}<em>{N} \mathbf{W}</em>{N}\right)^{-1} \mathbf{C}<em>{N}\right]</em>{n n} \sigma_{n}^{\star}\left(1-\sigma_{n}^{\star}\right)\left(1-2 \sigma_{n}^{\star}\right) \frac{\partial a_{n}^{\star}}{\partial \theta_{j}}
\end{aligned} \tag{6.92}
$$
および
$$
\frac{\partial a_{n}^{\star}}{\partial \theta_{j}}=\left(\mathbf{I}+\mathbf{W}<em>{N} \mathbf{C}</em>{N}\right)^{-1} \frac{\partial \mathbf{C}<em>{N}}{\partial \theta</em>{j}}\left(\mathbf{t}<em>{N}-\boldsymbol{\sigma}</em>{N}\right) \tag{6.94}
$$
を対数尤度の勾配を用いて表せ．</p>
</div>
<p>(4.135)式
$$
Z=\int f(\mathbf{z}) \mathrm{d} \mathbf{z}\
=f(z_{0})\int exp(-\frac{1}{2} (\mathbf{z}- \mathbf{z_{0}}) A (\mathbf{z}- \mathbf{z_{0}})) \mathrm{d} \mathbf{z}\
=f(z_{0})\frac{(2\pi)^{\frac{M}{2}}}{|A|^\frac{1}{2}} \
Z=p(\mathbf{t_{N}}\mid\theta), f(\mathbf{z})=p(\mathbf{t_{N}}\mid\mathbf{a_{N}})p(\mathbf{a_{N}}\mid\theta), \mathbf{z}=\mathbf{a_{N}}と置くと、\
p(\mathbf{t_{N}}\mid\theta)=p(\mathbf{t_{N}}\mid\mathbf{a^<em>_{N}})p(\mathbf{a^</em><em>{N}}\mid\theta)\frac{(2\pi)^{\frac{N}{2}}}{|H|^\frac{1}{2}}\
=exp(\Psi\left(\mathbf{a}</em>{N}^{\star}\right))\frac{(2\pi)^{\frac{N}{2}}}{|H|^\frac{1}{2}}\
両辺の対数を取って、\
\ln p\left(\mathbf{t}<em>{N} \mid \theta\right)=\Psi\left(\mathbf{a}</em>{N}^{\star}\right)-\frac{1}{2} \ln \left|\mathbf{W}<em>{N}+\mathbf{C}</em>{N}^{-1}\right|+\frac{N}{2} \ln (2 \pi)
$$</p>
<p>※</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="prml第7章演習問題解答"><a class="header" href="#prml第7章演習問題解答">PRML第7章演習問題解答</a></h1>
<head>
<style>
  div.panel-primary {
	border: 1px solid #000;
    margin: 10px 5px;
    padding: 16px 10px 0px;
  }
</style>
</head>
<h2 id="演習-71"><a class="header" href="#演習-71">演習 7.1</a></h2>
<div class="panel-primary">
<p>今，入力ベクトルのデータ集合${\mathbf{x}_n}$とそれに対応する目標値$t_n \in {-1, 1}$が与えられ，かつ，それぞれのクラス分布をカーネル関数$k(\mathbf{x}, \mathbf{x}^{\prime})$を用いてParzen推定法(2.5.1節参照)でモデル化したとするそれぞれのクラスの事前確率が等しいとしたとき，誤分類率が最も小さくなる分類規則を求めよ．またカーネルが$k(\mathbf{x}, \mathbf{x}^{\prime}) = \mathbf{x}^{\mathrm T}\mathbf{x}^{\prime}$どという形で表される場合，分類規則は単に重心との距離が近い方のクラスを新しい入力ベクトルに割り当てる， という形になることを示せ．最後にカーネルが$k(\mathbf{x}, \mathbf{x}^{\prime}) = \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\boldsymbol{\phi}(\mathbf{x}^{\prime})$という形の場合は，分類規則は特徴空間$\boldsymbol{\phi}(\mathbf{x})$において最も重心が近いクラスを割り当てることに等しいことを示せ．</p>
</div>
<p>式 (2.249) に従い、$p(\mathbf{x}|t)$を
\begin{align}
p(\mathbf{x}|t) \propto
\begin{cases}
\frac{1}{N_{+1}} \sum_{t = +1} k(\mathbf{x}, \mathbf{x}<em>n) \ \ t = +1\
\frac{1}{N</em>{-1}} \sum_{t = -1} k(\mathbf{x}, \mathbf{x}<em>n) \ \ t = -1
\end{cases}
\end{align}
と書ける。各クラスの事前確率が等しいと仮定するので、事後確率$p(t|\mathbf{x})$は
\begin{align}
p(t|\mathbf{x}) \propto
\begin{cases}
\frac{1}{N</em>{+1}} \sum_{t = +1} k(\mathbf{x}, \mathbf{x}<em>n) \ \ t = +1 \
\frac{1}{N</em>{-1}} \sum_{t = -1} k(\mathbf{x}, \mathbf{x}<em>n) \ \ t = -1
\end{cases}
\end{align}
となる。新しい$\mathbf{x}^{\star}$を分類するには、$p(t|\mathbf{x}^{\star})$を最大化する$t^{\star}$を探せばいいので、
\begin{align}
t^{\star} =
\begin{cases}
+1\ \ \mathrm{if}\ \ \frac{1}{N</em>{+1}} \sum_{t = +1} k(\mathbf{x}^{\star}, \mathbf{x}<em>n) \geq \frac{1}{N</em>{-1}} \sum_{t = -1} k(\mathbf{x}^{\star}, \mathbf{x}<em>n) \
-1\ \ \mathrm{if}\ \ \frac{1}{N</em>{+1}} \sum_{t = +1} k(\mathbf{x}^{\star}, \mathbf{x}<em>n) \leq \frac{1}{N</em>{-1}} \sum_{t = -1} k(\mathbf{x}^{\star}, \mathbf{x}<em>n)
\end{cases}
\end{align}
ここで$k(\mathbf{x}, \mathbf{x}') = \mathbf{x}^{T}\mathbf{x}'$のとき、
\begin{align}
\frac{1}{N</em>{+1}} \sum_{t = +1} k(\mathbf{x}, \mathbf{x}<em>n) &amp; = \frac{1}{N</em>{+1}} \sum_{t = +1} \mathbf{x}^{T}\mathbf{x}<em>n \
&amp;= \frac{1}{N</em>{+1}} \sum_{i = 1}^{N_{+1}} x_1 (x_{n1} + x_{n2} + \cdots + x_{nd}) + \cdots + x_d (x_{n1} + x_{n2} + \cdots + x_{nd}) \
&amp;= x_1 (\bar{x}<em>{+1,1} + \bar{x}</em>{+1,2} + \cdots + \bar{x}<em>{+1,d}) + \cdots + x_d (\bar{x}</em>{+1,1} + \bar{x}<em>{+1,2} + \cdots + \bar{x}</em>{+1,d}) \
&amp;= \mathbf{x}^{T}\mathbf{\bar{x}}<em>{+1}
\end{align}
同様に、
\begin{align}
\frac{1}{N</em>{+1}} \sum_{t = -1} k(\mathbf{x}, \mathbf{x}<em>n) = \mathbf{x}^{T}\mathbf{\bar{x}}</em>{-1}
\end{align}
よって、上記の分類規則は
\begin{align}
t^{\star} =
\begin{cases}
+1\ \ \mathrm{if}\ \ \mathbf{x}^{T}\mathbf{\bar{x}}<em>{+1} \geq \mathbf{x}^{T}\mathbf{\bar{x}}</em>{-1} \
-1\ \ \mathrm{if}\ \ \mathbf{x}^{T}\mathbf{\bar{x}}<em>{+1} \leq \mathbf{x}^{T}\mathbf{\bar{x}}</em>{-1}
\end{cases}
\end{align}
となる。
$k(\mathbf{x}, \mathbf{x}') = \phi(\mathbf{x})^{T}\phi(\mathbf{x}')$としたときも、同様の計算により、
\begin{align}
t^{\star} =
\begin{cases}
+1\ \ \mathrm{if}\ \ \phi(\mathbf{x})^{T}\bar{\phi}(\mathbf{x})<em>{+1} \geq \phi(\mathbf{x})^{T}\bar{\phi}(\mathbf{x})</em>{-1} \
-1\ \ \mathrm{if}\ \ \phi(\mathbf{x})^{T}\bar{\phi}(\mathbf{x})<em>{+1} \leq \phi(\mathbf{x})^{T}\bar{\phi}(\mathbf{x})</em>{-1}
\end{cases}
\end{align}
ここで$\bar{\phi}(\mathbf{x})<em>{+1} = \frac{1}{N</em>{+1}} \sum_{n = 1}^{N_{+1}} \phi(\mathbf{x}<em>n)$、$\bar{\phi}(\mathbf{x})</em>{-1} = \frac{1}{N_{+1}} \sum_{n = 1}^{N_{-1}} \phi(\mathbf{x}_n)$</p>
<h2 id="演習-72"><a class="header" href="#演習-72">演習 7.2</a></h2>
<div class="panel-primary">
<p>制約式
$$
t_{n}\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)+b\right) \geqslant 1, \quad n=1, \ldots, N \tag{7.5}
$$
において，右辺の$1$を任意の正数$\gamma$で置き換えても，マージン最大の超平面は変化しないことを示せ．</p>
</div>
<p>$$
t_{n}\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}<em>{n}\right)+b\right) \geqslant \gamma, \quad n=1, \cdots, N \tag{7.5.a}
$$
と置き換えると、
$\mathbf{w}^{\prime}=\frac{\mathbf{w}}{\gamma}, \quad b^{\prime}=\frac{b}{\gamma} .$として
$$
t</em>{n}\left(\mathbf{w}^{\prime\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}<em>{n}\right)+b^{\prime}\right) \geqslant 1, \quad n=1, \ldots, N \tag{7.5.b}
$$
と書け、マージンは
$$
\min <em>{n} \frac{\left[t</em>{n}\left(\mathbf{w}^{\prime\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}</em>{n}\right)+b^{\prime}\right)\right]}{|\mathbf{w}^{\prime}|}
=\min <em>{n} \frac{\left[t</em>{n}\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)+b\right)\right]}{|\mathbf{w}|}
$$
と変化しない。</p>
<h2 id="演習-73"><a class="header" href="#演習-73">演習 7.3</a></h2>
<div class="panel-primary">
<p>データ空間の次元数によらず各クラスに一つずつデータが存在すれば， 2つのデータ点だけから成るデータ集合でマージン最大の超平面を決定できることを示せ．</p>
</div>
<p>※</p>
<p>各クラスに一つずつデータ点が与えられたとき，その2点を $\mathbf{x}<em>1\in\mathit{C}</em>+ (t_1 = +1)$， $\mathbf{x}<em>2\in\mathit{C}</em>- (t_2 = -1)$ とすると以下の制約式のもとで式(7.6)を解くことでマージンを最大化する超平面が得られる</p>
<p>$$
arg,min_{\mathbf{w}, b},\frac{1}{2}||\mathbf{w}||^2\tag{7.6}
$$</p>
<p>$$
\mathbf{w}^T\mathbf{x}_1+b= +1\tag{1}
$$</p>
<p>$$
\mathbf{w}^T\mathbf{x}_2+b= -1\tag{2}
$$</p>
<p>(7.6)式をラグランジュ乗数$\lambda$と$\eta$を用いて解くと</p>
<p>$$
arg,min_{\mathbf{w}, b},\left{\frac{1}{2}||\mathbf{w}||^2+\lambda(\mathbf{w}^T\mathbf{x}_1+b-1)+\eta(\mathbf{w}^T\mathbf{x}_2+b+1)\right}\tag{3}
$$</p>
<p>(3)式の$\mathbf{w}$とbについて微分した式を0とおくと</p>
<p>$$
0=\mathbf{w}+\lambda\mathbf{x}_1+\eta\mathbf{x}_2\tag{4}
$$
$$
0=\lambda+\eta\tag{5}
$$</p>
<p>が得られ，(4)，(5)式から</p>
<p>$$
\mathbf{w}=\lambda(\mathbf{x}_2-\mathbf{x}_1)\tag{6}
$$</p>
<p>また(1)，(2)式からbは</p>
<p>$$
2b=-\mathbf{w}^T(\mathbf{x}_1+\mathbf{x}_2)
$$</p>
<p>であり，これと(6)式と合わせて</p>
<p>$$
\begin{aligned}
b=&amp;-\frac{\lambda}{2}(\mathbf{x}_1-\mathbf{x}_2)^T(\mathbf{x}_1+\mathbf{x}_2)\=&amp;-\frac{\lambda}{2}(\mathbf{x}_1^T\mathbf{x}_1-\mathbf{x}_2^T\mathbf{x}_2)
\end{aligned}
$$</p>
<p>のように求まり，マージンを最大化する超平面が定まる．</p>
<h2 id="演習-74"><a class="header" href="#演習-74">演習 7.4</a></h2>
<div class="panel-primary">
<p>マージン最大の超平面のマージン$\rho$は，以下の式を満たすことを示せ．</p>
<p>$$
\frac{1}{\rho^2}= \sum_{n=1}^N a_n \tag{7.123}
$$</p>
<p>ただし${a_n}$は</p>
<p>$$
\widetilde{L}(\mathbf{a})=\sum_{n=1}^{N} a_{n}-\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} a_{n} a_{m} t_{n} t_{m} k\left(\mathbf{x}<em>{n}, \mathbf{x}</em>{m}\right) \tag{7.10}
$$</p>
<p>を制約条件</p>
<p>$$
a_{n} \geqslant 0, \hspace{2em} n=1, \ldots, N \tag{7.11}
$$</p>
<p>$$
\sum_{n=1}^{N} a_{n} t_{n}=0 \tag{7.12}
$$</p>
<p>の下で解いて得られる解とする．</p>
</div>
<p>今、定義と、(7, 2)より、
\begin{align}
\rho = \frac{t_n y(x_n)}{||\mathbf{w}||} = \frac{t_n (\mathbf{w}^T \phi(\mathbf{x}<em>n)+b)}{||\mathbf{w}||}
\end{align}
である。今、分子と分母を定数倍すると、ある$\mathbf{w}^{\star}$において、
\begin{align}
\rho = \frac{1}{||\mathbf{w}^{\star}||}
\end{align}
が成り立つ。よって、$\frac{1}{\rho^2} = ||\mathbf{w}^{\star}||^2$であり、$||\mathbf{w}^{\star}||^2 = \sum</em>{n=1}^N a_n$を証明すれば題意は満たされる。今、(7, 10)より
\begin{align}
\widetilde{L}(\mathbf{a}) &amp;= \sum_n a_n - \frac{1}{2}\sum_n \sum_m a_n a_m t_n t_m k(\mathbf{x}_n, \mathbf{x}_m)　\
&amp;= \sum_n a_n - \frac{1}{2}\sum_n a_n t_n \phi(\mathbf{x}_n) \sum_m  a_m  t_m  \phi(\mathbf{x}_m)　\
&amp;= \sum_n a_n - \frac{1}{2}||\mathbf{w^{\star}}||^2 &amp;(\because　(7, 8))
\end{align}
また、ラグランジュ乗数法の定義より。
\begin{align}
\widetilde{L}(a) = L(\mathbf{w^{\star}}, b, \mathbf{a} ) = \frac{1}{2}||\mathbf{w}^{\star}||^2 &amp;(\because (7.7))
\end{align}
よって、$\sum_n a_n - \frac{1}{2}||\mathbf{w^{\star}}||^2 = \frac{1}{2}||\mathbf{w}^{\star}||^2$であり、整理すると、題意が導かれる。</p>
<h2 id="演習-75"><a class="header" href="#演習-75">演習 7.5</a></h2>
<div class="panel-primary">
<p>前問における$\rho$および${a_n}$は，次の式を満たすことを示せ．</p>
<p>$$
\frac{1}{\rho^{2}}=2 \widetilde{L}(\mathbf{a}) \tag{7.124}
$$</p>
<p>ここで，$\widetilde{L}(\mathbf{a})$は</p>
<p>$$
\widetilde{L}(\mathbf{a})=\sum_{n=1}^{N} a_{n}-\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} a_{n} a_{m} t_{n} t_{m} k\left(\mathbf{x}<em>{n}, \mathbf{x}</em>{m}\right) \tag{7.10}
$$</p>
<p>で定義される関数である同様に以下の関係が成り立つことを示せ．</p>
<p>$$
\frac{1}{\rho^{2}}= |\mathbf{w}|^2 \tag{7.125}
$$</p>
</div>
<p>本問については、7.4ですでに示されている.</p>
<p>\begin{align}
(
\because
\widetilde{L}(a) = \frac{1}{2}||\mathbf{w}'||^2 ,
\rho = \frac{1}{||\mathbf{w}'||}
)
\end{align}</p>
<p>※</p>
<h2 id="演習-76"><a class="header" href="#演習-76">演習 7.6</a></h2>
<div class="panel-primary">
<p>出力値が$t\in{-1, 1}$であるロジスティック回帰モデルについて考える．</p>
<p>$$
y(\mathbf{x})=\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x})+b \tag{7.1}
$$</p>
<p>という形の$y(\mathbf{x})$を用いて，$p(t=1|y) = \sigma(y)$とすると，対数尤度(の符号を反転したもの)に2乗ノルムの正則化項を加えたものは</p>
<p>$$
\sum_{n=1}^{N} E_{\mathrm{LR}}\left(y_{n} t_{n}\right)+\lambda|\mathbf{w}|^{2} \tag{7.47}
$$</p>
<p>という形を取ることを示せ．ただし</p>
<p>$$
E_{\mathrm{LR}}(y t)=\ln (1+\exp (-y t)) \tag{7.48}
$$</p>
<p>である．</p>
</div>
<p>ロジスティック回帰モデルはinputデータに対して各クラスの事後確率を求め
最も高い確率のクラスに分類する手法。</p>
<p>各クラスの事後確率はロジスティックシグモイド関数として以下のように書ける。</p>
<p>入力データがクラス1である確率：$p(t=1 \mid y)=\sigma(y)$
入力データがクラス2である確率：$p(t=-1 \mid y)=1-\sigma(y)=\sigma(-y)$
$※\sigma(y)=\frac{1}{1+e^{-y}}　y(\mathbf{x})=\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x})+b$</p>
<p>学習データとして、各学習データがi.i.dの$\mathcal{D}=\left{\left(t_{1}, \mathbf{x}<em>{n}\right), \ldots,\left(t</em>{N}, \mathbf{x}_{N}\right)\right}$が与えられると
最適パラメータ($\mathbf{w},b$)は以下の尤度を最大化することで得られる。</p>
<p>$$p(\mathcal{D})=\prod_{t_{n}=1} \sigma\left(y_{n}\right) \prod_{t_{n^{\prime}}=-1} \sigma\left(-y_{n^{\prime}}\right)=\prod_{n=1}^{N} \sigma\left(t_{n} y_{n}\right)　※y_{n}=y\left(\mathbf{x}<em>{n}\right) , t</em>{n} \in{-1,1}$$</p>
<p>これは、各学習データが正分類される確率を全データに対して掛け合わせたものを表しており
正しく正分類されているほど、尤度は大きくなる。負の対数尤度を取ると以下となる。</p>
<p>$$\begin{aligned}-\ln p(\mathcal{D}) &amp;=-\ln \prod_{n=1}^{N} \sigma\left(t_{n} y_{n}\right) \ &amp;=\sum_{n=1}^{N} \ln \sigma\left(t_{n} y_{n}\right)^{\mathrm{-1}}  \ &amp;=\sum_{n=1}^{N} \ln \left(1+\exp \left(-t_{n} y_{n}\right)\right) \end{aligned}$$</p>
<p>これに、$\lambda|\mathbf{w}|^{2}$を加えると(7.47)という形を取る。</p>
<h2 id="演習-77"><a class="header" href="#演習-77">演習 7.7</a></h2>
<div class="panel-primary">
<p>SVM回帰モデルのラグランジュ関数</p>
<p>$$
\begin{aligned}
L=&amp;\ C \sum_{n=1}^{N}\left(\xi_{n}+\widehat{\xi}<em>{n}\right)+\frac{1}{2}|\mathbf{w}|^{2}-\sum</em>{n=1}^{N}\left(\mu_{n} \xi_{n}+\widehat{\mu}<em>{n} \widehat{\xi}</em>{n}\right) \
&amp;-\sum_{n=1}^{N} a_{n}\left(\epsilon+\xi_{n}+y_{n}-t_{n}\right)-\sum_{n=1}^{N} \widehat{a}<em>{n}\left(\epsilon+\widehat{\xi}</em>{n}-y_{n}+t_{n}\right) .
\end{aligned} \tag{7.56}
$$</p>
<p>について考える．$(7.56)$の $\mathbf{w}, b, \xi_{n}, \widehat{\xi}_{n}$に対する偏微分をそれぞれ零とおき，その結果を代入することで双対ラグランジュ関数</p>
<p>$$
\begin{aligned}
\widetilde{L}(\mathbf{a}, \widehat{\mathbf{a}})=&amp;-\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N}\left(a_{n}-\widehat{a}<em>{n}\right)\left(a</em>{m}-\widehat{a}<em>{m}\right) k\left(\mathbf{x}</em>{n}, \mathbf{x}<em>{m}\right) \ &amp;-\epsilon \sum</em>{n=1}^{N}\left(a_{n}+\widehat{a}<em>{n}\right)+\sum</em>{n=1}^{N}\left(a_{n}-\widehat{a}<em>{n}\right) t</em>{n}
\end{aligned} \tag{7.61}
$$</p>
<p>が得られることを示せ．</p>
</div>
<p>(7.56)に$y(\mathbf{x})=\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x})+b$を代入して、スラック変数を分解すると以下の式が得られる。</p>
<p>$$\begin{aligned} L=&amp; \sum_{n=1}^{N} C \xi_{n}+\sum_{n=1}^{N} C\widehat{\xi}<em>{n}+\frac{1}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w}-\sum</em>{n=1}^{N}\left(\mu_{n} \xi_{n}+\widehat{\mu}<em>{n} \widehat{\xi}</em>{n}\right) \ &amp;-\sum_{n=1}^{N} a_{n}\left(\epsilon+\xi_{n}+\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}<em>{n}\right)+b-t</em>{n}\right) \ &amp;-\sum_{n=1}^{N} \widehat{a}<em>{n}\left(\epsilon+\widehat{\xi}</em>{n}-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}<em>{n}\right)-b+t</em>{n}\right) \qquad (*)\end{aligned}$$</p>
<p>ラグランジュ関数(7.56)の$\mathbf{w}, b, \xi_{n}, \widehat{\xi}_{n}$に対する偏微分をそれぞれ零とおくことで
以下の式が得られる。</p>
<p>$$
\begin{aligned}
&amp;\frac{\partial L}{\partial \mathrm{w}}=0 \Rightarrow \mathrm{w}=\sum_{n=1}^{N}\left(a_{n}-\widehat{a}<em>{n}\right) \phi\left(\mathrm{x}</em>{n}\right)\qquad (7.57)\
&amp;\frac{\partial L}{\partial b}=0 \Rightarrow \sum_{n=1}^{N}\left(a_{n}-\widehat{a}<em>{n}\right)=0\qquad (7.58)\
&amp;\frac{\partial L}{\partial \xi</em>{n}}=0 \Rightarrow a_{n}+\mu_{n}=C\qquad (7.59)\
&amp;\frac{\partial L}{\partial \widehat{\xi}<em>{n}}=0 \Rightarrow \widehat{a}</em>{n}+\widehat{\mu}_{n}=C\qquad (7.60)<br />
\end{aligned}
$$</p>
<p>(*)式に、(7.57) , (7.59) , (7.60)を代入すると以下の式となる。</p>
<p>$$
\begin{aligned}
L=&amp; \sum_{n=1}^{N}\left(a_{n}+\mu_{n}\right) \xi_{n}+\sum_{n=1}^{N}\left(\widehat{a}<em>{n}+\widehat{\mu}</em>{n}\right) \widehat{\xi}<em>{n} \
&amp;+\frac{1}{2} \sum</em>{n=1}^{N} \sum_{m=1}^{N}\left(a_{n}-\widehat{a}<em>{n}\right)\left(a</em>{m}-\widehat{a}<em>{m}\right) \phi\left(\mathbf{x}</em>{n}\right)^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}<em>{m}\right)-\sum</em>{n=1}^{N}\left(\mu_{n} \xi_{n}+\widehat{\mu}<em>{n} \widehat{\xi}</em>{n}\right) \
&amp;-\sum_{n=1}^{N}\left(a_{n} \xi_{n}+\widehat{a}<em>{n} \widehat{\xi}</em>{n}\right)-\epsilon \sum_{n=1}^{N}\left(a_{n}+\widehat{a}<em>{n}\right)+\sum</em>{n=1}^{N}\left(a_{n}-\widehat{a}<em>{n}\right) t</em>{n} \
&amp;-\sum_{n=1}^{N} \sum_{m=1}^{N}\left(a_{n}-\widehat{a}<em>{n}\right)\left(a</em>{m}-\widehat{a}<em>{m}\right) \phi\left(\mathbf{x}</em>{n}\right)^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}<em>{m}\right)-b \sum</em>{n=1}^{N}\left(a_{n}-\widehat{a}_{n}\right) .
\end{aligned}
$$</p>
<p>この式の第1,2項は、第4,5項と丁度打ち消しあう。また、式(7.58)により最後の項は0になるので
まとめると(7.61)が得られる。</p>
<h2 id="演習-78"><a class="header" href="#演習-78">演習 7.8</a></h2>
<div class="panel-primary">
<p>7.1.4節で議論した SVM回帰モデルについて，$\xi_{n} \gt 0$が成り立つ訓練データ点については$a_n = C$，同様に$\widehat{\xi}<em>{n} \gt 0$が成り立つ訓練データ点については$\widehat{a}</em>{n} = C$が成立することを示せ．</p>
</div>
<p>※(7.67),(7.68)から明らか。</p>
<h2 id="演習-79"><a class="header" href="#演習-79">演習 7.9</a></h2>
<div class="panel-primary">
<p>RVM回帰モデルについて，重みに対する事後確率分布の平均および共分散が</p>
<p>\begin{align}
\mathbf{m}&amp;=\beta \mathbf{\Sigma} \Phi^{\mathrm{T}} \mathbf{t} \tag{7.82} \
\mathbf{\Sigma}&amp;=\left(\mathbf{A}+\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \tag{7.83}
\end{align}</p>
<p>で与えられることを示せ．</p>
</div>
<p>(7.79)式の直後の段落に記載された仮定（各$w_i$の事前確率分布の平均が0、分散が$\alpha_i$）により、(3.49)〜(3.51)式と(7.81)〜(7.83)式との対応関係は、
\begin{eqnarray}
\mathbf{m}_0 &amp;=&amp; \mathbf{0} \
\mathbf{S}_0 &amp;=&amp; \rm{diag}(\alpha_i^{-1}) \
\mathbf{m}_N &amp;=&amp; \mathbf{m} \rm{\ \ :definition} \
\mathbf{S}_N &amp;=&amp; \mathbf{\Sigma} \rm{\ \ :definition}
\end{eqnarray}</p>
<p>である。従って、
\begin{eqnarray}
\mathbf{m}
&amp;=&amp; S_N \left( S_0^{-1}\mathbf{m}_0 + \beta \mathbf{\Phi}^\mathrm{T} \mathbf{t} \right) \
&amp;=&amp; \beta \Sigma \Phi ^\mathrm{T} \mathbf{t}
\end{eqnarray}</p>
<p>\begin{eqnarray}
\Sigma ^{-1} &amp;=&amp; \left( \rm{diag}(\alpha_i^{-1} )\right) ^{-1} + \beta \Phi^\mathrm{T} \Phi \
&amp;=&amp; \rm{diag}(\alpha_i) + \beta \Phi^\mathrm{T} \Phi \
\Sigma &amp;=&amp; \left( A + \beta \Phi^\mathrm{T} \Phi \right) ^{-1}
\end{eqnarray}
となる。ただし、$A=$diag$(\alpha _i)$と定義した。</p>
<h2 id="演習-710"><a class="header" href="#演習-710">演習 7.10</a></h2>
<div class="panel-primary">
<p>RVM回帰モデルについて周辺化尤度関数の式</p>
<p>$$
\begin{aligned} \ln p(\mathbf{t} \mid \mathbf{X}, \boldsymbol{\alpha}, \beta) &amp;=\ln \mathcal{N}(\mathbf{t} \mid \mathbf{0}, \mathbf{C}) \ &amp;=-\frac{1}{2}\left{N \ln (2 \pi)+\ln |\mathbf{C}|+\mathbf{t}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t}\right} \end{aligned} \tag{7.85}
$$</p>
<p>を，</p>
<p>$$
p(\boldsymbol{t} \mid \mathbf{X}, \boldsymbol{\alpha}, \beta)=\int p(\mathbf{t} \mid \mathbf{X}, \mathbf{w}, \beta) p(\mathbf{w} \mid \boldsymbol{\alpha}) \mathrm{d} \mathbf{w} \tag{7.84}
$$</p>
<p>の$\mathbf{w}$に対する積分を実行することで導け．(指数に現れる2次式を平方完成するとよい．)</p>
</div>
<p>$$
\begin{aligned}
p(\mathbf{t} \mid \mathbf{X}, \boldsymbol{\alpha}, \beta) &amp;=\int p(\mathbf{t} \mid \mathbf{X}, \mathbf{\mathbf{w}}, \beta) p(\mathbf{w} \mid \alpha) d \mathbf{w} \
&amp;=\int \prod_{n=1}^{N} \mathcal{N}\left(t_{n} \mid \mathbf{w}^{\mathrm T} \boldsymbol{\phi}(\mathbf{x}), \beta^{-1}\right) \prod_{i=1}^{M} \mathcal{N}\left(w_{i} \mid 0, \alpha_{i}^{-1}\right) d \mathbf{w} \
&amp;=\int\left(\frac{\beta}{2 \pi}\right)^{\frac{N}{2}} \prod_{n=1}^{N} \exp \left{-\frac{\left(t_{n}-\mathbf{w}^{\mathrm T} \boldsymbol{\phi}(\mathbf{x})\right)^{2}}{2 \beta^{-1}}\right}\left(\frac{1}{2 \pi}\right)^{\frac{M}{2}} \prod_{i=1}^{M} \alpha_{i}^{\frac{1}{2}} \exp \left{-\frac{w_{i}^{2}}{2 \alpha_{i}^{-1}}\right} d \mathbf{w} \
&amp;=\left(\frac{\beta}{2 \pi}\right)^{\frac{N}{2}}\left(\frac{1}{2 \pi}\right)^{\frac{M}{2}} \prod_{i=1}^{M} \alpha_{i}^{\frac{1}{2}} \int \exp \left{-\frac{\beta}{2}|\mathbf{t}-\mathbf{\Phi} \mathbf{w}|^{2}-\frac{1}{2} \mathbf{w}^{\mathrm T} \mathbf{A} \mathbf{w}\right} d \mathbf{w}
\end{aligned}
$$</p>
<p>ここで$\mathbf{A} = \operatorname{diag}(\alpha_i)$である。指数部分を整理すると</p>
<p>$$
\begin{aligned}
-\frac{\beta}{2}|\mathbf{t}-\mathbf{\Phi} \mathbf{w}|^{2}-\frac{1}{2} \mathbf{w}^{\mathrm T} \mathbf{A} \mathbf{w} &amp;=-\frac{1}{2}\left{\beta\left(\mathbf{t}^{\mathrm T} \mathbf{t}-2 \mathbf{t}^{\mathrm T} \mathbf{\Phi} \mathbf{w}+\mathbf{w}^{\mathrm T} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi} \mathbf{w}\right)+\mathbf{w}^{\mathrm T} \mathbf{A} \mathbf{w}\right} \
&amp;=-\frac{1}{2}\left{\mathbf{w}^{\mathrm T}\left(\mathbf{A}+\beta \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right) \mathbf{w}-2 \beta \mathbf{t}^{\mathrm T} \mathbf{\Phi} \mathbf{w}+\beta \mathbf{t}^{\mathrm T} \mathbf{t}\right} \
&amp;=-\frac{1}{2}\left{(\mathbf{w}-\mathbf{m})^{\mathrm T} \mathbf{\Sigma}^{-1}(\mathbf{w}-\mathbf{m})+\beta \mathbf{t}^{\mathrm T} \mathbf{t}-\mathbf{m}^{\mathrm T} \mathbf{\Sigma}^{-1} \mathbf{m}\right}
\end{aligned}
$$</p>
<p>ここで$(3.49)$の平方完成にならって$\mathbf{\Sigma} = \left(\mathbf{A}+\beta \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right)^{-1}$, $\mathbf{m} = \beta \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T}\mathbf{t}$とした。これより$\mathbf{w}$についての積分が行えるので</p>
<p>$$
\begin{aligned}
p(\mathbf{t} \mid \mathbf{X}, \boldsymbol{\alpha}, \beta) &amp;=\left(\frac{\beta}{2 \pi}\right)^{\frac{N}{2}}\left(\frac{1}{2 \pi}\right)^{\frac{M}{2}} \prod_{i=1}^{M} \alpha_{i}^{\frac{1}{2}} \int \exp \left{-\frac{1}{2}\left{(\mathbf{w}-\mathbf{m})^{\mathrm T} \mathbf{\Sigma}^{-1}(\mathbf{w}-\mathbf{m})+\beta \mathbf{t}^{\mathrm T} \mathbf{t}-\mathbf{m}^{\mathrm T} \mathbf{\Sigma}^{-1} \mathbf{m}\right}\right} d \mathbf{w} \
&amp;=\left(\frac{\beta}{2 \pi}\right)^{\frac{N}{2}}\left(\frac{1}{2 \pi}\right)^{\frac{M}{2}} \prod_{i=1}^{M} \alpha_{i}^{\frac{1}{2}}\left(2\pi\right)^{\frac{M}{2}}|\mathbf{\Sigma}|^{\frac{1}{2}} \exp \left{-\frac{1}{2}\left(\beta \mathbf{t}^{\mathrm T} \mathbf{t}-\mathbf{m}^{\mathrm T} \mathbf{\Sigma}^{-1} \mathbf{m}\right)\right}
\end{aligned}
$$</p>
<p>となる。そしてさらに$\mathbf{t}$について再度指数部分を整理すると</p>
<p>$$
\begin{aligned}
-\frac{1}{2}\left(\beta \mathbf{t}^{\mathrm{T}} \mathbf{t}-\mathbf{m}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \mathbf{m}\right) &amp;= - \frac{1}{2}\left(\beta \mathbf{t}^{\mathrm{T}} \mathbf{t}-\beta \mathbf{t}^{\mathrm{T}} \mathbf{\Phi} \mathbf{\Sigma} \mathbf{\Sigma}^{-1} \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm{T}} \mathbf{t} \beta\right) \
&amp;= -\frac{1}{2} \mathbf{t}^{\mathrm{T}}\left(\beta \mathbf{I}-\beta \mathbf{\Phi} \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm{T}} \beta\right) \mathbf{t} \
&amp;= -\frac{1}{2} \mathbf{t}^{\mathrm{T}}\left(\beta \mathbf{I}-\beta \mathbf{\Phi}\left(\mathbf{A}+\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm{T}} \beta\right) \mathbf{t} \
&amp;= -\frac{1}{2}  \mathbf{t}^{\mathrm T}\left(\left(\beta^{-1} \mathbf{I}\right)^{-1}-\left(\beta^{-1} \mathbf{I}\right)^{-1} \mathbf{\Phi}\left(\mathbf{A}+\mathbf{\Phi}^{\mathrm T}\left(\beta^{-1} \mathbf{I}\right)^{-1} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm T}\left(\beta^{-1} \mathbf{I}\right)^{-1}\right) \mathbf{t} \
&amp;= -\frac{1}{2} \mathbf{t}^{\mathrm{T}}\left(\beta^{-1} \mathbf{I}+\mathbf{\Phi} \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}}\right)^{-1} \mathbf{t} \hspace{1em} (\because \textrm{Woodburyの公式}, \textrm{(C.7)})\
&amp;= -\frac{1}{2} \mathbf{t}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t}
\end{aligned}
$$</p>
<p>となる。ただし、最後で$\mathbf{C} = \beta^{-1} \mathbf{I}+\mathbf{\Phi} \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}}$とした。以上から対数を取ることで</p>
<p>$$
\begin{aligned}
\ln p(\mathbf{t} \mid \mathbf{X}, \boldsymbol{\alpha}, \beta) &amp;= \frac{N}{2}(\ln\beta -\ln (2\pi)) + \frac{1}{2}\ln |\mathbf{\Sigma}| + \frac{1}{2}\sum_{i=1}^{M}\ln \alpha_i -\frac{1}{2} \mathbf{t}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t} \
&amp;= -\frac{N}{2}\ln(2\pi) + \frac{N}{2}\ln \beta +\frac{1}{2}\ln\left| \mathbf{\Sigma} \right| + \frac{1}{2}\sum_{i=1}^{M}\ln \alpha_i -\frac{1}{2} \mathbf{t}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t} \
&amp;= -\frac{N}{2}\ln(2\pi) - \frac{1}{2}\ln |\mathbf{C}| -\frac{1}{2} \mathbf{t}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t}
\end{aligned}
$$</p>
<p>となり、展開することで$(7.85)$式を得られる。</p>
<p>※ 最後の式変形部分について、$\displaystyle \frac{N}{2}\ln \beta + \frac{1}{2}\ln |\mathbf{\Sigma}| + \frac{1}{2}\sum_{i=1}^{M}\ln \alpha_i = -\frac{1}{2}\ln \left|\beta^{-1} \mathbf{I}+\mathbf{\Phi} \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}}\right|$を示す。</p>
<p>これは$\displaystyle \ln \left(\beta^{N} \cdot |\mathbf{\Sigma}| \cdot \prod_{i=1}^{M} \alpha_{i} \right) = \ln \left|\beta^{-1} \mathbf{I}+\mathbf{\Phi} \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}}\right|^{-1}$を示せれば良い。</p>
<p>$$
\begin{aligned}
\ln \left(\beta^{N} \cdot |\mathbf{\Sigma}| \cdot \prod_{i=1}^{M} \alpha_{i}\right) &amp;=\ln (|\beta \mathbf{I}| |\mathbf{A}| |\mathbf{\Sigma}|) \quad (\because |\beta\mathbf{I}| = \beta^N, |\mathbf{A}||\mathbf{B}| = |\mathbf{B}||\mathbf{A}|)\
&amp;=\ln \left(\left|(\beta \mathbf{I})^{-1}\right|^{-1}\left|\mathbf{A}^{-1}\right|^{-1}\left|\mathbf{A}+\mathbf{\Phi}^{\mathrm{T}}(\beta \mathbf{I}) \mathbf{\Phi}\right|^{-1}\right) \quad (\because(\mathrm{C}. 3)) \
&amp;=\ln \left|(\beta \mathbf{I})^{-1} \mathbf{A}^{-1}\left(\mathbf{A}+\mathbf{\Phi}^{\mathrm{T}}(\beta \mathbf{I}) \mathbf{\Phi}\right)\right|^{-1} \quad (\because(\mathrm{C}. 12)) \
&amp;=\ln\left|(\beta \mathbf{I})^{-1}\left(\mathbf{I}+\mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}}(\beta \mathbf{I}) \mathbf{\Phi}\right)\right|^{-1} \
&amp;=\ln \left|(\beta \mathbf{I})^{-1}\left(\mathbf{I}+\left(\mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}}\right)^{\mathrm{T}}((\beta \mathbf{I}) \mathbf{\Phi})^{\mathrm{T}}\right)\right|^{-1}\quad (\because(\mathrm{C} .14)) \
&amp;=\ln \left|(\beta \mathbf{I})^{-1}\left(\mathbf{I}+\mathbf{\Phi} \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}}(\beta \mathbf{I})\right)\right|^{-1} \quad \left(\because\left(\mathbf{A}^{-1}\right)^{\mathrm{T}}=\mathbf{A}^{-1}\right) \
&amp;=\ln \left|\left(\mathbf{I}+\mathbf{\Phi} \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}}(\beta \mathbf{I})\right)(\beta \mathbf{I})^{-1}\right|^{-1} \quad \left(\because |\mathbf{AB}| = |\mathbf{BA}|\right) \
&amp;=\ln \left|\beta^{-1} \mathbf{I}+\mathbf{\Phi} \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}}\right|^{-1}
\end{aligned}
$$</p>
<h2 id="演習-711"><a class="header" href="#演習-711">演習 7.11</a></h2>
<div class="panel-primary">
<p>前問を，</p>
<p>$$
p(\mathbf{y})=\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \boldsymbol{\mu}+\mathbf{b}, \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}\right) \tag{2.115}
$$</p>
<p>の結果を用いて解け．</p>
</div>
<p>※$(2.115)$式に代入するだけで求まる。</p>
<p>演習7.10の途中式から</p>
<p>$$
\begin{aligned}
p(\mathbf{t} \mid \mathbf{X}, \boldsymbol{\alpha}, \beta) &amp;=\int p(\mathbf{t} \mid \mathbf{X}, \mathbf{w}, \beta) p(\mathbf{w} \mid \alpha) d \mathbf{w} \
&amp;=\int \prod_{n=1}^{N} \mathcal{N}\left(t_{n} \mid \mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x}), \beta^{-1}\right) \prod_{i=1}^{M} \mathcal{N}\left(w_{i} \mid 0, \alpha_{i}^{-1}\right) d \mathbf{w} \
&amp;=\int \mathcal{N} \left( \mathbf{t} \mid \mathbf{\Phi w}, \beta^{-1}\mathbf{I} \right) \mathcal{N} \left( \mathbf{w} \mid \mathbf{0}, \mathbf{A}^{-1} \right) d\mathbf{w}
\end{aligned}
$$</p>
<p>$(2.115)$式を使って周辺化すると</p>
<p>$$
\begin{aligned}
p(\mathbf{t} \mid \mathbf{X}, \boldsymbol{\alpha}, \beta) &amp;= \mathcal{N}\left(\mathbf{t} \mid \mathbf{\Phi 0}+\left(\beta^{-1} \mathbf{I}\right)+\mathbf{\Phi} \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm T}\right) \
&amp;=\mathcal{N}\left(\mathbf{t} \mid \mathbf{0}, \mathbf{C}\right)
\end{aligned}
$$</p>
<p>となるので、$(7.85)$式が求められた。</p>
<h2 id="演習-712"><a class="header" href="#演習-712">演習 7.12</a></h2>
<div class="panel-primary">
<p>RVM回帰モデルについて周辺化対数尤度</p>
<p>$$
\begin{aligned} \ln p(\mathbf{t} \mid \mathbf{X}, \boldsymbol{\alpha}, \beta) &amp;=\ln \mathcal{N}(\mathbf{t} \mid \mathbf{0}, \mathbf{C}) \ &amp;=-\frac{1}{2}\left{N \ln (2 \pi)+\ln |\mathbf{C}|+\mathbf{t}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t}\right} \end{aligned} \tag{7.85}
$$</p>
<p>を直接最大化すると，更新式</p>
<p>$$
\alpha_{i}^{\text {new }}=\frac{\gamma_{i}}{m_{i}^{2}} \tag{7.87}
$$</p>
<p>および</p>
<p>$$
\left(\beta^{\text {new}}\right)^{-1}=\frac{|\mathbf{t}-\Phi \mathbf{m}|^{2}}{N-\sum_{i} \gamma_{i}} \tag{7.88}
$$</p>
<p>が得られることを示せ．ただし$\gamma_i$は</p>
<p>$$
\mathbf{\Sigma}=\left(\mathbf{A}+\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \tag{7.83}
$$</p>
<p>で定義される共分散行列$\mathbf{\Sigma}$の$i$番目の対角成分を用いて</p>
<p>$$
\gamma_i = 1-\alpha_i \Sigma_{ii} \tag{7.89}
$$</p>
<p>で与えられるものとする．</p>
</div>
<p>※$\mathbf{\Phi},\mathbf{\Phi}^{\mathrm T},\mathbf{\Sigma}$はそれぞれ$M\times N,N\times M, N\times N$行列、$\mathbf{t}, \mathbf{m}$はそれぞれ$M, N$次元ベクトルである。</p>
<p>演習 7.10または7.11の結果から</p>
<p>$$
\ln p(\mathbf{t} \mid \mathbf{X}, \alpha, \beta)=\frac{N}{2} \ln \beta-\frac{N}{2} \ln (2 \pi)+\frac{1}{2} \ln |\mathbf{\Sigma}|+\frac{1}{2} \sum_{i=1}^{M} \ln \alpha_{i}-\frac{1}{2} \mathbf{t}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t}
$$</p>
<p>となる。次にテキスト58ページのように、この対数尤度の微分を$0$とする。</p>
<p>まず$\alpha_i$について偏微分するが、準備として$\mathbf{I}_{ii}$を$ii$成分のみ$1$で残りを$0$とする行列とする。これを用いて上式第3項の$\alpha_i$についての偏微分は</p>
<p>$$
\begin{aligned}
\frac{\partial}{\partial \alpha_{i}} \ln |\mathbf{\Sigma}| &amp;=-\frac{\partial}{\partial \alpha_{i}} \ln \left|\mathbf{\Sigma}^{-1}\right| \
&amp;=-\operatorname{Tr}\left[\mathbf{\Sigma} \frac{\partial \mathbf{\Sigma}^{-1}}{\partial \alpha_{i}}\right] \quad(\because \textrm{(C.22)}) \
&amp;=-\operatorname{Tr}\left[\mathbf{\Sigma} \frac{\partial}{\partial \alpha_{i}}\left(\mathbf{A}+\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)\right] \
&amp;=-\operatorname{Tr}\left[\mathbf{\Sigma} \mathbf{I}<em>{i i}\right] \
&amp;=-\Sigma</em>{i i}
\end{aligned}
$$</p>
<p>第5項の$\alpha_i$についての偏微分は、$\mathbf{\Sigma}$が対称行列であることと$\mathbf{\Sigma} = \left(\mathbf{A}+\beta \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right)^{-1}$, $\mathbf{m} = \beta \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T}\mathbf{t}$を利用して</p>
<p>$$
\begin{aligned}
\frac{\partial}{\partial \alpha_{i}}\left(\mathbf{t}^{\mathrm{T}} \mathbf{C} \mathbf{t}\right) &amp;=\frac{\partial}{\partial \alpha_{i}}\left(\beta \mathbf{t}^{\mathrm{T}} \mathbf{t}-\mathbf{m}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \mathbf{m}\right) \quad (\because 演習7.10)\
&amp;=-\frac{\partial}{\partial \alpha_{i}}\left(\mathbf{m}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \mathbf{m}\right) \
&amp;=-\frac{\partial}{\partial \alpha_{i}}\left(\beta \mathbf{t}^{\mathrm{T}} \mathbf{\Phi} \mathbf{\Sigma} \mathbf{\Sigma}^{-1} \beta \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm{T}} \mathbf{t}\right) \quad (\because \mathbf{m} = \beta \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm{T}} \mathbf{t})\
&amp;=-\frac{\partial}{\partial \alpha_{i}}\left(\beta^{2} \mathbf{t}^{\mathrm{T}} \mathbf{\Phi} \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm{T}} \mathbf{t}\right) \
&amp;=-\operatorname{Tr}\left[ \left( \frac{\partial}{\partial \mathbf{\Sigma}^{-1}} \beta^{2} \mathbf{t}^{\mathrm{T}} \mathbf{\Phi} \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm{T}} \mathbf{t} \right)^{\mathrm T} \frac{\partial \mathbf{\Sigma}^{-1}}{\partial \alpha_i}\right] \quad (\because \textrm{Matrix Cookbook (137)}) \
&amp;=-\operatorname{Tr}\left[\beta^{2}\left(-\mathbf{\Sigma}\left(\mathbf{\Phi}^{\mathrm T} \mathbf{t}\right)\left(\mathbf{\Phi}^{\mathrm T} \mathbf{t}\right)^{\mathrm T} \mathbf{\Sigma}\right)^{\mathrm T} \mathbf{I}<em>{i i}\right] \quad (\because \textrm{Matrix Cookbook (61)}) \
&amp;=\operatorname{Tr}\left[\left(\mathbf{mm}^{\mathrm T}\right)^{\mathrm T} \mathbf{I}</em>{i i}\right] \
&amp;=m_{i}^2
\end{aligned}
$$</p>
<p>となる。$m_i$は$(7.82)$で定義される事後平均$\mathbf{m}$の$i$番目の要素である。また途中の式変形で<a href="https://www.math.uwaterloo.ca/%7Ehwolkowi/matrixcookbook.pdf">Matrix Cookbook</a>に掲載されている行列の微分の公式を用いた。</p>
<p>$$
\frac{\partial}{\partial \alpha_{i}}\ln p(\mathbf{t} \mid \mathbf{X}, \alpha, \beta) = -\frac{1}{2}\Sigma_{ii} + \frac{1}{2\alpha_i}-\frac{1}{2}m_i^{2}
$$</p>
<p>これを$0$として移項すると</p>
<p>$$
\begin{aligned}
&amp; \alpha_{i} m_{i}^{2} = 1-\alpha_{i} \Sigma_{i i} \
&amp; \therefore \alpha_{i} = \frac{1-\alpha_{i} \Sigma_{i i}}{m_{i}^{2}}=\frac{\gamma_{i}}{m_{i}^{2}}
\end{aligned}
$$</p>
<p>これが求める$\alpha_i^{\textrm{new}}$となる。</p>
<p>同様にして$\beta$について偏微分する。$\ln p$の第3項について</p>
<p>$$
\begin{aligned}
\frac{\partial}{\partial \beta}\ln | \mathbf{\Sigma} | &amp;= -\frac{\partial}{\partial \beta} \ln \left|\mathbf{\Sigma}^{-1}\right| \
&amp;=-\operatorname{Tr}\left[\mathbf{\Sigma} \frac{\partial \mathbf{\Sigma}^{-1}}{\partial \beta}\right] \
&amp;=-\operatorname{Tr}\left[\mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right] \end{aligned}
$$</p>
<p>第5項について</p>
<p>$$
\begin{aligned}
\frac{\partial}{\partial \beta}\left(\mathbf{t}^{\mathrm T} \mathbf{C} \mathbf{t}\right) &amp;=\frac{\partial}{\partial \beta}\left(\beta \mathbf{t}^{\mathrm T} \mathbf{t}-\mathbf{m}^{\mathrm T} \mathbf{C} \mathbf{m}\right) \
&amp;=\mathbf{t}^{\mathrm T} \mathbf{t}-\frac{\partial}{\partial \beta}\left(\beta^{2} \mathbf{t}^{\mathrm T} \mathbf{\Phi} \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T} \mathbf{t}\right) \
&amp;=\mathbf{t}^{\mathrm T} \mathbf{t}-2 \beta\left(\mathbf{t}^{\mathrm T} \mathbf{\Phi} \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T} \mathbf{t}\right)-\beta^{2} \frac{\partial}{\partial \beta}\left(\mathbf{t}^{\mathrm T} \mathbf{\Phi} \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T} \mathbf{t}\right) \
&amp;=\mathbf{t}^{\mathrm T} \mathbf{t}-2 \mathbf{t}^{\mathrm T} \mathbf{\Phi} \mathbf{m}-\beta^{2} \operatorname{Tr}\left[\frac{\partial}{\partial \mathbf{\Sigma}^{-1}}\left(\left(\mathbf{\Phi}^{\mathrm T} \mathbf{t}\right)^{\mathrm T} \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T} \mathbf{t}\right)^{\mathrm T} \frac{\partial \mathbf{\Sigma}^{-1}}{\partial \beta}\right] \
&amp;=\mathbf{t}^{\mathrm T} \mathbf{t}-2 \mathbf{t}^{\mathrm T} \mathbf{\Phi} \mathbf{m}+\beta^{2} \operatorname{Tr}\left[\mathbf{\Sigma}\left(\mathbf{\Phi}^{\mathrm T} \mathbf{t}\right)(\mathbf{\Phi}^{\mathrm T} \mathbf{t})^{\mathrm T} \mathbf{\Sigma} \cdot\left(\mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right)\right] \
&amp;=\mathbf{t}^{\mathrm T} \mathbf{t}-2 \mathbf{t}^{\mathrm T} \mathbf{\Phi} \mathbf{m}+\operatorname{Tr}\left[\mathbf{m} \mathbf{m}^{\mathrm T} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right] \
&amp;=\mathbf{t}^{2} \mathbf{t}-2 \mathbf{t}^{\mathrm T} \mathbf{\Phi} \mathbf{m}+\operatorname{Tr}\left[\mathbf{m}^{\mathrm T} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi} \mathbf{m}\right] \
&amp;=\mathbf{t}^{2} \mathbf{t}-2 \mathbf{t}^{\mathrm T} \mathbf{\Phi} \mathbf{m}+(\mathbf{\Phi} \mathbf{m})^{\mathrm T} \mathbf{\Phi} \mathbf{m} \
&amp;=|\mathbf{t}-\mathbf{\Phi} \mathbf{m}|^{2}
\end{aligned}
$$</p>
<p>これより、</p>
<p>$$
\frac{\partial}{\partial \beta}\ln p(\mathbf{t} \mid \mathbf{X}, \alpha, \beta)=\frac{1}{2}\left(\frac{N}{\beta}-\operatorname{Tr}\left[\mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right]-|\mathbf{t}-\mathbf{\Phi} \mathbf{m}|^{2}\right)
$$</p>
<p>となる。このうち$\operatorname{Tr}\left[\mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right]$について</p>
<p>$$
\begin{aligned}
\mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi} &amp;=\mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}+\beta^{-1} \mathbf{\Sigma} \mathbf{A}-\beta^{-1} \mathbf{\Sigma} \mathbf{A} \
&amp;=\mathbf{\Sigma}\left(\beta \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}+\mathbf{A}\right) \beta^{-1}-\beta^{-1} \mathbf{\Sigma} \mathbf{A} \
&amp;=\mathbf{I} \beta^{-1}-\beta^{-1} \Sigma \mathbf{A} \
&amp;=\beta^{-1}(\mathbf{I}-\mathbf{\Sigma} \mathbf{A})
\end{aligned}
$$</p>
<p>となるので、</p>
<p>$$
\begin{aligned}
\ &amp; \frac{\partial}{\partial \beta}\ln p(\mathbf{t} \mid \mathbf{X}, \alpha, \beta) = 0 \
\Leftrightarrow &amp;\ \frac{1}{2}\left(\frac{N}{\beta}-\operatorname{Tr}\left[\mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right]-|\mathbf{t}-\mathbf{\Phi} \mathbf{m}|^{2}\right) = 0 \
\Leftrightarrow &amp;\ \beta^{-1} = \frac{|\mathbf{t}-\mathbf{\Phi} \mathbf{m}|^{2}}{N-\operatorname{Tr}(\mathbf{I}-\mathbf{\Sigma A})}=\frac{|\mathbf{t}-\mathbf{\Phi} \mathbf{m}|^{2}}{N-\sum_{i} \gamma_{i}}
\end{aligned}
$$</p>
<p>これが$\left(\beta^{\text {new}}\right)^{-1}$となる。</p>
<p>※ $\alpha_{i}^{\text {new }}$も$\left(\beta^{\text {new}}\right)^{-1}$も1つ前の$\alpha_i, \beta^{-1}$の値に依存しているので、これら超パラメータの学習はP.58に書かれているように、適当な初期値を決めてから更新していき、適当な収束条件が満たされるまで繰り返される。</p>
<h2 id="演習-713"><a class="header" href="#演習-713">演習 7.13</a></h2>
<div class="panel-primary">
<p>本文では，RVM回帰モデルについて，</p>
<p>$$
\begin{aligned} \ln p(\mathbf{t} \mid \mathbf{X}, \boldsymbol{\alpha}, \beta) &amp;=\ln \mathcal{N}(\mathbf{t} \mid \mathbf{0}, \mathbf{C}) \ &amp;=-\frac{1}{2}\left{N \ln (2 \pi)+\ln |\mathbf{C}|+\mathbf{t}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t}\right} \end{aligned} \tag{7.85}
$$</p>
<p>の周辺化尤度の最大化から，更新式</p>
<p>$$
\alpha_{i}^{\text {new }}=\frac{\gamma_{i}}{m_{i}^{2}} \tag{7.87}
$$</p>
<p>および</p>
<p>$$
\left(\beta^{\text {new}}\right)^{-1}=\frac{|\mathbf{t}-\Phi \mathbf{m}|^{2}}{N-\sum_{i} \gamma_{i}} \tag{7.88}
$$</p>
<p>を導いた．超パラメータの事前分布を</p>
<p>$$
\operatorname{Gam}(\tau \mid a, b)=\frac{1}{\Gamma(a)} b^{a} \tau^{a-1} e^{-b \tau} \tag{B.26}
$$</p>
<p>の形のガンマ分布に変更したときの$\boldsymbol{\alpha}$と$\beta$に対する更新式を，同様に事後確率$p(\mathbf{t}, \boldsymbol{\alpha}, \beta \mid \mathbf{X})$を$\boldsymbol{\alpha}$と$\beta$に対して最大化することで導出せよ．</p>
</div>
<p>題意により、$\mathbf{\alpha}_i$と$\beta$の事前分布を以下のように定める。ここで、全ての$\alpha_i$についてパラメータ$a,b$は共通とした。（本文では$\alpha$が確率変数ではないので、$i$に応じて異なるパラメータにしないと関連度自動決定の議論に繋がらないが、$\alpha$を確率変数とみなすことで、各$i$について同一のパラメータを採用することができる。）</p>
<p>\begin{aligned}
p(\alpha_i) = \operatorname{Gam}(\alpha_i \mid a, b) = \frac{1} {\Gamma(a)} b^{a} \alpha_i{}^{a-1} e^{-b \alpha_i} \
p(\beta) = \operatorname{Gam}(\beta \mid \tilde{a}, \tilde{b}) = \frac{1}{\Gamma(\tilde{a})} \tilde{b}^{\tilde{a}} \beta^{\tilde{a}-1} e^{-\tilde{b} \beta}
\end{aligned}</p>
<p>尤度関数$p(\mathbf{t}, \mathbf{\alpha}, \beta \mid \mathbf{X} ) = p(\mathbf{t} \mid \mathbf{X}, \mathbf{\alpha}, \beta) \prod_i p(\alpha_i) p(\beta)$を最大化する$\mathbf{\alpha}$と$\beta$を求める。</p>
<p>対数尤度関数は、以下の通り。</p>
<p>\begin{aligned}
\ln p(\mathbf{t}, \mathbf{\alpha}, \beta \mid \mathbf{X} )
=&amp;
\frac{N}{2} \ln \beta-\frac{N}{2} \ln (2 \pi)+\frac{1}{2} \ln |\mathbf{\Sigma}|+\frac{1}{2} \sum_{j=1}^{M} \ln \alpha_{j}-\frac{1}{2} \mathbf{t}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t} \
&amp;+
\sum_{j=1}^M \left{ a \ln b + (a-1)\ln \alpha_j - b \alpha_j - \ln \Gamma (a) \right} \
&amp;+
\left{ \tilde{a} \ln \tilde{b} + (\tilde{a}-1)\ln \beta - \tilde{b} \beta - \ln \Gamma (\tilde{a}) \right}
\end{aligned}</p>
<p>対数尤度関数を$\alpha_i$と$\beta$で偏微分する。１行目の偏微分は演習(7.12)に登場する式変形を参照。</p>
<p>\begin{aligned}
\frac{\partial}{\partial \alpha_i} \ln p(\mathbf{t}, \mathbf{\alpha}, \beta \mid \mathbf{X} )
=&amp; \left( -\frac{1}{2}\Sigma_{ii} + \frac{1}{2\alpha_i}-\frac{1}{2}m_i^{2} \right) + \left( \frac{a-1}{\alpha_i} - b \right) \
=&amp; -\frac{1}{2} \frac{1 - \gamma_i}{\alpha_i} + \frac{1}{2\alpha_i}-\frac{1}{2}m_i^{2} + \frac{a-1}{\alpha_i} - b \
=&amp; \frac{1}{2 \alpha_i} \left{ -(1 - \gamma_i) + 1 + 2(a-1) \right} - \frac{1}{2} (m_i^2 + 2b)\
=&amp; \frac{1}{2 \alpha_i} \left( \gamma_i + 2a -2 \right) - \frac{1}{2} (m_i^2 + 2b)
\end{aligned}
右辺$=0$を解いて、
\begin{aligned}
\alpha_i =\frac{\gamma_i + 2a -2}{m_i^2 + 2b}
\end{aligned}</p>
<p>となる。でも、公式解答は</p>
<p>\begin{aligned}
\alpha_i =\frac{\gamma_i + 2a -2}{m_i^2 - 2b}
\end{aligned}</p>
<p>となっている・・・。次に、</p>
<p>\begin{aligned}
\frac{\partial}{\partial \beta} \ln p(\mathbf{t}, \mathbf{\alpha}, \beta \mid \mathbf{X} )
=&amp; \frac{1}{2}\left(\frac{N}{\beta}-\operatorname{Tr}\left[\mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right]-|\mathbf{t}-\mathbf{\Phi} \mathbf{m}|^{2}\right) +(\tilde{a} -1)\frac{1}{\beta}-\tilde{b} \
=&amp; \frac{1}{2}\left(\frac{N}{\beta}-\frac{\sum_i \gamma_i}{\beta}-|\mathbf{t}-\mathbf{\Phi} \mathbf{m}|^{2}\right) +(\tilde{a} -1)\frac{1}{\beta}-\tilde{b} \
=&amp; \frac{1}{2}\left{ \frac{ N-\sum_i \gamma_i +2(\tilde{a}-1) }{\beta} - \left( |\mathbf{t}-\mathbf{\Phi} \mathbf{m}|^{2} + 2\tilde{b} \right) \right}
\end{aligned}
右辺$=0$を解いて、
\begin{aligned}
\beta^{-1} =\frac
{|\mathbf{t}-\mathbf{\Phi} \mathbf{m}|^{2} + 2\tilde{b}}
{2\tilde{a}-2+N-\sum_i \gamma_i}
\end{aligned}
となる。でも、公式解答は</p>
<p>\begin{aligned}
\beta^{-1} =\frac
{|\mathbf{t}-\mathbf{\Phi} \mathbf{m}|^{2} + 2\tilde{b}}
{\tilde{a}+2+N-\sum_i \gamma_i}
\end{aligned}
となっている・・・。</p>
<h2 id="演習-714"><a class="header" href="#演習-714">演習 7.14</a></h2>
<div class="panel-primary">
<p>RVM回帰モデルの予測確率分布が</p>
<p>$$
\begin{aligned} p\left(t \mid \mathbf{x}, \mathbf{X}, \mathbf{t}, \alpha^{\star}, \beta^{\star}\right) &amp;=\int p\left(t \mid \mathbf{x}, \mathbf{w}, \beta^{\star}\right) p\left(\mathbf{w} \mid \mathbf{X}, \mathbf{t}, \boldsymbol{\alpha}^{\star}, \beta^{\star}\right) \mathrm{d} \mathbf{w} \ &amp;=\mathcal{N}\left(t \mid \mathbf{m}^{\mathrm{T}} \phi(\mathbf{x}), \sigma^{2}(\mathbf{x})\right) \end{aligned} \tag{7.90}
$$</p>
<p>で与えられることを示せ．また，その予測分布の分散が</p>
<p>$$
\sigma^{2}(\mathbf{x})=\left(\beta^{\star}\right)^{-1}+\boldsymbol{\phi}(\mathbf{x})^{\mathrm{T}} \mathbf{\Sigma} \boldsymbol{\phi}(\mathbf{x}) \tag{7.91}
$$</p>
<p>で与えられることも示せ．ここで，$\mathbf{\Sigma}$は</p>
<p>$$
\mathbf{\Sigma} = \left(\mathbf{A}+\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \tag{7.83}
$$</p>
<p>において$\alpha = \alpha^{\star}$および$\beta = \beta^{\star}$としたものである．</p>
</div>
<p>$(7.76)$式,$(7.81)$式から</p>
<p>\begin{aligned} p\left(t \mid \mathbf{x}, \mathbf{w}, \beta^{\star}\right)  &amp;=\mathcal{N}\left(t \mid \mathbf{w}^{\mathrm{T}} \phi(\mathbf{x}), (\beta^{\star})^{-1}\right )
\ &amp;
\end{aligned}
\begin{aligned} p\left(\mathbf{w} \mid \mathbf{X}, \mathbf{t}, \boldsymbol{\alpha}^{\star}, \beta^{\star}\right)&amp;=\mathcal{N}\left(\mathbf{w} \mid \mathbf{m}, \mathbf{\Sigma} \right )
\ &amp;
\end{aligned}
$$
(2.115)式において、A\mathbf{x}を\mathbf{w}^{\mathrm{T}} \phi(\mathbf{x})に、L^{-1}を(\beta^{\star})^{-1}に、\muを\mathbf{m}に、\Lambda^{-1}を\Sigmaに置き換えると、
$$
\begin{aligned} p\left(t \mid \mathbf{x}, \mathbf{X}, \mathbf{t}, \alpha^{\star}, \beta^{\star}\right) &amp;=\int p\left(t \mid \mathbf{x}, \mathbf{w}, \beta^{\star}\right) p\left(\mathbf{w} \mid \mathbf{X}, \mathbf{t}, \boldsymbol{\alpha}^{\star}, \beta^{\star}\right) \mathrm{d} \mathbf{w} \ &amp;=\mathcal{N}\left(t \mid \mathbf{m}^{\mathrm{T}} \phi(\mathbf{x}), \left(\beta^{\star}\right)^{-1}+\boldsymbol{\phi}(\mathbf{x})^{\mathrm{T}} \mathbf{\Sigma} \boldsymbol{\phi}(\mathbf{x})\right) \end{aligned}
となる。</p>
<h2 id="演習-715"><a class="header" href="#演習-715">演習 7.15</a></h2>
<div class="panel-primary">
<p>$$
|\mathbf{C}| =\left|\mathbf{C}<em>{-i}\right|\left(1+\alpha</em>{i}^{-1} \boldsymbol{\varphi}<em>{i}^{\mathrm{T}} \mathbf{C}</em>{-i}^{-1} \boldsymbol{\varphi}_{i}\right) \tag{7.94}
$$</p>
<p>および</p>
<p>$$
\mathbf{C}^{-1} =\mathbf{C}<em>{-i}^{-1}-\frac{\mathbf{C}</em>{-i}^{-1} \boldsymbol{\varphi}<em>{i} \boldsymbol{\varphi}</em>{i}^{\mathrm{T}} \mathbf{C}<em>{-i}^{-1}}{\alpha</em>{i}+\boldsymbol{\varphi}<em>{i}^{\mathrm{T}} \mathbf{C}</em>{-i}^{-1} \boldsymbol{\varphi}_{i}} \tag{7.95}
$$</p>
<p>を用いて，周辺化尤度</p>
<p>$$
\begin{aligned} \ln p(\mathbf{t} \mid \mathbf{X}, \boldsymbol{\alpha}, \beta) &amp;=\ln \mathcal{N}(\mathbf{t} \mid \mathbf{0}, \mathbf{C}) \ &amp;=-\frac{1}{2}\left{N \ln (2 \pi)+\ln |\mathbf{C}|+\mathbf{t}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t}\right} \end{aligned} \tag{7.85}
$$</p>
<p>が</p>
<p>$$
L(\boldsymbol{\alpha})=L\left(\boldsymbol{\alpha}<em>{-i}\right)+\lambda\left(\alpha</em>{i}\right) \tag{7.96}
$$</p>
<p>の形に変形できることを示せ．ただし$\lambda(\alpha_n)$および品質/疎性パラメータはそれぞれ</p>
<p>$$
\lambda\left(\alpha_{i}\right)=\frac{1}{2}\left[\ln \alpha_{i}-\ln \left(\alpha_{i}+s_{i}\right)+\frac{q_{i}^{2}}{\alpha_{i}+s_{i}}\right] \tag{7.97}
$$</p>
<p>$$
s_{i}=\boldsymbol{\varphi}<em>{i}^{\mathrm{T}} \mathbf{C}</em>{-i}^{-1} \boldsymbol{\varphi}_{i} \tag{7.98}
$$</p>
<p>$$
q_{i}=\boldsymbol{\varphi}<em>{i}^{\mathrm{T}} \mathbf{C}</em>{-i}^{-1} \mathbf{t} \tag{7.99}
$$</p>
<p>で定義されているとする．</p>
</div>
<p>$(7.94)$式は</p>
<p>$$
\begin{aligned}
|\mathbf{C}| &amp;= \left| \mathbf{C}<em>{-i}\left(\mathbf{I}+\alpha</em>{i}^{-1} \mathbf{C}<em>{-i}^{-1} \varphi</em>{i} \varphi_{i}^{\mathrm T}\right)\right| \
&amp;= \left| \mathbf{C}<em>{-i} \right| \left| \mathbf{I}+\alpha</em>{i}^{-1} \mathbf{C}<em>{-i}^{-1} \varphi</em>{i} \varphi_{i}^{\mathrm T}\right| \
&amp;=\left|\mathbf{C}<em>{-i}\right|\left(1+\alpha</em>{i}^{-1}\left(\mathbf{C}<em>{-i}^{-1} \varphi</em>{i}\right)^{\mathrm T} \varphi_{i}\right) \quad (\because (\textrm{C}. 15)) \
&amp;=\left|\mathbf{C}<em>{-i}\right|\left(1+\alpha</em>{i}^{-1} \varphi_{i}^{\mathrm T} \mathbf{C}<em>{-i}^{-1} \varphi</em>{i}\right) \quad \left(\because \mathbf{C}<em>{-i}^{-1} = \left( \mathbf{C}</em>{-i}^{-1} \right)^{\mathrm T} \right)
\end{aligned}
$$</p>
<p>$(7.95)$式はWoodburyの公式を用いて求められる。
$$
\begin{aligned}
\left(\mathbf{C}<em>{-i}+\alpha</em>{i}^{-1} \varphi_{i} \varphi_{i}^{\mathrm T}\right)^{-1}
&amp;=\mathbf{C}<em>{-i}^{-1}-\mathbf{C}</em>{-i}^{-1} \varphi_{i}\left(\alpha_{i} \mathbf{I}+\varphi_{i}^{\mathrm T} \mathbf{C}<em>{-i}^{-1} \varphi</em>{i}\right)^{-1} \varphi_{i}^{\mathrm T} \mathbf{C}<em>{-i}^{-1} \
&amp;=\mathbf{C}</em>{-i}^{-1}-\frac{\mathbf{C}<em>{-i}^{-1} \varphi</em>{i} \varphi_{i}^{\mathrm T} \mathbf{C}<em>{-i}^{-1}}{\alpha</em>{i}+\varphi_{i}^{\mathrm T} \mathbf{C}<em>{-i}^{-1} \varphi</em>{i}}
\end{aligned}
$$</p>
<p>これらを用いて対数周辺尤度$\displaystyle \ln p(\mathbf{t} \mid \mathbf{X}, \boldsymbol{\alpha}, \beta) =-\frac{1}{2}\left{N \ln (2 \pi)+\ln |\mathbf{C}|+\mathbf{t}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t}\right}$を計算すると</p>
<p>$$
\begin{aligned}
L(\boldsymbol{\alpha})=&amp;-\frac{1}{2}\left{N \ln (2 \pi)+\ln |\mathbf{C}|+\mathbf{t}^{\mathrm T} \mathbf{C}^{-1} \mathbf{t}\right} \
=&amp;-\frac{1}{2}\left{N \ln (2 \pi)+\ln \left(\left|\mathbf{C}<em>{-i}\right|\left(1+\alpha</em>{i}^{-1} \varphi_{i}^{\mathrm T} \mathbf{C}<em>{-1}^{-1} \varphi</em>{i}\right)\right)+\mathbf{t}^{\mathrm T}\left(\mathbf{C}<em>{-i}^{-1} - \frac{\mathbf{C}</em>{-i}^{-1} \varphi_{i} \varphi_{i}^{\mathrm T} \mathbf{C}<em>{-i}^{-1}}{\alpha</em>{i}+\varphi_{i}^{\mathrm T} \mathbf{C}<em>{-i}^{-1} \varphi</em>{i}}\right) \mathbf{t}\right} \
=&amp;-\frac{1}{2}\left{N \ln (2 \pi)+\ln \left(\left|\mathbf{C}<em>{-i}\right|\left(1+\alpha</em>{i}^{-1} s_{i}\right)\right)+\mathbf{t}^{\mathrm T} \mathbf{C}<em>{-i}^{-1} \mathbf{t} - \frac{q</em>{i}^{2}}{\alpha_{i}+s_{i}}\right} \
&amp;(\because q_{i}^{2}=q_{i}^{\mathbf{T}}q_{i}=(\varphi_{i}^{\mathbf{T}}\mathbf{C}<em>{-i}^{-1}\mathbf{t})^{\mathbf{T}}(\varphi</em>{i}^{\mathbf{T}}\mathbf{C}<em>{-i}^{-1}\mathbf{t})=\mathbf{t}^{\mathbf{T}}(\mathbf{C}</em>{-i}^{-1})^{\mathbf{T}}\varphi_{i}\varphi_{i}^{\mathbf{T}}\mathbf{C}<em>{-i}^{-1}\mathbf{t})
\
=&amp;-\frac{1}{2}\left{N \ln (2 \pi)+\ln |\mathbf{C}</em>{-i}|+\mathbf{t}^{\mathrm T} \mathbf{C}<em>{-i}^{-1} \mathbf{t} \right} -\frac{1}{2} \ln \left(\frac{\alpha</em>{i}+s_{i}}{\alpha_{i}}\right) + \frac{1}{2} \frac{q_{i}^{2}}{\alpha_{i}+s_{i}} \
=&amp;\ L(\boldsymbol{\alpha}<em>{-i})+\frac{1}{2}\left[\ln \alpha</em>{i}-\ln \left(\alpha_{i}+s_{i}\right)+\frac{q_{i}{ }^{2}}{\alpha_{i}+s_{i}}\right] \
=&amp;\ L(\boldsymbol{\alpha}_{-i})+\lambda(\alpha_i)
\end{aligned}
$$</p>
<p>以上より、$(7.96)$式が導出された。</p>
<h2 id="演習-716"><a class="header" href="#演習-716">演習 7.16</a></h2>
<div class="panel-primary">
<p>超パラメータ$\alpha_i$に対して， RVM回帰モデルの周辺化対数尤度</p>
<p>\begin{align}
\lambda(\alpha_{i}) = \frac{1}{2}[ \ln \alpha_i - \ln(\alpha_i + s_i) +\frac{q_i^2}{\alpha_1 + s_i} ]
\end{align}</p>
<p>の2階微分を取ることで，</p>
<p>$$
\alpha_{i}=\frac{s_{i}^{2}}{q_{i}^{2}-s_{i}} \tag{7.101}
$$</p>
<p>で与えられる停留点が周辺化尤度の極大値であることを示せ．</p>
</div>
<p>$\lambda(\alpha_i)$を一階微分すると、
\begin{align}
\frac{\partial \lambda(\alpha_i)}{\partial \alpha_i} = \frac{\alpha_i^{-1}s_i^2 - (q_i^2 -s_i)}{2(\alpha_i + s_i )^2}
\end{align}
である。よって、その分子が0をとるとき、$\alpha_i$は極値をとる。よって、</p>
<p>\begin{align}
&amp;\alpha_i^{-1}s_i^2 - (q_i^2 -s_i) = 0 \
&amp;\Rightarrow \alpha_i =\frac{s_i^2}{q_i^2 - s_i}
\end{align}</p>
<p>次に、２階微分は以下になる。
\begin{align}
\frac{\partial^2 \lambda(\alpha_i)}{\partial^2 \alpha_i} = \frac{1}{2}[-\frac{1}{\alpha_i^2}+\frac{1}{(\alpha_i+s_i)^2 }+\frac{2q_i^2}{(\alpha_i+s_i)^3} ]
\end{align}</p>
<p>次に、２階微分に$\alpha_i =\frac{s_i^2}{q_i^2 - s_i}$を代入した際に、0未満であれば、その$\alpha_i$は極大値であることが明らかになる。
\begin{align}
\frac{1}{2}[-\frac{1}{\alpha_i^2}+\frac{1}{(\alpha_i+s_i)^2 }+\frac{2q_i^2}{(\alpha_i+s_i)^3} ] &amp;=
\frac{1}{2}[-\frac{1}{(\frac{s_i^2}{q_i^2 - s_i})^2}+\frac{1}{(\frac{s_i^2}{q_i^2 - s_i}+s_i)^2 }+\frac{2q_i^2}{(\frac{s_i^2}{q_i^2 - s_i}+s_i)^3} ] \
&amp;= \frac{1}{2}[-\frac{(q_i^2 - s_i)^2}{s_i^4}+\frac{(q_i^2 - s_i)^2}{s_i^2 q_i^4}+\frac{2(q_i^2 - s_i)^3}{s_i^3 q_i^4}] \
&amp;= -\frac{1}{2}{(q_i^2 - s_i)^4}{q_i^4 s_i^2} &lt; 0 &amp;(\because q_i^2 - s_i &gt; 0)
\end{align}
よって、$\alpha_i =\frac{s_i^2}{q_i^2 - s_i}$において極大値を取る。</p>
<h2 id="演習-717"><a class="header" href="#演習-717">演習 7.17</a></h2>
<div class="panel-primary">
<p>\begin{align}
\boldsymbol{\Sigma}&amp;=\left(\mathbf{A}+\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \tag{7.83} \
\mathbf{C}&amp;=\beta^{-1} \mathbf{I}+\mathbf{\Phi} \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}} \tag{7.87}\
\left(\mathbf{A}+\mathbf{B D}^{-1} \mathbf{C}\right)^{-1}&amp;=\mathbf{A}^{-1}-\mathbf{A}^{-1} \mathbf{B}\left(\mathbf{D}+\mathbf{C A}^{-1} \mathbf{B}\right)^{-1} \mathbf{C A}^{-1} \tag{C.7}
\end{align}</p>
<p>を用いて，</p>
<p>\begin{align}
Q_{i}&amp;=\boldsymbol{\varphi}<em>{i}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t} \tag{7.102} \
S</em>{i}&amp;=\boldsymbol{\varphi}<em>{i}^{\mathrm{T}} \mathbf{C}^{-1} \boldsymbol{\varphi}</em>{i} \tag{7.103}
\end{align}</p>
<p>で定義される$Q_n, S_n$が，</p>
<p>\begin{align}
Q_{i}=\beta \boldsymbol{\varphi}<em>{i}^{\mathrm{T}} \mathbf{t}-\beta^{2} \boldsymbol{\varphi}</em>{i}^{\mathrm{T}} \boldsymbol{\Phi} \boldsymbol{\Sigma} \boldsymbol{\Phi}^{\mathrm{T}} \mathbf{t} \tag{7.106} \
S_{i}=\beta \boldsymbol{\varphi}<em>{i}^{\mathrm{T}} \boldsymbol{\varphi}</em>{i}-\beta^{2} \boldsymbol{\varphi}<em>{i}^{\mathrm{T}} \boldsymbol{\Phi} \boldsymbol{\Sigma} \boldsymbol{\Phi}^{\mathrm{T}} \boldsymbol{\varphi}</em>{i} \tag{7.107}
\end{align}</p>
<p>に変形できることを示せ．</p>
</div>
<p>※
(7.102)式に(7.87)式を代入して</p>
<p>$$
\begin{aligned}
Q_{i}&amp;=\boldsymbol{\varphi}<em>{i}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t}\
&amp;=\boldsymbol{\varphi}</em>{i}^{T}(\beta^{-1} \mathbf{I}+\mathbf{\Phi} \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}})^{-1}\mathbf{t}\
&amp;=\boldsymbol{\varphi}<em>{i}^{T}\left{\beta\mathbf{I}-\beta^{2}\mathbf{\Phi}(\mathbf{A}^{-1}+\beta\mathbf{\Phi}^{T}\mathbf{\Phi})^{-1}\mathbf{\Phi}^{T}\right}\mathbf{t}\
&amp;=\beta\boldsymbol{\varphi}</em>{i}^{T}\mathbf{t}-\beta^{2}\boldsymbol{\varphi}<em>{i}^{T}\mathbf{\Phi}(\mathbf{A}^{-1}+\beta\mathbf{\Phi}^{T}\mathbf{\Phi})^{-1}\mathbf{\Phi}^{T}\mathbf{t}\
&amp;=\beta\boldsymbol{\varphi}</em>{i}^{T}\mathbf{t}-\beta^{2}\boldsymbol{\varphi}_{i}^{T}\mathbf{\Phi}\boldsymbol{\Sigma}\mathbf{\Phi}^{T}\mathbf{t}\
\end{aligned}
$$</p>
<p>よって(7.106)式が得られる．2行目から3行目への式変形に(C.7)式を用い，4行目から5行目の式変形で(7.83)式を用いた．
また$\mathbf{t}$を$\boldsymbol{\varphi}_i$として上記と同様の計算を行うことで$S_i$についての式(7.107)が求まる．</p>
<h2 id="演習-718"><a class="header" href="#演習-718">演習 7.18</a></h2>
<div class="panel-primary">
<p>RVM分類モデルの対数事後確率分布</p>
<p>$$
\begin{aligned}
\ln p(\mathbf{w} \mid \mathbf{t}, \boldsymbol{\alpha})&amp;=\ln {p(\mathbf{t} \mid \mathbf{w}) p(\mathbf{w} \mid \boldsymbol{\alpha})}-\ln p(\mathbf{t} \mid \alpha) \
&amp;=\sum_{n=1}^{N}\left{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right}-\frac{1}{2} \mathbf{w}^{\mathrm{T}} \mathbf{A} \mathbf{w}+\text { const. }
\end{aligned} \tag{7.109}
$$</p>
<p>の勾配ベクトルおよびへシアン行列は</p>
<p>\begin{align}
\nabla \ln p(\mathbf{w} \mid \mathbf{t}, \boldsymbol{\alpha}) &amp;=\boldsymbol{\Phi}^{\mathrm{T}}(\mathbf{t}-\mathbf{y})-\mathbf{A} \mathbf{w} \tag{7.110} \
\nabla \nabla \ln p(\mathbf{w} \mid \mathbf{t}, \boldsymbol{\alpha}) &amp;=-\left(\Phi^{\mathrm{T}} \mathbf{B} \Phi+\mathbf{A}\right) \tag{7.111}
\end{align}</p>
<p>で与えられることを示せ．</p>
</div>
<p>$p(\mathbf{w} \mid \boldsymbol{\alpha})$は$(7.80)$から</p>
<p>$$
\begin{aligned}
p(\mathbf{w} \mid \boldsymbol{\alpha}) &amp;=\prod_{i=1}^{M} \mathcal{N}\left(w_{i} \mid 0, \alpha_{i}^{-1}\right) \
\ln p(\mathbf{w} \mid \boldsymbol{\alpha}) &amp;=\sum_{i=1}^{M} \ln \left[\left(\frac{\alpha_{i}}{2 \pi}\right)^{\frac{1}{2}} \exp \left{-\frac{\alpha_{i} w_{i}^{2}}{2}\right}\right]=-\frac{1}{2} \mathbf{w}^{\mathrm T} \mathbf{Aw} + \textrm{const.}
\end{aligned}
$$</p>
<p>である。</p>
<p>$p(\mathbf{t} \mid \mathbf{w})$は$(4.90)$式のクロスエントロピー誤差関数$E(\mathbf{w})$の符号を反転させたもの</p>
<p>$$
\ln p(\mathbf{t} \mid \mathbf{w})=\sum_{n=1}^{N}\left{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right}
$$</p>
<p>である。</p>
<p>演習4.13と同様に$\ln p(\mathbf{t} \mid \mathbf{w})$の$\mathbf{w}$についての勾配は</p>
<p>$$
\begin{aligned}
\nabla_{\mathbf{w}} \ln p &amp;=\frac{\partial \ln p}{\partial y_{n}} \frac{\partial y_{n}}{\partial a_{n}} \nabla_{\mathbf{w}} a_{n} \
\frac{\partial \ln p}{\partial y_{n}} &amp;=\sum_{n=1}^{N}\left(\frac{t_{n}}{y_{n}}-\frac{1-t_{n}}{1-y_{n}}\right) \ &amp;=\sum_{n=1}^{N} \frac{t_{n}-y_{n}}{y_{n}\left(1-y_{n}\right)} \
\frac{\partial y_{n}}{\partial a_{n}} &amp;= \sigma\left(a_{n}\right)\left(1-\sigma\left(a_{n}\right)\right)=y_{n}\left(1-y_{n}\right) \
\nabla_{\mathbf{w}} a_{n}&amp;=\boldsymbol{\phi}_{n}
\end{aligned}
$$</p>
<p>よって</p>
<p>$$
\begin{aligned}
\nabla_{\mathbf{w}} \ln p(\mathbf{w} \mid \mathbf{t}, \boldsymbol{\alpha}) &amp;=\sum_{n=1}^{N}\left(t_{n}-y_{n}\right) \boldsymbol{\phi}_{n}-\frac{1}{2} \cdot 2 \mathbf{Aw} \ &amp;=\mathbf{\Phi}^{\mathrm T}(\mathbf{t}-\mathbf{y})-\mathbf{Aw}
\end{aligned}
$$</p>
<p>ヘッセ行列は</p>
<p>$$
\begin{aligned}
\nabla_{\mathbf{w}}\left(\mathbf{\Phi}^{\mathrm T}(\mathbf{t}-\mathbf{y})-\mathbf{Aw}\right) &amp;=-\sum_{n=1}^{N}\left(\frac{\partial y_{n}}{\partial a_{n}} \nabla_{\mathbf{w}} a_{n}\right) \boldsymbol{\phi}<em>{n}^{\mathrm T}-\mathbf{A}^{\mathrm T} \
&amp;=-\sum</em>{n=1}^{N} y_{n}\left(1-y_{n}\right) \boldsymbol{\phi}<em>{n} \boldsymbol{\phi}</em>{n}^{\mathrm T}-\mathbf{A} \
&amp;=-\left(\mathbf{\Phi}^{\mathrm T} \mathbf{B} \mathbf{\Phi}+\mathbf{A}\right)
\end{aligned}
$$</p>
<p>となる。</p>
<h2 id="演習-719"><a class="header" href="#演習-719">演習 7.19</a></h2>
<div class="panel-primary">
<p>RVM分類モデルにおいて，周辺尤度関数の近似式</p>
<p>$$
\begin{aligned}
p(\mathbf{t} \mid \boldsymbol{\boldsymbol{\alpha} }) &amp;=\int p(\mathbf{t} \mid \mathbf{w}) p(\mathbf{w} \mid \boldsymbol{\boldsymbol{\alpha} }) \mathrm{d} \mathbf{w} \
&amp; \simeq p\left(\mathbf{t} \mid \mathbf{w}^{\star}\right) p\left(\mathbf{w}^{\star} \mid \boldsymbol{\boldsymbol{\alpha} }\right)(2 \pi)^{M / 2}|\mathbf{\Sigma}|^{1 / 2}
\end{aligned} \tag{7.114}
$$</p>
<p>を最大化すると，超パラメータの更新式</p>
<p>$$
\alpha_{i}^{\text {new }}=\frac{\gamma_{i}}{\left(w_{i}^{\star}\right)^{2}} \tag{7.116}
$$</p>
<p>が得られることを示せ．</p>
</div>
<p>$\mathbf{w}^{\star}$を用いると条件付き確率$(4.89)$、事前分布$(7.80)$はそれぞれ</p>
<p>$$
\begin{aligned}
p\left(\mathbf{t} \mid \mathbf{w}^{\star}\right) &amp;= \prod_{n=1}^{N} y_{n}^{t_n}\left(1-y_{n}\right)^{1-t_{n}} \
p\left(\mathbf{w}^{\star} \mid \boldsymbol{\alpha} \right) &amp;= \prod_{i=1}^{M} \mathcal{N} \left(w_{i}^{*} \mid 0, \alpha_{i}^{-1}\right) = \left(\frac{1}{2 \pi}\right)^{\frac{M}{2}} \prod_{i=1}^{M} \alpha_{i}^{\frac{1}{2}} \exp \left{-\frac{\alpha_{i}{w_{i}^{\star}}^{2}}{2}\right}
\end{aligned}
$$</p>
<p>であるから$(7.114)$式の対数をとって対数周辺化尤度を求めると</p>
<p>$$
\begin{aligned}
\ln p(\mathbf{t} \mid \boldsymbol{\alpha} ) &amp;=
\ln p\left(\mathbf{t} \mid \mathbf{w}^{\star}\right)+\ln p\left(\mathbf{w}^{\star} \mid \boldsymbol{\alpha} \right)+\frac{M}{2} \ln (2 \pi)+\frac{1}{2}\ln |\mathbf{\Sigma}| \
&amp;=\sum_{n=1}^{N}\left{t_{n} \ln y_{n}^{<em>}+\left(1-t_{n}\right) \ln \left(1-y_{n}^{</em>}\right)\right} \
&amp; -\frac{1}{2} \sum_{i=1}^{M} \alpha_{i} w_{i}^{<em>^{2}}+\frac{1}{2} \sum_{i=1}^{N} \ln \alpha_{i}-\frac{M}{2} \ln (2 \pi)+\frac{M}{2} \ln (2 \pi)+\frac{1}{2} \ln |\mathbf{\Sigma}| \
&amp;=\left[\sum_{n=1}^{N}\left{t_{n} \ln y_{n}{ }^{</em>}+\left(1-t_{n}\right) \ln \left(1-y_{n}{ }^{<em>}\right)\right}\right]-\frac{1}{2} \sum_{i=1}^{M} \alpha_{i} w_{i}^{</em>^{2}}+\frac{1}{2} \sum_{i=1}^{N} \ln \alpha_{i}+\frac{1}{2} \ln |\mathbf{\Sigma}|
\end{aligned}
$$</p>
<p>$\alpha_i$についての微分を$0$とすると、今$\mathbf{w} = \mathbf{w}^{\star}$で固定されているので、$y_n^{\star} = \sigma(a_n) = \sigma({\mathbf{w}^{\star}}^{\mathrm T}\boldsymbol{\phi}_n)$も固定されている。つまり$[\ ]$以外の項について微分を取れば良い。</p>
<p>$$
\begin{aligned}
\frac{\partial}{\partial \alpha_{i}}\left[-\frac{1}{2} \sum_{i=1}^{M} \alpha_{i} w_{i}^{<em>^{2}}+\frac{1}{2} \sum_{i=1}^{M} \ln \alpha_{i}+\frac{1}{2} \ln |\mathbf{\Sigma}|\right]=0 \
-\frac{1}{2}\left(w_{i}^{</em>}\right)^{2}+\frac{1}{2} \frac{\partial}{\partial \alpha_{i}}\left(\ln \alpha_{i}\right)+\frac{1}{2} \frac{\partial}{\partial \alpha_{i}} \ln |\mathbf{\Sigma}|=0 \
-\frac{1}{2}\left(w_{i}^{*}\right)^{2}+\frac{1}{2 \alpha_{i}}-\frac{1}{2} \Sigma_{i i}=0 \quad (\because 演習 7.12)
\end{aligned}
$$</p>
<p>以上から$(7.115)$式が得られた。これに$\gamma_i = 1 - \alpha_{i} \Sigma_{ii}$を導入すれば</p>
<p>$$
\begin{aligned}
\alpha_{i}\left(w_{i}^{<em>}\right)^{2} &amp;= 1-\alpha_{i} \Sigma_{i i}=\gamma_{i} \ \therefore \ \alpha_{i} &amp;= \frac{\gamma_{i}}{\left(w_{i}^{</em>}\right)^{2}}
\end{aligned}
$$</p>
<p>これが$\alpha_{i}$の更新式となり$\displaystyle \left( \alpha_{i}^{\textrm {(new)}} \leftarrow \frac{\gamma_{i}}{\left(w_{i}^{*}\right)^{2}} \right)$、$(7.87)$と同一である。</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="prml第8章演習問題解答"><a class="header" href="#prml第8章演習問題解答">PRML第8章演習問題解答</a></h1>
<head>
<style>
  div.panel-primary {
	border: 1px solid #000;
    margin: 10px 5px;
    padding: 16px 10px 0px;
  }
</style>
</head>
<h2 id="演習-81"><a class="header" href="#演習-81">演習 8.1</a></h2>
<div class="panel-primary">
<p>変数を1つずつ周辺化することによって，有向グラフの同時分布の表現</p>
<p>$$
p(\mathbf{x})=\prod_{k=1}^{K} p\left(x_{k} \mid \mathrm{pa}_{k}\right) \tag{8.5}
$$</p>
<p>が正しく規格化されていることを示せ．ただし個々の条件付き分布は正しく規格化されていると仮定する．</p>
</div>
<p>$\int p(\mathbf{x}) d\mathbf{x} = 1$であることを示せばよい。</p>
<p>\begin{align}
\int p(\mathbf{x}) d\mathbf{x} &amp;= \int \prod^K_{k = 1} p(x_k|\mathrm{pa}_k) d\mathbf{x} \
&amp;= \idotsint p(x_K|\mathrm{pa}<em>K) \prod^{K - 1}</em>{k = 1} p(x_k|\mathbf{pa}<em>k) dx_1x_2\cdots x_K \
&amp;= \idotsint \Big[\prod^{K - 1}</em>{k = 1} p(x_k|\mathbf{pa}<em>k) \int p(x_K|\mathrm{pa}<em>K) dx_K\Big] dx_1x_2\cdots x</em>{K - 1} \
&amp;= \idotsint \prod^{K - 1}</em>{k = 1} p(x_k|\mathbf{pa}<em>k) dx_1x_2\cdots x</em>{K - 1} \
&amp;= \cdots \
&amp;= 1.
\end{align}</p>
<h2 id="演習-82"><a class="header" href="#演習-82">演習 8.2</a></h2>
<div class="panel-primary">
<p>有向グラフにおいて，すべてのノードについて，自分より小さい番号を持つノードに向かうリンクが存在しないようにノードを順序付けることができるなら，有向閉路は存在しないことを示せ．</p>
</div>
<p>$N$個の変数$x_1, x_2, \cdots x_N$を仮定する。題意より
\begin{align}
x_1 \rightarrow \cdots \rightarrow x_N
\end{align}
となる経路は存在しうるが、
\begin{align}
x_N \rightarrow \cdots \rightarrow x_1
\end{align}
となる経路は存在しないため、有向閉路は存在しない。</p>
<h2 id="演習-83"><a class="header" href="#演習-83">演習 8.3</a></h2>
<div class="panel-primary">
<p>表8.2で与えられる同時分布を持つ3つの2値変数$a,b,c \in {0, 1}$を考える．この分布が以下の特性を持つことを直接計算によって示せ．$a$および$b$は周辺依存である．すなわち$p(a, b) \neq p(a)p(b)$．しかし$c$で条件付けられると独立である．すなわち$c=0$および$c=1$のいずれの場合でも$p(a, b \mid c) = p(a\mid c)p(b\mid c)$である．
<img src="https://i.imgur.com/FfoEAQa.png" alt="" /></p>
</div>
<p>各確率を周辺化することで地道に求めていく。</p>
<p>$$
\begin{aligned}
p(a)&amp;=\sum_{b \in{0,1}} \sum_{c \in{0,1}} p(a, b, c) \
p(b)&amp;=\sum_{a \in{0,1}} \sum_{c \in{0,1}} p(a, b, c) \
p(a,b)&amp;=\sum_{c \in{0,1}} p(a,b,c)
\end{aligned}
$$</p>
<p>これを$a=0, b=0$について求めると</p>
<p>$$
\begin{aligned}
p(a=0) &amp;= \frac{192+144+48+216}{1000}=\frac{600}{1000},\ p(b=0) = \frac{192+144+192+64}{1000} =\frac{592}{1000} \
p(a=0, b=0) &amp;= \frac{192+144}{1000} = \frac{336}{1000}
\end{aligned}
$$</p>
<p>これより$p(a=0, b=0) \neq p(a=0)p(b=0)$であることが示された。同様にして、いずれの$a,b$の組み合わせでも$p(a, b) \neq p(a)p(b)$となる。</p>
<p>次に$c$で条件付けられた場合、ベイズの定理から</p>
<p>$$
p(a, b \mid c)= \frac{p(a,b,c)}{p(c)} =\frac{p(a, b, c)}{\sum_{a \in{0,1}} \sum_{b \in{0,1}} p(a, b, c)}
$$</p>
<p>同様に</p>
<p>$$
\begin{aligned}
p(a \mid c)&amp;=\frac{\sum_{b \in{0,1}} p(a, b, c)}{\sum_{a \in{0,1}} \sum_{b \in{0,1}} p(a, b, c)} \
p(b \mid c)&amp;=\frac{\sum_{a \in{0,1}} p(a, b, c)}{\sum_{a \in{0,1}} \sum_{b \in{0,1}} p(a, b, c)}
\end{aligned}
$$</p>
<p>である。それぞれ計算していくと</p>
<p>$$
\begin{aligned}
p(c=0) &amp;= \frac{192+48+192+48}{1000} = \frac{480}{1000} \
p(a=0, b=0 \mid c=0) &amp;= \frac{p(a=0, b=0, c=0)}{p(c=0)} = \frac{192}{480} = \frac{2}{5} \
p(a=0 \mid c=0) &amp;= \frac{p(a=0, c=0)}{p(c=0)} = \frac{240}{480} = \frac{1}{2} \
p(b=0 \mid c=0) &amp;= \frac{p(b=0, c=0)}{p(c=0)} = \frac{384}{480} = \frac{4}{5} \
\end{aligned}
$$</p>
<p>これより、$p(a=0, b=0 \mid c=0) = p(a=0 \mid c=0)p(b=0 \mid c=0)$が成立していることがわかる。同様にしてすべて計算していくと結果は以下の表の通りになる。</p>
<p>$$
\begin{array}{|c|c|c|c|}\hline a &amp; b &amp; \mathrm{c} &amp; p(a, b \mid c) \ \hline \hline 0 &amp; 0 &amp; 0 &amp; 0.400 \ \hline 0 &amp; 1 &amp; 0 &amp; 0.100 \ \hline 1 &amp; 0 &amp; 0 &amp; 0.400 \ \hline 1 &amp; 1 &amp; 0 &amp; 0.100 \ \hline \hline 0 &amp; 0 &amp; 1 &amp; 0.277 \ \hline 0 &amp; 1 &amp; 1 &amp; 0.415 \ \hline 1 &amp; 0 &amp; 1 &amp; 0.123 \ \hline 1 &amp; 1 &amp; 1 &amp; 0.185 \ \hline\end{array} \hspace{2em} \begin{array}{|c|c|c|c|}\hline a &amp; b &amp; c &amp; p(a \mid c) p(b \mid c) \ \hline \hline 0 &amp; 0 &amp; 0 &amp; 0.400 \ \hline 0 &amp; 1 &amp; 0 &amp; 0.100 \ \hline 1 &amp; 0 &amp; 0 &amp; 0.400 \ \hline 1 &amp; 1 &amp; 0 &amp; 0.100 \ \hline \hline 0 &amp; 0 &amp; 1 &amp; 0.277 \ \hline 0 &amp; 1 &amp; 1 &amp; 0.415 \ \hline 1 &amp; 0 &amp; 1 &amp; 0.123 \ \hline 1 &amp; 1 &amp; 1 &amp; 0.185 \ \hline\end{array}
$$</p>
<p>これより$p(a,b\mid c) = p(a\mid c)p(b\mid c)$が成立していることが示された。つまり$c$で条件付けられた場合に$a,b$は独立である。</p>
<h2 id="演習-84"><a class="header" href="#演習-84">演習 8.4</a></h2>
<div class="panel-primary">
<p>表8.2で与えられる同時分布に対して分布$p(a)$, $p(b\mid c)$および$p(c\mid a)$を計算せよ．その結果から$p(a, b, c) = p(a)p(c\mid a)p(b\mid c)$を直接計算して示し，対応する有向グラフを描け．</p>
</div>
<p>演習8.3と同様に$p(c|a)$を計算する。</p>
<p>$$
\begin{aligned}
p(c=0|a=0) &amp;= \frac{192+48}{600} = 0.4\
p(c=0|a=1) &amp;= \frac{192+48}{400} = 0.6\
p(c=1|a=0) &amp;= \frac{144+216}{600} = 0.4\
p(c=1|a=1) &amp;= \frac{64+96}{400} = 0.6\
\end{aligned}
$$
これを用いて$p(a)p(c|a)p(b|c)$を計算すると、以下の通り表8.2の$p(a,b,c)$に一致する。
$$
\begin{array}{|c|c|c|c|}\hline a &amp; b &amp; \mathrm{c} &amp; p(a)p(c|a)p(b|c) \ \hline \hline 0 &amp; 0 &amp; 0 &amp; 0.6\times 0.4\times 0.8=0.192 \ \hline 0 &amp; 0 &amp; 1 &amp; 0.6\times 0.6\times 0.4=0.144 \ \hline 0 &amp; 1 &amp; 0 &amp; 0.6\times 0.4\times 0.2=0.048 \ \hline 0 &amp; 1 &amp; 1 &amp; 0.6\times 0.6\times 0.6=0.216 \ \hline \hline 1 &amp; 0 &amp; 0 &amp; 0.4\times 0.6\times 0.8=0.192 \ \hline 1 &amp; 0 &amp; 1 &amp; 0.4\times 0.4\times 0.4=0.064 \ \hline 1 &amp; 1 &amp; 0 &amp; 0.4\times 0.6\times 0.2=0.048 \ \hline 1 &amp; 1 &amp; 1 &amp; 0.4\times 0.4\times 0.6=0.096 \ \hline\end{array}
$$
従って、有向グラフは「a→c→b」となる。</p>
<h2 id="演習-85"><a class="header" href="#演習-85">演習 8.5</a></h2>
<div class="panel-primary">
<p>$$
p(\mathbf{t} \mid \mathbf{X}, \mathbf{w}, \beta)=\prod_{n=1}^{N} p\left(t_{n} \mid \mathbf{x}_{n}, \mathbf{w}, \beta\right) \tag{7.79}
$$</p>
<p>および</p>
<p>$$
p(\mathbf{w} \mid \boldsymbol{\alpha})=\prod_{i=1}^{M} \mathcal{N} \left( w_i \mid 0, \alpha_i^{-1} \right) \tag{7.80}
$$</p>
<p>によって記述される．RVMに対応する有向確率的グラフィカルモデルを描け．</p>
</div>
<p>教科書P.57から</p>
<blockquote>
<p>RVMでは1つの超パラメータの代わりに個々の重みパラメータ$w_i$ごとに異なった超パラメータ$\alpha_i$を用いる</p>
</blockquote>
<p>ので$(7.80)$となっている。そこでこの式をまず有向グラフにすると</p>
<p><img src="https://i.imgur.com/HTu4sfH.png" alt="" /></p>
<p>のようになる。ノードは$M$個存在する。</p>
<p>同様に$(7.79)$を有向グラフにすると</p>
<p><img src="https://i.imgur.com/ABmm7oZ.png" alt="" /></p>
<p>ノードは$N$個存在する。$t_n$は$\mathbf{x}_n$,$\mathbf{w}_i$, $\beta$より生成される。なお、$t_n$は観測変数なのでノードに影をつけておく。</p>
<p>$(7.80)$で得た重み$\mathbf{w}$が$(7.79)$の$t_n$への親ノードになっているので、これらを繋いで</p>
<p><img src="https://i.imgur.com/VjGdT1K.png" alt="" /></p>
<p>という図を得る。</p>
<h2 id="演習-86"><a class="header" href="#演習-86">演習 8.6</a></h2>
<div class="panel-primary">
<p><img src="https://i.imgur.com/zX33hmn.png" alt="" /></p>
<p>図8.13に示されるモデルにおいて，条件付き分布$p(y\mid x_1, \ldots, x_M)$（ただし$x_i \in {0,1}$）を規定するのに必要なパラメータ数は，ロジスティックシグモイド関数表現</p>
<p>$$
p\left(y=1 \mid x_{1}, \ldots, x_{M}\right)=\sigma\left(w_{0}+\sum_{i=1}^{M} w_{i} x_{i}\right)=\sigma\left(\mathbf{w}^{\mathrm{T}} \mathbf{x}\right) \tag{8.10}
$$</p>
<p>を用いれば$2^M$から$M + 1$に減らせることを示した．別の表現(Pearl, 1988) として，</p>
<p>$$
p\left(y=1 \mid x_{1}, \ldots, x_{M}\right)=1-\left(1-\mu_{0}\right) \prod_{i=1}^{M}\left(1-\mu_{i}\right)^{x_{i}} \tag{8.104}
$$</p>
<p>で与えられるものもある．ただし，$0 \leqslant \mu_i \leqslant 1 (i = 0,\ldots, M)$であり，条件付き分布$(8. 104)$は<strong>noisy OR</strong>として知られる．この表現が論理的OR関数（すなわち，少なくとも1つの$i$に対して$x_i = 1$であれば常に$y=1$を与える関数）を「ソフト」 （確率的） にしたものであると解釈できることを示せ．また，$\mu_i$の解釈について論ぜよ．</p>
</div>
<p>まず、$\mu_0 = 0$、$\mu_i = 1$ for $i = 1, \ldots$の場合を考える。この時、8.104式は、
$$
p\left(y=1 \mid x_{1}, \ldots, x_{M}\right)=1- \prod_{i=1}^{M}0^{x_{i}} =
\begin{cases}
0 &amp;\text{if every } x_i = 0\
1 &amp;\text{else}
\end{cases}
$$
と表される。これは、論理的OR関数（すなわち，少なくとも1つの$i$に対して$x_i = 1$であれば常に$y=1$を与える関数）に等しい。そして、$\mu_0 \approx 0$,  $\mu_1 \approx 1$の時は、OR関数に近似できる。</p>
<p>次に、$\mu_0, \mu_i$について考える。$\mathbf{x} = \mathbf{0}$の時、
\begin{align}
p(y = 1|\mathbf{x} = \mathbf{0}) = 1 - (1-\mu_0)\prod_i (1-\mu_i)^0 = 1 - (1-\mu_0) = \mu_0
\end{align}
すなわち、$\mu_0$は、$\mathbf{x} = \mathbf{0}$の時の$y = 1$の確率と見ることができる。</p>
<p>次に、$x_i = 1$、$\mathbf{x}<em>{-i} = \mathbf{0}$を考えると、
\begin{align}
p(y = 1|x_i =1, \mathbf{x}</em>{-i} = \mathbf{0})  = 1 - (1-\mu_0)(1-\mu_i) = \mu_0 + \mu_i -\mu_0 \mu_i
\end{align}</p>
<p>今、$\mu_0 \approx 0$とすると、
\begin{align}
p(y = 1|x_i =1, \mathbf{x}<em>{-i}) \approx \mu_i
\end{align}
である。すなわち、$\mu_i$とは、$\mu_0 = 0$である時に、$x_i = 1$、$\mathbf{x}</em>{-i} = \mathbf{0}$である確率に等しい。以上から、8.104式は、論理的OR関数をソフトに(確率的に)したものであると言える。</p>
<h2 id="演習-87"><a class="header" href="#演習-87">演習 8.7</a></h2>
<div class="panel-primary">
<p>再帰的関係</p>
<p>$$
\mathbb{E}\left[x_{i}\right]=\sum_{j \in \mathrm{pa}<em>{i}} w</em>{i j} \mathbb{E}\left[x_{j}\right]+b_{i} \tag{8.15}
$$</p>
<p>および</p>
<p>$$
\begin{aligned} \operatorname{cov}\left[x_{i}, x_{j}\right] &amp;=\mathbb{E}\left[\left(x_{i}-\mathbb{E}\left[x_{i}\right]\right)\left(x_{j}-\mathbb{E}\left[x_{j}\right]\right)\right] \ &amp;=\mathbb{E}\left[\left(x_{i}-\mathbb{E}\left[x_{i}\right]\right)\left{\sum_{k \in \mathrm{pa}<em>{j}} w</em>{j k}\left(x_{k}-\mathbb{E}\left[x_{k}\right]\right)+\sqrt{v_{j}} \epsilon_{j}\right}\right] \ &amp;=\sum_{k \in \mathrm{pa}<em>{j}} w</em>{j k} \operatorname{cov}\left[x_{i}, x_{k}\right]+I_{i j} v_{j} . \end{aligned} \tag{8.16}
$$</p>
<p>を用いて，図8.14に示されるグラフの同時分布の平均および共分散が，それぞれ</p>
<p>$$
\mu=\left(b_{1}, b_{2}+w_{21} b_{1}, b_{3}+w_{32} b_{2}+w_{32} w_{21} b_{1}\right)^{\mathrm{T}} \tag{8.17}
$$</p>
<p>および</p>
<p>$$
\Sigma=\left(\begin{array}{ccc}v_{1} &amp; w_{21} v_{1} &amp; w_{32} w_{21} v_{1} \ w_{21} v_{1} &amp; v_{2}+w_{21}^{2} v_{1} &amp; w_{32}\left(v_{2}+w_{21}^{2} v_{1}\right) \ w_{32} w_{21} v_{1} &amp; w_{32}\left(v_{2}+w_{21}^{2} v_{1}\right) &amp; v_{3}+w_{32}^{2}\left(v_{2}+w_{21}^{2} v_{1}\right)\end{array}\right) \tag{8.18}
$$</p>
<p>で与えられることを示せ．</p>
<p><img src="https://i.imgur.com/mD7v5q8.png" alt="" /></p>
</div>
<p>図8.14のグラフに示された変数間の依存関係 ($x_1 \rightarrow x_2 \rightarrow x_3$) より、
\begin{align}
\mathbb{E}\left[x_1\right] &amp;= b_1 \
\mathbb{E}\left[x_2\right] &amp;= w_{21} \mathbb{E}\left[x_1\right] + b_2 \
&amp;= w_{21} b_1 + b_2 \
\mathbb{E}\left[x_3\right] &amp;= w_{32} \mathbb{E}\left[x_2\right] + b_3 \
&amp;= w_{32} w_{21} b_1 + w_{32} b_2 + b_3 \
\mathrm{var}\left[x_1\right] &amp;= v_1 \
\mathrm{cov}\left[x_1, x_2\right] &amp;= w_{21} \mathrm{cov}\left[x_1, x_1\right] \
&amp;= w_{21} v_1 = \mathrm{cov}\left[x_2, x_1\right] \
\mathrm{var}\left[x_2\right] &amp;= w_{21} \mathrm{cov}\left[x_2, x_1\right] + v_2 \
&amp;= w^2_{21} v_1 + v_2 \
\mathrm{cov}\left[x_1, x_3\right] &amp;= w_{32} \mathrm{cov}\left[x_1, x_2\right] \
&amp;= w_{32}w_{21} v_1 = \mathrm{cov}\left[x_3, x_1\right] \
\mathrm{cov}\left[x_2, x_3\right] &amp;= w_{32} \mathrm{cov}\left[x_2, x_2\right] \
&amp;= w_{32}w^2_{21} v_1 + w_{32} v_2 = w_{32}(w^2_{21} v_1 + v_2) = \mathrm{cov}\left[x_3, x_2\right] \
\mathrm{var}\left[x_3\right] &amp;= w_{32} \mathrm{cov}\left[x_3, x_2\right] + v_3 \
&amp;= w^2_{32}(w^2_{21} v_1 + v_2) + v_3
\end{align}</p>
<h2 id="演習-88"><a class="header" href="#演習-88">演習 8.8</a></h2>
<div class="panel-primary">
<p>$a \perp !!! \perp b, c \mid d$ならば$a \perp !!! \perp b \mid d$であることを示せ．</p>
</div>
<p>$a \perp !!! \perp b, c \mid d$ より条件付き独立の定義から</p>
<p>$$
p(a, b, c \mid d)=p(a \mid d)p(b, c \mid d)
$$</p>
<p>である．両辺をcについて周辺化することで</p>
<p>$$
p(a,b \mid d) =p(a \mid d)p(b \mid d)
$$</p>
<p>が得られ，条件付き独立の定義から</p>
<p>$$
a \perp !!! \perp b \mid d
$$</p>
<p>である．以上により示された．</p>
<h2 id="演習-89"><a class="header" href="#演習-89">演習 8.9</a></h2>
<div class="panel-primary">
<p>有向グラフにおいて，マルコフブランケット内のすべてのノードに条件付けられたノード$\mathbf{x}$の条件付き分布が，グラフの残りの変数に対して独立であることを有向分離規準を用いて示せ．</p>
</div>
<p>マルコフブランケット内のすべてのノードとは$\mathbf{x}$の親、子、共同親の全てのノードである。これらが全て条件付けられている時、$\mathbf{x}$が条件付けられていないノードに対して独立であることを示す。まず、条件付けられていないノードは、
(1) $\mathbf{x}$の親ノードの親
(2) $\mathbf{x}$の親ノードの子
(3) $\mathbf{x}$の子ノードの子
(4) $\mathbf{x}$の共同親ノードの親
(5) $\mathbf{x}$の共同親ノードの子
のいずれかが$\mathbf{x}$との経路に存在する。よって(1)~(5)のいずれも$\mathbf{x}$と独立であることを示せば良い。</p>
<p>p.91の2条件を改めて示すと、
ノード$\mathbf{y}$で経路が遮断されている条件は
(a)$\mathbf{y}$が条件づけられていて、経路に含まれる矢印が$\mathbf{y}$でhead-to-tailもしくはtail-to-tail
(b)$\mathbf{y}$がその子孫とともに条件づけられておらず、経路に含まれる矢印が$\mathbf{y}$でhead-to-head</p>
<p>(1)について、(1)から$\mathbf{x}$への経路は、$\mathbf{x}$の親ノードが(a)に当てはまり(head-to-tail)、遮断されている。</p>
<p>(2)について、(2)から$\mathbf{x}$への経路は、$\mathbf{x}$の親ノードが(a)に当てはまり(tail-to-tail)、遮断されている。</p>
<p>(3)について、$\mathbf{x}$から(3)への経路は、$\mathbf{x}$の子ノードが(a)に当てはまり(tail-to-tail)、遮断されている。</p>
<p>(4)について、(4)から$\mathbf{x}$への経路は、$\mathbf{x}$の共同親ノードが(a)に当てはまり(head-to-tail)、遮断されている。</p>
<p>(5)について、(5)から$\mathbf{x}$への経路は、$\mathbf{x}$の共同親ノードが(a)に当てはまり(tail-to-tail)、遮断されている。</p>
<p>以上により示された。</p>
<h2 id="演習-810"><a class="header" href="#演習-810">演習 8.10</a></h2>
<div class="panel-primary">
<p>すべての変数が観測されていない図8.54に示される有向グラフを考える．$a \perp !!! \perp b \mid \emptyset$を示せ．今，$d$を観測したとする．一般に$a \not\perp !!! \perp b \mid d$であることを示せ．</p>
<p><img src="https://i.imgur.com/TYGV2ye.png" alt="" /></p>
</div>
<p>まず、図8.54のグラフより、
\begin{align}
p(a, b, c, d) = p(a)p(b)p(c|a, b)p(d|c).
\end{align}
上式を$c$と$d$に関して周辺化することで、
\begin{align}
p(a, b) &amp;= p(a)p(b)\sum_{c}\sum_{d}p(c|a, b)p(d|c) \
&amp;= p(a)p(b)\sum_{c}p(c|a, b)\sum_{d}p(d|c) \
&amp;= p(a)p(b) \times 1 \times 1 \
&amp;= p(a)p(b)
\end{align}
となるので、$a \perp !!! \perp b \mid \emptyset$。
次に、
\begin{align}
p(a, b|d) &amp;= \frac{\sum_{c} p(a, b, c, d)}{\sum_{a}\sum_{b}\sum_{c} p(a, b, c, d)} \
&amp;= \frac{p(d|a, b)p(a)p(b)}{p(d)} \
&amp;= \frac{p(d|a, b)p(d)}{p(d)} \frac{p(a)p(b)}{p(d)}.
\end{align}
しかし、
\begin{align}
p(a|d)(b|d) &amp;= \frac{p(a, d)}{p(d)} \frac{p(b, d)}{p(d)} \
&amp;= \frac{p(d|a)p(a)}{p(d)} \frac{p(d|b)p(b)}{p(d)} \
&amp;= \frac{p(d|a)p(d|b)}{p(d)} \frac{p(a)p(b)}{p(d)} \
&amp;\neq p(a, b|d)
\end{align}
なので、$a \not\perp !!! \perp b \mid d$。</p>
<h2 id="演習-811"><a class="header" href="#演習-811">演習 8.11</a></h2>
<div class="panel-primary">
<p><img src="https://i.imgur.com/oMm0sit.png" alt="" />
図8.21 に示される車の燃料装置の例を考える．燃料計$G$の状態を直接観測する代わりに，燃料計が運転手$D$によって観測され，彼が燃料計の読みを我々に報告すると仮定する．この報告は 燃料計が満タンを指している$D = 1$かあるいは空を指している$D = 0$かのいずれかである．この運転手はいささか信頼性に欠け，以下の確率に従うとする．</p>
<p>$$
p(D=1 \mid G=1)=0.9 \tag{8.105}
$$</p>
<p>$$
p(D=0 \mid G=0)=0.9 \tag{8.106}
$$</p>
<p>今運転手が燃料計が空を指していることを報告したとする．すなわち我々は$D=0$を観測した．この観測値だけが与えられたときの，タンクが空である確率を求めよ．同様に，バッテリが切れているという観測も得られたときのタンクが空である確率を求め，後者の確率の方が低いことを確認せよ．この結果の直感的解釈について議論し，図8.54との関係を説明せよ．</p>
<p><img src="https://i.imgur.com/TYGV2ye.png" alt="" /></p>
</div>
<p>教科書P.89の問題設定から</p>
<p>$$
\begin{array}{l}p(G=1 \mid B=1, F=1)=0.8 \ p(G=1 \mid B=1, F=0)=0.2 \ p(G=1 \mid B=0, F=1)=0.2 \ p(G=1 \mid B=0, F=0)=0.1 .\end{array} \hspace{1em}
\begin{array}{l}p(G=0 \mid B=1, F=1)=0.2 \ p(G=0 \mid B=1, F=0)=0.8 \ p(G=0 \mid B=0, F=1)=0.8 \ p(G=0 \mid B=0, F=0)=0.9 .\end{array}
$$</p>
<p>である。また$p(D=0\mid G=1) = 0.1, p(D=0 \mid G=0) = 0.9$である。</p>
<p>求めたいのは$p(F=0 \mid D=0)$なので、式変形をすると</p>
<p>$$
p(F=0 \mid D=0)=\frac{p(D=0 \mid F=0) p(F=0)}{p(D=0)}=\frac{\sum_{G \in {0,1}} p(D=0, G \mid F=0) p(F=0)}{p(D=0)} \tag{*}
$$</p>
<p>である。まず$p(G=0)$を計算する。これは$(8.30)$にも出ているが</p>
<p>$$
\begin{aligned} p(G=0) &amp;=\sum_{B \in{0,1}}\sum_{F \in{0.1}} p(G=0 \mid B, F) p(B) p(F) \
&amp;=p(G=0 \mid B=0, F=0)p(B=0)p(F=0) + p(G=0 \mid B=0, F=1)p(B=0)p(F=1) \
&amp;+ p(G=0 \mid B=1, F=0)p(B=1)p(F=0) + p(G=0 \mid B=1, F=1)p(B=1)p(F=1) \
&amp;=0.9 \times 0.1 \times 0.1+0.8 \times 0.1 \times 0.9+0.8 \times 0.9 \times 0.1+0.2 \times 0.9 \times 0.9 \ &amp;=0.315 \end{aligned}
$$</p>
<p>であり、$p(G=1) = 1-0.315 = 0.685$が求まる。</p>
<p>$(*)$式の分母を計算すると</p>
<p>$$
\begin{aligned}
p(D=0) &amp;=\sum_{G \in{0,1}} p(D=0 \mid G) p(G) \ &amp;=0.9 \times 0.315+0.1 \times 0.685=0.352
\end{aligned}
$$</p>
<p>となる。次に$\sum_{G \in {0,1}} p(D=0, G \mid F=0)$を計算する。</p>
<p>$$
\begin{aligned}
\sum_{G \in[0,1]} p(D=0, G \mid F=0) &amp;=\sum_{G \in{0,1}} p(D=0 \mid G, F=0) p(G \mid F=0) \
&amp;=\sum_{G \in{0,1}} p(D=0 \mid G) p(G \mid F=0) \
&amp;=0.9 \times 0.81+0.1 \times 0.19=0.748
\end{aligned}
$$</p>
<p>ここで問題設定から$D$は$G$のみに依存しているので$p(D=0\mid G, F=0) = p(D=0 \mid G)$であることを用いた。よってこれらの値を用いることで、$(*)$式は</p>
<p>$$
p(F=0\mid D=0) =\frac{0.748 \times 0.1}{0.352} = 0.2125
$$</p>
<p>次に、$B=0$が観測されたときの求める確率は</p>
<p>$$
\begin{aligned}
p(F=0 \mid D=0, B=0) &amp;=\frac{p(D=0, B=0, F=0)}{p(D=0, B=0)} \ &amp;=\frac{\sum_{G} p(D=0, B=0, F=0, G)}{\sum_{G} p(D=0, B=0, G)} \ &amp;=\frac{\sum_{G} p(B=0, F=0, G) p(D=0 \mid B=0, F=0, G)}{\sum_{G} p(B=0, G) p(D=0 \mid B=0, G)} \ &amp;=\frac{\sum_{G} p(B=0, F=0, G) p(D=0 \mid G)}{\sum_{G} p(B=0, G) p(D=0 \mid G)}
\end{aligned}\tag{**}
$$</p>
<p>これを計算する。分子の$p(B=0, F=0, G)$について</p>
<p>$$
\begin{aligned} p(F=0, B=0, G=0) &amp;=p(G=0 \mid B=0, F=0) p(B=0, F=0) \ &amp;=0.9 \times p(B=0) \times p(F=0) \ &amp;=0.9 \times 0.1 \times 0.1=0.009 \
p(F=0, B=0, G=1) &amp;=0.1 \times p(B=0) \times p(F=0) \ &amp;=0.001 \end{aligned}
$$</p>
<p>分母の$p(B=0, G)$について</p>
<p>$$
\begin{aligned} p(B=0, G=0) &amp;=\sum_{F} p(B=0, G=0, F) \
&amp;=p(F=0, B=0, G=0)+p(F=1, B=0, G=0) \
&amp;=0.009+p(G=0 \mid B=0, F=1) p(B=0) p(F=1) \
&amp;=0.009+0.8 \times 0.1 \times 0.9 \ &amp;=0.081 \
p(B=0, G=1) &amp;=p(F=0, B=0, G=1)+p(F=1, B=0, G=1) \ &amp;=0.1 \times 0.1 \times 0.1+0.2 \times 0.1 \times 0.9 \ &amp;=0.001+0.018=0.019
\end{aligned}
$$</p>
<p>以上の計算から</p>
<p>$$
\begin{aligned} p(F=0 \mid D=0, B=0) &amp;=\frac{\sum_{G} p(D=0 \mid G) p(F=0, B=0, G)}{\sum_{G} p(D=0 \mid G) p(B=0, G)} \ &amp;=\frac{0.9 \times 0.009+0.1 \times 0.001}{0.9 \times 0.081+0.1 \times 0.019} \ &amp;=\frac{9 \times 9+1 \times 1}{9 \times 81+1 \times 19}=\frac{41}{374}=0.1096 \cdots \end{aligned}
$$</p>
<p>これらの計算から、$p(F=0 \mid D=0) \gt p(F=0 \mid D=0, B=0)$となっている。すなわちバッテリの状態を確認したことでタンクが空の確率は減少した。この減少はバッテリの状態$B=0$が$G=0$を弁明してしまうため、$F=0$の可能性が低くなる直感と一致する。</p>
<p>また、図8.54と結びつけると、ノード$c$は$G$，ノード$d$は$D$に対応する。</p>
<h2 id="演習-812"><a class="header" href="#演習-812">演習 8.12</a></h2>
<div class="panel-primary">
<p>$M$個の異なる確率変数集合に対して$2^{M(M-1)/2}$個の異なる無向グラフが存在することを示せ．$M=3$の場合における8個の可能なグラフをすべて描け．</p>
</div>
<p>※</p>
<p><img src="https://i.imgur.com/X9gxRKa.png" alt="" /></p>
<h2 id="演習-813"><a class="header" href="#演習-813">演習 8.13</a></h2>
<div class="panel-primary">
<p>反復条件付きモード(ICM) を使って</p>
<p>$$
E(\mathbf{x}, \mathbf{y})=h \sum_{i} x_{i}-\beta \sum_{{i, j}} x_{i} x_{j}-\eta \sum_{i} x_{i} y_{i} \tag{8.42}
$$</p>
<p>で与えられるエネルギー関数を最小化することを考える．すべての他の変数を固定して，ある1つの変数$x_j$に関する2状態のエネルギー値の差を書き下せ．またその表現が，グラフにおける$x_j$の近傍の局所的な量だけに依存することを示せ．</p>
</div>
<p>※PRML下巻pp.102~103を参照。</p>
<p>教科書中の問題設定から、すべての$\mathbf{x}, \mathbf{y}$は${+1, -1}$の2値である。</p>
<p>ある変数$x_k$がエネルギー関数$E(\mathbf{x, y})$に与える影響を考えるために、$(8.42)$式を変形すると</p>
<p>$$
E(\mathbf{x}, \mathbf{y})=h\left(\sum_{i \neq k} x_{i}+x_{k}\right)-\beta\left(\sum_{i\neq k} x_{i} x_{j}+x_{k} \sum_{l} x_{l}\right) - \eta \left( \sum_{i\neq k} x_i y_i + x_k y_k \right)
$$</p>
<p>となる。ここで問題設定から$x_l$は$x_k$に隣接する変数である。</p>
<p>$\mathbf{x,y}$のうち、$x_j$のみ$1, -1$の2状態を考え、残りの変数は固定されているのならば、そのときのエネルギー差は</p>
<p>$$
\begin{aligned}
E(\mathbf{x,y})|<em>{x_j=1} - E(\mathbf{x,y})|</em>{x_j=-1}
&amp;=\left(h-\beta \sum_{l} x_{l}-\eta y_{j}\right)-\left(-h+\beta \sum_{l} x_{l}+\eta y_{j}\right) \
&amp;=2h -2\beta \sum_{l}x_l -2\eta y_j
\end{aligned}
$$</p>
<p>となる。このエネルギー差は確かにグラフにおける$x_j$の近傍の局所的な量だけに依存している。</p>
<h2 id="演習-814"><a class="header" href="#演習-814">演習 8.14</a></h2>
<div class="panel-primary">
<p>エネルギー関数が</p>
<p>$$
E(\mathbf{x}, \mathbf{y})=h \sum_{i} x_{i}-\beta \sum_{{i, j}} x_{i} x_{j}-\eta \sum_{i} x_{i} y_{i} \tag{8.42}
$$</p>
<p>で与えられ，その係数が$\beta = h = 0$である場合を考える．最も確からしい潜在変数の値はすべての$i$に対して$x_i = y_i$であることを示せ．</p>
</div>
<p>エネルギー関数は</p>
<p>$$
E(\mathbf{x}, \mathbf{y}) = -\eta \sum_{i} x_{i} y_{i}
$$</p>
<p>となる。これは$x_i = y_i$であれば$E=-\eta$, $x_i \neq y_i$であれば$E=\eta$となる。</p>
<p>潜在変数$x_i$と観測値$y_i$に対し、エネルギー関数$E$が負になれば$\mathbf{x}$,$\mathbf{y}$の同時分布</p>
<p>$$
p(\mathbf{x}, \mathbf{y})=\frac{1}{Z} \exp {-E(\mathbf{x}, \mathbf{y})} \tag{8.43}
$$</p>
<p>が大きくなり、$\mathbf{y}$が観測されているので$p(\mathbf{x}\mid \mathbf{y}) = p(\mathbf{x}, \mathbf{y})/p(\mathbf{y})$も大きくなる。</p>
<p>すなわち最も確からしい$x_i$の値はすべての$i$に対して$x_i = y_i$であり、これはノイズ付加画像そのものである。</p>
<h2 id="演習-815"><a class="header" href="#演習-815">演習 8.15</a></h2>
<div class="panel-primary">
<p><img src="https://i.imgur.com/F4Kumwm.png" alt="" /></p>
<p>図8.38に示されるグラフにおいて，2 つの隣接ノード上の同時分布$p(x_{n-1}, x_n)$が</p>
<p>$$
p\left(x_{n-1}, x_{n}\right)=\frac{1}{Z} \mu_{\alpha}\left(x_{n-1}\right) \psi_{n-1, n}\left(x_{n-1}, x_{n}\right) \mu_{\beta}\left(x_{n}\right) \tag{8.58}
$$</p>
<p>の形で表現されることを示せ．</p>
</div>
<p>8.4.1節の議論と同様に考えれば良い。求める周辺分布は</p>
<p>$$
p(x_{n-1},x_n) = \sum_{x_1}\sum_{x_1}\cdots\sum_{x_{n-2}}\sum_{x_{n+1}}\cdots\sum_{x_N}p(\mathbf{x})
$$</p>
<p>である。$(8.52)$式と同様にポテンシャル関数を用いると、図8.38に書かれてある2つのメッセージ$\mu_{\alpha}(x_n)$と$\mu_{\beta}(x_n)$を用いて</p>
<p>$$
\begin{aligned}
p\left(x_{n-1}, x_{n}\right) &amp;=\frac{1}{Z}\left[\sum_{x_{n-2}} \psi_{n-2, n-1}\left(x_{n-2}, x_{n-1}\right) \ldots\left[\sum_{x_{2}} \psi_{2,3}\left(x_{2}, x_{3}\right)\left[\sum_{x_{1}} \psi_{1,2}\left(x_{1}, x_{2}\right)\right]\right] \ldots\right] \
&amp; \times \psi_{n-1, n}\left(x_{n-1}, x_{n}\right) \ &amp; \times\left[\sum_{x_{n+1}} \psi_{n, n+1}\left(x_{n}, x_{n+1}\right) \ldots\left[\sum_{x_{N}} \psi_{N-1, N}\left(x_{N-1}, x_{N}\right)\right] \ldots\right] \
&amp;=\frac{1}{Z} \mu_{\alpha}\left(x_{n-1}\right) \psi_{n-1, n}\left(x_{n-1}, x_{n}\right) \mu_{\beta}\left(x_{n}\right)
\end{aligned}
$$</p>
<p>と書ける。</p>
<h2 id="演習-816"><a class="header" href="#演習-816">演習 8.16</a></h2>
<div class="panel-primary">
<p><img src="https://i.imgur.com/F4Kumwm.png" alt="" /></p>
<p>図8.38に示されるグラフにおいて，すべてのノード$n\in {1, \ldots ,N-1}$に対して$p(x_n \mid x_N)$を計算する推論問題を考える．この問題を効率的に解くために8.4.1 節で議論したメッセージパッシングアルゴリズムが利用できることを示し，どのメッセージがどのように修正されるかについて議論せよ．</p>
</div>
<p>教科書p.112の上段に記載のとおり、ポテンシャル関数に$I(x_N,\hat{x}_N)$を掛けるだけで良い（$\hat{x}_N$は変数$x_N$の観測値）。</p>
<p>$\mu_\alpha (x_n)$の再帰式は教科書の議論と同じ。</p>
<p>$\mu_\beta (x_n)$の再帰式は、初期条件を通常の</p>
<p>$$
\begin{aligned}
\mu_\beta (x_{N-1}) &amp;= \sum <em>{x_N} \psi</em>{N-1,N}(x_{N-1},x_N)
\end{aligned}
$$</p>
<p>に代えて、</p>
<p>$$
\begin{aligned}
\mu_\beta (x_{N-1}) &amp;= \sum <em>{x_N} I(x_N,\hat
x_N)\psi</em>{N-1,N}(x_{N-1},x_N)\
&amp;= \psi_{N-1,N}(x_{N-1},\hat{x}_N)
\end{aligned}
$$</p>
<p>とすれば良い。</p>
<h2 id="演習-817"><a class="header" href="#演習-817">演習 8.17</a></h2>
<div class="panel-primary">
<p><img src="https://i.imgur.com/F4Kumwm.png" alt="" /></p>
<p>図8.38に示される形の$N=5$ノードのグラフを考える．ただし$x_3$および$x_5$は観測されているとする．有向分離性を使って$x_2 \perp !!! \perp x_5 \mid x_3$を示せ．8.4.1 節のメッセージパッシングアルゴリズムを$p(x_2 \mid x_3, x_5)$の計算に用いたとき，結果が$x_5$の値に依存しないことを示せ．</p>
</div>
<p>（前半部分）
$x_2$と$x_5$を結ぶ経路は$x_3$が観測された時遮断されるため$x_2 \perp !!! \perp x_5 \mid x_3$である．</p>
<p>（後半部分）</p>
<p>ベイズの定理により
$$
\begin{aligned}
p(x_2 \mid x_3, x_5)&amp;=\frac{p(x_2 , x_3, x_5)}{p(x_3, x_5)}\
&amp;=\frac{\sum_{x_1, x_4}\psi_{1, 2}\psi_{2, 3}\psi_{3, 4}\psi_{4, 5}}{\sum_{x_1,x_2,x_4}\psi_{1, 2}\psi_{2, 3}\psi_{3, 4}\psi_{4, 5}}\
&amp;=\frac{\sum_{x_1}\psi_{1, 2}\psi_{2, 3}\sum_{x_4}\psi_{3, 4}\psi_{4, 5}}{\sum_{x_1,x_2}\psi_{1, 2}\psi_{2, 3}\sum_{x_4}\psi_{3, 4}\psi_{4, 5}}\
&amp;=\frac{\sum_{x_1}\psi_{1, 2}\psi_{2, 3}}{\sum_{x_1,x_2}\psi_{1, 2}\psi_{2, 3}}
\end{aligned}
$$</p>
<h2 id="演習-818"><a class="header" href="#演習-818">演習 8.18</a></h2>
<div class="panel-primary">
<p>有向木によって表現される分布が，対応する無向木上の等価な分布によって（自明に）表現されることを示せ．無向木で表現される分布が，クリークポテンシャルを適切に規格化することにより，有向木で表現可能であることも示せ．ある与えられた無向木から構築できる異なる有向木の数を計算せよ．</p>
</div>
<p>$x_1$を親に持ち、$x_2$を子に持つような木の一部分を抜き出して考える。ポテンシャル関数
$$
\psi_{2,1} = p(x_2 \mid x_1)
$$
とすることで自明に無向木に変換できる(木では親が一つのためモラル化の必要はない)
また、無向木から有向木への変換は、
$$
p(x_2 \mid x_1) = \frac{\psi_{2,1}}{\sum <em>{x_2} \psi</em>{2,1}}
$$
として規格化すればよい。その際、有向木の作り方は根の選び方に対応する$N$通り(要素数の数)である。</p>
<h2 id="演習-819"><a class="header" href="#演習-819">演習 8.19</a></h2>
<div class="panel-primary">
<p>8.4.4 節において導出した積和アルゴリズムを，8.4.1 節において議論したノードの連鎖モデルに適用し，結果</p>
<p>$$
p\left(x_{n}\right)=\frac{1}{Z} \mu_{\alpha}\left(x_{n}\right) \mu_{\beta}\left(x_{n}\right) \tag{8.54}
$$
$$
\begin{aligned} \mu_{\alpha}\left(x_{n}\right) &amp;=\sum_{x_{n-1}} \psi_{n-1, n}\left(x_{n-1}, x_{n}\right)\left[\sum_{x_{n-2}} \cdots\right] \ &amp;=\sum_{x_{n-1}} \psi_{n-1, n}\left(x_{n-1}, x_{n}\right) \mu_{\alpha}\left(x_{n-1}\right) \end{aligned} \tag{8.55}
$$
および
$$
\begin{aligned} \mu_{\beta}\left(x_{n}\right) &amp;=\sum_{x_{n+1}} \psi_{n, n+1}\left(x_{n}, x_{n+1}\right)\left[\sum_{x_{n+2}} \cdots\right] \ &amp;=\sum_{x_{n+1}} \psi_{n, n+1}\left(x_{n}, x_{n+1}\right) \mu_{\beta}\left(x_{n+1}\right) \end{aligned} \tag{8.57}
$$
が特別な場合として得られることを示せ．</p>
</div>
<p>規格化項$Z$を因子$f$などに包含させて、$\psi$を$f$に対応させれば、それぞれ</p>
<p>$$
\begin{aligned} \mu_{x_{m} \rightarrow f_{s}}\left(x_{m}\right) &amp;=\prod_{l \in \operatorname{ne}\left(x_{m}\right) \backslash f_{s}}\left[\sum_{X_{l m}} F_{l}\left(x_{m}, X_{l m}\right)\right] \ &amp;=\prod_{l \in \operatorname{ne}\left(x_{m}\right) \backslash f_{s}} \mu_{f_{l} \rightarrow x_{m}}\left(x_{m}\right) \end{aligned} \tag{8.69}
$$</p>
<p>$$
p\left(\mathbf{x}<em>{s}\right)=f</em>{s}\left(\mathbf{x}<em>{s}\right) \prod</em>{i \in \operatorname{ne}\left(f_{s}\right)} \mu_{x_{i} \rightarrow f_{s}}\left(x_{i}\right) \tag{8.72}
$$</p>
<p>の特殊な場合(直鎖状に限定したもの)とみなせる。</p>
<h2 id="演習-820"><a class="header" href="#演習-820">演習 8.20</a></h2>
<div class="panel-primary">
<p>木構造因子グラフにおける積和アルゴリズムのメッセージパッシングの手続きについて考える．メッセージはまずすべての葉ノードから任意に選ばれた根ノードに向かって伝播され，その後根ノードから葉ノードヘと伝播される各ステップにおいて，メッセージを送るべきノードが，そのメッセージを計算するために必要なすべてのメッセージをすでに受け取っているようにメッセージパッシングのスケジュールを組むことが可能であることを，帰納法を用いて示せ．</p>
</div>
<p>ノードが1個の時はあきらかに成り立つ．
ノードが$N$個のとき題意を満たすスケジュールを組むことができると仮定する．題意を満たすスケジュールを考えたとき$N$個のノードには根が存在するため増やすノードは葉ノードとなる．葉ノードを増やすときすべてのメッセージが伝播されるようにスケジュールを組むことができる木構造因子グラフのノードに付け加えるので$N+1$個の時も題意を満たすようなスケジュールを組むことが可能である．</p>
<p>以上により帰納法から任意のNについて題意を満たすスケジュールを組むことができる．</p>
<h2 id="演習-821"><a class="header" href="#演習-821">演習 8.21</a></h2>
<div class="panel-primary">
<p>因子グラフにおいて，積和メッセージパッシングアルゴリズムを実行した後，</p>
<p>$$
p\left(\mathbf{x}<em>{s}\right)=f</em>{s}\left(\mathbf{x}<em>{s}\right) \prod</em>{i \in \operatorname{ne}\left(f_{s}\right)} \mu_{x_{i} \rightarrow f_{s}}\left(x_{i}\right) \tag{8.72}
$$</p>
<p>を適用することにより，各因子$f_s(\mathbf{x}_s)$に関連する変数$\mathbf{x}_s$全体上の周辺分布$p(\mathbf{x}_s)$が計算できることを示せ．</p>
</div>
<p>因子$f_s$と繋がっている変数の組$\mathbf{x}_s$上の周辺分布を計算する。
（教科書の議論では$\mathbf{x}_s$の構成要素のうち、根ノード側の変数${ x }$と葉ノード側の変数${ x_1, \cdots x_M }$を区別したが、今回は$\mathbf{x}_s$のすべての要素について横並びの議論をするので、その必要はない。）</p>
<p>$(8.61)$式に変えて、周辺分布は、</p>
<p>$$
\begin{aligned}
p(\mathbf{x}<em>s)
=\sum</em>{\mathbf{x}\backslash{\mathbf{x}_s}}
p(\mathbf{x})
\end{aligned}
$$</p>
<p>となる。</p>
<p><img src="https://i.imgur.com/7tmg1xL.jpg" alt="" /></p>
<p>上図にて、$x_i$に隣接する因子（$f_s$を除く）の積は、
$$
\begin{aligned}
\prod_{j \in \mathrm{ne} (x_i) \backslash f_s}
F_{ij} (x_i, X_{ij})
\end{aligned}
$$
なので、同時分布$p(\mathbf{x})$は、
$$
\begin{aligned}
p(\mathbf{x} ) = f_s(\mathbf{x} <em>s)
\prod</em>{i \in \mathrm{ne} (f_s) }
\prod_{j \in \mathrm{ne} (x_i) \backslash f_s}
F_{ij} (x_i, X_{ij})
\end{aligned}
$$
と書ける。従って、
$$
\begin{aligned}
p(\mathbf{x}<em>s)
&amp;=\sum</em>{\mathbf{x}\backslash{\mathbf{x}<em>s}}
f_s(\mathbf{x} <em>s)
\prod</em>{i \in \mathrm{ne} (f_s) }
\prod</em>{j \in \mathrm{ne} (x_i) \backslash f_s}
F_{ij} (x_i, X_{ij})　\
&amp;=f_s(\mathbf{x} <em>s)
\prod</em>{i \in \mathrm{ne} (f_s) }
\sum_{\mathbf{x}\backslash{\mathbf{x}<em>s}}
\prod</em>{j \in \mathrm{ne} (x_i) \backslash f_s}
F_{ij} (x_i, X_{ij})　\
&amp;=f_s(\mathbf{x} <em>s)
\prod</em>{i \in \mathrm{ne} (f_s) }
\mu _{x_i \rightarrow f_s} (x_i)
\end{aligned}
$$
と題意の周辺分布が計算された。</p>
<p>なお、１行目から２行目への式変形で、変数$x_i$での和が$i$番目以外のツリー内の因子に影響を与えないこと、すなわち
$$
\begin{aligned}
&amp;\sum_{x_1}\sum_{x_2}\cdots \left( F_{11}F_{12}F_{13}F_{21}F_{22}\cdots \right)\
=&amp; \left( \sum_{x_1} F_{11} F_{12} F_{13} \right)
\left( \sum_{x_2} F_{21} F_{22} \right) \cdots
\end{aligned}
$$
を用いた。また、2行目から3行目の式変形で、教科書と同様に
$$
\begin{aligned}
\mu <em>{x_i \rightarrow f_s} (x_i)
=\sum</em>{\mathbf{x}\backslash{\mathbf{x}<em>s}}
\prod</em>{j \in \mathrm{ne} (x_i) \backslash f_s}
F_{ij} (x_i, X_{ij})　
\end{aligned}
$$
と定義した。</p>
<h2 id="演習-822"><a class="header" href="#演習-822">演習 8.22</a></h2>
<div class="panel-primary">
<p>木構造因子グラフを考える．連結部分グラフであるような変数ノードの部分集合が与えられたとする．（すなわち，その部分集合の任意の変数ノードが，少なくとも他の1つの変数ノードに1つの因子ノードを通して連結されているとする．）積和アルゴリズムを利用して，その部分集合上の周辺分布を計算する方法を示せ．</p>
</div>
<p>与えられた連結部分グラフであるような変数ノードの部分集合を$\mathbf{x}_a$とする。残りの変数ノードを$\mathbf{x}_b$とする。</p>
<p>定義$(8.61)$から、$\mathbf{x}_a$についての周辺分布は$\mathbf{x}_a$を除くすべての変数、すなわち$\mathbf{x}_b$についての同時分布の和を取れば良い。</p>
<p>$$
p(\mathbf{x}<em>a) = \sum</em>{\mathbf{x}_b}p(\mathbf{x})
$$</p>
<p>また、因子グラフを使った同時分布$p(\mathbf{x})$は$(8.59)$や$(8.60)$のように、連結されている変数ノードと因子の積で表現される。</p>
<p>$$
p(\mathbf{x}) =\prod_{s}f_s(\mathbf{x}_s)
$$</p>
<p>この2式から</p>
<p>$$
p(\mathbf{x}<em>a) = \sum</em>{\mathbf{x}<em>b}\prod</em>{s}f_s(\mathbf{x}_s)
$$</p>
<p>となる。ここで、この$f_s(\mathbf{x}_s)$は2つのパターンで構成される。すなわち、</p>
<ol>
<li>$\mathbf{x}_b$に属する変数ノードとリンクせず、$\mathbf{x}_a$に属する変数ノード間でのみリンクしている因子ノード$f_s^{'}$</li>
<li>$\mathbf{x}_a$に含まれる変数ノードと$\mathbf{x}_b$に含まれる変数ノード間のリンクを1つ以上持つ因子ノード$f_s^{''}$</li>
</ol>
<p>である。注意すべき点として、1の方は$\sum_{\mathbf{x}_b}$と入れ替えることができないのでメッセージ$\mu$の形で書くことはできず、純粋に$f^{'}_s$の積が残ることになる。2のうち$\mathbf{x}_b$が関わる場合は積と和を交換することでメッセージとして表現可能である。</p>
<p>$f_s$を$\mathbf{x}_a$と$\mathbf{x}<em>b$間に存在する因子ノード、$x_s$を$f_s$と接続している変数ノード（$x_s \in \mathbf{x}<em>a$）、$f</em>{s_a}$を$\mathbf{x}<em>a$に属する変数ノード間にのみ存在する因子ノード、$\mathbf{x}</em>{s_a}$を$f</em>{s_a}$と接続している変数ノードの部分集合であるとする。この記法を用いて、1と2の場合の積で表現して</p>
<p>$$
\begin{aligned}
p\left(\mathbf{x}<em>{a}\right) &amp;=\prod</em>{s_{a}} f_{s_{a}}\left(\mathbf{x}<em>{s</em>{a}}\right) \prod_{s \in \operatorname{ne} \mathbf{x}<em>{a}} \mu</em>{f_{s} \rightarrow x_{s}}\left(x_{s}\right)
\end{aligned}
$$</p>
<p>と書ける。</p>
<hr>
<img src="https://i.imgur.com/Sor4lG6.png" width="100%">
<p>この図を例にすると、$\mathbf{x}<em>a={x_2, x_8, x_9}$としたとき、$f</em>{s_a} = f_6, \mathbf{x}_{s_a}={x_8, x_9}$となる。</p>
<p>$p(\mathbf{x}) = f_1(x_1, x_2)f_2(x_2, x_3, x_6)f_3(x_2,x_5,x_8)f_4(x_4, x_5)f_5(x_5,x_7)f_6(x_8,x_9)f_7(x_9, x_{10})$であるから、</p>
<p>$$
\begin{aligned}
p(\mathbf{x}<em>a) &amp;= p(x_2, x_8, x_9) \
&amp;= \sum</em>{x_1,x_3,x_6,x_4,x_5,x_7,x_{10}}p(\mathbf{x}) \
&amp;=f_6(x_8,x_9)\mu_{f_1 \rightarrow x_2}(x_2)\mu_{f_2 \rightarrow x_3}(x_3)\mu_{x_5 \rightarrow f_3}(x_5)\mu_{f_7 \rightarrow x_9}(x_9) \
&amp;=f_6(x_8,x_9)\mu_{f_1 \rightarrow x_2}(x_2)\mu_{f_2 \rightarrow x_3}(x_3)\left[ \mu_{f_4 \rightarrow x_5}(x_5)\mu_{f_5 \rightarrow x_5}(x_5) \right]\mu_{f_7 \rightarrow x_9}(x_9) \
&amp;= f_6(x_8,x_9)\prod_{s \in \operatorname{ne} \mathbf{x}<em>{a}} \mu</em>{f_{s} \rightarrow x_{s}}\left(x_{s}\right)
\end{aligned}
$$</p>
<p>のように書ける。</p>
<h2 id="演習-823"><a class="header" href="#演習-823">演習 8.23</a></h2>
<div class="panel-primary">
<p>8.4.4 節において，因子グラフの変数ノード$x_i$上の周辺分布$p(x_i)$が，隣接因子ノードからこのノードに伝わるメッセージの積として</p>
<p>$$
\begin{aligned} p(x) &amp;=\prod_{s \in \operatorname{ne}(x)}\left[\sum_{X_{s}} F_{s}\left(x, X_{s}\right)\right] \ &amp;=\prod_{s \in \operatorname{ne}(x)} \mu_{f_{s} \rightarrow x}(x) \end{aligned} \tag{8.63}
$$</p>
<p>の形で与えられることを示した．$x_i$に接続されるリンクを1つ選んだとする．周辺分布$p(x_i)$はこの1つのリンクに沿って入ってくるメッセージと同じリンクに沿って出ていくメッセージとの積として書くこともできることを示せ．</p>
</div>
<p>$$
\begin{aligned}
p(x_i) &amp;= \prod_{s \in \operatorname{ne}(x_i)} \mu_{f_{s} \rightarrow x_i}(x_i) \ \ (\because \mathrm{Eq.} 8.63) \
&amp;= \mu_{f_{s} \rightarrow x_i}(x_i) \prod_{t \in \operatorname{ne}(x_i)\setminus f_s} \mu_{f_{t} \rightarrow x_i}(x_i) \
&amp;= \mu_{f_{s} \rightarrow x_i}(x_i) \mu_{x_i \rightarrow f_t}(x_i). \ \ (\because \mathrm{Eq.} 8.69)
\end{aligned}
$$</p>
<h2 id="演習-824"><a class="header" href="#演習-824">演習 8.24</a></h2>
<div class="panel-primary">
<p>木構造因子グラフにおいて，積和メッセージパッシングアルゴリズムを実行したとする．因子$f_s(\mathbf{x}_s)$の変数$\mathbf{x}_s$上の周辺分布が</p>
<p>$$
p\left(\mathbf{x}<em>{s}\right)=f</em>{s}\left(\mathbf{x}<em>{s}\right) \prod</em>{i \in \operatorname{ne}\left(f_{s}\right)} \mu_{x_{i} \rightarrow f_{s}}\left(x_{i}\right) \tag{8.72}
$$</p>
<p>の形に書けることを示せ．これは，この因子ノードに接続されるすべてのリンクに沿って入ってきたメッセージの積に，局所的な因子$f_{s}(\mathbf{x}_s)$を掛けたものである．</p>
</div>
<p>演習8.21と同様</p>
<h2 id="演習-825"><a class="header" href="#演習-825">演習 8.25</a></h2>
<div class="panel-primary">
<p><img src="https://i.imgur.com/nKLqMUL.png" alt="" /></p>
<p>図8.51 のグラフにおいて，ノード$x_3$を根ノードとして積和アルゴリズムを実行すると$x_2$上の正しい周辺分布が得られる．このことは</p>
<p>$$
\begin{aligned} \widetilde{p}\left(x_{2}\right) &amp;=\mu_{f_{a} \rightarrow x_{2}}\left(x_{2}\right) \mu_{f_{b} \rightarrow x_{2}}\left(x_{2}\right) \mu_{f_{c} \rightarrow x_{2}}\left(x_{2}\right) \ &amp;=\left[\sum_{x_{1}} f_{a}\left(x_{1}, x_{2}\right)\right]\left[\sum_{x_{3}} f_{b}\left(x_{2}, x_{3}\right)\right]\left[\sum_{x_{4}} f_{c}\left(x_{2}, x_{4}\right)\right] \ &amp;=\sum_{x_{1}} \sum_{x_{3}} \sum_{x_{4}} f_{a}\left(x_{1}, x_{2}\right) f_{b}\left(x_{2}, x_{3}\right) f_{c}\left(x_{2}, x_{4}\right) \ &amp;=\sum_{x_{1}} \sum_{x_{3}} \sum_{x_{4}} \widetilde{p}(\mathrm{x}) \end{aligned} \tag{8.86}
$$</p>
<p>において確かめられた．では，$x_1$および$x_3$についても正しい周辺分布が得られることを示せ．同様に，このグラフにおいて積和アルゴリズムを実行した後，</p>
<p>$$
p\left(\mathbf{x}<em>{s}\right)=f</em>{s}\left(\mathbf{x}<em>{s}\right) \prod</em>{i \in \operatorname{ne}\left(f_{s}\right)} \mu_{x_{i} \rightarrow f_{s}}\left(x_{i}\right) \tag{8.72}
$$</p>
<p>の結果を適用すれば，$x_1$および$x_2$上の正しい同時分布が得られることを示せ．</p>
</div>
<p>p. 124と同様に進める。$x_1$について新たに必要なメッセージシークエンスを求めると、</p>
<p>$$
\begin{aligned}
&amp;\mu_{f_a \rightarrow x_1}(x_1) = \sum_{x_2} f_a(x_1, x_2) \mu_{x_2 \rightarrow f_a} (x_2) \
&amp;\mu_{x_2 \rightarrow f_a} (x_2) = \mu_{f_b \rightarrow x_2}(x_2) \mu_{f_c \rightarrow x_2}(x_2) \
&amp;\mu_{f_b \rightarrow x_2}(x_2) = \sum_{x_3} f_b(x_2, x_3)
\end{aligned}
$$</p>
<p>になる。これを代入して、</p>
<p>$$
\begin{aligned}
\tilde{p}(x_1) &amp;= \mu_{f_a \rightarrow x_1}(x_1) \
&amp;= \sum_{x_2} f_a(x_1, x_2) \mu_{x_2 \rightarrow f_a} (x_2)\
&amp;= \sum_{x_2} f_a(x_1, x_2) \mu_{f_b \rightarrow x_2}(x_2) \mu_{f_c \rightarrow x_2}(x_2) \
&amp;=\sum_{x_2} f_a(x_1, x_2) \sum_{x_3} f_b(x_2, x_3) \sum_{x_4} f_b(x_2, x_4)\
&amp;= \sum_{x_2}\sum_{x_3}\sum_{x_4} f_a(x_1, x_2)  f_b(x_2, x_3)  f_b(x_2, x_4)\
&amp;=  \sum_{x_2}\sum_{x_3}\sum_{x_4} \tilde{p}(\mathbf{x})&amp;(\because (8.73))
\end{aligned}
$$</p>
<p>また、$x_3$について、同様に新たに必要なメッセージシークエンスを求めて代入すると、</p>
<p>$$
\begin{aligned}
\tilde{p}(x_3) &amp;= \mu_{f_b \rightarrow x_3}(x_3) \
&amp;= \sum_{x_2} f_b(x_3, x_2) \mu_{x_2 \rightarrow f_b} (x_2)\
&amp;= \sum_{x_2} f_b(x_3, x_2) \mu_{f_a \rightarrow x_2}(x_2) \mu_{f_c \rightarrow x_2}(x_2) \
&amp;=\sum_{x_2} f_b(x_3, x_2) \sum_{x_1} f_a(x_1, x_2) \sum_{x_4} f_c(x_2, x_4)\
&amp;= \sum_{x_1}\sum_{x_2}\sum_{x_4} f_a(x_1, x_2)  f_b(x_2, x_3)  f_c(x_2, x_4)\
&amp;=  \sum_{x_1}\sum_{x_2}\sum_{x_4} \tilde{p}(\mathbf{x})
\end{aligned}
$$</p>
<p>になる。</p>
<p>次に$x_1$と$x_2$の同時分布を求める。(8.72)と$\tilde{p}(x_1)$導出過程から、</p>
<p>$$
\begin{aligned}
p(x_1, x_2) &amp;= f_a(x_1, x_2)\mu_{x_2 \rightarrow f_a}(x_2) \
&amp;= f_a(x_1, x_2)\mu_{x_2 \rightarrow f_a} (x_2)\
&amp;= f_a(x_1, x_2) \sum_{x_3} f_b(x_2, x_3) \sum_{x_4} f_b(x_2, x_4) \
&amp;= \sum_{x_3}  \sum_{x_4}\tilde{p}(\mathbf{x})
\end{aligned}
$$</p>
<h2 id="演習-826"><a class="header" href="#演習-826">演習 8.26</a></h2>
<div class="panel-primary">
<p>離散変数上の木構造因子グラフを考える．共通の因子に属さない2つの変数$x_a$および$x_b$上の同時分布$p(x_a,x_b)$を計算したいとする．積和アルゴリズムを使ってこの同時分布を計算する手順を示せ．ただしその手順では，それらの変数のうちの1つが，取り得る値のうちのそれぞれに各ステップで固定される．</p>
</div>
<p>(8.72)を繰り返し用いて、$x_a,x_b$が共通する因子を持つようにすれば良い。</p>
<p>参考：https://tips-memo.com/prml-8-26</p>
<h2 id="演習-827"><a class="header" href="#演習-827">演習 8.27</a></h2>
<div class="panel-primary">
<p>2つの3状態離散変数$x$および$y$を考える．例えば$x,y \in {0, 1, 2}$である．これらの変数上の同時分布$p(x,y)$であって，周辺分布$p(x)$を最大にする値$\hat{x}$と周辺分布$p(y)$を最大にする値$\hat{y}$とを組み合わせると，同時分布の確率が$0$となる（すなわち$p(\hat{x}, \hat{y})= 0$となる）ようなものを作れ．</p>
</div>
<p><img src="https://i.imgur.com/nmRAC1i.png" alt="" /></p>
<h2 id="演習-828"><a class="header" href="#演習-828">演習 8.28</a></h2>
<div class="panel-primary">
<p>8.4.7 節で，因子グラフに対する積和アルゴリズムにおける<strong>保留</strong>メッセージの概念が定義された．グラフが1つ以上の閉路を持つ場合，アルゴリズムをどんなに長く実行しても少なくとも1つの保留メッセージが常に存在することを示せ．</p>
</div>
<p>閉路上のノード又は因子を1つ選びnとする。閉路に沿ってnの隣のノード又は因子をn+1とする。
閉路に沿ってn+1の隣りでnと反対側のノード又は因子をn+2とする。</p>
<p>閉路上のノード又は因子が閉路上のリンクに保留メッセージを持つとする。
何回か保留メッセージを送信した時、閉路上から保留メッセージが消えたとする。</p>
<p>これはノードの因子の上では起こり得ない。
なぜならnがn+1とのリンク上の保留メッセージを送信するとn+1が保留メッセージを持つため
保留メッセージが消えないからである。</p>
<p>閉路上どこのノード又は因子をnとしても良いので
結局、閉路上どのノード又は因子においても閉路上の保留メッセージを送信した後
保留メッセージが消えることはない。</p>
<h2 id="演習-829"><a class="header" href="#演習-829">演習 8.29</a></h2>
<div class="panel-primary">
<p>積和アルゴリズムを（ループのない）木構造因子グラフに対して実行した場合，ある有限個のメッセージが送られた後，保留メッセージが存在しなくなることを示せ．</p>
</div>
<p><img src="https://i.imgur.com/nyLXN4W.png" alt="" /></p>
<p>あるツリーがN個のメッセージを送ったあと保留メッセージがなくなったとする。
このときのメッセージ列を$M_{1}, M_{2}, \cdots, M_{N}$とする。</p>
<p>このツリーの任意のノードpに葉ノードoを追加する。新しくできたツリーのメッセージ列は
(1) メッセージ列の先頭にノードo → ノードpのメッセージMoを追加する。
(2) メッセージ列の末尾にノードp → ノードoのメッセージMn+1を追加する。
(3) メッセージ列のうちノードpからo以外の隣接ノードへの送信メッセージMp1・・・Moを含むように変える。</p>
<p>という手順で得ることができる、得られたメッセージは
$M_{0}, M_{1}, M_{2}, \cdots, M_{p 1}, \cdots, M_{N}, M_{N+1}$となり、これを送信したあと保留メッセージは残らない。
よって、有限個のメッセージ送信で保留メッセージがなくなるツリーに葉ノードを1つ付け加えたものは有限個のメッセージで保留メッセージがなくなると言える。</p>
<p>次に、ノード1つからなるツリーは保留メッセージを持たない。
また、全てのツリーはノード1つから初めて1つづつ葉ノードを追加していくことで作ることができる。</p>
<p>以上より全てのツリーは有限個のメッセージを送信したあと保留メッセージが無くなるといえる。</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="prml第9章演習問題解答"><a class="header" href="#prml第9章演習問題解答">PRML第9章演習問題解答</a></h1>
<head>
<style>
  div.panel-primary {
	border: 1px solid #000;
    margin: 10px 5px;
    padding: 16px 10px 0px;
  }
</style>
</head>
<h2 id="演習-91"><a class="header" href="#演習-91">演習 9.1</a></h2>
<div class="panel-primary">
<p>9.1節で議論した<em>K</em>-meansアルゴリズムを考える．離散的な支持変数$r_{nk}$の状態が有限通りしかないこと，そしてそのような状態の各々について，${\boldsymbol{\mu}_k}$のただ1つの最適値があることの帰結として<em>K</em>-meansアルゴリズムは有限回の繰り返しで収束することを示せ．</p>
</div>
<p>※教科書pp.140–142参照。</p>
<p>教科書にあるように、<em>K</em>-meansアルゴリズムはEステップとMステップに分かれている。Eステップではデータ点$\mathbf{x}_n$と$\boldsymbol{\mu}<em>k$は固定されている。各$n$について、$r</em>{nk}=1$としたときに$|\mathbf{x}_n - \boldsymbol{\mu}<em>k|^2$が最小となるような$k\ (k=1\ldots K)$を選んで$r</em>{nk}=1$とし、残りを$0$とする。式で書くと</p>
<p>$$r_{n k}=\left{\begin{array}{ll}1 &amp; k=\arg \min <em>{j}\left|\mathrm{x}</em>{n}-\mu_{j}\right|^{2} \text { のとき } \ 0 &amp; \text { それ以外. }\end{array}\right. \tag{9.2}$$</p>
<p>言い換えると、各$n$について$r_{nk}=1$の選び方は$K$通り存在するので、$N$個のデータ点について言えば全部で$K^N$通りである。一方で、Eステップの操作によって必ず$(9.1)$式の歪み尺度(distortion measure)$J$は小さくなる(図9.2を参照)。</p>
<p>Mステップについては$r_{nk}$を固定した状況下で歪み尺度$J$を最小化する。このとき最適な$\boldsymbol{\mu}<em>k$は$(9.3)$式にしたがって<strong>解析的に</strong>求まる。言い換えれば、前回のMステップから$r</em>{nk}$の割り当てが変わっている場合、Mステップでの$\boldsymbol{\mu}<em>k$の更新操作で必ず$J$は低下するが、Eステップでの$r</em>{nk}$の割り当てが変わらない場合はMステップでの$\boldsymbol{\mu}_k$も変化しない。このとき<em>K</em>-meansアルゴリズムは収束解を得る。</p>
<p>したがって、すべてのデータ点$\mathbf{x}_n$について最適なクラス割り当てが見つかるまでEステップとMステップが繰り返されるわけだが、これは高々$K^N$回の繰り返しの中で必ず収束することとなる。（※ただし、この収束解は大域的な最適解になるとは限らないことに注意。）</p>
<h2 id="演習-92"><a class="header" href="#演習-92">演習 9.2</a></h2>
<div class="panel-primary">
<p>2.3.5節で述べた Robbins-Monroの逐次的推定法を
$$
J=\sum_{n=1}^{N} \sum_{k=1}^{K} r_{n k}\left|\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}\right|^{2} \tag{9.1}
$$
で与えた$J$の$\boldsymbol{\mu}<em>k$による微分で得られる回帰関数の根を求める問題に適用せよ．また，これから各データ点$\mathbf{x}<em>n$について，最も近いプロトタイプ$\boldsymbol{\mu}<em>k$が
$$
\boldsymbol{\mu}</em>{k}^{\text {new }}=\boldsymbol{\mu}</em>{k}^{\text {old }}+\eta</em>{n}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}^{\text {old }}\right) \tag{9.5}
$$
によって更新されるような確率的<em>K</em>-meansアルゴリズムが導かれることを示せ．</p>
</div>
<p>$\frac{\partial J}{\partial \boldsymbol{\mu} <em>k}=0$となるように、$J$に含まれるパラメータ$r</em>{nk}, \boldsymbol{\mu}_k$を最適化する問題を、データ系列の数$N$について逐次的に解く。
データ点${ \mathbf{x}<em>1, \mathbf{x}<em>2,\cdots \mathbf{x}</em>{N-1} }$に基づいてパラメータ${r</em>{nk}, \boldsymbol{\mu}<em>k}</em>{k=1,2,\cdots,K, n=1,2,\cdots, N-1}$が最適化されている（つまり、$\left(\frac{\partial J}{\partial \boldsymbol{\mu} _k}\right)^{(N-1)}=0$となっている）とき、データ点$\mathbf{x}<em>N$を追加で入手した時にパラメータ${r</em>{nk}, \boldsymbol{\mu}<em>k}</em>{k=1,2,\cdots,K, n=1,2,\cdots, N}$を最適化する（つまり、$\left(\frac{\partial J}{\partial \boldsymbol{\mu} _k}\right)^{(N)}=0$とする）ことを考える。</p>
<p>$$
\begin{aligned}
\left( \frac{\partial J}{\partial \boldsymbol{\mu} <em>k}
\right)^{(N)}
&amp;=\frac{\partial}{\partial \boldsymbol{\mu} <em>k}
\left( \sum</em>{n=1}^{N} \sum</em>{k'=1}^{K} r_{n k'}\left|\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k'}\right|^{2} \right) \
&amp;=\sum_{n=1}^{N} \sum_{k'=1}^{K} r_{n k'} (-2) (\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k'}) \delta_{kk'} \
&amp;=-2 \sum_{n=1}^{N} r_{n k} (\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k})\
&amp;=
\left( \frac{\partial J}{\partial \boldsymbol{\mu} <em>k}
\right)^{(N-1)}
-2 r</em>{N k} (\mathbf{x}<em>{N}-\boldsymbol{\mu}</em>{k})
\end{aligned}
$$</p>
<p>$\mathbf{x}<em>{N}$に最も近い${\boldsymbol{\mu}</em>{1},\boldsymbol{\mu}<em>{2}\cdots\boldsymbol{\mu}</em>{K} }$が$\boldsymbol{\mu}<em>{k}$と異なる場合、Eステップにおいて$r</em>{Nk}=0$とする。この場合、Mステップにおいて${\boldsymbol{\mu}<em>{1},\boldsymbol{\mu}</em>{2}\cdots\boldsymbol{\mu}_{K} }$の値は更新されない。</p>
<p>$\mathbf{x}<em>{N}$に最も近い${\boldsymbol{\mu}</em>{1},\boldsymbol{\mu}<em>{2}\cdots\boldsymbol{\mu}</em>{K} }$が$\boldsymbol{\mu}<em>{k}$に一致している場合、Eステップにおいて$r</em>{Nk}=1$とする。この場合、Mステップにおける更新式（上式）は、</p>
<p>$$
\begin{aligned}
\left( \frac{\partial J}{\partial \boldsymbol{\mu} <em>k}
\right)^{(N)}
&amp;=
\left( \frac{\partial J}{\partial \boldsymbol{\mu} <em>k}
\right)^{(N-1)}
-2 (\mathbf{x}</em>{N}-\boldsymbol{\mu}</em>{k})
\end{aligned}
$$</p>
<p>となる。</p>
<p>Robbins-Monroの逐次推定の$(2.129)$式</p>
<p>$$
\theta^{(N)}=\theta^{(N-1)}-a_{N-1} z\left(\theta^{(N-1)}\right)
$$</p>
<p>に代入して、</p>
<p>$$
\begin{aligned}
\boldsymbol{\mu}<em>k ^{(N)}
= \boldsymbol{\mu}<em>k ^{(N-1)} + 2 a</em>{N-1} (\mathbf{x}</em>{N}-\boldsymbol{\mu}_{k}^{(N-1)})
\end{aligned}
$$</p>
<p>となる。
$2 a_{N-1} = \eta_N$と変数を変換すれば、(9.5)式を得る。</p>
<h2 id="演習-93"><a class="header" href="#演習-93">演習 9.3</a></h2>
<div class="panel-primary">
<p>混合ガウスモデルを考え.潜在変数の周辺分布$p(\mathbf{z})$が
$$p(\mathbf{z})=\prod_{k=1}^{K} \pi_{k}^{z_{k}} \tag{9.10}$$
で，観測変数の条件付き分布$p(\mathbf{x}\mid\mathbf{z})$が
$$p(\mathbf{x} \mid \mathbf{z})=\prod_{k=1}^{K} \mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}<em>{k}, \mathbf{\Sigma}</em>{k}\right)^{z_{k}} \tag{9.11}$$
でそれぞれ与えられると仮定する．$p(\mathbf{z})p(\mathbf{x}\mid\mathbf{z})$を$\mathbf{z}$の可能な値について足して得られる周辺分布$p(\mathbf{x})$が
$$p(\mathbf{x})=\sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\mathbf{x} \mid \mu_{k}, \mathbf{\Sigma}_{k}\right) \tag{9.7}$$
の形の混合ガウス分布になることを示せ．</p>
</div>
<p>導入に従ってとけばよい。</p>
<p>$$
\begin{aligned}
p(\mathbf{x}) &amp;= \sum_{\mathbf{z}} p(\mathbf{z})p(\mathbf{x}|\mathbf{z}) \
&amp;=\sum_{\mathbf{z}}\prod_{k} \pi_k^{z_k} \prod_k  N(\mathbf{x}|\mathbf{\mu}<em>k, \mathbf{\Sigma_k})^{z_k} &amp;(\because (9.10), (9.11)))\
&amp;=\sum</em>{\mathbf{z}} \prod_{k} \pi_k^{z_k}  N(\mathbf{x}|\mathbf{\mu}<em>k, \mathbf{\Sigma_k})^{z_k}\
&amp;= \sum</em>{\mathbf{z}}\prod_{k}( \pi_k N(\mathbf{x}|\mathbf{\mu}<em>k, \mathbf{\Sigma_k}))^{z_k}\
&amp;= \sum</em>{k} \pi_k N(\mathbf{x}|\mathbf{\mu}_k, \mathbf{\Sigma_k}) &amp;(\because \text{1-of-K})
\end{aligned}
$$</p>
<h2 id="演習-94"><a class="header" href="#演習-94">演習 9.4</a></h2>
<div class="panel-primary">
<p>潜在変数を持つモデルに関して，データ集合$\mathbf{X}$を観測した下での事後分布$p(\boldsymbol{\theta}\mid \mathbf{X})$を，EMアルゴリスムを用いて$\boldsymbol{\theta}$について最大化する問題を考える．このとき，Eステップは最尤推定問題の場合と同じであるのに対し，Mステップでは，最大化すべき量が$\mathcal{Q}(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text {old }})+\ln p(\boldsymbol{\theta})$で与えられることを示せ．ただし，$\mathcal{Q}(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text {old }})$は
$$\mathcal{Q}\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text {old }}\right)=\sum_{\mathbf{Z}} p\left(\mathbf{Z} \mid \mathbf{X}, \theta^{\text {old }}\right) \ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta}) \tag{9.30}$$
で定義されている．</p>
</div>
<p>ベイズの定理より</p>
<p>$$
p(\mathbf{\theta}|\mathbf{X})=\frac{p(\mathbf{X}|\mathbf{\theta})p(\mathbf{\theta})}{p(\mathbf{X})}
$$</p>
<p>である．対数尤度を取って，</p>
<p>$$
\begin{aligned}
\ln p(\mathbf{\theta}|\mathbf{X})&amp;= \ln p(\mathbf{X}|\mathbf{\theta}) + \ln p(\mathbf{\theta}) + Const\
&amp;=\ln\sum_kp(\mathbf{X},\mathbf{Z}|\theta) + \ln p(\mathbf{\theta}) + Const\
\end{aligned}
$$</p>
<p>ここで第一項については教科書の(9.29), (9.30)の議論と同様に対数の中の総和への処理，および潜在変数$\mathbf{Z}$についての事後確率の期待値を考えるといった工夫をすることで$\mathcal{Q}(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text {old }})$を用いて表すことができて，</p>
<p>$$
\begin{aligned}
\mathcal{Q}(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text {old }}) + \ln p(\mathbf{\theta})
\end{aligned}
$$</p>
<p>を最大化すればいいことがわかる</p>
<h2 id="演習-95"><a class="header" href="#演習-95">演習 9.5</a></h2>
<div class="panel-primary">
<p>図9.6で示すような，混合ガウスモデルの有向グラフを考える．8.2節で議論した有向分離の規準を利用して，潜在変数の事後分布が次式のように各データ点ごとの事後分布の積になることを示せ．</p>
<p>$$p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\mu}, \mathbf{\Sigma}, \boldsymbol{\pi})=\prod_{n=1}^{N} p\left(\mathbf{z}<em>{n} \mid \mathbf{x}</em>{n}, \boldsymbol{\mu}, \mathbf{\Sigma}, \boldsymbol{\pi}\right) \tag{9.80}$$</p>
</div>
<p>まず、図9.9より、$\mathbf{\pi}$と$\mathbf{\mu}$、$\mathbf{\mu}$ と$\mathbf{\Sigma}$、$\mathbf{\Sigma}$と$\mathbf{\pi}$は、tail-to-tailなので、$\mathbf{Z}$と$\mathbf{X}$が観察された時、遮断されている。すなわち条件付き独立が成り立つ。よって、以下では、$\mathbf{X}$と$\mathbf{Z}$について考えていく。</p>
<p>今、図9.9より、$n \in {1, 2, \cdots  N}$について、それぞれ$\mathbf{z}_n \rightarrow \mathbf{x}_n$が成り立つ。また、i.i.dデータ集合であることが仮定されているので、$p(\mathbf{X}) = \prod_n p(\mathbf{x}_n)$である。</p>
<p>以上から、
$$
\begin{aligned}
p(\mathbf{Z}| \mathbf{X}, \boldsymbol{\mu}, \mathbf{\Sigma}, \boldsymbol{\pi}) &amp;=
p(\mathbf{Z}|\mathbf{X}) \
&amp;= \prod_n p(\mathbf{z}<em>n|\mathbf{x}<em>n)
\end{aligned}
$$
が成り立つ。最後に、条件付き独立であるパラメータを加えて、
$$p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\mu}, \mathbf{\Sigma}, \boldsymbol{\pi})=\prod</em>{n=1}^{N} p\left(\mathbf{z}</em>{n} \mid \mathbf{x}_{n}, \boldsymbol{\mu}, \mathbf{\Sigma}, \boldsymbol{\pi}\right) \tag{9.80}$$
が導かれる。</p>
<h2 id="演習-96"><a class="header" href="#演習-96">演習 9.6</a></h2>
<div class="panel-primary">
<p>混合ガウスモデルについて，各混合要素の共分散行列$\mathbf{\Sigma}_k$すべてが共通の値$\mathbf{\Sigma}$に制限された特別な場合を考える．そのようなモデルにおいて，尤度関数を最大化するEM方程式を導け．</p>
</div>
<p>(9.17),(9.18),(9.19),(9.22)に$\mathbf{\Sigma}<em>{k}=\mathbf{\Sigma}</em>{j}=\mathbf{\Sigma}$を代入する。</p>
<p>$$
\begin{aligned}
&amp;N_{k}=\sum_{n=1}^{N} \gamma\left(z_{n k}\right) \
&amp;\left(\gamma\left(z_{n k}\right)=\frac{\pi_{k} \mathcal{N}\left(X_{n} \mid \boldsymbol{\mu}<em>{k}, \mathbf{\Sigma}\right)}{\sum</em>{j} \pi_j \mathcal{N}\left(\mathbf{x}<em>{n} \mid \boldsymbol{\mu}</em>{j}, \mathbf{\Sigma}\right)}\right) \
&amp;\boldsymbol{\mu}<em>{k}=\frac{1}{N</em>{k}} \sum_{n=1}^{N} \gamma\left(z_{n k}\right) \mathbf{x}<em>{n} \
&amp;\mathbf{\Sigma}=\frac{1}{N</em>{k}} \sum_{n=1}^{N} \gamma\left(z_{n k}\right)\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}\right)\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}\right)^{\mathrm{T}} \
&amp;\pi_{k}=\frac{N_{k}}{N}
\end{aligned}
$$</p>
<h2 id="演習-97"><a class="header" href="#演習-97">演習 9.7</a></h2>
<div class="panel-primary">
<p>混合ガウスモデルに関する完全データ対数尤度関数
$$\ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})=\sum_{n=1}^{N} \sum_{k=1}^{K} z_{n k}\left{\ln \pi_{k}+\ln \mathcal{N}\left(\mathbf{x}<em>{n} \mid \boldsymbol{\mu}</em>{k}, \mathbf{\Sigma}_{k}\right)\right} \tag{9.36}$$
を最大化することは，各混合要素について独立に平均と共分散をその混合要素に対応するデータ点集合にあてはめ，かつ混合係数を各グループに属するデータの数の割合に一致させるという結果になることを示せ．</p>
</div>
<p>(9.36)を$\mathbf{\mu}_{k}$で微分すると、</p>
<p>$$
\begin{aligned}
\frac{\partial}{\partial\mathbf{\mu}<em>k} \ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\mu}, \mathbf{\Sigma}, \boldsymbol{\pi}) &amp;= \sum</em>{n=1}^{N} \sum_{k=1}^{K} \frac{\partial}{\partial\mathbf{\mu}<em>k}{ z</em>{n k}\left{\ln \pi_{k}+\ln \mathcal{N}\left(\mathbf{x}<em>{n} \mid \boldsymbol{\mu}</em>{k}, \mathbf{\Sigma}<em>{k}\right)\right}}\
&amp;= \sum</em>{x_n \in C_k} \frac{\partial}{\partial\mathbf{\mu}<em>k} {\ln \pi</em>{k} + \ln \mathcal{N}\left(\mathbf{x}<em>{n} \mid \boldsymbol{\mu}</em>{k}, \mathbf{\Sigma}<em>{k}\right)}\
&amp;= \sum</em>{x_n \in C_k} \frac{\partial}{\partial\mathbf{\mu}<em>k} {\ln \mathcal{N}\left(\mathbf{x}</em>{n} \mid \boldsymbol{\mu}<em>{k}, \mathbf{\Sigma}</em>{k}\right)}
\end{aligned}
$$</p>
<p>これは、「ガウス分布でクラスタ$C_k$に属するデータ点を利用した場合の
パラメータ最適化」と同じである。</p>
<p>$\pi_i$については、$\sum_{k=1}^{K}\pi_k=1$...(A)という制限があるため、ラグランジュの未定乗数法を用いて
$$L=\ln p(\mathbf{X},\mathbf{Z}|\mathbf{\mu},\mathbf{\Sigma},\mathbf{\pi}) + \lambda(\mathbf{\Sigma}<em>{k}\pi</em>{k}-1) ...\mathrm{(B)}$$</p>
<p>$$\frac{\partial L}{\partial \pi_{k}} = \sum_{n=1}^{N}\frac{z_{nk}}{\pi_k} + \lambda=0$$</p>
<p>両辺に$\pi_k$をかけて$\mathbf{\Sigma}<em>{k}$をとると、
$$\sum</em>{k=1}^K \sum_{n=1}^N z_{nk} + \sum_{k=1}^K \lambda\pi_k=0$$
制約条件(A), $\mathbf{\Sigma}<em>{k}\mathbf{\Sigma}</em>{n}z_{nk}=N$より、</p>
<p>$$\sum_{k=1}^K \sum_{n=1}^N z_{nk} + \lambda=0 $$
$$ N + \lambda = 0, \lambda = -N$$
(B)に代入して、
$$∴\pi_{k} = \frac{1}{N}\sum_{n=1}^N z_{nk}$$</p>
<h2 id="演習-98"><a class="header" href="#演習-98">演習 9.8</a></h2>
<div class="panel-primary">
<p>もし負担率$\gamma(z_{nk})$を固定した下で，
$$\mathbb{E}<em>{\mathbf{Z}}[\ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})]=\sum</em>{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right)\left{\ln \pi_{k}+\ln \mathcal{N}\left(\mathbf{x}<em>{n} \mid \boldsymbol{\mu}</em>{k}, \boldsymbol{\Sigma}_{k}\right)\right} \tag{9.40}$$
を$\boldsymbol{\mu}_k$についての最大化をしようとすると，$(9.17)$で与えられる陽な解が得られることを示せ．</p>
<p>$$
\begin{aligned}
\mathbf{\mu}<em>k = \frac{1}{N_k}\sum_n \gamma(z</em>{nk})\mathbf{x}_n \tag{9.17}
\end{aligned}
$$</p>
<p>ただし、</p>
<p>$$
\begin{aligned}
N_k = \sum_n \gamma(z_{nk}) \tag{9/18}
\end{aligned}
$$</p>
</div>
<p>(9.40)式を$\mathbf{\mu}_k$について微分し、0となる$\mathbf{\mu}_k$を求めれば良い。</p>
<p>$$
\begin{aligned}
&amp;\frac{d}{d\mathbf{\mu}<em>k} \mathbb{E}</em>{\mathbf{Z}}[\ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})] =
\sum_n \gamma (z_{nk})\Sigma^{-1}(\mathbf{x}_n-\mathbf{\mu}<em>k)  = 0 \
&amp;\Rightarrow \sum_n \gamma (z</em>{nk})(\mathbf{x}_n-\mathbf{\mu}_k)  = 0  \
&amp;\Rightarrow \mathbf{\mu}<em>k = \frac{1}{N_k}\sum_n \gamma(z</em>{nk})\mathbf{x}_n &amp;(\because (9.18))
\end{aligned}
$$</p>
<h2 id="演習-99"><a class="header" href="#演習-99">演習 9.9</a></h2>
<div class="panel-primary">
<p>もし負担率$\gamma(z_{nk})$を固定した下で，
$$\mathbb{E}<em>{\mathbf{Z}}[\ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})]=\sum</em>{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right)\left{\ln \pi_{k}+\ln \mathcal{N}\left(\mathbf{x}<em>{n} \mid \boldsymbol{\mu}</em>{k}, \boldsymbol{\Sigma}<em>{k}\right)\right} \tag{9.40}$$
を$\boldsymbol{\Sigma}</em>{k}$と$\pi_k$についての最大化をしようとすると，
$$\mathbf{\Sigma}<em>{k}=\frac{1}{N</em>{k}} \sum_{n=1}^{N} \gamma\left(z_{n k}\right)\left(\mathbf{x}<em>{n}-\mu</em>{k}\right)\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}\right)^{\mathrm{T}} \tag{9.19}$$
と
$$\pi_k = \frac{N_k}{N} \tag{9.22}$$
で与えられる陽な解が得られることを示せ．</p>
</div>
<p>演習(9.8)と同様に最適化したい変数の微分を0として解くことで目的の解を得る．</p>
<p>$\Sigma_k^{-1}$について(9.40)を偏微分する</p>
<p>$$
\begin{aligned}
\frac{\partial}{\partial\Sigma_k^{-1}}\mathbb{E}<em>{\mathbf{Z}}[\ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})]&amp;=\frac{\partial}{\partial\Sigma_k^{-1}}\sum</em>{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right)\left{\ln \pi_{k}+\ln \mathcal{N}\left(\mathbf{x}<em>{n} \mid \boldsymbol{\mu}</em>{k}, \boldsymbol{\Sigma}<em>{k}\right)\right}\
&amp;=\frac{\partial}{\partial\Sigma_k^{-1}}\sum</em>{n=1}^{N} \gamma\left(z_{n k}\right)\ln \mathcal{N}\left(\mathbf{x}<em>{n} \mid \boldsymbol{\mu}</em>{k}, \boldsymbol{\Sigma}<em>{k}\right)\
&amp;=\frac{\partial}{\partial\Sigma_k^{-1}}\sum</em>{n=1}^{N} \gamma\left(z_{n k}\right)\left{-\frac{|D|}{2}\ln 2\pi+\frac{1}{2}\ln |\Sigma_k|^{-1}-\frac{1}{2}(\mathbf{x}_n-\mathbf{\mu}_k)^T\Sigma_k^{-1}(\mathbf{x}<em>n-\mathbf{\mu}<em>k)\right}\
&amp;=\frac{\partial}{\partial\Sigma_k^{-1}}\sum</em>{n=1}^{N} \frac{1}{2}\gamma\left(z</em>{n k}\right)\left{\ln |\Sigma_k^{-1}|-(\mathbf{x}_n-\mathbf{\mu}_k)^T\Sigma_k^{-1}(\mathbf{x}<em>n-\mathbf{\mu}<em>k)\right}\
&amp;=\sum</em>{n=1}^{N} \frac{1}{2}\gamma\left(z</em>{n k}\right)\left{\Sigma_k^T-(\mathbf{x}_n-\mathbf{\mu}_k)(\mathbf{x}<em>n-\mathbf{\mu}<em>k)^T\right}\
&amp;=\sum</em>{n=1}^{N} \frac{1}{2}\gamma\left(z</em>{n k}\right)\left{\Sigma_k-(\mathbf{x}_n-\mathbf{\mu}_k)(\mathbf{x}_n-\mathbf{\mu}_k)^T\right}\
\end{aligned}
$$</p>
<p>$|A^{-1}|=|A|^{-1}$,$\frac{\partial}{\partial{A}}\ln|A|=(A^{-1})^T$を用いた．
これを0として</p>
<p>$$
\begin{aligned}
\sum_{n=1}^{N} \frac{1}{2}\gamma\left(z_{n k}\right)\left{\mathbf{\Sigma}_k-(\mathbf{x}_n-\mathbf{\mu}_k)^T(\mathbf{x}_n-\mathbf{\mu}_k)\right}=0
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
\mathbf{\Sigma}<em>k\sum</em>{n=1}^{N} \gamma\left(z_{n k}\right)=\sum_{n=1}^{N} \gamma\left(z_{n k}\right)(\mathbf{x}_n-\mathbf{\mu}_k)^T(\mathbf{x}_n-\mathbf{\mu}_k)
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
\mathbf{\Sigma}<em>k=\frac{1}{N}\sum</em>{n=1}^{N} \gamma\left(z_{n k}\right)(\mathbf{x}_n-\mathbf{\mu}_k)^T(\mathbf{x}_n-\mathbf{\mu}_k)
\end{aligned}
$$</p>
<p>が得られる．</p>
<p>次に$\pi_k$について考える．これは演習(9.7)を応用して考えることができて，$\mathbf{z}<em>{nk}$を$\gamma(\mathbf{z}</em>{nk})$で置き換え，同様にラグランジュ未定乗数法を用いて計算することで</p>
<p>$$
\begin{aligned}
\boldsymbol{\pi}<em>k&amp;=\frac{1}{N}\sum</em>{n=1}^N\gamma(\mathbf{z}_{nk})\
&amp;=\frac{N_k}{N}
\end{aligned}
$$</p>
<p>以上から目的の解が得られた</p>
<h2 id="演習-910"><a class="header" href="#演習-910">演習 9.10</a></h2>
<div class="panel-primary">
<p>$$p(\mathbf{x})=\sum_{k=1}^{K} \pi_{k} p(\mathbf{x} \mid k) \tag{9.81}$$</p>
<p>の形の混合分布で与えられる密度のモデルを考え，ベクトル$\mathbf{x}$を$\mathbf{x} = (\mathbf{x}_a, \mathbf{x}_b)$のように2つの部分に分解すると仮定する．このとき，条件付き密度$p(\mathbf{x}_b\mid \mathbf{x}_a)$自体が混合分布であることを示し，混合係数と各混合要素の表式を求めよ．</p>
</div>
<p>(9.81)より
$$
p\left(\mathbf{x}<em>{a}, \mathbf{x}</em>{b}\right)=\sum_{k=1}^{K} \pi_{k} p\left(\mathbf{x}<em>{a}, \mathbf{x}</em>{b} \mid k\right)
$$
であることを用いて、</p>
<p>$$
\begin{aligned}
p\left(\mathbf{x}<em>{b} \mid \mathbf{x}</em>{a}\right)&amp;=\frac{p\left(\mathbf{x}<em>{a}, \mathbf{x}</em>{b}\right)}{p\left(\mathbf{x}<em>{a}\right)}
\&amp;=\frac{\sum</em>{k=1}^{K} \pi_{k} p\left(\mathbf{x}<em>{a}, \mathbf{x}</em>{b} \mid k\right)}{p\left(\mathbf{x}<em>{a}\right)}\&amp;=\frac{\sum</em>{k=1}^{K} \pi_{k} p\left(\mathbf{x}<em>{b} \mid \mathbf{x}</em>{a}, k\right) p\left(\mathbf{x}<em>{a} \mid k\right)}{p\left(\mathbf{x}</em>{a}\right)}
\&amp;=\sum_{k=1}^{K} \lambda_{k} p\left(\mathbf{x}<em>{b} \mid \mathbf{x}</em>{a}, k\right)
\end{aligned}
$$</p>
<p>ただし混合係数は</p>
<p>$$
\begin{aligned}
\lambda_{k} &amp;\equiv \frac{\pi_{k} p\left(\mathbf{x}<em>{a} \mid k\right)}{p\left(\mathbf{x}</em>{a}\right)}\&amp;=\frac{\pi_{k} p\left(\mathbf{x}<em>{a} \mid k\right)}{\sum</em>{k=1}^{K} \pi_{k} p\left(\mathbf{x}_{a} \mid k\right)}
\end{aligned}
$$
である。</p>
<h2 id="演習-911"><a class="header" href="#演習-911">演習 9.11</a></h2>
<div class="panel-primary">
<p>9.3.2節においてすべての混合要素の共分散が$\epsilon\mathbf{I}$である混合ガウス分布を考えることで，<em>K</em>-means法とEMアルゴリズムの関係を導いた．ここに，$\epsilon \to 0$の極限において，このモデルの期待完全データ対数尤度
$$\mathbb{E}<em>{\mathbf{Z}}[\ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\mu}, \boldsymbol{\Sigma}, \boldsymbol{\pi})]=\sum</em>{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right)\left{\ln \pi_{k}+\ln \mathcal{N}\left(\mathbf{x}<em>{n} \mid \boldsymbol{\mu}</em>{k}, \boldsymbol{\Sigma}<em>{k}\right)\right} \tag{9.40}$$
の最大化が，<em>K</em>-means法の歪み尺度
$$J=\sum</em>{n=1}^{N} \sum_{k=1}^{K} r_{n k}\left|\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}\right|^{2} \tag{9.1}$$
の最小化と等価になることを示せ．</p>
</div>
<p>式 (9.37) より$\pi_k$は常に非負。従って、$\epsilon \to 0$となるとき、$\gamma(z_{nk}) \to r_{nk}$である (p.160) ことから、式 (9.40) の最大化は</p>
<p>$$
\begin{aligned}
\sum_{n = 1}^N \sum_{k = 1}^K r_{nk} \left(-\frac{1}{2\epsilon}\left|\mathbf{x}_n - \mathbf{\mu}_k\right|^2\right)
\end{aligned}
$$</p>
<p>の最大化に等しい。上式に最適化の対象である$\mathbf{\mu}_k$を含まないscaling factor $\left(-2\epsilon\right)$ をかけると式 (9.1) に等しくなることから、題意が示される。</p>
<h2 id="演習-912"><a class="header" href="#演習-912">演習 9.12</a></h2>
<div class="panel-primary">
<p>$$p(\mathbf{x})=\sum_{k=1}^{K} \pi_{k} p(\mathbf{x} \mid k) \tag{9.82}$$</p>
<p>の形の混合分布を考える．ここに，$\mathbf{x}$は離散，連続，その組み合わせのいずれでもよい．$p(\mathbf{x}\mid k)$の平均と共分散をそれぞれ$\boldsymbol{\mu}<em>k$と$\mathbf{\Sigma}<em>k$で表す．混合分布の平均と共分散がそれぞれ
$$\mathbb{E}[\mathbf{x}] =\sum</em>{k=1}^{K} \pi</em>{k} \boldsymbol{\mu}<em>{k} \tag{9.49}$$
$$\operatorname{cov}[\mathbf{x}] =\sum</em>{k=1}^{K} \pi_{k}\left{\mathbf{\Sigma}<em>{k}+\boldsymbol{\mu}</em>{k} \boldsymbol{\mu}_{k}^{\mathrm{T}}\right}-\mathbb{E}[\mathbf{x}] \mathbb{E}[\mathbf{x}]^{\mathrm{T}} \tag{9.50}$$
で与えられることを示せ．</p>
</div>
<p>問題設定より、$p(\mathbf{x}\mid k)$（の分布の下での$\mathbf{x}$）の平均と共分散がそれぞれ$\boldsymbol{\mu}_k$, $\mathbf{\Sigma}_k$となるので、上巻pp.19-20, $(1.36)$の記法にしたがって</p>
<p>$$\mathbb{E}<em>{k}[\mathbf{x}]=\int \mathbf{x} p(\mathbf{x} \mid k) d \mathbf{x}=\boldsymbol{\mu}</em>{k},\  \operatorname{cov}<em>{k}[\mathbf{x}]=\mathbf{\Sigma}</em>{k}$$</p>
<p>と書ける。</p>
<p>$\mathbb{E}[\mathbf{x}]$について、期待値の定義から</p>
<p>$$\begin{aligned}
\mathbb{E}[\mathbf{x}] &amp;=\int \mathbf{x} p(\mathbf{x}) d \mathbf{x} \
&amp;=\int \mathbf{x} \sum_{k=1}^{K} \pi_{k} p(\mathbf{x} \mid k) d \mathbf{x} \
&amp;=\sum_{k=1}^{K} \pi_{k} \int \mathbf{x} p(\mathbf{x} \mid k) d \mathbf{x} \
&amp;=\sum_{k=1}^{K} \pi_{k} \boldsymbol{\mu}_{k} \end{aligned}$$</p>
<p>となる。次に共分散$\operatorname{cov}[\mathbf{x}]$について、</p>
<p>$$\begin{aligned} \operatorname{cov}[\mathbf{x}]
&amp;=\mathbb{E}\left[(\mathbf{x}-\mathbb{E}[\mathbf{x}])(\mathbf{x}-\mathbb{E}[\mathbf{x}])^{\mathrm{T}}\right] \quad(\because(1.42)) \
&amp;=\mathbb{E}\left[\mathbf{xx}^{\mathrm{T}}-2 \mathbb{E}[\mathbf{x}]\mathbb{E}[\mathbf{x}]^{\mathrm{T}}+\mathbb{E}[\mathbf{x}] \mathbb{E}[\mathbf{x}]^{\mathrm{T}}\right] \
&amp;=\mathbb{E}\left[\mathbf{xx}^{\mathrm{T}}\right]-\mathbb{E}[\mathbf{x}] \mathbb{E}[\mathbf{x}]^{\mathrm{T}} \
&amp;=\int \mathbf{xx}^{\mathrm{T}} \sum_{k=1}^{K} \pi_{k} p(\mathbf{x} \mid k) d x-\mathbb{E}[\mathbf{x}] \mathbb{E}[\mathbf{x}]^{\mathrm{T}} \
&amp;=\sum_{k=1}^{K} \pi_{k} \int \mathbf{xx}^{\mathrm{T}} p(\mathbf{x} \mid k) d x-\mathbb{E}[\mathbf{x}] \mathbb{E}[\mathbf{x}]^{\mathrm{T}} \
&amp;=\sum_{k=1}^{K} \pi_{k} \mathbb{E}_{k}\left[\mathbf{xx}^{\mathrm{T}}\right]-\mathbb{E}[\mathbf{x}] \mathbb{E}[\mathbf{x}]^{\mathrm{T}}
\end{aligned}$$</p>
<p>ここで$\mathbb{E}_{k}[\mathbf{xx}^{\mathrm{T}}]$について、$(1.42)$の共分散行列の定義から</p>
<p>$$\begin{aligned} \mathbf{\Sigma}<em>{k}=\operatorname{cov}</em>{k}[\mathbf{x}]
&amp;=\mathbb{E}<em>{k}\left[\left(\mathbf{x}-\mathbb{E}</em>{k}[\mathbf{x}]\right)\left(\mathbf{x}-\mathbb{E}<em>{k}[\mathbf{x}]\right)^{\mathrm{T}}\right] \
&amp;=\mathbb{E}</em>{k}\left[\left(\mathbf{x}-\boldsymbol{\mu}<em>{k}\right)\left(\mathbf{x}-\boldsymbol{\mu}</em>{k}\right)^{\mathrm{T}}\right] \
&amp;=\mathbb{E}<em>{k}\left[\mathbf{xx}^{\mathrm{T}}\right]-\boldsymbol{\mu}</em>{k} \boldsymbol{\mu}_{k}^{\mathrm{T}} \end{aligned}$$</p>
<p>であるから、$\mathbb{E}<em>{k}\left[\mathbf{xx}^{\mathrm{T}}\right] = \mathbf{\Sigma}</em>{k}+\boldsymbol{\mu}<em>{k} \boldsymbol{\mu}</em>{k}^{\mathrm{T}}$が得られる。以上を代入して、</p>
<p>$$\operatorname{cov}[\mathbf{x}] =\sum_{k=1}^{K} \pi_{k}\left{\mathbf{\Sigma}<em>{k}+\boldsymbol{\mu}</em>{k} \boldsymbol{\mu}_{k}^{\mathrm{T}}\right}-\mathbb{E}[\mathbf{x}] \mathbb{E}[\mathbf{x}]^{\mathrm{T}}$$</p>
<p>となる。</p>
<h2 id="演習-913"><a class="header" href="#演習-913">演習 9.13</a></h2>
<div class="panel-primary">
<p>ベルヌーイ分布の混合は，パラメータを最尤解のときの値に一致させた場合</p>
<p>$$\mathbb{E}[\mathrm{x}]=\frac{1}{N} \sum_{n=1}^{N} \mathrm{x}_{n} \equiv \overline{\mathrm{x}} \tag{9.83}$$</p>
<p>を満たすことをEMアルゴリズムの更新式を用いて示せ．さらにこのモデルにおいて,すべての混合要素の平均値を$\boldsymbol{\mu}_{k}=\widehat{\boldsymbol{\mu}}(k=1, \ldots, K)$と同ーの値になるようにパラメータを初期化すると，混合係数の初期値が何であっても， EMアルゴリズムは一度の繰り返しで収束し，$\boldsymbol{\mu}_k = \overline{\mathbf{x}}$となることを示せ．これは，すべての混合要素が同ーになる縮退したケースを表しており，実用上，我々は適切な初期化によってこのような解を避ける努力をする．</p>
</div>
<p>$$
\begin{aligned}
p(\mathbf{x}|\boldsymbol{\mu}, \boldsymbol{\pi}) = \sum_{k=1}^K \pi_k p(\mathbf{x}|\boldsymbol{\mu}_k )  \tag{9.47}
\end{aligned}
$$</p>
<p>なので、</p>
<p>$$
\begin{aligned}
\mathbb{E}[\mathbf{x}] &amp;= \sum_{k=1}^K \pi_k \boldsymbol{\mu}<em>k \
&amp;= \sum</em>{k=1}^K \pi_k \overline{\mathbf{x}<em>k} \
&amp;= \sum</em>{k=1}^K \pi_k \frac{1}{N_k} \sum_{n=1}^N \gamma (z_{nk}) \mathbf{x}<em>n \
&amp;= \frac{1}{N} \sum</em>{n=1}^N \sum_{k=1}^K \gamma (z_{nk}) \mathbf{x}<em>n \
&amp;= \frac{1}{N} \sum</em>{n=1}^N \mathbf{x}_n \
\end{aligned}
$$</p>
<p>となる。ここで、$\gamma (z_{nk})$は負担率であることから、$k$について和をとると$1$になることを用いた。</p>
<p>次に、後半部分を証明する。Eステップの更新式は(9.56)式で表され、</p>
<p>$$
\begin{aligned}
\gamma (z_{nk})
&amp;= \frac{\pi _k p(\mathbf{x}_n | \boldsymbol{\mu}<em>k)}{\sum</em>{j=1}^K \pi _j p(\mathbf{x}_n | \boldsymbol{\mu}_j)} \
&amp;= \frac{\pi <em>k \hat{\boldsymbol{\mu}}}{\sum</em>{j=1}^K \pi _j \hat{\boldsymbol{\mu}}} \
&amp;= \pi_k
\end{aligned}
$$</p>
<p>となり、$n$によらない。
Mステップの更新式は(9.59)式で表され、</p>
<p>$$
\begin{aligned}
\boldsymbol{\mu}<em>k
&amp;= \overline{x_k} \
&amp;= \frac{1}{N_k} \sum</em>{n=1}^N \gamma (z_{nk}) \mathbf{x}<em>n \
&amp;= \frac{1}{N_k} \pi_k \sum</em>{n=1}^N \mathbf{x}<em>n \
&amp;= \frac{1}{N} \sum</em>{n=1}^N \mathbf{x}_n \
\end{aligned}
$$</p>
<p>となり、$k$によらない。
従って、逐次的に更新式を適用しても2回目以降は値が変化しない。結果として、純粋な（混合分布ではない）ベルヌーイ分布と同じ確率分布しか表現できない。</p>
<h2 id="演習-914"><a class="header" href="#演習-914">演習 9.14</a></h2>
<div class="panel-primary">
<p>$$p(\mathbf{x} \mid \mathbf{z}, \boldsymbol{\mu})=\prod_{k=1}^{K} p\left(\mathbf{x} \mid \boldsymbol{\mu}<em>{k}\right)^{z</em>{k}} \tag{9.52}$$
と
$$p(\mathbf{z} \mid \boldsymbol{\pi})=\prod_{k=1}^{K} \pi_{k}^{z_{k}} \tag{9.53}$$
の積でベルヌーイ分布の潜在変数と観測変数の同時分布を構成しよう．この同時分布を$\mathbf{z}$について周辺化すると
$$p(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\pi})=\sum_{k=1}^{K} \pi_{k} p\left(\mathbf{x} \mid \boldsymbol{\mu}_{k}\right) \tag{9.47}$$
が得られることを示せ．</p>
</div>
<p>乗法定理から</p>
<p>$$
\begin{aligned}
p(\mathbf{x},\mathbf{z},\mid \boldsymbol{\mu},\boldsymbol{\pi})&amp;=p(\mathbf{x} \mid \mathbf{z}, \boldsymbol{\mu})p(\mathbf{z} \mid \boldsymbol{\pi})
\end{aligned}
$$</p>
<p>である．これを$\mathbf{z}$について周辺化し，(9.52), (9.53)を代入すると</p>
<p>$$
\begin{aligned}
p(\mathbf{x}\mid \boldsymbol{\mu},\boldsymbol{\pi})&amp;=\sum_\mathbf{z} p(\mathbf{x} \mid \mathbf{z}, \boldsymbol{\mu})p(\mathbf{z} \mid \boldsymbol{\pi})\
&amp;=\sum_{\mathbf{z}}\prod_{k=1}^{K} \pi_{k}^{z_{k}}\prod_{k=1}^{K} p\left(\mathbf{x} \mid \boldsymbol{\mu}<em>{k}\right)^{z</em>{k}}\
&amp;=\sum_{\mathbf{z}}\prod_{k=1}^{K} \pi_{k}^{z_{k}}p\left(\mathbf{x} \mid \boldsymbol{\mu}<em>{k}\right)^{z</em>{k}}\
&amp;=\sum_{k=1}^K \pi_{k}p\left(\mathbf{x} \mid \boldsymbol{\mu}_{k}\right)
\end{aligned}
$$</p>
<p>が得られる．
ここで$z_k$はone-hot-vectorであるため0か1の値を取る．したがって$z_k=0$のとき$\pi_{k}^{0}p\left(\mathbf{x} \mid \boldsymbol{\mu}_{k}\right)^{0}=1$であり，積を考える場合には$z_k=1$となる場合に注目すれば良いことがわかる．以上により最後の行の式に変形できる．</p>
<h2 id="演習-915"><a class="header" href="#演習-915">演習 9.15</a></h2>
<div class="panel-primary">
<p>混合ベルヌーイ分布についての期待完全データ対数尤度関数
$$\mathbb{E}<em>{\mathbf{Z}}[\ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\mu}, \boldsymbol{\pi})]=\sum</em>{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right)\Biggl{
\ln \pi_{k} \ \left.+\sum_{i=1}^{D}\left[x_{n i} \ln \mu_{k i}+\left(1-x_{n i}\right) \ln \left(1-\mu_{k i}\right)\right]\right} \tag{9.55}$$
を$\boldsymbol{\mu}<em>k$について最大化すると，Mステップ方程式
$$\boldsymbol{\mu}</em>{k}=\overline{\mathbf{x}}_{k} \tag{9.59}$$
が得られることを示せ．</p>
</div>
<p>問題文の通り、$\mathbb{E}_{\mathbf{Z}}[\ln p]$を$\boldsymbol{\mu}<em>k$について最大化する。このために$\mu</em>{ki}$での微分を考える。</p>
<p>$$\begin{aligned} \frac{\partial}{\partial \mu_{k i}} \mathbb{E}<em>{\mathbf{Z}}[\ln p]
&amp;=\sum</em>{n=1}^{N} \gamma\left(z_{n k}\right)\left(\frac{x_{n i}}{\mu_{k i}}-\frac{1-x_{n i}}{1-\mu_{k i}}\right) \
&amp;=\frac{\sum_{n=1}^{N} \gamma\left(z_{n k}\right) x_{n i}-\sum_{n=1}^{N} \gamma\left(z_{n k}\right) \mu_{k i}}{\mu_{k i}\left(1-\mu_{k i}\right)} \end{aligned}$$</p>
<p>この式を0とおいて、</p>
<p>$$\begin{aligned}&amp; \sum_{n=1}^{N} \gamma\left(z_{n k}\right) \mu_{k i}=\sum_{n=1}^{N} \gamma\left(z_{nk}\right) x_{n i} \
&amp; \mu_{k i}=\frac{\sum_{n=1}^{N} \gamma\left(z_{n k}\right) x_{n i}}{\sum_{n=1}^{N} \gamma\left(z_{n k}\right)}=\frac{1}{N_{k}} \sum_{n=1}^{N} \gamma\left(z_{n k}\right) x_{n i}\hspace{1em}(\because (9.57))\end{aligned}$$</p>
<p>を得る。これを踏まえると、</p>
<p>$$\begin{aligned} \frac{\partial \mathbb{E}<em>{\mathbf{Z}}[\ln p]}{\partial \boldsymbol{\mu}</em>{k}} &amp;=\left(\frac{\partial \mathbb{E}<em>{\mathbf{Z}}[\ln p]}{\partial \mu</em>{k 1}}, \frac{\partial \mathbb{E}<em>{\mathbf{Z}}[\ln p]}{\partial \mu</em>{k 2}}, \cdots\right)^{\mathrm{T}} \
&amp;=\frac{1}{N_{k}}\left(\sum_{n=1}^{N} \gamma \left(z_{n k}\right) x_{n 1}, \sum_{n=1}^{N} \gamma\left(z_{n k}\right) x_{n 2}, \cdots\right)^{\mathrm{T}} \
&amp;=\frac{1}{N_{k}} \sum_{n=1}^{N} \gamma\left(z_{n k}\right) \mathbf{x}<em>{n} \
&amp;=\overline{\mathbf{x}}</em>{k}
\end{aligned}$$</p>
<p>したがって、Mステップでは$\boldsymbol{\mu}<em>{k}=\overline{\mathbf{x}}</em>{k}$となる。</p>
<h2 id="演習-916"><a class="header" href="#演習-916">演習 9.16</a></h2>
<div class="panel-primary">
<p>変数混合ベルヌーイ分布についての期待完全データ対数尤度関数
$$\mathbb{E}<em>{\mathbf{Z}}[\ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\mu}, \boldsymbol{\pi})]=\sum</em>{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right)\left{\ln \pi_{k}\right. \ \left.+\sum_{i=1}^{D}\left[x_{n i} \ln \mu_{k i}+\left(1-x_{n i}\right) \ln \left(1-\mu_{k i}\right)\right]\right} \tag{9.55}$$
を，ラグランジュ未定乗数法を用いて混合係数$\pi_k$の総和を一定に保ちつつ$\pi_k$について最大化すると，Mステップ方程式
$$\pi_{k} = \frac{N_k}{N} \tag{9.60}$$
が得られることを示せ．</p>
</div>
<p>ラグランジュの未定乗数法を用いると</p>
<p>$$
L=\mathbb{E}<em>{\mathbf{Z}}[\ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\mu}, \boldsymbol{\pi})]+\lambda\left(\sum</em>{k=1}^{K} \pi_{k}-1\right)
$$</p>
<p>これを$\pi_{k}$で微分してゼロとおくと</p>
<p>$$\frac{\partial L}{\partial \pi_{k}}=\sum_{n=1}^{N} \frac{\gamma\left(z_{n k}\right)}{\pi_{k}}+\lambda=0-①$$</p>
<p>この式の両辺に$\pi_{k}$をかけると</p>
<p>$$
\sum_{n=1}^{N} \gamma\left(z_{n k}\right)+\lambda \pi_{k}=0
$$</p>
<p>$k$について総和を取れば</p>
<p>$$
\sum_{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right)+\lambda=0 \Leftrightarrow \lambda=-\sum_{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right)
$$</p>
<p>これを①に代入すれば
$$
\pi_{k}=\frac{\sum_{n} \gamma\left(z_{n k}\right)}{\sum_{n, k} \gamma\left(z_{n k}\right)}=\frac{N_{k}}{N}
$$</p>
<p>※ (9.57)を使用</p>
<h2 id="演習-917"><a class="header" href="#演習-917">演習 9.17</a></h2>
<div class="panel-primary">
<p>混合ベルヌーイ分布については離散変数$\mathbf{x}<em>n$について$0 \leqslant p\left(\mathbf{x}</em>{n} \mid \boldsymbol{\mu}_{k}\right) \leqslant 1$という制約があるために，不完全データ対数尤度関数は上に有界であり，よって尤度関数が発散する特異点が存在しないことを示せ．</p>
</div>
<p>不完全データ対数尤度関数$\ln p\left(\mathbf{x}<em>{n} \mid \boldsymbol{\mu}, \boldsymbol{\pi}\right)$は、(9.51)で与えられる。
$0\leqslant p\left(\mathbf{x}</em>{n} \mid \boldsymbol{\mu}_{k}\right) \leqslant 1$より、</p>
<p>$$\begin{aligned} \ln p\left(\mathbf{x}<em>{n} \mid \boldsymbol{\mu}, \boldsymbol{\pi}\right) &amp;=\sum</em>{n=1}^{N} \ln \left(\sum_{k=1}^{K} \pi_{k} p\left(\mathbf{x}<em>{n} \mid \boldsymbol{\mu}</em>{k}\right)\right) ...(9.51)\ &amp; \leqslant \sum_{n=1}^{N} \ln \left(\sum_{k=1}^{K} \pi_{k}\right) \ &amp;=\sum_{n=1}^{N} \ln 1=0 \end{aligned}$$</p>
<p>したがって、対数尤度関数は上に有界であり、発散することはない。つまり、そのような特異点は存在しない。</p>
<p>※ 連続変数である混合ガウス分布のときは発散していた（pp.149-150）との対比</p>
<h2 id="演習-918"><a class="header" href="#演習-918">演習 9.18</a></h2>
<div class="panel-primary">
<p>9.3.3節で論じたように，混合ベルヌーイモデルに事前分布を仮定する．ただし各パラメータベクトル$\boldsymbol{\mu}<em>k$の分布$ρ(\boldsymbol{\mu}<em>k\mid a_k, b_k)$において，$\boldsymbol{\mu}<em>k$の各成分はそれぞれ独立にパラメータ$a_k$と$b_k$を持つベータ分布
$$\operatorname{Beta}(\mu \mid a, b)=\frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} \mu^{a-1}(1-\mu)^{b-1} \tag{2.13}$$
に従うものとし，また，$p(\boldsymbol{\pi}\mid\boldsymbol{\alpha})$はディリクレ分布
$$\operatorname{Dir}(\mu \mid \alpha)=\frac{\Gamma\left(\alpha</em>{0}\right)}{\Gamma\left(\alpha</em>{1}\right) \cdots \Gamma\left(\alpha</em>{K}\right)} \prod_{k=1}^{K} \mu_{k}^{\alpha_{k}-1} \tag{2.38}$$
であるとする．このとき，事後分布$p(\boldsymbol{\mu},\boldsymbol{\pi}\mid \mathbf{X})$を最大化するEMアルゴリズムを導け．</p>
</div>
<p>演習9.4より、対数尤度関数ではなく対数事後分布について最大化する場合は、</p>
<p>$$
\begin{aligned}
\mathcal{Q}(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text {old }}) = \mathcal{Q}_l(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text {old }}) + \ln p(\mathbf{\theta})
\end{aligned}
$$</p>
<p>を最大化すれば良い。ただし、ここでは、</p>
<p>$$
\begin{aligned}
p(\theta) = p(\mathbf{\mu}|\mathbf{a, b})p(\mathbf{\pi}|\mathbf{\alpha})
\end{aligned}
$$</p>
<p>であり、$\mathcal{Q}_l(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text {old }})$は(9.55)式で表される。</p>
<p>今、(2.13)より、</p>
<p>$$
\begin{aligned}
\ln p(\mathbf{\mu}|\mathbf{a, b}) &amp;= \sum_{\mathbf{z}} \ln p(\mathbf{\mu}<em>k|a_i, bi) \
&amp;= \sum</em>{\mathbf{z}} \ln \prod_k \mu_{ki}^{a_k-1}(1-\mu_{ki})^{b_k-1} + const. \
&amp;= \sum_{\mathbf{z}} \sum_{k} {(a_k -1) \ln \mu_{ki}+(b_k-1)\ln (1-\mu_{ki})} + const.
\end{aligned}
$$</p>
<p>そして、同様に、(2.14)より、</p>
<p>$$
\begin{aligned}
\ln p(\mathbf{\pi}|\mathbf{\alpha})
&amp;= \sum_k (\alpha_k -1)\ln  \pi_k + const.
\end{aligned}
$$</p>
<p>したがって、</p>
<p>$$
\begin{aligned}
\mathcal{Q}(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text {old }}) = E_{\mathbf{z}}[\ln p] + \sum_{\mathbf{z}} \sum_{k} {(a_k -1) \ln \mu_{ki}+(b_k-1)\ln (1-\mu_{ki})}  + \sum_k (\alpha_k -1)\ln  \pi_k \tag{9.Ex18.1}
\end{aligned}
$$</p>
<p>を最大化する問題を考えれば良い。よって、$\mathcal{Q}$を$\mu_{ki}$について微分を、$\pi_k$につ未定乗数法を用いて更新式を導出する。</p>
<p>$$
\begin{aligned}
\frac{\partial \mathcal{Q}}{\partial \mu_{ki}} &amp;= \frac{\partial E_{z}[\ln p] }{\partial \mu_{ki}} + \frac{a_i -1}{\mu_{ki}} - \frac{b_i -1 }{1 - \mu_{ki}} \
&amp;= \sum_n \gamma(z_{nk})(\frac{x_{ni}}{\mu_{ki}} -\frac{1-x_{ni}}{1-\mu_{ki}}) +\frac{a_i -1}{\mu_{ki}} - \frac{b_i -1 }{1 - \mu_{ki}} &amp;(\because Ex. 9.15)\
&amp;= \frac{N_k \bar{x}<em>{ki}+a_i -1 }{\mu</em>{ki}} - \frac{N_k - N_k \bar{x}<em>{ki}+b_i -1}{1 - \mu</em>{ki}} &amp;(\because (9.57), (9.58))
\end{aligned}
$$
よって、整理して</p>
<p>$$
\begin{aligned}
\mu_{ki} = \frac{N_k \bar{x}<em>{ki}+a_i -1 }{N_k \bar{x}</em>{ki}+a_i -1 +b_i -1}
\end{aligned}
$$</p>
<p>次に、$\pi_k$について、ラグランジュ未定乗数法を用いて$\pi_k$を求める。以下の式を考える。</p>
<p>$$
\begin{aligned}
L \propto E_z + \sum_k (\alpha_k -1)\ln \pi_k +\lambda (\sum \pi_k -1)
\end{aligned}
$$
これを$\pi_k$について微分して、</p>
<p>$$
\begin{aligned}
\frac{\partial L}{\partial \pi_k} = \sum_n \frac{\gamma(z_{nk})}{\pi_k} + \frac{a_k-1}{\pi_k}+ \lambda = 0 &amp;&amp;(\because Ex. 9.16)
\end{aligned}
$$
そして、両辺に$\pi_k$を乗じ、$k$について和をとると、</p>
<p>$$
\begin{aligned}
\sum_k \sum_n \gamma(z_{nk}) + \sum_k(\alpha_k -1)+ \lambda = 0 &amp;&amp;(\because \sum_k \pi_k = 1)
\end{aligned}
$$</p>
<p>これを整理すると、</p>
<p>$$
\begin{aligned}
\lambda = -N -\alpha_0 + K &amp;&amp;(\because \alpha_0 = \sum_k \alpha_k)
\end{aligned}
$$</p>
<p>これを微分式に代入して整理すると</p>
<p>$$
\begin{aligned}
\pi_k = \frac{\sum_n \gamma(z_{nk})+\alpha_k -1}{-\lambda} = \frac{N_k + \alpha_k -1}{N + \alpha_0 - K} &amp;&amp;(\because (9.57))
\end{aligned}
$$</p>
<h2 id="演習-919"><a class="header" href="#演習-919">演習 9.19</a></h2>
<div class="panel-primary">
<p>$\mathbf{x}$を，各成分が$x_{i j}(i=1, \ldots, D, j=1, \ldots, M)$である$D\times M$次元のベクトル値の確率変数とする．ただし，$x_{ij}$の各々は${0,1}$に値を取り，すべての$I$に対し制約条件$\sum_{j}x_{ij}=1$を満たすものとする．$\mathbf{x}$の分布は，以下のように，2.2節で議論した($N=1$の)多項分布の混合であるとする．</p>
<p>$$p(\mathbf{x})=\sum_{k=1}^{K} \pi_{k} p\left(\mathbf{x} \mid \boldsymbol{\mu}_{k}\right) \tag{9.84}$$</p>
<p>ただし</p>
<p>$$p\left(\mathbf{x} \mid \boldsymbol{\mu}<em>{k}\right)=\prod</em>{i=1}^{D} \prod_{j=1}^{M} \mu_{k i j}^{x_{i j}} \tag{9.85}$$</p>
<p>である．パラメータ$\mu_{kij}$は，確率$p(x_{ij} =1 \mid \boldsymbol{\mu}<em>k)$を表し，$0 \leqslant \mu</em>{kij} \leqslant 1$と，各$k, i$の値について制約条件$\sum_{j}\mu_{kij} = 1$を満たさなくてはならない．観測データ集合$\left{\mathbf{x}<em>{n}\right}(n=1, \ldots, N)$が与えられた下で，この分布の混合係数$\pi_k$と要素のパラメータ$\mu</em>{kij}$を最尤推定するEMアルゴリズムの，EステップとMステップの更新式を求めよ．</p>
</div>
<p>観測変数$\mathbf{x}$に対応する潜在変数$\mathbf{z}$を導入する．このとき観測データセットの条件付き分布は</p>
<p>$$
\begin{aligned}
p(\mathbf{X} \mid\mathbf{Z},\boldsymbol{\mu})&amp;=\prod_{n=1}^N\prod_{k=1}^Kp(\mathbf{x}<em>n\mid\boldsymbol{\mu}<em>k)^{z</em>{nk}}\
&amp;=\prod</em>{n=1}^N\prod_{k=1}^K\prod_{i=1}^{D} \prod_{j=1}^{M} \mu_{k i j}^{x_{i j}z_{nk}}
\end{aligned}
$$</p>
<p>と表せ，同様に潜在変数の分布は</p>
<p>$$
p(\mathbf{Z}\mid\pi)=\prod_{n=1}^N\prod_{k=1}^K\pi^{z_{nk}}_k
$$</p>
<p>以上から完全データ尤度関数は</p>
<p>$$\begin{aligned}
p(\mathbf{X},\mathbf{Z}\mid\boldsymbol{\mu},\pi)&amp;=p(\mathbf{X} \mid\mathbf{Z},\boldsymbol{\mu})p(\mathbf{Z}\mid\pi)\
&amp;=\prod_{n=1}^N\prod_{k=1}^K\prod_{i=1}^{D} \prod_{j=1}^{M} \mu_{k i j}^{x_{i j}z_{nk}}\prod_{n=1}^N\prod_{k=1}^K\pi^{z_{nk}}<em>k\
&amp;=\prod</em>{n=1}^N\prod_{k=1}^K\prod_{i=1}^{D} \prod_{j=1}^{M} (\pi_k\mu_{k i j}^{x_{i j}})^{z_{nk}}
\end{aligned}
$$</p>
<p>対数をとって添字に注意しながら整理すると</p>
<p>$$
\ln p(\mathbf{X},\mathbf{Z}\mid\boldsymbol{\mu},\pi)=\sum_{n=1}^{N} \sum_{k=1}^{K} z_{n k}\left{\ln \pi_{k}+\sum_{i=1}^{D} \sum_{j=1}^{M}\mathbf{x}<em>{nij}\ln\mu</em>{kij}\right}
$$</p>
<p>以上のような完全データ対数尤度関数が得られる．</p>
<p>E-stepでは以下の式で$p(\mathbf{Z}\mid\pi)$による$z_{nk}$の期待値計算を行う．</p>
<p>$$
\begin{aligned}
\mathbb{E}[z_{nk}]=\frac{\pi_kp\left(\mathbf{x} \mid \boldsymbol{\mu}<em>{k}\right)}{\sum_j\pi_jp\left(\mathbf{x} \mid \boldsymbol{\mu}</em>{j}\right)}=\gamma(z_{nk})
\end{aligned}
$$</p>
<p>M-stepではラグランジュ未定乗数法を用いて各種パラメータの最大化を行う.ラグランジアンが制約条件と係数$\lambda_1,\lambda_2$を用いて以下のようにかけて</p>
<p>$$
\mathcal{L}=\sum_{n=1}^{N} \sum_{k=1}^{K} \gamma(z_{n k})\left{\ln \pi_{k}+\sum_{i=1}^{D} \sum_{j=1}^{M}\mathbf{x}<em>{nij}\ln\mu</em>{kij}\right}+\lambda_1(\sum_{k=1}^K\pi_k-1)+\sum_{k=1}^K\sum_{i=1}^D\lambda_{2ki}(\sum_{j=1}^M\mu_{kij}-1)
$$</p>
<p>これを$\pi_k,\mu_{kij}$について微分しものを0とおいてそれぞれ最大化を行う．</p>
<p>まず$\pi_k$について</p>
<p>$$
\begin{aligned}
\frac{\partial\mathcal{L}}{\partial\pi_k}&amp;=\sum_{n=1}^N\frac{\gamma(z_{nk})}{\pi_k}+\lambda_1=0
\end{aligned}
$$</p>
<p>$\sum_n\gamma(z_{nk})=N_k,\sum_k\pi_k=1$を用いて$\lambda_1$についてこれを解くと</p>
<p>$$
\frac{N_k}{\pi_k}+\lambda_1=0
$$</p>
<p>$$
N_k=-\lambda_1\pi_k
$$</p>
<p>$$
\lambda_1=-N
$$</p>
<p>よって</p>
<p>$$
\pi_k=\frac{N_k}{N}
$$</p>
<p>が得られる．次に$\mu_{kij}$について偏微分を0として</p>
<p>$$
\begin{aligned}
\frac{\partial\mathcal{L}}{\partial\mu_{kij}}&amp;=\sum_{n=1}^N\frac{\gamma(z_{nk})\mathbf{x}<em>{nij}}{\mu</em>{kij}}+\lambda_{2ki}=0
\end{aligned}
$$</p>
<p>$\sum_n\gamma(z_{nk})=N_k,\sum_{j}\mu_{kij}=1$を用いて$\lambda_{2ki}$について解くと</p>
<p>$$
\sum_{n=1}^N\gamma(z_{nk})\mathbf{x}<em>{nij}+\lambda</em>{2ki}\mu_{kij}=0
$$</p>
<p>$$
\sum_{n=1}^N\sum_{j=1}^M\gamma(z_{nk})\mathbf{x}<em>{nij}+\lambda</em>{2ki}\sum_{j=1}^M\mu_{kij}=0
$$
$$
\begin{aligned}
\lambda_{2ki}=-\sum_{n=1}^N\gamma(z_{nk})\sum_{j=1}^M\mathbf{x}_{nij}=-N_k
\end{aligned}
$$</p>
<p>これを$\lambda_{2ki}$に代入して式を整理すると</p>
<p>$$
\mu_{kij}=\frac{1}{N_k}\sum_{n=1}^N\gamma(z_{nk})\mathbf{x}_{nij}
$$</p>
<p>となる．</p>
<h2 id="演習-920"><a class="header" href="#演習-920">演習 9.20</a></h2>
<div class="panel-primary">
<p>ベイズ線形回帰モデルについて，期待完全データ対数尤度関数
$$\begin{aligned} \mathbb{E}[\ln p(\mathbf{t}, \mathbf{w} \mid \alpha, \beta)]=&amp; \frac{M}{2} \ln \left(\frac{\alpha}{2 \pi}\right)-\frac{\alpha}{2} \mathbb{E}\left[\mathbf{w}^{\mathrm{T}} \mathbf{w}\right]+\frac{N}{2} \ln \left(\frac{\beta}{2 \pi}\right) \ &amp;-\frac{\beta}{2} \sum_{n=1}^{N} \mathbb{E}\left[\left(t_{n}-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}<em>{n}\right)^{2}\right] \end{aligned} \tag{9.62}$$
の最大化は$\alpha$に関するMステップの更新式
$$\alpha=\frac{M}{\mathbb{E}\left[\mathbf{w}^{\mathrm{T}} \mathbf{w}\right]}=\frac{M}{\mathbf{m}</em>{N}^{\mathrm{T}} \mathbf{m}<em>{N}+\operatorname{Tr}\left(\mathbf{S}</em>{N}\right)} \tag{9.63}$$
を導くことを示せ．</p>
</div>
<p>$\mathbf{w}$の事後分布は$(3.49)$より以下になる。
$$p(\mathbf{w} \mid \mathbf{t})=\mathcal{N}\left(\mathbf{w} \mid \mathbf{m}<em>{N}, \mathbf{S}</em>{N}\right)$$</p>
<p>(9.62) を$\alpha$で微分して0とおくと</p>
<p>$$\frac{\partial \mathbb{E}}{\partial \alpha}=\frac{M}{2} \frac{1}{\alpha}-\frac{1}{2} \mathbb{E}\left[\mathbf{w}^{\mathrm{T}} \mathbf{w}\right]=0 \ \therefore \alpha=\frac{M}{\mathbb{E}\left[\mathbf{w}^{\mathrm{T}} \mathbf{w}\right]}$$</p>
<p>$$\begin{aligned} \mathbb{E}\left[\mathbf{w}^{\mathrm{T}} \mathbf{w}\right] &amp;=\mathbb{E}\left[\mathbf{w}<em>{1}^{2}+\mathbf{w}</em>{2}^{2}+\cdots \cdot\right] \ &amp;=\mathbb{E}\left[\mathbf{w}<em>{1}^{2}\right]+\mathbb{E}\left[\mathbf{w}</em>{2}^{2}\right]+\cdots \cdot \ &amp;=\operatorname{Tr}\left(\mathbb{E}\left[\mathbf{w} \mathbf{w}^{\mathrm{T}}\right]\right) \quad \because \operatorname{Tr}\left(\mathbb{E}\left[\mathbf{w} \mathbf{w}^{\mathrm{T}}\right]\right)=E\left[\mathbf{w}<em>{1}^{2}\right]+\cdots \cdot\ &amp;=\operatorname{Tr}\left(\mathbf{m}</em>{N} \mathbf{m}<em>{N}^{\mathrm{T}}+\mathbf{S}</em>{N}\right)\hspace{1em} \because(2.62) E\left[\mathbf{w} \mathbf{w}^{\mathrm{T}}\right]=\mathbf{m}<em>{N} \mathbf{m}</em>{N}^{\mathrm{T}}+\mathbf{S}<em>{N} \ &amp;=\mathbf{m}</em>{N}^{\mathrm{T}} \mathbf{m}<em>{N}+\operatorname{Tr}\left(\mathbf{S}</em>{N}\right) \end{aligned}$$</p>
<p>ゆえに(9.63)が導かれる。
$$\alpha=\frac{M}{\mathbb{E}\left[\mathbf{\mathbf{w}}^{\mathrm{T}} \mathbf{\mathbf{w}}\right]}=\frac{M}{\mathbf{m}<em>{N}^{\mathrm{T}} \mathbf{m}</em>{N}+\operatorname{Tr}\left(\mathbf{S}_{N}\right)}$$</p>
<h2 id="演習-921"><a class="header" href="#演習-921">演習 9.21</a></h2>
<div class="panel-primary">
<p>ベイズ線形回帰モデルについて，3.5節におけるエビデンスの枠組みを用いて，パラメータ$\alpha$に関する
$$\alpha=\frac{M}{\mathbb{E}\left[\mathbf{w}^{\mathrm{T}} \mathbf{w}\right]}=\frac{M}{\mathbf{m}<em>{N}^{\mathrm{T}} \mathbf{m}</em>{N}+\operatorname{Tr}\left(\mathbf{S}_{N}\right)} \tag{9.63}$$
と同様の，パラメータ$\beta$に関するMステップ更新式を導け．</p>
</div>
<p>※教科書P.164より、この目的は第3章のエビデンス近似のEMアルゴリズムを用いた$\alpha, \beta$の計算法の再定義。</p>
<p>Eステップではパラメータ$\alpha, \beta$の現在の値に基づく$\mathbf{w}$の事後分布の計算を行う。$(3.49)$ですでに$\mathbf{w}$の事後分布を求めてあるので</p>
<p>$$p(\mathbf{w} \mid t)=\mathcal{N}\left(\mathbf{w} \mid \mathbf{m}<em>{N}, \mathbf{S}</em>{N}\right) \tag{3.49}$$</p>
<p>Mステップではこの量を$\alpha, \beta$について最大化する。完全データ対数尤度</p>
<p>$$\ln p(\mathbf{t}, \mathbf{w} \mid \alpha, \beta)=\ln p(\mathbf{t} \mid \mathbf{w}, \beta)+\ln p(\mathbf{w} \mid \alpha) \tag{5.61}$$</p>
<p>$\mathbf{w}$について期待値を取ると、</p>
<p>$$\begin{aligned} \mathbb{E}[\ln p(\mathbf{t}, \mathbf{w} \mid \alpha, \beta)]=&amp; \frac{M}{2} \ln \left(\frac{\alpha}{2 \pi}\right)-\frac{\alpha}{2} \mathbb{E}\left[\mathbf{w}^{\mathrm{T}} \mathbf{w}\right]+\frac{N}{2} \ln \left(\frac{\beta}{2 \pi}\right) \ &amp;-\frac{\beta}{2} \sum_{n=1}^{N} \mathbb{E}\left[\left(t_{n}-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}_{n}\right)^{2}\right] \end{aligned} \tag{9.62}$$</p>
<p>これを$\beta$で微分して$0$とおくと</p>
<p>$$\frac{\partial \mathbb{E}}{\partial \beta}=\frac{N}{2} \frac{1}{\beta}-\frac{1}{2} \sum_{n} \mathbb{E}\left[\left(t_{n}-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}_{n}\right)^{2}\right]=0$$</p>
<p>$$\begin{aligned}
\mathbb{E}\left[\left(t_{n}-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}<em>{n}\right)^{2}\right]
&amp;=\mathbb{E}\left[t</em>{n}^{2}-2 t_{n} \mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}<em>{n}+\boldsymbol{\phi}</em>{n}^{\mathrm{T}} \mathbf{w} \mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}<em>{n}\right] \
&amp;=\mathbb{E}[t</em>{n}]^{2}-2 t_{n} \mathbb{E}[\mathbf{w}]^{\mathrm{T}} \boldsymbol{\phi}<em>{n}+\boldsymbol{\phi}</em>{n}^{\mathrm{T}} \mathbb{E}\left[\mathbf{w}\mathbf{w}^{\mathrm{T}}\right] \boldsymbol{\phi}<em>{n} \
&amp;=t</em>{n}^{2}-2 t_{n} \mathbf{m}<em>{N}^{\mathrm T} \boldsymbol{\phi}</em>{n}+\boldsymbol{\phi}<em>{n}^{\mathrm T}\left(\mathbf{m}</em>{N} \mathbf{m}<em>{N}^{\mathrm{T}}+\mathbf{S}</em>{N}\right) \boldsymbol{\phi}<em>{n}\qquad \because (2.59),(2.62)\
&amp;=\left(t</em>{n}-\mathbf{m}<em>{N}^{\mathrm{T}} \boldsymbol{\phi}</em>{n}\right)^{2}+\boldsymbol{\phi}<em>{n}^{\mathrm{T}} \mathbf{S}</em>{N} \boldsymbol{\phi}_{n}
\end{aligned}$$</p>
<p>なので以下となる。</p>
<p>$$\frac{N}{\beta}-\sum_{n=1}^N\left{\left(t_{n}-\mathbf{m}<em>{N}^{\mathrm{T}} \boldsymbol{\phi}</em>{n}\right)^{2}+\boldsymbol{\phi}<em>{n}^{\mathrm{T}} \mathbf{S}</em>{N} \boldsymbol{\phi}_{n}\right}=0$$</p>
<p>$$\begin{aligned}
\frac{1}{\beta} &amp;=\frac{1}{N} \sum_{n=1}^N\left{\left(t_{n}-\mathbf{m}<em>{N}^{\mathrm{T}} \boldsymbol{\phi}</em>{n}\right)^{2}+\boldsymbol{\phi}<em>{n}^{\mathrm{T}} \mathbf{S}</em>{N} \boldsymbol{\phi}<em>{n}\right} \
&amp;=\frac{1}{N}\left{\sum</em>{n=1}^N\left(t_{n}-\mathbf{m}<em>{N}^{\mathrm{T}} \boldsymbol{\phi}</em>{n}\right)^{2}+\operatorname{Tr}\left(\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \mathbf{S}_{N}\right)\right}
\end{aligned}$$</p>
<p>※最後の変形は以下のようになる。$N \times M$の計画行列$\mathbf{\Phi}\ (3.16)$を利用して</p>
<p>$$\mathbf{\Phi} = \begin{pmatrix} \boldsymbol{\phi}_1^{\mathrm T} \ \vdots \ \boldsymbol{\phi}_N^{\mathrm T} \end{pmatrix} \
\mathbf{\Phi S}_N\mathbf{\Phi}^{\mathrm T} = \begin{pmatrix} \boldsymbol{\phi}_1^{\mathrm T} \ \vdots \ \boldsymbol{\phi}_N^{\mathrm T} \end{pmatrix} \mathbf{S}_N \begin{pmatrix} \boldsymbol{\phi}_1 &amp; \ldots \ \boldsymbol{\phi}_N \end{pmatrix} =
\begin{pmatrix} \boldsymbol{\phi}_1^{\mathrm T}\mathbf{S}_N\boldsymbol{\phi}_1 &amp; \cdots &amp; \boldsymbol{\phi}_1^{\mathrm T}\mathbf{S}_N\boldsymbol{\phi}_N \ \vdots &amp; \ddots &amp; \vdots \ \boldsymbol{\phi}_N^{\mathrm T}\mathbf{S}_N\boldsymbol{\phi}_1 &amp; \cdots &amp; \boldsymbol{\phi}_N^{\mathrm T}\mathbf{S}_N\boldsymbol{\phi}_N \end{pmatrix}$$</p>
<p>この結果から、$\sum_{n=1}^{N}\boldsymbol{\phi}_n^{\mathrm T}\mathbf{S}<em>N\boldsymbol{\phi}<em>n = \operatorname{Tr}\left(\mathbf{\Phi} \mathbf{S}</em>{N} \mathbf{\Phi}^{\mathrm{T}}\right) = \operatorname{Tr}\left(\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \mathbf{S}</em>{N}\right)$となることがわかる。</p>
<h2 id="演習-922"><a class="header" href="#演習-922">演習 9.22</a></h2>
<div class="panel-primary">
<p>期待完全データ対数尤度
$$\mathbb{E}<em>{\mathbf{w}}[{\ln p(\mathbf{t} \mid \mathbf{X}, \mathbf{w}, \beta) p(\mathbf{w} \mid \boldsymbol{\alpha})}] \tag{9.66}$$
を最大化することで.回帰問題のためのRVM(関連ベクトルマシン)の超パラメータについてのMステップ更新式
$$\alpha</em>{i}^{\text {new }} =\frac{1}{m_{i}^{2}+\Sigma_{i i}} \tag{9.67}$$
$$\left(\beta^{\text {new }}\right)^{-1} =\frac{|\mathbf{t}-\Phi \mathbf{m}|^{2}+\beta^{-1} \sum_{i} \gamma_{i}}{N} \tag{9.68}$$
を導け．</p>
</div>
<p>$$p(t \mid \mathbf{x}, \mathbf{w}, \beta)=\mathcal{N}\left(t \mid y(\mathbf{x}), \beta^{-1}\right) \tag{7.76}$$
$$p(\mathbf{t} \mid \mathbf{X}, \mathbf{w}, \beta)=\prod_{n=1}^{N} p\left(t_{n} \mid \mathbf{x}<em>{n}, \mathbf{w}, \beta\right) \tag{7.79}$$
$$p(\mathbf{w} \mid \boldsymbol{\alpha})=\prod</em>{i=1}^{M} \mathcal{N}\left(w_{i} \mid 0, \alpha_{i}^{-1}\right) \tag{7.80}$$
よりまずは対数尤度関数を計算する。</p>
<p>$$\begin{aligned}
\ln p(\mathbf{t} \mid \mathbf{X}, \mathbf{w}, \beta) p(\mathbf{w} \mid \boldsymbol{\alpha}) &amp;=\ln p(\mathbf{t} \mid \mathbf{X}, \mathbf{w}, \beta)+\ln p(\mathbf{w} \mid \boldsymbol{\alpha}) \
&amp;=\sum_{n=1}^{N} \ln p\left(t_{n} \mid x_{n}, \mathbf{w}, \beta^{-1}\right)+\sum_{i=1}^{M} \ln \mathcal{N}\left(w_{i} \mid 0, \alpha_{i}^{-1}\right) \
&amp;=\sum_{n=1}^{N} \ln \mathcal{N}\left(t_{n} \mid \mathbf{w}^{\mathrm T} \boldsymbol{\phi}<em>{n}, \beta^{-1}\right)+\sum</em>{i=1}^{M} \ln \mathcal{N}\left(w_{i} \mid 0, \alpha_{i}^{-1}\right) \
&amp;=\frac{N}{2} \ln \frac{\beta}{2 \pi}-\frac{\beta}{2} \sum_{n=1}^{N}\left(t_{n}-\mathbf{w}^{\mathrm T} \boldsymbol{\phi}<em>{n}\right)^{2}+\frac{1}{2} \sum</em>{i=1}^{M} \ln \frac{\alpha_{i}}{2 \pi}-\sum_{i=1}^{M} \frac{\alpha_{i}}{2} w_{i}^{2}
\end{aligned}$$</p>
<p>Mステップでは、この対数尤度関数の潜在変数$\mathbf{w}$についての期待値を取る。</p>
<p>期待値を取る$\mathbf{w}$の事後分布は、
$$p(\mathbf{w} \mid \mathbf{t}, \mathbf{X}, \boldsymbol{\alpha}, \beta)=\mathcal{N}(\mathbf{w} \mid \mathbf{m}, \mathbf{\Sigma})\tag{7.81}$$
を利用する。</p>
<p>$$\mathbb{E}<em>{\mathbf{w}}[\ln p]=\frac{N}{2} \ln \frac{\beta}{2 \pi}-\frac{\beta}{2} \sum</em>{n=1}^{N} \mathbb{E}<em>{\mathbf{w}}\left[\left(t</em>{n}-\mathbf{w}^{\mathrm T} \boldsymbol{\phi}<em>{n}\right)^{2}\right]+\frac{1}{2} \sum</em>{i=1}^{M} \ln \frac{\alpha_{i}}{2 \pi}-\sum_{i=1}^{M} \frac{\alpha_{i}}{2} \mathbb{E}<em>{\mathbf{w}}\left[w</em>{i}^{2}\right]$$</p>
<p>$\alpha_{i}$に関して微分したものを$0$とおくと</p>
<p>$$\frac{1}{2} \cdot \frac{2 \pi}{\alpha_{i}} \cdot \frac{1}{2 \pi}-\frac{1}{2} \mathbb{E}<em>{\mathbf{w}}\left[w</em>{i}^{2}\right] =0$$</p>
<p>$$\begin{aligned}
\alpha_{i}^{\textrm{new}} &amp;=\frac{1}{\mathbb{E}<em>{\mathbf{w}}\left[w</em>{i}^{2}\right]} \
&amp;=\frac{1}{\mathbb{E}<em>{\mathbf{w}}\left[\mathbf{ww}^{\mathrm T}\right]</em>{ii} }\
\end{aligned}$$</p>
<p>ここで、$\mathbb{E}<em>{\mathbf{w}}\left[\mathbf{ww}^{\mathrm T}\right]</em>{ii}$は$\mathbb{E}_{\mathbf{w}}\left[\mathbf{ww}^{\mathrm T}\right]$の行列$ii$成分を表している。今、$(7.81)$式から$\mathbf{w}$は$\mathcal{N}(\mathbf{w}\mid \mathbf{m}, \mathbf{\Sigma})$から生成されているので（つまり$\mathbf{w} \sim \mathcal{N}(\mathbf{w}\mid \mathbf{m}, \mathbf{\Sigma})$）、$(2.62)$の結果から</p>
<p>$$\mathbb{E}\left[\mathbf{w} \mathbf{w}^{\mathrm T}\right]=\mathbf{m} \mathbf{m}^{\mathrm T}+\mathbf{\Sigma}$$</p>
<p>これを用いて</p>
<p>$$\alpha_{i}=\frac{1}{\mathbb{E}<em>{\mathbf{w}}\left[\mathbf{w} \mathbf{w}^{\mathrm T}\right]</em>{ii}}=\frac{1}{\left(\mathbf{m m}^{\mathrm T}+\mathbf{\Sigma}\right)<em>{ii}}=\frac{1}{m</em>{i}^{2}+\Sigma_{i i}}$$</p>
<p>同様に$\beta$を微分したものを$0$とおくと</p>
<p>$$\frac{N}{2} \cdot \frac{2 \pi}{\beta} \cdot \frac{1}{2 \pi}-\frac{1}{2} \sum_{n=1}^{N} \mathbb{E}<em>{\mathbf{w}}\left[\left(t</em>{n}-\mathbf{w}^{\mathrm T} \boldsymbol{\phi}_{n}\right)^{2}\right]=0$$</p>
<p>$$\begin{aligned}\beta^{\textrm{new}} &amp;= \frac{N}{\sum_{n=1}^{N} \mathbb{E}<em>{\mathbf{w}}\left[\left(t</em>{n}-\mathbf{w}^{\mathrm T} \boldsymbol{\phi}<em>{n}\right)^{2}\right]} \
\left(\beta^{\textrm{new}}\right)^{-1} &amp;=\frac{1}{N}\sum</em>{n=1}^{N} \mathbb{E}<em>{\mathbf{w}}\left[\left(t</em>{n}-\mathbf{w}^{\mathrm T} \boldsymbol{\phi}<em>{n}\right)^{2}\right] \
&amp;=\frac{1}{N} \sum</em>{n=1}^{N}\left{\left(t_{n}-\mathbf{m}^{\mathrm T} \boldsymbol{\phi}<em>{N}\right)^{2}+\operatorname{Tr}\left[\boldsymbol{\phi}</em>{n}^{\mathrm{T}} \mathbf{\Sigma} \boldsymbol{\phi}_{n}\right]\right} \
&amp;=\frac{1}{N}\left{|\mathbf{t}-\mathbf{\Phi} \mathbf{m}|^{2}+\operatorname{Tr}\left[\mathbf{\Phi}^{\mathrm T} \mathbf{\Phi} \mathbf{\Sigma}\right]\right} \end{aligned}
$$</p>
<p>ここで$\sum_{n=1}^{N} \mathbb{E}<em>{\mathbf{w}}\left[\left(t</em>{n}-\mathbf{w}^{\mathrm T} \boldsymbol{\phi}_{n}\right)^{2}\right]$部分については演習9.21と同様に変形した。</p>
<p>以下では$\operatorname{Tr}\left[\mathbf{\Phi}^{\mathrm T} \mathbf{\Phi} \mathbf{\Sigma}\right] = \beta^{-1} \sum_{i} \gamma_{i}$を示す。RVMでは$\mathbf{\Phi} , \mathbf{\Sigma}$に関する情報は</p>
<p>$$\mathbf{\Sigma}=\left(\mathbf{A}+\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \tag{7.83}$$</p>
<p>で与えられる。この両辺にまず$\left(\mathbf{A}+\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)$をかけて</p>
<p>$$\begin{aligned} \left(\mathbf{A}+\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)\mathbf{\Sigma} &amp;= \mathbf{I} \
\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\mathbf{\Sigma} &amp;= \beta^{-1}\left(\mathbf{I}-\mathbf{A\Sigma}\right)
\end{aligned}$$</p>
<p>これを$\operatorname{Tr}\left(\mathbf{\Phi} \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T}\right)=\operatorname{Tr}\left(\mathbf{\Phi}^{\mathrm T} \mathbf{\Phi} \mathbf{\Sigma} \right)$に代入すれば</p>
<p>$$\begin{aligned} \operatorname{Tr}\left(\mathbf{\Phi}^{\mathrm T} \mathbf{\Phi} \mathbf{\Sigma} \right) &amp;=\beta^{-1} \operatorname{Tr}(\mathbf{I}-\mathbf{A\Sigma}) \
&amp;=\beta^{-1} \sum_{i}\left(1-\alpha_{i} \Sigma_{ii}\right) \ &amp;=\beta^{-1} \sum_{i} \gamma_{i} \end{aligned}$$</p>
<p>以上で$(9.68)$式が導かれた。</p>
<h2 id="演習-923"><a class="header" href="#演習-923">演習 9.23</a></h2>
<div class="panel-primary">
<p>7.2.1節において回帰問題のためのRVMの超パラメータ$\boldsymbol{\alpha}$と$\beta$の更新式$(7.87)$と$(7.88)$を導くのに周辺尤度を直接最大化した．同様に9.3.4節において，同じ周辺尤度を最大化するのにEMアルゴリズムを用いて更新式
$$\alpha_{i}^{\textrm{new}} =\frac{1}{m_{i}^{2}+\Sigma_{i i}} \tag{9.67}$$
$$\left(\beta^{\textrm{new}}\right)^{-1} =\frac{|\mathbf{t}-\mathbf{\Phi} \mathbf{m}|^{2}+\beta^{-1} \sum_{i} \gamma_{i}}{N} \tag{9.68}$$
を求めた．任意の停留点においてこれら2組の更新式が厳密に等価であることを示せ．</p>
</div>
<p>$$
\begin{aligned}
\alpha_{i}^{\textrm{new}} &amp;=\frac{\gamma_{i}}{m_{i}^{2}}\qquad　(7.87) \
\left(\beta^{\mathrm{new}}\right)^{-1} &amp;=\frac{|\mathbf{t}-\mathbf{\Phi} \mathbf{m}|^{2}}{N-\sum_{i} \gamma_{i}}\quad　(7.88)\
\end{aligned}
$$
$$
\gamma_{i}=1-\alpha_{i} \Sigma_{i i}\quad　(7.89)
$$</p>
<p>この問題では「$(7.87)$かつ$(7.88)$」⇔「$(9.67)$かつ$(9.68)$」を示せばよい。$(7.87)$に$(7.89)$を代入すれば</p>
<p>$$\begin{aligned}
\alpha_{i}^{\textrm{new}} &amp;=\frac{1-\alpha_{i}^{\textrm{new}} \Sigma_{i i}}{m_{i}^{2}} \
\Leftrightarrow \alpha_{i}^{\textrm{new}}&amp;=\frac{1}{m_{i}^{2}+\sum_{i i}}\quad (\Leftrightarrow(9.67))
\end{aligned}$$</p>
<p>βに関しては、逆に導出する。(9.68)において$\beta$を$\beta^{\textrm{new}}$に置き換えると</p>
<p>$$\begin{aligned}
\left(\beta^{\textrm{new}}\right)^{-1}&amp;=\frac{|\mathbf{t}-\mathbf{\Phi} \mathbf{m}|^{2}+\left(\beta^{\textrm{new}}\right)^{-1} \sum_{i} \gamma_{i}}{N} \
\Leftrightarrow\left(\beta^{\textrm{new}}\right)^{-1}&amp;=\frac{|\mathbf{t}-\mathbf{\Phi} \mathbf{m}|^{2}}{N-\sum_{i} \gamma_{i}}
\end{aligned}$$</p>
<h2 id="演習-924"><a class="header" href="#演習-924">演習 9.24</a></h2>
<div class="panel-primary">
<p>$$\ln p(\mathbf{X} \mid \boldsymbol{\theta})=\mathcal{L}(q, \theta)+\mathrm{KL}(q | p) \tag{9.70}$$
を示せ．ただし$\mathcal{L}(q, \boldsymbol{\theta})$と$\mathrm{KL}(q | p)$はそれぞれ
$$\mathcal{L}(q, \boldsymbol{\theta})=\sum_{\mathbf{Z}} q(\mathbf{Z}) \ln \left{\frac{p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})}{q(\mathbf{Z})}\right} \tag{9.71}$$
$$\mathrm{KL}(q | p)=-\sum_{\mathbf{Z}} q(\mathbf{Z}) \ln \left{\frac{p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta})}{q(\mathbf{Z})}\right}\tag{9.72}$$
で定義されている．</p>
</div>
<p>乗法定理より、$\ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})=\ln p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta})+\ln p(\mathbf{X} \mid \boldsymbol{\theta})$であるから</p>
<p>$$\begin{aligned} \mathcal{L}(q, \boldsymbol{\theta}) &amp;=\sum_{\mathbf{Z}} q(\mathbf{Z}){\ln p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta})+\ln p(\mathbf{X} \mid \boldsymbol{\theta})-\ln q(\mathbf{Z})} \ &amp;=\ln p(\mathbf{X} \mid \boldsymbol{\theta}) \sum_{\mathbf{Z}} q(\mathbf{Z})+\sum_{\mathbf{Z}} q(\mathbf{Z}) \ln \frac{p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta})}{q(\mathbf{Z})} \ &amp;=\ln p(\mathbf{X} \mid \boldsymbol{\theta})-\operatorname{KL}(q | p) \end{aligned}$$</p>
<p>より、$(9.70)$が示された。</p>
<h2 id="演習-925"><a class="header" href="#演習-925">演習 9.25</a></h2>
<div class="panel-primary">
<p>対数尤度関数$\ln p(\mathbf{X}\mid \boldsymbol{\theta})$とその下界
$$\mathcal{L}(q, \boldsymbol{\theta})=\sum_{\mathbf{Z}} q(\mathbf{Z}) \ln \left{\frac{p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})}{q(\mathbf{Z})}\right} \tag{9.71}$$
(ただし$q(\mathbf{Z}) = p(\mathbf{Z}\mid \mathbf{X}, \boldsymbol{\theta}^{\textrm{old}})$) は，点$\boldsymbol{\theta} = \boldsymbol{\theta}^{\textrm{old}}$において同じ勾配を持つことを示せ．</p>
</div>
<p>前提条件である、$q(\mathbf{Z})=p\left(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta}^{\text {old }}\right)$ の場合、
$$\mathrm{KL}(q | p)=-\sum_{\mathbf{Z}} q(\mathbf{Z}) \ln \left{\frac{p(\mathbf{Z} \mid \mathbf{X}, \boldsymbol{\theta})}{q(\mathbf{Z})}\right} = 0$$
となっている。また常に$\mathrm{KL}(q | p) \ge 0$なので、点$\boldsymbol{\theta} = \boldsymbol{\theta}^{\textrm{old}}$において
$$
\left.\frac{\partial}{\partial \boldsymbol{\theta}} \mathrm{KL}(q | p)\right|<em>{\boldsymbol{\theta}=\boldsymbol{\theta}</em>{\textrm{old}}} = \mathbf{0}
$$</p>
<p>となる。
$$\ln p(\mathbf{X} \mid \boldsymbol{\theta})=\mathcal{L}(q, \theta)+\mathrm{KL}(q | p) \tag{9.70}$$
式の$\boldsymbol{\theta}$についての両辺の微分をとり、点$\boldsymbol{\theta} = \boldsymbol{\theta}^{\textrm{old}}$において右辺の第2項は$\mathbf{0}$になるので、左辺の微分（勾配）と右辺の第1項の微分（勾配）は同じになる。すなわち、</p>
<p>$$\left.\frac{\partial}{\partial \boldsymbol{\theta}} \operatorname{ln}p(\mathbf{X} \mid \boldsymbol{\theta})\right|<em>{\boldsymbol{\theta}=\boldsymbol{\theta}</em>{\textrm{old}}} = \left.\frac{\partial}{\partial \boldsymbol{\theta}} \mathcal{L}(q, \boldsymbol{\theta})\right|<em>{\boldsymbol{\theta}=\boldsymbol{\theta}</em>{\textrm{old}}}$$</p>
<p>を得る。</p>
<h2 id="演習-926"><a class="header" href="#演習-926">演習 9.26</a></h2>
<div class="panel-primary">
<p>混合ガウス分布について，負担率を1つのデータ点$\mathbf{x}<em>m$のみについてしか更新しない逐次型EMアルゴリズムを考える．Mステップの式$(9.17)$と$(9.18)$から始めて，混合要素の平均を更新する式
$$\boldsymbol{\mu}</em>{k}^{\text {new }}=\boldsymbol{\mu}<em>{k}^{\text {old }}+\left(\frac{\gamma^{\text {new }}\left(z</em>{m k}\right)-\gamma^{\text {old }}\left(z_{m k}\right)}{N_{k}^{\text {new }}}\right)\left(\mathbf{x}<em>{m}-\boldsymbol{\mu}</em>{k}^{\text {old }}\right) \tag{9.78}$$
$$N_{k}^{\text {new }}=N_{k}^{\text {old }}+\gamma^{\text {new }}\left(z_{m k}\right)-\gamma^{\text {old }}\left(z_{m k}\right)\tag{9.79}$$
を導け．</p>
</div>
<p>※P.171の流れの通り、Eステップで$n=m$となる1つのデータ点$m$のみについて$\gamma(z_{nk})$を更新する。混合分布が指数型分布族の場合は、負担率が十分統計量となるので、$n=m$についての更新差分だけ考慮すれば良い。</p>
<p>$$N_{k}=\sum_{n=1}^{N} \gamma\left(z_{n k}\right) \tag{9.18}$$
より
$$\begin{aligned}
N_{k}^{\textrm{new}} &amp;=\gamma^{\textrm{old}}\left(z_{1 k}\right)+\gamma^{\textrm{old}}\left(z_{2 k}\right)+\cdots+\gamma^{\textrm{new}}\left(z_{m k}\right)+\cdots+\gamma^{\textrm{old}}\left(z_{N k}\right) \
&amp;=\sum_{n=1}^{N} \gamma^{\textrm{old}}\left(z_{n k}\right)-\gamma^{\textrm{old}}\left(z_{m k}\right)+\gamma^{\textrm{new}}\left(z_{m k}\right) \
&amp;=N_{k}^{\textrm{old}}+\gamma^{\textrm{new}}\left(z_{m k}\right)-\gamma^{\textrm{old}}\left(z_{m k}\right)
\end{aligned}$$</p>
<p>以上から$(9.79)$が示された。</p>
<p>$$\mu_{k}=\frac{1}{N_{k}} \sum_{n=1}^{N} \gamma\left(z_{n k}\right) \mathbf{x}_{n} \tag{9.17}$$</p>
<p>より</p>
<p>$$\begin{aligned}
\boldsymbol{\mu}<em>{k}^{\textrm{new}}&amp;=\frac{1}{N</em>{k}^{\textrm{new}}}\left{\gamma^{\textrm{old}}\left(z_{1 k}\right) \mathbf{x}<em>{1}+\gamma^{\textrm{old}}\left(z</em>{2 k}\right) \mathbf{x}<em>{2}+\cdots+\gamma^{\textrm{new}}\left(z</em>{m k}\right) \mathbf{x}<em>{m}+\cdots+\gamma^{\textrm{old}}\left(z</em>{N k}\right) \mathbf{x}<em>{N}\right}\
&amp;=\frac{1}{N</em>{k}^{\textrm{new}}}\left{\sum_{n=1}^{N} \gamma^{\textrm{old}}\left(z_{n k}\right) \mathbf{x}<em>{n}-\gamma^{\textrm{old}}\left(z</em>{m k}\right) \mathbf{x}<em>{m}+\gamma^{\textrm{new}}\left(z</em>{m k}\right) \mathbf{x}<em>{m}\right}\
&amp;=\frac{1}{N</em>{k}^{\textrm{new}}}\left{N_{k}^{\textrm{old}} \boldsymbol{\mu}<em>{k}^{\textrm{old}}-\gamma^{\textrm{old}}\left(z</em>{m k}\right) \mathbf{x}<em>{m}+\gamma^{\textrm{new}}\left(z</em>{m k}\right) \mathbf{x}<em>{m}\right}\
&amp;=\frac{1}{N</em>{k}^{\textrm{new}}}\left[\underbrace{\left{N_{k}^{\textrm{new}}+\gamma^{\textrm{old}}\left(z_{m k}\right)-\gamma^{\textrm{new}}\left(z_{m k}\right)\right.}<em>{(\because(9.79))}} \boldsymbol{\mu}</em>{k}^{\textrm{old}}-\gamma^{\textrm{old}}\left(z_{m k}\right) \mathbf{x}<em>{m}+\gamma^{\textrm{new}}\left(z</em>{m k}\right) \mathbf{x}<em>{m}\right] \
&amp;=\frac{1}{N</em>{k}^{\textrm{new}}}\left[N_{k}^{\textrm{new}} \boldsymbol{\mu}<em>{k}^{\textrm{old}}-\left{\gamma^{\textrm{new}}\left(z</em>{m k}\right)-\gamma^{\textrm{old}}\left(z_{m k}\right)\right} \boldsymbol{\mu}<em>{k}^{\textrm{old}}+\left{\gamma^{\textrm{new}}\left(z</em>{m k}\right)-\gamma^{\textrm{old}}\left(z_{m k}\right)\right} \mathbf{x}<em>{m}\right]\
&amp;=\boldsymbol{\mu}</em>{k}^{\textrm{old}}+\frac{1}{N_{k}^{\textrm{new}}}\left{\gamma^{\textrm{new}}\left(z_{m k}\right)-\gamma^{\textrm{old}}\left(z_{m k}\right)\right}\left(\mathbf{x}<em>{m}-\boldsymbol{\mu}</em>{k}^{\textrm{old}}\right)
\end{aligned}$$</p>
<p>以上から$(9.78)$が示された。</p>
<h2 id="演習-927"><a class="header" href="#演習-927">演習 9.27</a></h2>
<div class="panel-primary">
<p>平均の更新式
$$\boldsymbol{\mu}<em>{k}^{\text {new }}=\boldsymbol{\mu}</em>{k}^{\text {old }}+\left(\frac{\gamma^{\text {new }}\left(z_{m k}\right)-\gamma^{\text {old }}\left(z_{m k}\right)}{N_{k}^{\text {new }}}\right)\left(\mathbf{x}<em>{m}-\boldsymbol{\mu}</em>{k}^{\text {old }}\right) \tag{9.78}$$
と同様に，負担率を逐次型で更新する場合の混合ガウス分布の共分散行列と混合係数を更新するMステップの更新式を求めよ．</p>
</div>
<p>教科書P.154の混合ガウス分布のためのEMアルゴリズムから、$(9.24)-(9.26)$を利用して、Mステップでの更新後の混合係数は</p>
<p>$$\begin{aligned}
\pi^{\textrm{new}} &amp;=\frac{N^{\textrm{new}}}{N}(\because(9.26)) \ &amp;=\frac{N_{k}^{\text {old}}+\gamma^{\text {new}}\left(z_{m k}\right)-\gamma^{\text {old}}\left(z_{m k}\right)}{N} \
&amp;=\pi^{\textrm{old}} + \frac{\gamma^{\text {new}}\left(z_{m k}\right)-\gamma^{\text {old}}\left(z_{m k}\right)}{N}
\end{aligned}
$$</p>
<p>となる。</p>
<p>次に更新後の共分散行列は、古い値は$(9.19)$を利用して</p>
<p>$$
\mathbf{\Sigma}<em>{k}^{\textrm{old}}=\frac{1}{N</em>{k}^{\textrm{old}}} \sum_{n=1}^{N} \gamma\left(z_{n k}\right)\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}^{\textrm{old}}\right)\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}^{\textrm{old}}\right)^{\mathrm{T}}
$$</p>
<p>である。これを$n=m$について更新すると</p>
<p>$$\begin{aligned}\mathbf{\Sigma}<em>{k}^{\textrm{new}}&amp;=\frac{1}{N</em>{k}^{\textrm{new}}}\left(\sum_{n \neq m} \gamma^{\textrm{old}}\left(z_{n k}\right)\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}^{\textrm{old}}\right)\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}^{\textrm{old}}\right)^{\mathrm{T}} +\gamma^{\textrm{new}}\left(z_{m k}\right)\left(\mathbf{x}<em>{m}-\boldsymbol{\mu}</em>{k}^{\textrm{new}}\right)\left(\mathbf{x}<em>{m}-\boldsymbol{\mu}</em>{k}^{\textrm{new}}\right)^{\mathrm{T}}\right) \
&amp;=\frac{1}{N_{k}^{\textrm{new}}}\left(N_{k}^{\textrm{old}} \mathbf{\Sigma}<em>{k}^{\textrm{old}}-\gamma^{\textrm{old}}\left(z</em>{m k}\right)\left(\mathbf{x}<em>{m}-\boldsymbol{\mu}</em>{k}^{\textrm{old}}\right)\left(\mathbf{x}<em>{m}-\boldsymbol{\mu}</em>{k}^{\textrm{old}}\right)^{\mathrm{T}}+\gamma^{\textrm{new}}\left(z_{m k}\right)\left(\mathbf{x}<em>{m}-\boldsymbol{\mu}</em>{k}^{\textrm{new}}\right)\left(\mathbf{x}<em>{m}-\boldsymbol{\mu}</em>{k}^{\textrm{new}}\right)^{\mathrm{T}}\right) \
&amp;=\mathbf{\Sigma}<em>{k}^{\mathrm{old}} -\frac{\gamma^{\textrm{old}}\left(z</em>{m k}\right)}{N_{k}^{\mathrm{new}}}\left(\left(\mathbf{x}<em>{m}-\boldsymbol{\mu}</em>{k}^{\textrm{old}}\right)\left(\mathbf{x}<em>{m}-\boldsymbol{\mu}</em>{k}^{\textrm{old}}\right)^{\mathrm{T}}-\mathbf{\Sigma}<em>{k}^{\textrm{old}}\right) +\frac{\gamma^{\textrm{new}}\left(z</em>{m k}\right)}{N_{k}^{\textrm{new}}}\left(\left(\mathbf{x}<em>{m}-\boldsymbol{\mu}</em>{k}^{\textrm{new}}\right)\left(\mathbf{x}<em>{m}-\boldsymbol{\mu}</em>{k}^{\textrm{new}}\right)^{\mathrm{T}}-\mathbf{\Sigma}_{k}^{\textrm{old}}\right)
\end{aligned}
$$</p>
<p>となる。途中で$(9.79)$式の変形$N_{k}^{\textrm{old}} = N_{k}^{\textrm{new}} - \gamma^{\text {new }}\left(z_{m k}\right)+ \gamma^{\text {old }}\left(z_{m k}\right)$を利用して$\mathbf{\Sigma}_{k}^{\mathrm{old}}$とその更新差分に分解した。</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="prml第10章演習問題解答-1018まで"><a class="header" href="#prml第10章演習問題解答-1018まで">PRML第10章演習問題解答 (10.18まで)</a></h1>
<head>
<style>
  div.panel-primary {
	border: 1px solid #000;
    margin: 10px 5px;
    padding: 16px 10px 0px;
  }
</style>
</head>
<h2 id="演習-101"><a class="header" href="#演習-101">演習 10.1</a></h2>
<div class="panel-primary">
<p>観測データの対数周辺尤度$\ln p(\mathbf{X})$は</p>
<p>$$\ln p(\mathbf{X})=\mathcal{L}(q)+\mathrm{KL}(q | p) \tag{10.2}$$</p>
<p>のように二つの項に分解できることを確かめよ．ここで，$\mathcal{L}(q)$, $\mathrm{KL}(q | p)$は$(10.3), (10.4)$</p>
<p>$$
\mathcal{L}(q) = \int q(\mathbf{Z}) \ln \left{\frac{p(\mathbf{X}, \mathbf{Z})}{q(\mathbf{Z})}\right} \mathrm{d} \mathbf{Z} \tag{10.3}
$$</p>
<p>$$
\mathrm{KL}(q | p) =-\int q(\mathbf{Z}) \ln \left{\frac{p(\mathbf{Z} \mid \mathbf{X})}{q(\mathbf{Z})}\right} \mathrm{d} \mathbf{Z} \tag{10.4}
$$</p>
<p>で与えられる．</p>
</div>
<p>$(10.3)$と$(10.4)$を足すと</p>
<p>$$
\begin{aligned}
\mathcal{L}(q)+\mathrm{KL}(q | p)
&amp;=\int q(\mathbf{Z})\left[\ln \frac{p(\mathbf{X}, \mathbf{Z})}{q(\mathbf{Z})}-\ln \frac{p(\mathbf{Z} \mid \mathbf{X})}{q(\mathbf{Z})}\right] d \mathbf{Z} \
&amp;=\int q(\mathbf{Z})[\ln p(\mathbf{X}, \mathbf{Z})-\ln p(\mathbf{Z} \mid \mathbf{X})] d \mathbf{Z} \
&amp;=\int q(\mathbf{Z})[\ln p(\mathbf{X}, \mathbf{Z})-\ln p(\mathbf{X}, \mathbf{Z})+\ln p(\mathbf{X})] d\mathbf{Z} \
&amp;=\ln p(\mathbf{X})\int q(\mathbf{Z})d \mathbf{Z} \
&amp;=\ln p(\mathbf{X})
\end{aligned}
$$</p>
<p>よって$(10.2)$式が示された。</p>
<h2 id="演習-102"><a class="header" href="#演習-102">演習 10.2</a></h2>
<div class="panel-primary">
<p>$\mathbb{E}\left[z_{1}\right]=m_{1}$および$\mathbb{E}\left[z_{2}\right]=m_{2}$を用いて連立方程式</p>
<p>$$
\begin{aligned}
m_{1}&amp;=\mu_{1}-\Lambda_{11}^{-1} \Lambda_{12}\left(\mathbb{E}\left[z_{2}\right]-\mu_{2}\right) \quad (10.13) \ m_{2}&amp;=\mu_{2}-\Lambda_{22}^{-1} \Lambda_{21}\left(\mathbb{E}\left[z_{1}\right]-\mu_{1}\right) \quad (10.15)
\end{aligned}
$$</p>
<p>を解き，もともとの分布$p(\mathbf{z})$が非特異ならば，近似された因子分布の平均についての一意な解は$\mathbb{E}\left[z_{1}\right]=\mu_{1}$および$\mathbb{E}\left[z_{2}\right]=\mu_{2}$となることを示せ．</p>
</div>
<p>(10.13)式と(10.15)式に、$\mathbb{E}\left[z_{1}\right]=m_{1}$と$\mathbb{E}\left[z_{2}\right]=m_{2}$を代入して、</p>
<p>$$
\begin{aligned}
m_{1}&amp;=\mu_{1}-\Lambda_{11}^{-1} \Lambda_{12}\left(m_2-\mu_{2}\right)\
m_{2}&amp;=\mu_{2}-\Lambda_{22}^{-1} \Lambda_{21}\left(m_1-\mu_{1}\right)
\end{aligned}
$$</p>
<p>これを行列の形で表すと、</p>
<p>$$
\begin{aligned}
\left[\begin{array}{cc}
1 &amp; \Lambda <em>{11}^{-1}\Lambda</em>{12} \
\Lambda_{22}^{-1}\Lambda_{21} &amp; 1
\end{array}\right]
\left[\begin{array}{rr}
m_1 - \mu_1 \
m_2- \mu_2
\end{array}\right]
=\left[\begin{array}{rr}
0 \
0
\end{array}\right]
\end{aligned}
$$</p>
<p>一番左の$2 \times 2$行列には逆行列が存在する(*)ので、左から逆行列をかけて$m_1=\mu_1, m_2=\mu_2$を得る。</p>
<p>(*) 一番左の行列に逆行列が存在しないと仮定すると、行列式が0、つまり</p>
<p>$$
\begin{aligned}
&amp;1-\Lambda <em>{11}^{-1}\Lambda</em>{12}
\Lambda_{22}^{-1}\Lambda_{21} =0\
\Leftrightarrow &amp; \Lambda <em>{11}\Lambda</em>{22}-
\Lambda_{12}\Lambda_{21} =0\
\Leftrightarrow &amp; \det \mathbf \Lambda =0
\end{aligned}
$$</p>
<p>となってしまい、元の分布$p(\mathbf{z})$が特異であることを意味する。（精度行列の逆行列が存在しない、すなわち共分散行列が定義できない。）</p>
<h2 id="演習-103"><a class="header" href="#演習-103">演習 10.3</a></h2>
<div class="panel-primary">
<p>$$q(\mathbf{Z})=\prod_{i=1}^{M} q_{i}\left(\mathbf{Z}<em>{i}\right) \tag{10.5}$$
の形の分解された変分分布$q(\mathbf{Z})$を考えよう．ラグランジュ乗数法を用いて，カルバック-ライブラーダイバージェンス$\textrm{KL}(p | q)$を因子の一つ$q_i(\mathbf{Z}<em>i)$について他の因子を固定して最小化すると，解
$$q</em>{j}^{\star}\left(\mathbf{Z}</em>{j}\right)=\int p(\mathbf{Z}) \prod_{i \neq j} \mathrm{~d} \mathbf{Z}<em>{i}=p\left(\mathbf{Z}</em>{j}\right) \tag{10.17}$$
が得られることを確かめよ．</p>
</div>
<p>※
(10.16)式からKLダイバージェンスは</p>
<p>$$
\begin{aligned}
KL(p \parallel q) &amp;= -\int p(\mathbf{Z})\left[\sum^M_{i=1}\ln q_i(\mathbf{Z}_i)\right]d\mathbf{Z}+const\
&amp;=-\int p(\mathbf{Z})\left[\ln q_j(\mathbf{Z}<em>j)+\sum^M</em>{i\neq j}\ln q_i(\mathbf{Z}_i)\right]d\mathbf{Z}+const\
&amp;=-\int p(\mathbf{Z})\ln q_j(\mathbf{Z}<em>j)d\mathbf{Z}+const\
&amp;=-\int\left[\int p(\mathbf{Z})\prod</em>{i\neq j}d\mathbf{Z}_i\right]\ln q_j(\mathbf{Z}_j)d\mathbf{Z}_j+const\
&amp;=-\int p(\mathbf{Z}_j)\ln q_j(\mathbf{Z}_j)d\mathbf{Z}_j+const
\end{aligned}
$$</p>
<p>と計算できる．ここでconstの項は同一の項にはなっていないことに留意．2行目から3行目への式変形では$q_j$に依存しない積分をconstに押し込んだ．最後の式変形では$\mathbf{Z}$の積分を各$\mathbf{Z}$の添字($1\dots i\dots j \dots M$)についてバラして添字$j$以外の積分の順序を入れ替え，$p(\mathbf{Z})$において$j$以外の添字で積分周辺化したため$p(\mathbf{Z}_j)$のみが残っている．</p>
<p>$q_j(\mathbf{Z}_j)$が正規化されているという条件を利用してラグランジュ乗数$\lambda$を導入して，ラグランジュ未定乗数法によりKLダイバージェンスの最小化は以下の式の最小化に書き換えることができて</p>
<p>$$
L = -\int p(\mathbf{Z}_j)\ln q_j(\mathbf{Z}_j)d\mathbf{Z}_j+\lambda\left(\int q_j(\mathbf{Z}_j)d\mathbf{Z}_j-1\right)
$$</p>
<p>を最小化すれば良いことがわかる．ここで元のKLダイバージェンスの式にあった定数項は$\mathbf{Z}_j$に依存しない項なので最小化に影響はなく無視した．</p>
<p>これを積分汎関数の形に変形して変分法を用いて解けるようにしたい．
$\mathbf{Z}_j$に依存しない項を積分に含めるために$\delta$関数を用いて$L$は
以下のように書き直すことができる</p>
<p>$$
L=\int\left{-p(\mathbf{Z}_j)\ln q_j(\mathbf{Z}_j)+\lambda q_j(\mathbf{Z}_j)-\lambda\delta(\mathbf{Z}_j)\right}d\mathbf{Z}_j
$$</p>
<p>被積分関数を</p>
<p>$$
G(p,q;\delta)=-p(\mathbf{Z}_j)\ln q_j(\mathbf{Z}_j)+\lambda q_j(\mathbf{Z}_j)-\lambda\delta(\mathbf{Z}_j)
$$</p>
<p>とおくと$L$を最小化する$q_j^*$はオイラー・ラグランジュ方程式から</p>
<p>$$
\frac{\partial G}{\partial q}=0
$$</p>
<p>$$
-\frac{p(\mathbf{Z}_j)}{q_j(\mathbf{Z}_j)} + \lambda = 0
$$</p>
<p>$\mathbf{Z}_j$について積分して</p>
<p>$$
\lambda=1
$$</p>
<p>よって</p>
<p>$$
q_j^*=p(\mathbf{Z}<em>j)=\int p(\mathbf{Z})\prod</em>{i\neq j}d\mathbf{Z}_i
$$</p>
<p>が得られる．</p>
<h3 id="変分法について補足"><a class="header" href="#変分法について補足">変分法について補足</a></h3>
<p>蛇足かもしれないけど変分法についてちょっと勉強したので補足．上巻の付録Dに変分法の説明が書いてあるけど被積分関数として関数一つとその一回導関数を含む場合についての説明だった．一般化した場合変分問題の解法であるオイラー・ラグランジュ方程式がどのような形になるか調べた．</p>
<p>高階導関数を含む(被積分関数が$G(y,y',y'',...,y^{(m)}, x)$と書ける)場合は</p>
<p>$$
\frac{\partial G}{\partial y}-\frac{d}{dx}\frac{\partial G}{\partial y'}+\frac{d^2}{dx^2}\frac{\partial G}{\partial y''}+...+(-1)^{(m)}\frac{d^m}{dx^m}\frac{\partial G}{\partial y^{(m)}}=0
$$</p>
<p>複数の関数を含む(被積分関数が$G(y,y',z, z',x)$と書ける)場合には</p>
<p>$$
\frac{\partial G}{\partial y}-\frac{d}{dx}\frac{\partial G}{\partial y'}=0
$$
$$
\frac{\partial G}{\partial z}-\frac{d}{dx}\frac{\partial G}{\partial z'}=0
$$</p>
<p>のように書けるらしい．今回の場合，最小化したい積分汎関数は同関数を含まず，複数の関数を含む形になっていたため単に注目する関数の偏微分を考えるだけでよかった（という理解であってますか...）</p>
<h2 id="演習-104"><a class="header" href="#演習-104">演習 10.4</a></h2>
<div class="panel-primary">
<p>ある固定された分布$p(\mathbf{x})$をガウス分布$q(\mathbf{x}) = \mathcal{N}(\mathbf{x}\mid \boldsymbol{\mu}, \mathbf{\Sigma})$を用いて近似したいとしよう．KLダイバージェンス$\textrm{KL}(p | q)$をガウス分布$q(\mathbf{x})$に関して書き下して微分することにより，$\textrm{KL}(p | q)$を$\boldsymbol{\mu}$および$\mathbf{\Sigma}$について最小化すると，結果として$\boldsymbol{\mu}$は$p(\mathbf{x})$の下での$\mathbf{x}$の期待値になり，$\mathbf{\Sigma}$はその共分散になることを示せ．</p>
</div>
<p>$$
\begin{aligned}
\mathrm{KL}(p | q) &amp;=-\int p(\mathbf{x}) \ln \left{\frac{q(\mathbf{x})}{p(\mathbf{x})}\right} d \mathbf{x} \
&amp;=-\int p(\mathbf{x}) \ln q(\mathbf{x}) d \mathbf{x}+\text { const } \
&amp;=-\int p(\mathbf{x})\left[-\frac{D}{2} \ln 2 \pi-\frac{1}{2} \ln |\boldsymbol{\Sigma}|-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right] d \mathbf{x}+\text { const } \
&amp;=\int p(\mathbf{x})\left[\frac{1}{2} \ln |\boldsymbol{\Sigma}|+\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right] d \mathbf{x}+\text { const } \
&amp;=\frac{1}{2} \ln |\boldsymbol{\Sigma}|+\int p(\mathbf{x})\left[\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T} \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right] d \mathbf{x}+\text { const } \
&amp;=\frac{1}{2} \ln |\boldsymbol{\Sigma}|+\int p(\mathbf{x}) \frac{1}{2}\left[\mathbf{x}^{T} \boldsymbol{\Sigma}^{-1} \mathbf{x}-2 \boldsymbol{\mu}^{T} \boldsymbol{\Sigma}^{-1} \mathbf{x}+\boldsymbol{\mu}^{T} \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}\right] d \mathbf{x}+\text { const } \
&amp;=\frac{1}{2} \ln |\boldsymbol{\Sigma}|+\frac{1}{2} \int p(\mathbf{x}) \operatorname{Tr}\left[\boldsymbol{\Sigma}^{-1}\left(\mathbf{x} \mathbf{x}^{T}\right)\right] d \mathbf{x}-\boldsymbol{\mu}^{T} \boldsymbol{\Sigma}^{-1} \mathbb{E}[\mathbf{x}]+\frac{1}{2} \boldsymbol{\mu}^{T} \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}+\text { const } \
&amp;=\frac{1}{2} \ln |\boldsymbol{\Sigma}|+\frac{1}{2} \operatorname{Tr}\left[\boldsymbol{\Sigma}^{-1} \mathbb{E}\left(\mathbf{x} \mathbf{x}^{T}\right)\right]-\boldsymbol{\mu}^{T} \boldsymbol{\Sigma}^{-1} \mathbb{E}[\mathbf{x}]+\frac{1}{2} \boldsymbol{\mu}^{T} \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}+\text { const }
\end{aligned}
$$
$D$ : $\mathbf{x}$の次元.
$\mathrm{KL}(p | q)$ を $\boldsymbol{\mu}$ について微分:
$$
\frac{\partial \mathrm{KL}}{\partial \boldsymbol{\mu}}=-\Sigma^{-1} \mathbb{E}[x]+\Sigma^{-1} \mu=0
$$
よって
$\boldsymbol{\mu}=\mathbb{E}[\mathbf{x}]$.  $\boldsymbol{\mu}=\mathbb{E}[\mathbf{x}]$ のとき, KL divergenceは:
$$
\mathrm{KL}(p | q)=\frac{1}{2} \ln |\boldsymbol{\Sigma}|+\frac{1}{2} \operatorname{Tr}\left[\boldsymbol{\Sigma}^{-1} \mathbb{E}\left(\mathbf{x} \mathbf{x}^{T}\right)\right]-\frac{1}{2} \boldsymbol{\mu}^{T} \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}+\text { const }
$$
この$\mathrm{KL}(p | q)$ を $\Sigma$ について微分すると:
$$
\frac{\partial \mathrm{KL}}{\partial \Sigma}=\frac{1}{2} \Sigma^{-1}-\frac{1}{2} \Sigma^{-1} \mathbb{E}\left[\mathbf{x} \mathbf{x}^{T}\right] \Sigma^{-1}+\frac{1}{2} \Sigma^{-1} \mu \mu^{T} \Sigma^{-1}=0
$$
'MatrixCookBook'内の$\mathrm{Eq}(61)$ と $\mathrm{Eq}(124)$ ,を用いた. $\Sigma, \mathbb{E}\left[\mathbf{x x}^{T}\right]$ は対象行列:
$\frac{\partial \mathbf{a}^{T} \mathbf{X}^{-1} \mathbf{b}}{\partial \mathbf{X}}=-\mathbf{X}^{-T} \mathbf{a b}^{T} \mathbf{X}^{-T} \quad$ and $\quad \frac{\partial \operatorname{Tr}\left(\mathbf{A} \mathbf{X}^{-1} \mathbf{B}\right)}{\partial \mathbf{X}}=-\mathbf{X}^{-T} \mathbf{A}^{T} \mathbf{B}^{T} \mathbf{X}^{-T}$
整理すると:
$$
\Sigma=\mathbb{E}\left[\mathbf{x x}^{T}\right]-\boldsymbol{\mu} \boldsymbol{\mu}^{T}=\mathbb{E}\left[\mathbf{x} \mathbf{x}^{T}\right]-\mathbb{E}[\mathbf{x}] \mathbb{E}[\mathbf{x}]^{T}=\operatorname{cov}[\mathbf{x}]
$$</p>
<h2 id="演習-105"><a class="header" href="#演習-105">演習 10.5</a></h2>
<div class="panel-primary">
<p>すべての隠れ確率変数の集合$\mathbf{Z}$が，潜在変数$\mathbf{z}$とモデルパラメータ$\boldsymbol{\theta}$に分けられるようなモデルを考える．この変分分布を潜在変数とパラメータに$q(\mathbf{z}, \boldsymbol{\theta}) = q_{\mathbf{z}}(\mathbf{z}) q_{\theta}(\boldsymbol{\theta})$のように分解し，分布$q_{\theta}(\boldsymbol{\theta})$を$q_{\theta}(\boldsymbol{\theta}) = \delta(\boldsymbol{\theta} - \boldsymbol{\theta}_0)$の形の点推定で近似することを考える．ここで，$\boldsymbol{\theta}<em>0$は自由パラメータのベクトルである．このとき，この分解された分布を変分ベイズ法により最適化することは， Eステップで$q</em>{\mathbf{z}}(\mathbf{z})$を最適化し， Mステップで$\boldsymbol{\theta}$の完全データの対数事後分布の期待値を$\boldsymbol{\theta}_0$について最大化するEMアルゴリズムと等価になることを示せ．</p>
</div>
<p>変分ベイズの点推定がEMアルゴリズムに相当することを確かめる問題。
10.1節で述べられている通り、EMアルゴリズムと変分推論の違いの一つは、Zにθを含めないか、含めるかである。今回はZとθを分離して考えているためEMアルゴリズムの枠組みで考えられる。変分ベイズ法では、Pをよく表すようなqをKLダイバージェンス基準で求める。つまり（10.2）において、KLダイバージェンスの項を最小化することに相当する（Eステップ）
（つまり変分ベイズはEEアルゴリズムのように捉えることもできる）</p>
<p>実際に計算をする。θを固定して</p>
<p>$$
\begin{aligned}
\mathrm{KL}(q | p) &amp;=-\iint q(\mathbf{Z}) \ln \left{\frac{p(\mathbf{Z} \mid \mathbf{X})}{q(\mathbf{Z})}\right} d \mathbf{Z} \
&amp;=-\iint q_{\mathbf{z}}(\mathbf{z}) q_{\boldsymbol{\theta}}(\boldsymbol{\theta}) \ln \left{\frac{p(\mathbf{z}, \boldsymbol{\theta} \mid \mathbf{X})}{q_{\mathbf{z}}(\mathbf{z}) q_{\boldsymbol{\theta}}(\boldsymbol{\theta})}\right} d \mathbf{z} d \boldsymbol{\theta} \
&amp;=-\iint q_{\mathbf{z}}(\mathbf{z}) q_{\boldsymbol{\theta}}(\boldsymbol{\theta}) \ln \left{\frac{p(\mathbf{z}, \boldsymbol{\theta} \mid \mathbf{X})}{q_{\mathbf{z}}(\mathbf{z})}\right} d \mathbf{z} d \boldsymbol{\theta}+\int q_{\boldsymbol{\theta}}(\boldsymbol{\theta}) \ln q_{\boldsymbol{\theta}}(\boldsymbol{\theta}) d \boldsymbol{\theta} \
&amp;=-\iint q_{\mathbf{z}}(\mathbf{z}) q_{\boldsymbol{\theta}}(\boldsymbol{\theta}) \ln \left{\frac{p(\mathbf{z}, \boldsymbol{\theta} \mid \mathbf{X})}{q_{\mathbf{z}}(\mathbf{z})}\right} d \mathbf{z} d \boldsymbol{\theta}+\text { const } \
&amp;=-\int q_{\boldsymbol{\theta}}(\boldsymbol{\theta})\left{\int q_{\mathbf{z}}(\mathbf{z}) \ln \left{\frac{p(\mathbf{z}, \boldsymbol{\theta} \mid \mathbf{X})}{q_{\mathbf{z}}(\mathbf{z})}\right} d \mathbf{z}\right} d \boldsymbol{\theta}+\text { const } \
&amp;=-\int q_{\mathbf{z}}(\mathbf{z}) \ln \left{\frac{p\left(\mathbf{z}, \boldsymbol{\theta}<em>{0} \mid \mathbf{X}\right)}{q</em>{\mathbf{z}}(\mathbf{z})}\right} d \mathbf{z}+\text { const } \
&amp;=-\int q_{\mathbf{z}}(\mathbf{z}) \ln \left{\frac{p\left(\mathbf{z} \mid \boldsymbol{\theta}<em>{0}, \mathbf{X}\right) p\left(\boldsymbol{\theta}</em>{0} \mid \mathbf{X}\right)}{q_{\mathbf{z}}(\mathbf{z})}\right} d \mathbf{z}+\text { const } \
&amp;=-\int q_{\mathbf{z}}(\mathbf{z}) \ln \left{\frac{p\left(\mathbf{z} \mid \boldsymbol{\theta}<em>{0}, \mathbf{X}\right)}{q</em>{\mathbf{z}}(\mathbf{z})}\right} d \mathbf{z}+\text { const }
\end{aligned}
$$</p>
<p>よって、$\mathrm{KL}(q | p)$を最小にする$q_{\mathbf{z}}(\mathbf{z})$は$p\left(\mathbf{z} \mid \boldsymbol{\theta}_{0}, \mathbf{X}\right)$が解となる。</p>
<p>続いて最適なθを求める。これは下限$\mathcal{L}(q)$を最大にするようなθを求めることに相当する。</p>
<p>$$
\begin{aligned}
L(q) &amp;=\iint q(\mathbf{Z}) \ln \left{\frac{p(\mathbf{X}, \mathbf{Z})}{q(\mathbf{Z})}\right} d \mathbf{Z} \
&amp;=\iint q_{\mathbf{z}}(\mathbf{z}) q_{\boldsymbol{\theta}}(\boldsymbol{\theta}) \ln \left{\frac{p(\mathbf{X}, \mathbf{z}, \boldsymbol{\theta})}{q_{\mathbf{z}}(\mathbf{z}) q_{\boldsymbol{\theta}}(\boldsymbol{\theta})}\right} d \mathbf{z} d \boldsymbol{\theta} \
&amp;=\iint q_{\mathbf{z}}(\mathbf{z}) q_{\boldsymbol{\theta}}(\boldsymbol{\theta}) \ln \left{\frac{p(\mathbf{X}, \mathbf{z}, \boldsymbol{\theta})}{q_{\mathbf{z}}(\mathbf{z})}\right} d \mathbf{z} d \boldsymbol{\theta}-\int q_{\boldsymbol{\theta}}(\boldsymbol{\theta}) \ln q_{\boldsymbol{\theta}}(\boldsymbol{\theta}) d \boldsymbol{\theta} \
&amp;=\iint q_{\mathbf{z}}(\mathbf{z}) q_{\boldsymbol{\theta}}(\boldsymbol{\theta}) \ln {p(\mathbf{X}, \mathbf{z}, \boldsymbol{\theta})} d \mathbf{z} d \boldsymbol{\theta}-\int q_{\boldsymbol{\theta}}(\boldsymbol{\theta}) \ln q_{\boldsymbol{\theta}}(\boldsymbol{\theta}) d \boldsymbol{\theta}+\text { const } \
&amp;=\int q_{\boldsymbol{\theta}}(\boldsymbol{\theta}) \mathbb{E}<em>{q</em>{\mathbf{z}}}[\ln p(\mathbf{X}, \mathbf{z}, \boldsymbol{\theta})] d \boldsymbol{\theta}-\int q_{\boldsymbol{\theta}}(\boldsymbol{\theta}) \ln q_{\boldsymbol{\theta}}(\boldsymbol{\theta}) d \boldsymbol{\theta}+\text { const } \
&amp;=\mathbb{E}<em>{q</em>{\mathbf{z}}(\mathbf{z})}\left[\ln p\left(\mathbf{X}, \mathbf{z}, \boldsymbol{\theta}<em>{0}\right)\right]-\int q</em>{\boldsymbol{\theta}}(\boldsymbol{\theta}) \ln q_{\boldsymbol{\theta}}(\boldsymbol{\theta}) d \boldsymbol{\theta}+\text { const }
\end{aligned}
$$</p>
<p>$\int q_{\boldsymbol{\theta}}(\boldsymbol{\theta}) \ln q_{\boldsymbol{\theta}}(\boldsymbol{\theta}) d \boldsymbol{\theta}$は−∞になるので無視して良いので、第一項の最大化を考えれば良い。
これは、対数事後分布の期待値を$\boldsymbol{\theta}_{0}$について最大化すれば良い。</p>
<h2 id="演習-106"><a class="header" href="#演習-106">演習 10.6</a></h2>
<div class="panel-primary">
<p>$\alpha$ダイバージェンスは
$$
\mathrm{D}_{\alpha}(p | q)=\frac{4}{1-\alpha^{2}}\left(1-\int p(x)^{(1+\alpha) / 2} q(x)^{(1-\alpha) / 2} \mathrm{~d} x\right) \tag{10.19}
$$
で定義される．カルバック-ライブラーダイバージェンス$\textrm{KL}(p | q)$はこのとき$\alpha \to 1$の場合に対応することを示せ．これには$p^{\epsilon} = \exp (\epsilon \ln p)=1+\epsilon \ln p+O\left(\epsilon^{2}\right)$と書き，$\epsilon \to 0$とすればよい．同様にして，$\textrm{KL}(q | p)$は$\alpha \to -1$の場合に対応することを示せ．</p>
</div>
<p>$\alpha\rightarrow1$の時は、$p^{\epsilon}=1+\epsilon \ln p+O\left(\epsilon^{2}\right)$を利用すべく、以下のように式変形する。</p>
<p>$$
\begin{aligned} D_{\alpha}(p | q) &amp;=\frac{4}{1-\alpha^{2}}\left(1-\int p^{(1+\alpha) / 2} q^{(1-\alpha) / 2} d x\right) \
&amp;=\frac{4}{1-\alpha^{2}}\left{1-\int \frac{p}{p^{(1-\alpha) / 2}}\left[1+\frac{1-\alpha}{2} \ln q+O\left(\frac{1-\alpha}{2}\right)^{2}\right] d x\right} \
&amp;=\frac{4}{1-\alpha^{2}}\left{1-\int p \cdot \frac{1+\frac{1-\alpha}{2} \ln q+O\left(\frac{1-\alpha}{2}\right)^{2}}{1+\frac{1-\alpha}{2} \ln p+O\left(\frac{1-\alpha}{2}\right)^{2}} d x\right} \ &amp;\approx \frac{4}{1-\alpha^{2}}\left{1-\int p \cdot \frac{1+\frac{1-\alpha}{2} \ln q}{1+\frac{1-\alpha}{2} \ln p} d x\right} \
&amp;=\frac{4}{1-\alpha^{2}}\left{-\int p \cdot\left[\frac{1+\frac{1-\alpha}{2} \ln q}{1+\frac{1-\alpha}{2} \ln p}-1\right] d x\right} \ &amp;=\frac{4}{(1+\alpha)(1-\alpha)}\left{-\int p \cdot \frac{\frac{1-\alpha}{2} \ln q-\frac{1-\alpha}{2} \ln p}{1+\frac{1-\alpha}{2} \ln p} d x\right} \ &amp;=\frac{2}{1+\alpha}\left{-\int p \cdot \frac{\ln q-\ln p}{1+\frac{1-\alpha}{2} \ln p} d x\right} \
&amp;D_{\alpha\rightarrow1}(p | q)= -\int p \cdot(\ln q-\ln p) d x=\int p \cdot \ln \frac{p}{q}dx = \textrm{KL}(p | q)
\end{aligned}
$$</p>
<p>同様に$\alpha\rightarrow-1$の時は、以下のように式変形する。</p>
<p>$$
\begin{aligned} D_{\alpha}(p | q) &amp;=\frac{4}{1-\alpha^{2}}\left(1-\int p^{(1+\alpha) / 2} q^{(1-\alpha) / 2} dx\right) \
&amp;=\frac{4}{1-\alpha^{2}}\left{1-\int \left[1+\frac{1+\alpha}{2} \ln p+O\left(\frac{1+\alpha}{2}\right)^{2}\right]\frac{q}{q^{(1+\alpha)/ 2}}dx\right} \
&amp;=\frac{4}{1-\alpha^{2}}\left{1-\int q \cdot \frac{1+\frac{1+\alpha}{2} \ln p+O\left(\frac{1+\alpha}{2}\right)^{2}}{1+\frac{1+\alpha}{2} \ln q+O\left(\frac{1+\alpha}{2}\right)^{2}} dx\right} \
&amp; \approx \frac{4}{1-\alpha^{2}}\left{1-\int q \cdot \frac{1+\frac{1+\alpha}{2} \ln p}{1+\frac{1+\alpha}{2} \ln q}dx\right} \
&amp;=\frac{4}{1-\alpha^{2}}\left{-\int q \cdot\left[\frac{1+\frac{1+\alpha}{2} \ln p}{1+\frac{1+\alpha}{2} \ln q}-1\right] d x\right} \ &amp;=\frac{4}{(1+\alpha)(1-\alpha)}\left{-\int q \cdot \frac{\frac{1+\alpha}{2} \ln p-\frac{1+\alpha}{2} \ln q}{1+\frac{1+\alpha}{2} \ln q}dx\right} \ &amp;=\frac{2}{1-\alpha}\left{-\int q \cdot \frac{\ln p-\ln q}{1+\frac{1+\alpha}{2} \ln q}dx\right} \
&amp;D_{\alpha\rightarrow-1}(p | q)= -\int q \cdot(\ln p-\ln q)dx=\int q \cdot \ln \frac{q}{p}dx = \textrm{KL}(q | p)
\end{aligned}
$$</p>
<h2 id="演習-107"><a class="header" href="#演習-107">演習 10.7</a></h2>
<div class="panel-primary">
<p>一変数ガウス分布の平均と精度を，分解した変分近似を用いて求める10.1.3節の問題を考える．このとき，因子$q_{\mu}(\mu)$はガウス分布$\mathcal{N}\left(\mu \mid \mu_{N}, \lambda_{N}^{-1}\right)$となり，この平均と精度はそれぞれ
$$\mu_{N} =\frac{\lambda_{0} \mu_{0}+N \bar{x}}{\lambda_{0}+N} \tag{10.26}$$
$$\lambda_{N} =\left(\lambda_{0}+N\right) \mathbb{E}[\tau] \tag{10.27}$$
で与えられることを示せ．同様にして因子$q_{\tau}(\tau)$はガンマ分布$\textrm{Gam}(\gamma \mid a_N, b_N)$となり，そのパラメータは
$$a_{N}=a_{0}+\frac{N+1}{2} \tag{10.29}$$
$$b_{N}=b_{0}+\frac{1}{2} \mathbb{E}<em>{\mu}\left[\sum</em>{n=1}^{N}\left(x_{n}-\mu\right)^{2}+\lambda_{0}\left(\mu-\mu_{0}\right)^{2}\right] \tag{10.30}$$
で与えられることを示せ．</p>
</div>
<p>※</p>
<p>$(10.25)$式から</p>
<p>$$
\begin{aligned} \ln q_{\mu}^{\star}(\mu) &amp;=-\frac{\mathbb{E}[\tau]}{2}\left{\lambda_{0}\left(\mu-\mu_{0}\right)^{2}+\sum_{n=1}^{N}\left(x_{n}-\mu\right)^{2}\right}+\text { const } \ &amp;=-\frac{\mathbb{E}[\tau]}{2}\left{\lambda_{0} \mu^{2}-2 \lambda_{0} \mu_{0} \mu+\lambda_{0} \mu_{0}^{2}+N \mu^{2}-2\left(\sum_{n=1}^{N} x_{n}\right) \mu+\sum_{n=1}^{N} x_{n}^{2}\right}+\text { const } \ &amp;=-\frac{\mathbb{E}[\tau]}{2}\left{\left(\lambda_{0}+N\right) \mu^{2}-2\left(\lambda_{0} \mu_{0}+\sum_{n=1}^{N} x_{n}\right) \mu+\left(\lambda_{0} \mu_{0}^{2}+\sum_{n=1}^{N} x_{n}^{2}\right)\right}+\text { const } \ &amp;=-\frac{\mathbb{E}[\tau]\left(\lambda_{0}+N\right)}{2}\left{\mu^{2}-2 \frac{\lambda_{0} \mu_{0}+\sum_{n=1}^{N} x_{n}}{\lambda_{0}+N} \mu+\frac{\lambda_{0} \mu_{0}^{2}+\sum_{n=1}^{N} x_{n}^{2}}{\lambda_{0}+N}\right}+\text { const } \end{aligned}
$$</p>
<h2 id="演習-108"><a class="header" href="#演習-108">演習 10.8</a></h2>
<div class="panel-primary">
<p>パラメータが
$$a_{N}=a_{0}+\frac{N+1}{2} \tag{10.29}$$
$$b_{N}=b_{0}+\frac{1}{2} \mathbb{E}<em>{\mu}\left[\sum</em>{n=1}^{N}\left(x_{n}-\mu\right)^{2}+\lambda_{0}\left(\mu-\mu_{0}\right)^{2}\right] \tag{10.30}$$
で与えられる一変数ガウス分布の精度の変分事後分布を考える．ガンマ分布の平均と分散についての標準的な結果
$$\mathbb{E}[\tau] =\frac{a}{b} \tag{B.27}$$
$$\operatorname{var}[\tau] =\frac{a}{b^{2}} \tag{B.28}$$
を用いて，$N\to \infty$のとき，この変分事後分布の期待値はデータの分散の最尤推定値の逆数となり，事後分布の分散は$0$に近づくことを示せ．</p>
</div>
<p>精度$\tau$はガンマ分布に従う。すなわち、</p>
<p>$$
\begin{aligned}
p(\tau) = \frac{1}{\Gamma(a_N)}b_N^{a_N}\tau^{a_N-1}e^{-b\tau}
\end{aligned}
$$</p>
<p>を満たす。今、ガンマ分布の標準的な結果$(B.27)$、$(B.28)$に代入すると、</p>
<p>$$
\begin{aligned}
\mathbb{E}[\tau] &amp;= \frac{a_N}{b_N} \
&amp;= \frac{a_{0}+\frac{N+1}{2}}{b_{0}+\frac{1}{2} \mathbb{E}<em>{\mu}\left[\sum</em>{n=1}^{N}\left(x_{n}-\mu\right)^{2}+\lambda_{0}\left(\mu-\mu_{0}\right)^{2}\right]} \
&amp;= \frac{2 a_{0}+N+1}{2 b_{0}+\mathbb{E}<em>{\mu}\left[\sum</em>{n=1}^{N}\left(x_{n}-\mu\right)^{2} + \lambda_{0}\left(\mu-\mu_{0}\right)\right]} \
&amp;\xrightarrow[N\to\infty]{} \frac{N}{\mathbb{E}<em>{\mu}\left[ \sum</em>{n=1}^{N}\left(x_{n}-\mu\right)^{2} \right]}
\end{aligned}
$$</p>
<p>これは$N\to \infty$の極限でデータ分散の最尤推定量$\displaystyle \frac{\sum_{n=1}^{N}(x_n-\mu)^2}{N}$の逆数になっている事がわかる。</p>
<p>分散は</p>
<p>$$
\begin{aligned}
\operatorname{var}[\tau] &amp;= \frac{a_N}{{b_N}^2} \
&amp;=\frac{2\mathbb{E}[\tau]}{2b_{0}+\mathbb{E}<em>{\mu}\left[\sum</em>{n=1}^{N}\left(x_{n}-\mu\right)^{2} + \lambda_{0}\left(\mu-\mu_{0}\right)\right]} \
&amp;\xrightarrow[N\to\infty]{} 0
\end{aligned}
$$</p>
<p>となる。</p>
<h2 id="演習-109"><a class="header" href="#演習-109">演習 10.9</a></h2>
<div class="panel-primary">
<p>ガンマ分布の平均が$\mathbb{E}[\tau] = a_N/b_N$になるという標準的な結果，および
$$\mu_{N} =\frac{\lambda_{0} \mu_{0}+N \bar{x}}{\lambda_{0}+N} \tag{10.26}$$
$$\lambda_{N} =\left(\lambda_{0}+N\right) \mathbb{E}[\tau] \tag{10.27}$$
$$a_{N}=a_{0}+\frac{N+1}{2} \tag{10.29}$$
$$b_{N}=b_{0}+\frac{1}{2} \mathbb{E}<em>{\mu}\left[\sum</em>{n=1}^{N}\left(x_{n}-\mu\right)^{2}+\lambda_{0}\left(\mu-\mu_{0}\right)^{2}\right] \tag{10.30}$$
を用いて，一変数ガウス分布の分解された変分近似の持つ精度の期待値の逆数についての結果
$$\frac{1}{\mathbb{E}[\tau]} =\overline{x^{2}}-\bar{x}^{2} =\frac{1}{N} \sum_{n=1}^{N}\left(x_{n}-\bar{x}\right)^{2} \tag{10.33}$$
を導け．</p>
</div>
<p>※問題文には書かれていないが、PRML下巻P.186の設定から$\mu_{0}=a_{0}=b_{0}=\lambda_{0}=0$であるとする。</p>
<p>まず$\displaystyle \frac{1}{\mathbb{E}[\tau]}$を計算する。</p>
<p>$$
\begin{aligned}
\dfrac{1}{{\mathbb E}[\tau]} &amp;= \left(\frac{a_N}{b_N}\right)^{-1} \
&amp;= \frac{b_N}{a_N}\
&amp;=\frac{b_0+\dfrac{1}{2}{\mathbb E}<em>\mu\left[\displaystyle\sum</em>{n=1}^N(x_n-\mu)^2+\lambda_0(\mu-\mu_0)^2\right]}{a_0+\dfrac{N+1}{2}} \
&amp;=\frac{{\mathbb E}<em>\mu\left[\displaystyle\sum</em>{n=1}^N(x_n-\mu)^2\right]}{N+1} \
&amp;=\frac{N}{N+1}\cdot\frac{1}{N}{\mathbb E}<em>\mu\left[\displaystyle\sum</em>{n=1}^N(x_n-\mu)^2\right] \
&amp;=\frac{N}{N+1}{\mathbb E}<em>\mu\left[\frac{1}{N}\sum</em>{n=1}^N(x_n-\mu)^2\right] \
&amp;=\frac{N}{N+1}{\mathbb E}<em>\mu\left[\frac{1}{N}\sum</em>{n=1}^N(x_n^2-2\mu x_n+\mu^2)\right] \
&amp;=\frac{N}{N+1}{\mathbb E}<em>\mu\left[\frac{1}{N}\sum</em>{n=1}^Nx_n^2-2\mu\frac{1}{N}\sum_{n=1}^Nx_n+\frac{1}{N}\sum_{n=1}^N\mu^2\right] \
&amp;=\frac{N}{N+1}{\mathbb E}<em>\mu\left[\overline{x^2}-2\overline{x}\mu+\mu^2\right] \
&amp;=\frac{N}{N+1}\left(\overline{x^2}-2\overline{x}{\mathbb E}</em>\mu[\mu]+{\mathbb E}_\mu[\mu^2]\right)
\end{aligned}
$$</p>
<p>これと</p>
<p>$$
\begin{aligned}
{\mathbb E}_\mu[\mu]&amp;= \mu_N \
&amp;=\frac{\lambda_0\mu_0+N\overline{x}}{\lambda_0+N} \
&amp;=\frac{N\overline{x}}{N}\ (\because \lambda_0 = \mu_0 = 0 )\
&amp;=\overline{x}
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
\mathbb{E}<em>{\mu}\left[\mu^{2}\right] &amp;=\operatorname{var}[\mu]+\mathbb{E}</em>{\mu}[\mu]^{2} \ &amp;=\lambda_{N}^{-1}+\overline{x}^{2} \ &amp;=\left(\left(\lambda_{0}+N\right) \mathbb{E}[\tau]\right)^{-1}+\overline{x}^{2} \ &amp;=(N \mathbb{E}[\tau])^{-1}+\overline{x}^{2} \ &amp;=\frac{1}{N \mathbb{E}[\tau]}+\overline{x}^{2}
\end{aligned}
$$</p>
<p>よって</p>
<p>$$
\begin{aligned}
\dfrac{1}{{\mathbb E}[\tau]} &amp;= \frac{N}{N+1}\left(\overline{x^2}-2\overline{x}\cdot\overline{x}+\frac{1}{N{\mathbb E}[\tau]}+\overline{x}^2\right) \
&amp;=\frac{N}{N+1}\left(\overline{x^2}-\overline{x}^2+\frac{1}{N{\mathbb E}[\tau]}\right) \
\therefore \dfrac{1}{{\mathbb E}[\tau]} &amp;= \overline{x^2}-\overline{x}^2
\end{aligned}
$$
一方で
$$
\begin{aligned}
\frac{1}{N} \sum_{n=1}^{N}\left(x_{n}-\overline{x}\right)^{2} &amp;=\frac{1}{N} \sum_{n=1}^{N}\left(x_{n}^{2}-2 \overline{x} x_{n}+\overline{x}^{2}\right) \ &amp;=\frac{1}{N} \sum_{n=1}^{N} x_{n}^{2}-2 \overline{x} \frac{1}{N} \sum_{n=1}^{N} x_{n}+\frac{1}{N} \sum_{n=1}^{N} \overline{x}^{2} \ &amp;=\overline{x^{2}}-2 \overline{x} \cdot \overline{x}+\overline{x}^{2} \ &amp;=\overline{x^{2}}-\overline{x}^{2}
\end{aligned}
$$</p>
<p>よって
$$
\frac{1}{\mathbb{E}[\tau]} =\overline{x^{2}}-\overline{x}^{2} =\frac{1}{N} \sum_{n=1}^{N}\left(x_{n}-\overline{x}\right)^{2} \tag{10.33}
$$</p>
<p>を得る。</p>
<h2 id="演習-1010"><a class="header" href="#演習-1010">演習 10.10</a></h2>
<div class="panel-primary">
<p>モデルの事後分布を変分推論を用いて近似する際の分解
$$\ln p(\mathbf{X})=\mathcal{L}-\sum_{m} \sum_{\mathbf{Z}} q(\mathbf{Z} \mid m) q(m) \ln \left{\frac{p(\mathbf{Z}, m \mid \mathbf{X})}{q(\mathbf{Z} \mid m) q(m)}\right} \tag{10.34}$$
を導け．</p>
</div>
<p>$$
\begin{aligned}
\mathcal{L} &amp;= \sum_m \sum_{\mathbf{Z}} q(\mathbf{Z}|m)q(m)\ln\left{\frac{p(\mathbf{X}, \mathbf{Z}, m)}{q(\mathbf{Z}|m)q(m)}\right} \
&amp;= \sum_m \sum_{\mathbf{Z}} q(\mathbf{Z}|m)q(m)\ln\left{\frac{p(\mathbf{Z}, m|\mathbf{X})p(\mathbf{X})}{q(\mathbf{Z}|m)q(m)}\right} \
&amp;= \sum_m \sum_{\mathbf{Z}} q(\mathbf{Z}|m)q(m)\ln\left{\frac{p(\mathbf{Z}, m|\mathbf{X})}{q(\mathbf{Z}|m)q(m)}\right} + \sum_m \sum_{\mathbf{Z}} q(\mathbf{Z}|m)q(m)\ln\left{p(\mathbf{X})\right} \
&amp;= \sum_m \sum_{\mathbf{Z}} q(\mathbf{Z}|m)q(m)\ln\left{\frac{p(\mathbf{Z}, m|\mathbf{X})}{q(\mathbf{Z}|m)q(m)}\right} + \ln p(\mathbf{X})
\end{aligned}
$$</p>
<p>上式を整理することで、式 (10.34) を得る。</p>
<h2 id="演習-1011"><a class="header" href="#演習-1011">演習 10.11</a></h2>
<div class="panel-primary">
<p>分布$q(m)$の正規化条件をラグランジュ乗数法を用いて扱うことにより，下限
$$\mathcal{L}=\sum_{m} \sum_{\mathbf{Z}} q(\mathbf{Z} \mid m) q(m) \ln \left{\frac{p(\mathbf{Z}, \mathbf{X}, m)}{q(\mathbf{Z} \mid m) q(m)}\right} \tag{10.35}$$
の最大値は
$$q(m) \propto p(m) \exp \left{\mathcal{L}_{m}\right} \tag{10.36}$$
によって得られることを示せ．</p>
</div>
<p>問題には「ラグランジュ乗数法を用いて」とあるが、ラグランジュ乗数法を用いない方が簡単に解ける (実際公式の解答も使っていない)。
まず、変分下限$\mathcal{L}$を式変形する。
$$
\begin{aligned}
\mathcal{L} &amp;= \sum_m \sum_{\mathbf{Z}} q(\mathbf{Z}|m)q(m)\ln\left{\frac{p(\mathbf{X}, \mathbf{Z}, m)}{q(\mathbf{Z}|m)q(m)}\right} \
&amp;= \sum_m \sum_{\mathbf{Z}} q(\mathbf{Z}|m)q(m)\ln\left{\frac{p(\mathbf{Z}, m|\mathbf{X})p(\mathbf{X})}{q(\mathbf{Z}|m)q(m)}\right} \
&amp;= \sum_m \sum_{\mathbf{Z}} q(\mathbf{Z}|m)q(m)\left{\ln p(\mathbf{Z}, m|\mathbf{X}) + \ln p(\mathbf{X}) - \ln q(\mathbf{Z}|m) - \ln q(m)\right} \
&amp;= \sum_m q(m) \left(\ln p(m) - \ln q(m) + \sum_{\mathbf{Z}} q(\mathbf{Z}|m)\left{\ln p(\mathbf{Z}, m|\mathbf{X}) - \ln q(\mathbf{Z}|m)\right} \right) \
&amp;= \sum_m q(m)\left{\ln\left(p(m)\exp(\mathcal{L}_m)\right) - \ln q(m)\right} \
&amp;= \sum_m q(m)\ln\left{\frac{p(m)\exp(\mathcal{L}_m)}{q(m)}\right}
\end{aligned}
$$</p>
<p>これは$p(m)\exp(\mathcal{L}_m)$と$q(m)$とのKLダイバージェンスに$-1$をかけたものに等しいので、
$$
q(m) \propto p(m)\exp(\mathcal{L}_m)
$$
のとき$\mathcal{L}$が最大となる。
($=$ではなく$\propto$なのは、$p(m)\exp(\mathcal{L}_m)$が正規化されているとは限らないため)</p>
<h2 id="演習-1012"><a class="header" href="#演習-1012">演習 10.12</a></h2>
<div class="panel-primary">
<p>同時分布
$$p(\mathbf{X}, \mathbf{Z}, \boldsymbol{\boldsymbol{\pi}}, \boldsymbol{\mu}, \mathbf{\Lambda})=p(\mathbf{X} \mid \mathbf{Z}, \boldsymbol{\mu}, \mathbf{\Lambda}) p(\mathbf{Z} \mid \boldsymbol{\boldsymbol{\pi}}) p(\boldsymbol{\boldsymbol{\pi}}) p(\boldsymbol{\mu} \mid \mathbf{\Lambda}) p(\mathbf{\Lambda}) \tag{10.41}$$
から始めて一般的な結果
$$\ln q_{j}^{\star}\left(\mathbf{Z}<em>{j}\right)= \mathbb{E}</em>{i \neq j}[\ln p(\mathbf{X}, \mathbf{Z})]+\mathrm{const} \tag{10.9}$$
を適用することで，ベイズ混合ガウス分布の潜在変数の最適な変分事後分布$q^{\star}(\mathbf{Z})$は
$$q^{\star}(\mathbf{Z})=\prod_{n=1}^{N} \prod_{k=1}^{K} r_{n k}^{z_{n k}} \tag{10.48}$$
で与えられることを，本文の段階を確かめることで示せ．</p>
</div>
<p>※教科書P.190の$(10.43)–(10.49)$の導出を確認する問題。</p>
<p>$$
\begin{aligned} \ln q^{\star}(\mathbf{Z}) &amp;=\mathbb{E}<em>{\boldsymbol{\boldsymbol{\pi}}, \boldsymbol{\mu} \mathbf{\Lambda}}[\ln p(\mathbf{X}, \mathbf{Z}, \boldsymbol{\mu}, \boldsymbol{\boldsymbol{\pi}}, \mathbf{\Lambda})]+\text { const. } \
&amp;=\mathbb{E}</em>{\boldsymbol{\boldsymbol{\pi}}, \boldsymbol{\mu}, \mathbf{\Lambda}}[\ln [p(\mathbf{Z} \mid \boldsymbol{\boldsymbol{\pi}}) p(\mathbf{X} \mid \mathbf{Z}, \boldsymbol{\mu}, \mathbf{\Lambda}) p(\boldsymbol{\boldsymbol{\pi}}) p(\boldsymbol{\mu} \mid \mathbf{\Lambda}) p(\mathbf{\Lambda})]]+\text { const } \end{aligned}
$$</p>
<p>$Z$に依存しない項はconst.となるので、</p>
<p>$$
\ln q^{\star}(\mathbf{Z})=\mathbb{E}<em>{\boldsymbol{\pi}}[\ln p(\mathbf{Z} \mid \boldsymbol{\boldsymbol{\pi}})]+\mathbb{E}</em>{\boldsymbol{\mu}<em>{k}, \mathbf{\Lambda}</em>{k}}[\ln p(\mathbf{X} \mid \mathbf{Z}, \boldsymbol{\mu}, \mathbf{\Lambda})]+\text { const. }
$$</p>
<p>$(10.37)$と$(10.38)$を代入して</p>
<p>$$
\begin{aligned} \ln q^{\star}(\mathbf{Z})&amp;=\mathbb{E}<em>{\boldsymbol{\pi}}\left[\sum</em>{n=1}^{N} \sum_{k=1}^{K} z_{nk} \ln \pi_{k} \right]+\mathbb{E}<em>{\boldsymbol{\mu}</em>{k}, \mathbf{\Lambda}<em>{k}}\left[\sum</em>{n=1}^{N} \sum_{k=1}^{K} z_{n k} \ln \mathcal{N}\left(\mathbf{x}<em>{n} \mid \boldsymbol{\mu}</em>{k}, \mathbf{\Lambda}<em>{k}^{-1}\right)\right] + \textrm{const.} \
&amp;=\sum</em>{n=1}^{N} \sum_{k=1}^{K}\left{z_{n k}\left(\mathbb{E}<em>{\boldsymbol{\pi}}\left[\ln \pi</em>{k} \right]+\mathbb{E}<em>{\boldsymbol{\mu}</em>{k}, \mathbf{\Lambda}<em>{k}}\left[\ln \mathcal{N}\left(\mathbf{x}</em>{n} \mid \boldsymbol{\mu}<em>{k}, \mathbf{\Lambda}</em>{k}^{-1}\right)\right]\right)\right} + \textrm{const.} \
&amp;=\sum_{n=1}^{N} \sum_{k=1}^{K}\left{z_{nk} \left(\mathbb{E}<em>{\boldsymbol{\pi}}\left[\ln \pi</em>{k} \right]+\frac{1}{2} \mathbb{E}[\ln \mathbf{\Lambda}]-\frac{D}{2} \ln (2 \pi)-\frac{1}{2} \mathbb{E}<em>{\boldsymbol{\mu}</em>{k}, \mathbf{\Lambda}<em>{k}}\left[\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}<em>{n}\right)^{\mathrm T} \mathbf{\Lambda}</em>{k}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{n}\right)\right]\right)\right} + \textrm{const.} \
&amp;\equiv\sum_{n=1}^{N} \sum_{k=1}^{K} z_{nk}\ln \rho_{nk} + \textrm{const.}
\end{aligned}
$$</p>
<p>最後に本文中で定義した</p>
<p>$$
\begin{aligned} \ln \rho_{n k} &amp;= \mathbb{E}\left[\ln \pi_{k}\right]+\frac{1}{2} \mathbb{E}\left[\ln \left|\mathbf{\Lambda}<em>{k}\right|\right]-\frac{D}{2} \ln (2 \pi) \ &amp;-\frac{1}{2} \mathbb{E}</em>{\boldsymbol{\mu}<em>{k}, \mathbf{\Lambda}</em>{k}}\left[\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}\right)^{\mathrm{T}} \mathbf{\Lambda}<em>{k}\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}_{k}\right)\right] \end{aligned} \tag{10.46}
$$</p>
<p>を用いた。</p>
<p>これを用いて両辺の指数を取れば</p>
<p>$$
q^{\star}(\mathbf{Z}) \propto \prod_{n=1}^{N} \prod_{k=1}^{K} \rho_{n k}^{z_{n k}} \tag{10.47}
$$</p>
<p>を得る。また、この分布は正規化されている必要があることと，各$n$の値について$z_{nk}$は二値ですべての$k$の値にわたる和が$1$であることに注意すると，$(10.48), (10.49)$を得る。</p>
<p>$$
q^{\star}(\mathbf{Z})=\prod_{n=1}^{N} \prod_{k=1}^{K} r_{n k}^{z_{n k}}, \quad r_{n k}=\frac{\rho_{n k}}{\sum_{j=1}^{K} \rho_{n j}}
$$</p>
<h2 id="演習-1013"><a class="header" href="#演習-1013">演習 10.13</a></h2>
<div class="panel-primary">
<p>$$
\begin{aligned} \ln q^{\star} &amp;(\boldsymbol{\pi}, \boldsymbol{\mu}, \mathbf{\Lambda})=\ln p(\boldsymbol{\pi})+\sum_{n=1}^{N} \ln p\left(\boldsymbol{\mu}<em>{k}, \mathbf{\Lambda}</em>{k}\right)+\mathbb{E}<em>{\mathbf{Z}}[\ln p(\mathbf{Z} \mid \boldsymbol{\pi})] \ &amp;+\sum</em>{n=1}^{N} \sum_{n=1}^{N} \mathbb{E}\left[z_{n k}\right] \ln \mathcal{N}\left(\mathbf{x}<em>{n} \mid \boldsymbol{\mu}</em>{k}, \mathbf{\Lambda}<em>{k}^{-1}\right)+\text { const. } \end{aligned} \tag{10.54}$$
から始めて，ベイズ混合ガウス分布における$\boldsymbol{\mu}<em>k$と$\mathbf{\Lambda}<em>k$の最適な変分事後分布についての結果
$$q^{\star}\left(\boldsymbol{\mu}</em>{k}, \mathbf{\Lambda}</em>{k}\right)=\mathcal{N}\left(\boldsymbol{\mu}</em>{k} \mid \mathbf{m}<em>{k},\left(\beta</em>{k} \mathbf{\Lambda}<em>{k}\right)^{-1}\right) \mathcal{W}\left(\mathbf{\Lambda}</em>{k} \mid \mathbf{W}<em>{k}, \nu</em>{k}\right) \tag{10.59}$$
を導き，この分布のパラメータが</p>
<p>$$
\begin{align} \beta_{k} &amp;=\beta_{0}+N_{k} \tag{10.60} \ \mathbf{m}<em>{k} &amp;=\frac{1}{\beta</em>{k}}\left(\beta_{0} \mathbf{m}<em>{0}+N</em>{k} \overline{\mathbf{x}}<em>{k}\right) \tag{10.61} \ \mathbf{W}</em>{k}^{-1} &amp;=\mathbf{W}<em>{0}^{-1}+N</em>{k} \mathbf{S}<em>{k}+\frac{\beta</em>{0} N_{k}}{\beta_{0}+N_{k}}\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{0}\right)\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm{T}} \tag{10.62} \ \nu_{k} &amp;=\nu_{0}+N_{k} \tag{10.63} \end{align}
$$</p>
<p>で与えられることを確かめよ．</p>
</div>
<p>※多変数で平均と精度がともに未知な場合、上巻P.100の$(2.157)$式にあるガウス–ウィシャート分布の形の共役事前分布を取ることを利用する。</p>
<p>$$
\mathcal{N}\left(\boldsymbol{\mu}<em>{k} \mid \mathbf{m}<em>0,\left(\beta</em>{0} \mathbf{\Lambda}</em>{k}\right)^{-1}\right)=\left(\frac{1}{2 \pi \beta_{0}}\right)^{\frac{D}{2}}\left(\left|\mathbf{\Lambda}<em>{k}\right|\right)^{\frac{1}{2}} \exp \left{-\frac{\beta</em>{0}}{2}\left(\boldsymbol{\mu}<em>{k}-\mathbf{m}<em>0\right)^{\mathrm T} \mathbf{\Lambda}</em>{k}\left(\boldsymbol{\mu}</em>{k}-\mathbf{m}_0\right)\right}
$$</p>
<p>$$
\mathcal{W}\left(\mathbf{\Lambda}<em>{k} \mid \mathbf{W}</em>{0}, \nu_{0}\right)=B\left(\mathbf{W}<em>{0}, \nu</em>{0}\right)\left|\mathbf{\Lambda}<em>{k}\right|^{\left(\nu</em>{0}-D-1\right) / 2} \exp \left(-\frac{1}{2} \operatorname{Tr}\left(\mathbf{W}<em>{0}^{-1} \mathbf{\Lambda}</em>{k}\right)\right) \tag{B .78}
$$</p>
<p>を利用して$(10.54)$式のうち$\boldsymbol{\mu}_k$と$\mathbf{\Lambda}_k$に依存する項を考える。ただし</p>
<p>$$q^{\star}(\boldsymbol{\pi}, \boldsymbol{\mu}, \mathbf{\Lambda})=q^{\star}(\boldsymbol{\pi}) \prod_{k=1}^{K} q^{\star}\left(\boldsymbol{\mu}<em>{k}, \mathbf{\Lambda}</em>{k}\right) \tag{10.55}$$</p>
<p>で示されているように、$\prod_{k=1}^{K}$の部分は外に出ていることに留意する。</p>
<p>$$
\begin{aligned}\ln q^{\star}(\boldsymbol{\mu}<em>k, \mathbf{\Lambda}<em>k) &amp;= \ln p\left(\boldsymbol{\mu}</em>{k}, \mathbf{\Lambda}</em>{k}\right)+\sum_{n=1}^{N} \mathbb{E}\left[z_{n k}\right] \ln \mathcal{N}\left(\mathbf{x}<em>n \mid \boldsymbol{\mu}</em>{k}, \mathbf{\Lambda}<em>{k}^{-1}\right) \
&amp;= \ln \left[\mathcal{N}\left(\boldsymbol{\mu}</em>{k} \mid \mathbf{m}<em>0,\left(\beta</em>{0} \mathbf{\Lambda}<em>{k}\right)^{-1}\right) \mathcal{W}\left(\mathbf{\Lambda}</em>{k} \mid \mathbf{W}<em>{0}, \nu</em>{0}\right)\right] +\sum_{n=1}^{N} \mathbb{E}\left[z_{n k}\right] \ln \mathcal{N}\left(\mathbf{x}<em>n \mid \boldsymbol{\mu}</em>{k}, \mathbf{\Lambda}<em>{k}^{-1}\right) \
&amp;= \ln \mathcal{N}\left(\boldsymbol{\mu}</em>{k} \mid \mathbf{m}<em>0,\left(\beta</em>{0} \mathbf{\Lambda}<em>{k}\right)^{-1}\right)+ \ln \mathcal{W}\left(\mathbf{\Lambda}</em>{k} \mid \mathbf{W}<em>{0}, \nu</em>{0}\right) +\sum_{n=1}^{N} \mathbb{E}\left[z_{n k}\right] \ln \mathcal{N}\left(\mathbf{x}<em>n \mid \boldsymbol{\mu}</em>{k}, \mathbf{\Lambda}<em>{k}^{-1}\right) \
&amp;=\frac{1}{2}\ln |\mathbf{\Lambda}<em>k| - \frac{\beta_0}{2}(\boldsymbol{\mu}<em>k - \mathbf{m}<em>0)^{\mathrm T}\mathbf{\Lambda}<em>k(\boldsymbol{\mu}<em>k - \mathbf{m}<em>0) +\frac{\nu</em>{0}-D-1}{2} \ln \left|\mathbf{\Lambda}</em>{k}\right|-\frac{1}{2} \operatorname{Tr}\left(\mathbf{W}</em>{0}^{-1} \mathbf{\Lambda}</em>{k}\right) \
&amp;+\sum</em>{n=1}^{N} \mathbb{E}\left[z</em>{nk}\right]\left(\frac{1}{2}\ln \left|\mathbf{\Lambda}</em>{k}\right|-\frac{1}{2}\left(\mathbf{x}<em>n-\boldsymbol{\mu}</em>{k}\right)^{\mathrm T} \mathbf{\Lambda}_{k}\left(\mathbf{x}<em>n-\boldsymbol{\mu}</em>{k}\right)\right)+\textrm{const.}
\end{aligned}
$$</p>
<p>これをさらに$\ln q^{\star}\left(\boldsymbol{\mu}<em>{k}, \mathbf{\Lambda}</em>{k}\right ) = \ln q^{\star}\left(\boldsymbol{\mu}<em>{k} \mid \mathbf{\Lambda}</em>{k}\right) + \ln q^{\star}\left(\mathbf{\Lambda}<em>{k}\right)$の形に分解する。$\boldsymbol{\mu}</em>{k}$に依存する項の部分を取り出す。</p>
<p>$$
\begin{aligned}\ln q^{*}\left(\boldsymbol{\mu}<em>{k} \mid \mathbf{\Lambda}</em>{k}\right) &amp;= -\frac{1}{2} \boldsymbol{\mu}<em>{k}^{\mathrm T}\left[\beta</em>{0}+\sum_{n=1}^{N} \mathbb{E}\left[z_{n k}\right]\right] \mathbf{\Lambda}<em>{k} \boldsymbol{\mu}</em>{k} +\boldsymbol{\mu}<em>{k}^{\mathrm T} \mathbf{\Lambda}</em>{k}\left[\beta_{0} \mathbf{m}<em>0+\sum</em>{n=1}^{N} \mathbb{E}\left[z_{nk}\right] \mathbf{x}<em>{n}\right]+\textrm{const.} \
&amp;= -\frac{1}{2} \boldsymbol{\mu}</em>{k}^{\mathrm T}(\beta_{0}+N_k) \mathbf{\Lambda}<em>{k} \boldsymbol{\mu}</em>{k} +\boldsymbol{\mu}<em>{k}^{\mathrm T} \mathbf{\Lambda}</em>{k}\left[\beta_{0} \mathbf{m}_0+ N_k \overline{\mathbf{x}}_k \right]+\textrm{const.}\quad (\because (10.50)-(10.52))\end{aligned}
$$</p>
<p>この形は$\boldsymbol{\mu}_{k}$についての二次形式となっており、両辺の指数を取れば多変数ガウス分布の形で</p>
<p>$$q^{\star}\left(\boldsymbol{\mu}<em>{k}\mid \mathbf{\Lambda}</em>{k}\right)=\mathcal{N}\left(\boldsymbol{\mu}<em>{k} \mid \mathbf{m}</em>{k},\left(\beta_{k} \mathbf{\Lambda}_{k}\right)^{-1}\right)$$</p>
<p>と書ける。ただし</p>
<p>$$
\begin{aligned} \beta_{k} &amp;=\beta_{0}+N_{k} \ \mathbf{m}<em>{k} &amp;=\frac{1}{\beta</em>{k}}\left(\beta_{0} \mathbf{m}<em>{0}+N</em>{k} \overline{\mathbf{x}}_{k}\right) \end{aligned}
$$</p>
<p>である。</p>
<p>続いて$q^{\star}(\mathbf{\Lambda}<em>k)$について、これは$\ln q^{\star}(\mathbf{\Lambda}<em>k) = \ln q^{\star}\left(\boldsymbol{\mu}</em>{k}, \mathbf{\Lambda}</em>{k}\right ) -\ln q^{\star}\left(\boldsymbol{\mu}<em>{k} \mid \mathbf{\Lambda}</em>{k}\right)$から求めると</p>
<p>$$
\begin{aligned}
\ln q^{*}\left(\mathbf{\Lambda}<em>{k}\right) &amp;=\frac{1}{2} \ln \left|\mathbf{\Lambda}</em>{k}\right|-\frac{\beta_{0}}{2}\left(\boldsymbol{\mu}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm T} \mathbf{\Lambda}<em>{k}\left(\boldsymbol{\mu}</em>{k}-\mathbf{m}<em>{0}\right) +\frac{\nu</em>{0}-D-1}{2}\ln \left|\mathbf{\Lambda}<em>{k}\right|-\frac{1}{2} \operatorname{Tr}\left(\mathbf{W}</em>{0}^{-1} \mathbf{\Lambda}<em>{k}\right) \
&amp;+\sum</em>{n=1}^{N} \mathbb{E}\left[z_{n k}\right]\left(\frac{1}{2}\ln \left|\mathbf{\Lambda}<em>{k}\right|-\frac{1}{2}\left(\mathbf{x}<em>n-\boldsymbol{\mu}</em>{k}\right)^{\mathrm T} \mathbf{\Lambda}</em>{k}\left(\mathbf{x}<em>n-\boldsymbol{\mu}</em>{k}\right)\right) - \ln q^{\star}\left(\boldsymbol{\mu}<em>{k}\mid \mathbf{\Lambda}</em>{k}\right) +\textrm{const.}\
&amp;= \frac{1}{2} \ln \left|\mathbf{\Lambda}<em>{k}\right|-\frac{\beta</em>{0}}{2}\left(\boldsymbol{\mu}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm T} \mathbf{\Lambda}<em>{k}\left(\boldsymbol{\mu}</em>{k}-\mathbf{m}<em>{0}\right) +\frac{\nu</em>{0}-D-1}{2}\ln \left|\mathbf{\Lambda}<em>{k}\right|-\frac{1}{2} \operatorname{Tr}\left(\mathbf{W}</em>{0}^{-1} \mathbf{\Lambda}<em>{k}\right) \
&amp;+\sum</em>{n=1}^{N} \mathbb{E}\left[z_{n k}\right]\left(\frac{1}{2}\ln \left|\mathbf{\Lambda}<em>{k}\right|-\frac{1}{2}\left(\mathbf{x}<em>n-\boldsymbol{\mu}</em>{k}\right)^{\mathrm T} \mathbf{\Lambda}</em>{k}\left(\mathbf{x}<em>n-\boldsymbol{\mu}</em>{k}\right)\right) \
&amp;-\frac{1}{2} \ln \left|\beta_{k} \mathbf{\Lambda}<em>{k}\right|-\frac{\beta</em>{k}}{2}\left(\boldsymbol{\mu}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm T} \mathbf{\Lambda}<em>{k}\left(\boldsymbol{\mu}</em>{k}-\mathbf{m}<em>{k}\right) +\textrm{const.}\
&amp;=\frac{\nu</em>{0}-D-1}{2}\ln|\mathbf{\Lambda}<em>k|+\frac{1}{2}\sum</em>{n=1}^{N}\mathbb{E}[z_{nk}]\ln |\mathbf{\Lambda}<em>k| \
&amp;-\frac{1}{2} \operatorname{Tr}\left[\left{\beta</em>{0}\left(\boldsymbol{\mu}<em>k-\mathbf{m}</em>{0}\right)\left(\boldsymbol{\mu}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm T}+\sum_{n=1}^{N} \mathbb{E}[z_{nk}]\left(\mathbf{x}<em>n-\boldsymbol{\mu}</em>{k}\right)\left(\mathbf{x}<em>n-\boldsymbol{\mu}<em>k\right)^{\mathrm T} \right.\right. \
&amp;\left.\left.-\beta</em>{k}\left(\boldsymbol{\mu}</em>{k}-\mathbf{m}<em>{k}\right)\left(\boldsymbol{\mu}</em>{k}-\mathbf{m}<em>{k}\right)^{\mathrm T}+\mathbf{W}</em>{0}^{-1}\right} \mathbf{\Lambda}_{k}\right]+\textrm{const.}
\end{aligned}
$$</p>
<p>これがウィシャート分布の対数形</p>
<p>$$
\ln \mathcal{W}=\ln B(\mathbf{W}<em>k, \nu</em>{k})+\frac{\nu_{k}-D-1}{2}\ln\left|\mathbf{\Lambda}<em>{k}\right|-\frac{1}{2} \operatorname{Tr}\left(\mathbf{W}</em>{k}^{-1} \mathbf{\Lambda}_{k}\right)
$$</p>
<p>となれば良い（$B(\mathbf{W}<em>{k},\nu</em>{k})$は正規化の定数項）。係数を比較して、</p>
<p>$$
\nu_{k}=\nu_{0}+\sum_{n=1}^{N} \mathbb{E}\left[z_{nk}\right]=\nu_{0}+N_{k}
$$</p>
<p>$$ \mathbf{W}<em>{k}^{-1}=\mathbf{W}</em>{0}^{-1}+\beta_{0}\left(\boldsymbol{\mu}<em>{k}-\mathbf{m}</em>{0}\right)\left(\boldsymbol{\mu}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm T}-\beta_{k}\left(\boldsymbol{\mu}<em>{k}-\mathbf{m}</em>{k}\right)\left(\boldsymbol{\mu}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm T}+\sum_{n=1}^{N} \mathbb{E}\left[z_{nk}\right]\left(\mathbf{x}<em>n-\boldsymbol{\mu}</em>{k}\right)\left(\mathbf{x}<em>n-\boldsymbol{\mu}</em>{k}\right)^{\mathrm T}
$$</p>
<p>となる。</p>
<p>最後の$\mathbf{W}_k^{-1}$が$(10.62)$の形になることを<strong>がんばって</strong>計算で示す。</p>
<p>$$
\begin{aligned}\mathbf{W}<em>{k}^{-1}&amp;=\mathbf{W}</em>{0}^{-1}+\beta_{0}\left(\boldsymbol{\mu}<em>{k}-\mathbf{m}</em>{0}\right)\left(\boldsymbol{\mu}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm T}-\beta_{k}\left(\boldsymbol{\mu}<em>{k}-\mathbf{m}</em>{k}\right)\left(\boldsymbol{\mu}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm T}+\sum_{n=1}^{N} \mathbb{E}\left[z_{nk}\right]\left(\mathbf{x}<em>n-\boldsymbol{\mu}</em>{k}\right)\left(\mathbf{x}<em>n-\boldsymbol{\mu}</em>{k}\right)^{\mathrm T} \
&amp;=\mathbf{W}<em>{0}^{-1}+\beta</em>{0} \boldsymbol{\mu}<em>{k} \boldsymbol{\mu}</em>{k}^{\mathrm T}-2 \beta_{0} \mathbf{m}<em>{0} \boldsymbol{\mu}</em>{k}^{\mathrm T}+\beta_{0} \mathbf{m}<em>{0} \mathbf{m}</em>{0}^{\mathrm T}-\beta_{k} \boldsymbol{\mu}<em>{k} \boldsymbol{\mu}</em>{k}^{\mathrm T}+2 \beta_{k} \mathbf{m}<em>{k} \boldsymbol{\mu}</em>{k}^{\mathrm T} -\beta_{k} \mathbf{m}<em>{k} \mathbf{m}</em>{k}^{\mathrm T}
+\sum_{n=1}^{N} r_{n k} \mathbf{x}<em>{n} \mathbf{x}</em>{n}^{\mathrm T}-2 \sum_{n=1}^{N} r_{n k} \mathbf{x}<em>{n} \boldsymbol{\mu}</em>{k}^{\mathrm T}+\sum_{n=1}^{N} r_{n k} \boldsymbol{\mu}<em>{k} \boldsymbol{\mu}</em>{k}^{\mathrm T} \
&amp;=\mathbf{W}<em>{0}^{-1}+\underbrace{\left( \sum</em>{n=1}^{N}r_{nk}+\beta_{0}-\beta_{k} \right)}<em>{0}\boldsymbol{\mu}</em>{k} \boldsymbol{\mu}<em>{k}^{\mathrm T} -2\underbrace{\left(\sum</em>{n=1}^{N} r_{n k} \mathbf{x}<em>{n}+\beta</em>{0} \mathbf{m}<em>{0}-\beta</em>{k} \mathbf{m}<em>{k}\right)}</em>{0} \boldsymbol{\mu}<em>{k}^{\mathrm T} + \sum</em>{n=1}^{N} r_{nk} \mathbf{x}<em>{n} \mathbf{x}</em>{n}^{\mathrm T}+\beta_{0} \mathbf{m}<em>{0} \mathbf{m}</em>{0}^{\mathrm T}-\beta_{k} \mathbf{m}<em>{k} \mathbf{m}</em>{k}^{\mathrm T} \
&amp;=\mathbf{W}<em>{0}^{-1}+ \underbrace{\sum</em>{n=1}^{N} r_{nk} \mathbf{x}<em>{n} \mathbf{x}</em>{n}^{\mathrm T}}<em>{(A)} + \underbrace{\beta</em>{0} \mathbf{m}<em>{0} \mathbf{m}</em>{0}^{\mathrm T}-\beta_{k} \mathbf{m}<em>{k} \mathbf{m}</em>{k}^{\mathrm T}}<em>{(B)} \quad (\because \beta</em>{k} \mathbf{m}<em>{k}=\beta</em>{0} \mathbf{m}<em>{0}+N</em>{k} \overline{\mathbf{x}}<em>{k}) \
&amp;=\mathbf{W}</em>{0}^{-1} + \underbrace{N_{k} \mathbf{S}<em>{k}+N</em>{k} \overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm T}}<em>{(A)} + \underbrace{\frac{\beta</em>{0} N_{k}}{\beta_{k}} \mathbf{m}<em>{0} \mathbf{m}</em>{0}^{\mathrm T}-\frac{N_{k}^{2}}{\beta_{k}} \overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm T}-\frac{\beta_{0} N_{k}}{\beta_{k}}\left(2 \mathbf{m}<em>{0} \overline{\mathbf{x}}</em>{k}^{\mathrm T}\right)}<em>{(B)} \
&amp;=\mathbf{W}</em>{0}^{-1} + N_{k} \mathbf{S}<em>{k} + \frac{\beta</em>{0}N_{k}}{\beta_{k}}\left( \mathbf{m}<em>{0} \mathbf{m}</em>{0}^{\mathrm T} -2\mathbf{m}<em>{0}\overline{\mathbf{x}}</em>{k}^{\mathrm T} + \overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm T} \right) \
&amp;=\mathbf{W}<em>{0}^{-1} + N</em>{k} \mathbf{S}<em>{k} + \frac{\beta</em>{0}N_{k}}{\beta_{0} + N_{k}}\left( \overline{\mathbf{x}}<em>{k} - \mathbf{m}</em>{0} \right)\left( \overline{\mathbf{x}}<em>{k} - \mathbf{m}</em>{0} \right)^{\mathrm T}
\end{aligned}
$$</p>
<p>以上で$(10.62)$が示された。</p>
<p>途中の式変形$(A)$について</p>
<p>$$
\begin{aligned}
\sum_{n=1}^{N} r_{n k} \mathbf{x}<em>{n} \mathbf{x}</em>{n}^{\mathrm T}&amp;=\sum_{n=1}^{N} r_{n k}\left[\left(\mathbf{x}<em>{n}-\overline{\mathbf{x}}</em>{k}\right)\left(\mathbf{x}<em>{n}-\overline{\mathbf{x}}</em>{k}\right)^{\mathrm T}-\overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm T}+2 \mathbf{x}<em>{n} \overline{\mathbf{x}}</em>{k}^{\mathrm T}\right]\
&amp;=\sum_{n=1}^{N} r_{n k}\left[\left(\mathbf{x}<em>{n}-\overline{\mathbf{x}}</em>{k}\right)\left(\mathbf{x}<em>{n}-\overline{\mathbf{x}}</em>{k}\right)^{\mathrm T}+\overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm T}+2\left(\mathbf{x}<em>{n}-\overline{\mathbf{x}}</em>{k}\right) \overline{\mathbf{x}}<em>{k}^{\mathrm T}\right]\
&amp;=N</em>{k} \mathbf{S}<em>{k}+\sum</em>{n=1}^{N} r_{n k} \overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm T}+2 \sum_{n=1}^{N} r_{n k}\left[\left(\mathbf{x}<em>{n}-\overline{\mathbf{x}}</em>{k}\right) \overline{\mathbf{x}}<em>{k}^{\mathrm T}\right] \
&amp;=N</em>{k} \mathbf{S}<em>{k}+\sum</em>{n=1}^{N} r_{n k} \overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm T}+2 \sum_{n=1}^{N} r_{n k} \mathbf{x}<em>{n} \overline{\mathbf{x}}</em>{k}^{\mathrm T}-2 \sum_{n=1}^{N} r_{nk} \overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm T}\
&amp;=N_{k} \mathbf{S}<em>{k}+N</em>{k} \overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm T}+2 N_{k} \overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm T}-2 N_{k} \overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm T}\
&amp;=N_{k} \mathbf{S}<em>{k}+N</em>{k} \overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm T}
\end{aligned}
$$</p>
<p>途中の式変形$(B)$について</p>
<p>$$
\begin{aligned}
\beta_{0} \mathbf{m}<em>{0} \mathbf{m}</em>{0}^{\mathrm T}-\beta_{k} \mathbf{m}<em>{k} \mathbf{m}</em>{k}^{\mathrm T} &amp;=\beta_{0} \mathbf{m}<em>{0} \mathbf{m}</em>{0}^{\mathrm T}-\frac{1}{\beta_{k}}\left(\beta_{0} \mathbf{m}<em>{0}+N</em>{k} \overline{\mathbf{x}}<em>{k}\right)\left(\beta</em>{0} \mathbf{m}<em>{0}+N</em>{k} \overline{\mathbf{x}}<em>{k}\right)^{\mathrm T} \
&amp;=\left(1-\frac{\beta</em>{0}}{\beta_{k}}\right) \beta_{0} \mathbf{m}<em>{0} \mathbf{m}</em>{0}^{\mathrm T}-\frac{N_{k}^{2}}{\beta_{k}} \overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm T}-\frac{2}{\beta_{k}} \beta_{0} N_{k} \mathbf{m}<em>{0} \overline{\mathbf{x}}</em>{k}^{\mathrm T} \
&amp;=\frac{\beta_{0} N_{k}}{\beta_{k}} \mathbf{m}<em>{0} \mathbf{m}</em>{0}^{\mathrm T}-\frac{N_{k}^{2}}{\beta_{k}} \overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm T}-\frac{\beta_{0} N_{k}}{\beta_{k}}\left(2 \mathbf{m}<em>{0} \overline{\mathbf{x}}</em>{k}^{\mathrm T}\right)
\end{aligned}
$$</p>
<p>となることを用いた。</p>
<h2 id="演習-1014"><a class="header" href="#演習-1014">演習 10.14</a></h2>
<div class="panel-primary">
<p>$$
q^{\star}\left(\boldsymbol{\mu}<em>{k}, \mathbf{\Lambda}</em>{k}\right)=\mathcal{N}\left(\boldsymbol{\mu}<em>{k} \mid \mathbf{m}</em>{k},\left(\beta_{k} \mathbf{\Lambda}<em>{k}\right)^{-1}\right) \mathcal{W}\left(\mathbf{\Lambda}</em>{k} \mid \mathbf{W}<em>{k}, \nu</em>{k}\right) \tag{10.59}
$$
の分布を使って，
$$
\begin{aligned}&amp; \mathbb{E}<em>{\boldsymbol{\mu}</em>{k}, \mathbf{\Lambda}<em>{k}}\left[\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}<em>{k}\right)^{\mathrm{T}} \mathbf{\Lambda}</em>{k}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}\right)\right] \
=&amp;\ D \beta_{k}^{-1}+\nu_{k}\left(\mathbf{x}<em>{n}-\mathbf{m}</em>{k}\right)^{\mathrm{T}} \mathbf{W}<em>{k}\left(\mathbf{x}</em>{n}-\mathbf{m}_{k}\right)\end{aligned}\tag{10.64}
$$
の結果を確かめよ．</p>
</div>
<p>期待値の定義を使って計算していく。</p>
<p>$$
\begin{aligned} &amp; \mathbb{E}<em>{\boldsymbol{\mu}</em>{k}, \mathbf{\Lambda}<em>{k}}\left[\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}<em>{k}\right)^{\mathrm T} \mathbf{\Lambda}</em>{k}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}\right)\right] \
=&amp; \iint\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}\right)^{\mathrm T} \mathbf{\Lambda}<em>{k}\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}<em>{k}\right) q^{\star}\left(\boldsymbol{\mu}</em>{k}, \mathbf{\Lambda}<em>{k}\right) d \boldsymbol{\mu}</em>{k} d \mathbf{\Lambda}<em>{k} \
=&amp;\int\left{\int\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}<em>{k}\right)^{\mathrm T} \mathbf{\Lambda}</em>{k}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}\right) q^{\star}\left(\boldsymbol{\mu}<em>{k} \mid \mathbf{\Lambda}</em>{k}\right) d \boldsymbol{\mu}<em>{k}\right} q^{\star}\left(\mathbf{\Lambda}</em>{k}\right) d \mathbf{\Lambda}<em>{k} \
=&amp;\int\underbrace{\left{\int\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}<em>{k}\right)^{\mathrm T} \mathbf{\Lambda}</em>{k}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}\right) \mathcal{N}\left(\boldsymbol{\mu}<em>{k} \mid \mathbf{m}</em>{k},\left(\beta_{k} \mathbf{\Lambda}<em>{k}\right)^{-1}\right) d \boldsymbol{\mu}</em>{k}\right}}<em>{(A)} q^{\star}\left(\mathbf{\Lambda}</em>{k}\right) d \mathbf{\Lambda}_{k}
\end{aligned}
$$</p>
<p>$(A)$について、</p>
<p>$$
\begin{aligned} &amp; \int\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}\right)^{\mathrm T} \mathbf{\Lambda}<em>{k}\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}<em>{k}\right) \mathcal{N}\left(\boldsymbol{\mu}</em>{k} \mid \mathbf{m}<em>{k},\left(\beta</em>{k} \Lambda_{A}\right)^{-1}\right) d \boldsymbol{\mu}<em>{k} \
=&amp;\ \mathbb{E}</em>{\boldsymbol{\mu}<em>{k}}\left[\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}<em>{k}\right)^{\mathrm T} \mathbf{\Lambda}</em>{k}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}\right)\right]\quad \left(\boldsymbol{\mu}<em>{k} \sim \mathcal{N}\left(\boldsymbol{\mu}</em>{k} \mid \mathbf{m}<em>{k},\left(\beta</em>{k} \mathbf{\Lambda}<em>{k}\right)^{-1}\right)\right) \
=&amp;\ \mathbb{E}</em>{\boldsymbol{\mu}<em>{k}}\left[\operatorname{Tr}\left[\mathbf{\Lambda}</em>{k}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}\right)\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}\right)^{\mathrm T}\right]\right] \
=&amp;\ \mathbb{E}<em>{\boldsymbol{\mu}</em>{k}}\left[\operatorname{Tr}\left[\mathbf{\Lambda}<em>{k}\left(\mathbf{x}</em>{n} \mathbf{x}<em>{n}^{\mathrm T}-2 \mathbf{x}</em>{n}^{\mathrm T} \boldsymbol{\mu}<em>{k}+\boldsymbol{\mu}</em>{k} \boldsymbol{\mu}<em>{k}^{\mathrm T}\right)\right]\right] \
=&amp;\operatorname{Tr}\left[\mathbb{E}</em>{\boldsymbol{\mu}<em>{k}}\left[\mathbf{\Lambda}</em>{k} \mathbf{x}<em>{n} \mathbf{x}</em>{n}^{\mathrm T}\right]-2 \mathbb{E}<em>{\boldsymbol{\mu}</em>{k}}\left[\mathbf{\Lambda}<em>{k} \mathbf{x}</em>{n}^{\mathrm T} \boldsymbol{\mu}<em>{k}\right]+\mathbb{E}</em>{\boldsymbol{\mu}<em>{k}}\left[\mathbf{\Lambda}</em>{k} \boldsymbol{\mu}<em>{k} \boldsymbol{\mu}</em>{k}^{\mathrm T}\right]\right] \
=&amp;\operatorname{Tr}\left[\mathbf{\Lambda}<em>{k}\left{\mathbf{x}</em>{n} \mathbf{x}<em>{n}^{\mathrm T}-2 \mathbf{x}</em>{n}^{\mathrm T} \mathbb{E}<em>{\boldsymbol{\mu}</em>{k}}\left[\boldsymbol{\mu}<em>{k}\right]+\mathbb{E}</em>{\boldsymbol{\mu}<em>{k}}\left[\boldsymbol{\mu}</em>{k} \boldsymbol{\mu}<em>{k}^{\mathrm T}\right]\right}\right] \
=&amp;\operatorname{Tr}\left[\mathbf{\Lambda}</em>{k}\left{\mathbf{x}<em>{n} \mathbf{x}</em>{n}^{\mathrm T}-2 \mathbf{x}<em>{n}^{\mathrm T} \mathbf{m}</em>{k}+\mathbf{m}<em>{k} \mathbf{m}</em>{k}^{\mathrm T}+\left(\beta_{k} \mathbf{\Lambda}<em>{k}\right)^{-1}\right}\right] \
=&amp;\operatorname{Tr}\left[\mathbf{\Lambda}</em>{k}\left(\mathbf{x}<em>{n} - \mathbf{m}</em>{k}\right)\left(\mathbf{x}<em>{n} - \mathbf{m}</em>{k}\right)^{\mathrm T}\right]+\operatorname{Tr}\left[\beta_{k}^{-1} \mathbf{I}\right] \
=&amp;\left(\mathbf{x}<em>{n} - \mathbf{m}</em>{k}\right)^{\mathrm T}\mathbf{\Lambda}<em>{k}\left(\mathbf{x}</em>{n} - \mathbf{m}<em>{k}\right)+D\beta</em>{k}^{-1}
\end{aligned}
$$</p>
<p>となる。ここで、$\mathbb{E}<em>{\boldsymbol{\mu}</em>{k}}\left[\boldsymbol{\mu}<em>{k}\right] = \mathbf{m}</em>{k}$と$\mathbb{E}<em>{\boldsymbol{\mu}</em>{k}}\left[\boldsymbol{\mu}<em>{k} \boldsymbol{\mu}</em>{k}^{\mathrm T}\right] = \mathbf{m}<em>k \mathbf{m}<em>k^{\mathrm T}+\left(\beta</em>{k} \mathbf{\Lambda}</em>{k}\right)^{-1}$、さらにトレース演算子と期待値演算子はともに線形演算子で交換可能であることを用いた。$D$は$\mathbf{x}_n$の次元数である。</p>
<p>これと演習問題10.13で得られた$q^{\star}(\mathbf{\Lambda}_k) = \mathcal{W}(\mathbf{\Lambda}_k \mid \mathbf{W}_k, \nu_k)$を用いると</p>
<p>$$
\begin{aligned}
\mathbb{E}<em>{\boldsymbol{\mu}</em>{k}, \mathbf{\Lambda}<em>{k}}\left[\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}<em>{k}\right)^{\mathrm T} \mathbf{\Lambda}</em>{k}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}\right)\right] &amp;= \int \left( \left(\mathbf{x}<em>{n} - \mathbf{m}</em>{k}\right)^{\mathrm T}\mathbf{\Lambda}<em>{k}\left(\mathbf{x}</em>{n} - \mathbf{m}<em>{k}\right)+D\beta</em>{k}^{-1} \right)q^{\star}(\mathbf{\Lambda}<em>k)d\mathbf{\Lambda}<em>k \
&amp;=\mathbb{E}</em>{\mathbf{\Lambda}<em>k}\left[ \left(\mathbf{x}</em>{n} - \mathbf{m}</em>{k}\right)^{\mathrm T}\mathbf{\Lambda}<em>{k}\left(\mathbf{x}</em>{n} - \mathbf{m}<em>{k}\right)+D\beta</em>{k}^{-1} \right] \quad \left( \mathbf{\Lambda}<em>{k} \sim \mathcal{W}(\mathbf{\Lambda}<em>k \mid \mathbf{W}<em>k, \nu_k) \right)\
&amp;=\mathbb{E}</em>{\mathbf{\Lambda}<em>k}[D \beta</em>{k}^{-1}]+\mathbb{E}</em>{\mathbf{\Lambda}</em>{k}}\left[\left(\mathbf{x}<em>{n} - \mathbf{m}</em>{k}\right)^{\mathrm T} \Lambda_{k}\left(\mathbf{x}<em>{n} - \mathbf{m}</em>{k}\right)\right] \
&amp;=D \beta_{k}^{-1}+\mathbb{E}<em>{\mathbf{\Lambda}</em>{k}}\left[\operatorname{Tr}\left[\mathbf{\Lambda}<em>{k} \left(\mathbf{x}</em>{n} - \mathbf{m}<em>{k}\right)\left(\mathbf{x}</em>{n} - \mathbf{m}<em>{k}\right)^{\mathrm T}\right]\right] \
&amp;=D \beta</em>{k}^{-1}+\operatorname{Tr}\left[\mathbb{E}<em>{\mathbf{\Lambda}</em>{k}}\left[\mathbf{\Lambda}<em>{k}\right] \left(\mathbf{x}</em>{n} - \mathbf{m}<em>{k}\right)\left(\mathbf{x}</em>{n} - \mathbf{m}<em>{k}\right)^{\mathrm T}\right] \
&amp;=D \beta</em>{k}^{-1}+\operatorname{Tr}\left[\nu_{k} \mathbf{W}<em>{k} \left(\mathbf{x}</em>{n} - \mathbf{m}<em>{k}\right)\left(\mathbf{x}</em>{n} - \mathbf{m}<em>{k}\right)^{\mathrm T}\right] \
&amp;=D \beta</em>{k}^{-1}+\nu_{k}\left(\mathbf{x}<em>{n} - \mathbf{m}</em>{k}\right)^{\mathrm T} \mathbf{W}<em>{k}\left(\mathbf{x}</em>{n} - \mathbf{m}_{k}\right)
\end{aligned}
$$</p>
<p>となり、$(10.64)$式が得られた。</p>
<blockquote>
<p>「これは容易に計算できて」とは？</p>
</blockquote>
<h2 id="演習-1015"><a class="header" href="#演習-1015">演習 10.15</a></h2>
<div class="panel-primary">
<p>$$
\mathbb{E}\left[\mu_{k}\right]=\frac{\alpha_{k}}{\widehat{\alpha}}=\frac{\alpha_k}{\sum_{k=1}^{K}\alpha_k}\tag{B.17}
$$
の結果を用いて，変分混合ガウス分布の混合係数の期待値は</p>
<p>$$
\mathbb{E}\left[\pi_{k}\right]=\frac{\alpha_{0}+N_{k}}{K \alpha_{0}+N}\tag{10.69}
$$</p>
<p>で与えられることを示せ．</p>
</div>
<p>単純に$\mu_k \to \pi_k$とし、$(10.58)$を用いて式を変形すれば求まる。</p>
<p>$$
\begin{aligned} \mathbb{E}\left[\pi_{k}\right] &amp;=\frac{\alpha_{k}}{\sum_{k=1}^{K} \alpha_{k}}\quad \because(\textrm{B} .17) \ &amp;=\frac{\alpha_{0}+N_{k}}{\sum_{k=1}^{K}\left(\alpha_{0}+N_{k}\right)}\quad \because(10.58) \
&amp;=\frac{\alpha_{0}+N_{k}}{K \alpha_{0}+\sum_{k=1}^{K} N_{k}}=\frac{\alpha_{0}+N_{k}}{K \alpha_{0}+N} \end{aligned}
$$</p>
<p>以上で$(10.69)$式が求められた。</p>
<h2 id="演習-1016"><a class="header" href="#演習-1016">演習 10.16</a></h2>
<div class="panel-primary">
<p>$$
\begin{aligned} \mathcal{L} &amp;=\sum_{\mathbf{Z}} \iiint q(\mathbf{Z}, \boldsymbol{\pi}, \boldsymbol{\mu}, \mathbf{\Lambda}) \ln \left{\frac{p(\mathbf{X}, \mathbf{Z}, \boldsymbol{\pi}, \boldsymbol{\mu}, \mathbf{\Lambda})}{q(\mathbf{Z}, \boldsymbol{\pi}, \boldsymbol{\mu}, \mathbf{\Lambda})}\right} \mathrm{d} \pi \mathrm{d} \boldsymbol{\mu} \mathrm{d} \mathbf{\Lambda} \ &amp;=\mathbb{E}[\ln p(\mathbf{X}, \mathbf{Z}, \boldsymbol{\pi}, \boldsymbol{\mu}, \mathbf{\Lambda})]-\mathbb{E}[\ln q(\mathbf{Z}, \boldsymbol{\pi}, \boldsymbol{\mu}, \mathbf{\Lambda})] \ &amp;= \mathbb{E}[\ln p(\mathbf{X} \mid \mathbf{Z}, \boldsymbol{\mu}, \mathbf{\Lambda})]+\mathbb{E}[\ln p(\mathbf{Z} \mid \boldsymbol{\pi})]+\mathbb{E}[\ln p(\boldsymbol{\pi})]+\mathbb{E}[\ln p(\boldsymbol{\mu}, \mathbf{\Lambda})] \ &amp;-\mathbb{E}[\ln q(\mathbf{Z})]-\mathbb{E}[\ln q(\boldsymbol{\pi})]-\mathbb{E}[\ln q(\boldsymbol{\mu}, \mathbf{\Lambda})] \end{aligned} \tag{10.70}$$
で与えられる変分ガウス混合モデルの下界の，最初の二項についての結果
$$
\begin{aligned} \mathbb{E}[\ln p(\mathbf{X} \mid \mathbf{Z}, \boldsymbol{\mu}, \mathbf{\Lambda})]&amp;= \frac{1}{2} \sum_{k=1}^{K} N_{k}\left{\ln \widetilde{\Lambda}<em>{k}-D \beta</em>{k}^{-1}-\nu_{k} \operatorname{Tr}\left(\mathbf{S}<em>{k} \mathbf{W}</em>{k}\right)\right.\ &amp;\left.-\nu_{k}\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm{T}} \mathbf{W}<em>{k}\left(\overline{\mathbf{x}}</em>{k}-\mathbf{m}<em>{k}\right)-D \ln (2 \pi)\right} \end{aligned} \tag{10.71}$$
$$
\begin{aligned} \mathbb{E}[\ln p(\mathbf{Z} \mid \boldsymbol{\pi})]= \sum</em>{n=1}^{N} \sum_{k=1}^{K} r_{n k} \ln \tilde{\pi}_{k} \end{aligned} \tag{10.72}$$
を確かめよ．</p>
</div>
<p>容易に計算できるらしいのでやってみる。$(10.71)$について$(10.38)$の観測データベクトルの条件付き分布の式</p>
<p>$$
p(\mathbf{X} \mid \mathbf{Z}, \boldsymbol{\mu}, \mathbf{\Lambda})=\prod_{n=1}^{N} \prod_{k=1}^{K} \mathcal{N}\left(\mathbf{x}<em>{n} \mid \boldsymbol{\mu}</em>{k}, \mathbf{\Lambda}<em>{k}^{-1}\right)^{z</em>{n k}} \tag{10.38}
$$</p>
<p>を用いると</p>
<p>$$
\begin{aligned}\mathbb{E}[\ln p(\mathbf{X} \mid \mathbf{Z}, \boldsymbol{\mu}, \mathbf{\Lambda})]&amp;=\mathbb{E}\left[z_{nk} \sum_{n=1}^{N} \sum_{k=1}^{K} \ln \mathcal{N}\left(\mathbf{x}<em>{n} \mid \boldsymbol{\mu}</em>{k}, \mathbf{\Lambda}<em>{k}^{-1}\right)\right] \
&amp;=\sum</em>{n=1}^{N} \sum_{k=1}^{K} \mathbb{E}\left[z_{n k}\left{-\frac{D}{2} \ln (2 \pi)+\frac{1}{2} \ln \left|\mathbf{\Lambda}<em>{k}\right|-\frac{1}{2}\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}<em>{k}\right)^{\mathrm T} \mathbf{\Lambda}</em>{k}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}\right)\right}\right]
\end{aligned}
$$</p>
<p>今は負担率$\mathbb{E}[z_{nk}]= r_{nk}$を固定したときのパラメータの変分事後分布を求めているので、$\mathbb{E}[z_{nk}]$は分離＆固定して考える（ってことで合ってるのか？）。</p>
<p>$$
\begin{aligned}
&amp;=\sum_{n=1}^{N} \sum_{k=1}^{K} \mathbb{E}\left[z_{n k}\right] \mathbb{E}\left[-\frac{D}{2} \ln (2 \pi)+\frac{1}{2} \ln \left|\mathbf{\Lambda}<em>{k}\right|-\frac{1}{2}\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}<em>{k}\right)^{\mathrm T} \mathbf{\Lambda}</em>{k}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}\right)\right] \
&amp;=\frac{1}{2} \sum_{n=1}^{N} \sum_{k=1}^{K} r_{n k} \mathbb{E}[-D \ln (2 \pi)]+\mathbb{E}<em>{\mathbf{\Lambda}</em>{k}}\left[\ln \left|\mathbf{\Lambda}<em>{k}\right|\right]-\mathbb{E}</em>{\boldsymbol{\mu}<em>{k}, \mathbf{\Lambda}</em>{k}}\left[\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}\right)^{\mathrm T} \mathbf{\Lambda}<em>{k}\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}<em>{k}\right)\right] \
&amp;=\frac{1}{2} \sum</em>{k=1}^{N} \sum_{k=1}^{K} r_{n k}\left[-D \ln (2 \pi)+\ln \tilde{\Lambda}<em>{k}-\left(D \beta</em>{k}^{-1}+\nu_{k}\left(\mathbf{x}<em>{n}-\mathbf{m}</em>{k}\right)^{\mathrm T} \mathbf{W}<em>{k}\left(\overline{\mathbf{x}}</em>{k}-\mathbf{m}<em>{k}\right)\right)\right] \quad (\because (10.64))\
&amp;=\frac{1}{2} \sum</em>{k=1}^{K}\left{\sum_{n=1}^{N} r_{n k}\left(-D \ln (2 \pi)+\ln \tilde{\Lambda}<em>{k}-D \beta</em>{k}^{-1}\right)-\sum_{n=1}^{N} r_{n k} \nu_{k}\left(\mathbf{x}<em>{n}-\mathbf{m}</em>{k}\right)^{\mathrm T} \mathbf{W}<em>{k}\left(\mathbf{x}</em>{n}-\mathbf{m}<em>{k}\right)\right} \
&amp;= \frac{1}{2} \sum</em>{k=1}^{K} \left{ N_{k}\left(-D \ln(2 \pi)+\ln \tilde{\Lambda}<em>{k}-D \beta</em>{k}^{-1}\right)-\sum_{n=1}^{N} r_{nk} \nu_{k}\left(\mathbf{x}<em>{n}-\mathbf{m}</em>{k}\right)^{\mathrm T} \mathbf{W}<em>{k}\left(\mathbf{x}</em>{n}-\mathbf{m}<em>{k}\right)\right} \
&amp;= \frac{1}{2} \sum</em>{k=1}^{K} N_{k} \left{ \ln \tilde{\Lambda}<em>{k}-D \beta</em>{k}^{-1}- \nu_{k}\operatorname{Tr}\left(\mathbf{S}<em>{k} \mathbf{W}</em>{k}\right) -\nu_{k}\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm T} \mathbf{W}<em>{k}\left(\overline{\mathbf{x}}</em>{k}-\mathbf{m}_{k}\right) -D \ln(2 \pi) \right} \quad (\because (*))
\end{aligned}
$$</p>
<p>以上で$(10.71)$式が示された。</p>
<hr>
<p>$(*)$の式変形の$\sum_{n=1}^{N} r_{nk} \nu_{k}\left(\mathbf{x}<em>{n}-\mathbf{m}</em>{k}\right)^{\mathrm T} \mathbf{W}<em>{k}\left(\mathbf{x}</em>{n}-\mathbf{m}_{k}\right)$について</p>
<p>$$
\begin{aligned}
\sum_{n=1}^{N} r_{nk} \nu_{k}\left(\mathbf{x}<em>{n}-\mathbf{m}</em>{k}\right)^{\mathrm T} \mathbf{W}<em>{k}\left(\mathbf{x}</em>{n}-\mathbf{m}<em>{k}\right)
&amp;=\nu</em>{k} \sum_{n=1}^{N} r_{n k}\left[\left(\mathbf{x}<em>{n}-\mathbf{m}</em>{k}\right)^{\mathrm T} \mathbf{W}<em>{k}\left(\mathbf{x}</em>{n}-\mathbf{m}<em>{k}\right)\right] \
&amp;=\nu</em>{k} \sum_{n=1}^{N} r_{n k}\left[\operatorname{Tr}\left(\mathbf{x}<em>{n}-\mathbf{m}</em>{k}\right)\left(\mathbf{x}<em>{n}-\mathbf{m}</em>{k}\right)^{\mathrm T} \mathbf{W}<em>{k}\right] \
&amp;=\nu</em>{k} \operatorname{Tr}\left[\sum_{n=1}^{N} r_{n k}\left(\mathbf{x}<em>{n}-\mathbf{m}</em>{k}\right)\left(\mathbf{x}<em>{n}-\mathbf{m}</em>{k}\right)^{\mathrm T} \mathbf{W}<em>{k}\right] \
&amp;=\nu</em>{k} N_{k} \operatorname{Tr}\left[\mathbf{S}<em>{k} \mathbf{W}</em>{k}+\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm T} \mathbf{W}<em>{k}\right] \quad (\because (**))\
&amp;=\nu</em>{k} N_{k}\left{\operatorname{Tr}\left[\mathbf{S}<em>{k} \mathbf{W}</em>{k}\right]+\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm T} \mathbf{W}<em>{k}\left(\overline{\mathbf{x}}</em>{k}-\mathbf{m}_{k}\right)\right}
\end{aligned}
$$</p>
<hr>
<p>$(**)$の式変形について</p>
<p>$$
\begin{aligned} \sum_{n=1}^{N} r_{n k}\left(\mathbf{x}<em>{n}-\mathbf{m}</em>{k}\right)\left(\mathbf{x}<em>{n}-\mathbf{m}</em>{k}\right)^{\mathrm T} &amp;= \sum_{n=1}^{N} r_{n k} \mathbf{x}<em>{n} \mathbf{x}</em>{n}^{\mathrm T}-2 \sum_{n=1}^{N} r_{n k} \mathbf{m}<em>{k}^{\mathrm T} \mathbf{x}</em>{n}+\sum_{n=1}^{N} r_{n k} \mathbf{m}<em>{k} \mathbf{m}</em>{k}^{\mathrm T} \
&amp;=N_{k} \mathbf{S}<em>{k}+N</em>{k} \overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm T}-2 N_{k} \mathbf{m}<em>{k}^{\mathrm T} \overline{\mathbf{x}}</em>{k}+N_{k} \mathbf{m}<em>{k} \mathbf{m}</em>{k}^{\mathrm T} \quad (\because 演習10.13の式変形(A))\
&amp;=N_{k}\left(\mathbf{S}<em>{k}+\left(\overline{\mathbf{x}}</em>{k}-\mathbf{m}<em>{k}\right)\left(\overline{\mathbf{x}}</em>{k}-\mathbf{m}_{k}\right)^{\mathrm T}\right) \end{aligned}
$$</p>
<hr>
<p>$(10.72)$については
$$
p(\mathbf{Z} \mid \boldsymbol{\pi})=\prod_{n=1}^{N} \prod_{k=1}^{K} \pi_{k}^{z_{n k}} \tag{10.37}
$$
から直ちに求められる。</p>
<p>$$
\begin{aligned}
\mathbb{E}<em>{\mathbf{Z},\boldsymbol{\pi}}[\ln p(\mathbf{Z}\mid \boldsymbol{\pi})] &amp;= \sum</em>{n=1}^{N}\sum_{k=1}^{K}\mathbb{E}<em>{\mathbf{Z},\boldsymbol{\pi}} \left[ z</em>{nk} \ln \pi_{k} \right] \
&amp;= \sum_{n=1}^{N}\sum_{k=1}^{K}\mathbb{E}<em>{\mathbf{Z}} \left[ z</em>{nk} \right]  \mathbb{E}<em>{\boldsymbol{\pi}} \left[\ln \pi</em>{k} \right] \
&amp;= \sum_{n=1}^{N}\sum_{k=1}^{K}r_{nk}\ln\tilde{\pi}_k \quad (\because (10.66))
\end{aligned}
$$</p>
<h2 id="演習-1017"><a class="header" href="#演習-1017">演習 10.17</a></h2>
<div class="panel-primary">
<p>$$
\begin{aligned} \mathcal{L} &amp;= \mathbb{E}[\ln p(\mathbf{X} \mid \mathbf{Z}, \boldsymbol{\mu}, \mathbf{\Lambda})]+\mathbb{E}[\ln p(\mathbf{Z} \mid \boldsymbol{\pi})]+\mathbb{E}[\ln p(\boldsymbol{\pi})]+\mathbb{E}[\ln p(\boldsymbol{\mu}, \mathbf{\Lambda})] \ &amp;-\mathbb{E}[\ln q(\mathbf{Z})]-\mathbb{E}[\ln q(\boldsymbol{\pi})]-\mathbb{E}[\ln q(\boldsymbol{\mu}, \mathbf{\Lambda})] \end{aligned} \tag{10.70}$$
で与えられる変分ガウス混合モデルの下界の，残りの項についての結果
$$\mathbb{E}[\ln p(\boldsymbol{\pi})]=\ln C(\boldsymbol{\alpha}<em>{0})+\left(\alpha</em>{0}-1\right) \sum_{k=1}^{K} \ln \widetilde{\pi}<em>{k} \tag{10.73}$$
$$
\begin{aligned}
\mathbb{E}[\ln p(\boldsymbol{\mu}, \mathbf{\Lambda})] &amp;=\frac{1}{2} \sum</em>{k=1}^{K}\left{D \ln \left(\beta_{0} / 2 \pi\right)+\ln \widetilde{\Lambda}<em>{k}-\frac{D \beta</em>{0}}{\beta_{k}}\right. \ &amp;\left.-\beta_{0} \nu_{k}\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm{T}} \mathbf{W}<em>{k}\left(\mathbf{m}</em>{k}-\mathbf{m}<em>{0}\right)\right}+K \ln B\left(\mathbf{W}</em>{0}, \nu_{0}\right) \ &amp;+\frac{\left(\nu_{0}-D-1\right)}{2} \sum_{k=1}^{K} \ln \widetilde{\Lambda}<em>{k}-\frac{1}{2} \sum</em>{k=1}^{K} \nu_{k} \operatorname{Tr}\left(\mathbf{W}<em>{0}^{-1} \mathbf{W}</em>{k}\right) \end{aligned}\tag{10.74}$$
$$\mathbb{E}[\ln q(\mathbf{Z})]=\sum_{n=1}^{N} \sum_{k=1}^{K} r_{n k} \ln r_{n k} \tag{10.75}$$
$$\mathbb{E}[\ln q(\boldsymbol{\pi})]=\sum_{k=1}^{K}\left(\alpha_{k}-1\right) \ln \tilde{\pi}<em>{k}+\ln C(\boldsymbol{\alpha}) \tag{10.76}$$
$$\mathbb{E}[\ln q(\boldsymbol{\mu}, \mathbf{\Lambda})]=\sum</em>{k=1}^{K}\left{\frac{1}{2} \ln \widetilde{\Lambda}<em>{k}+\frac{D}{2} \ln \left(\frac{\beta</em>{k}}{2 \pi}\right)-\frac{D}{2}-\mathrm{H}\left[q\left(\mathbf{\Lambda}_{k}\right)\right]\right} \tag{10.77}$$
を確かめよ．</p>
</div>
<p>$(10.39)$より$p(\boldsymbol{\pi}) = \operatorname{Dir}(\boldsymbol{\pi}\mid \boldsymbol{\alpha}_{0})$となることを用いて</p>
<p>$$
\begin{aligned}
\mathbb{E}[\ln p(\boldsymbol{\pi})] &amp;=\mathbb{E}\left[\ln C(\boldsymbol{\alpha}<em>{0}) \prod</em>{k=1}^{K} \pi_{k}^{\alpha_{0}-1}\right] \
&amp;=\mathbb{E}\left[\ln C(\boldsymbol{\alpha}<em>{0})\right]+\mathbb{E}\left[\sum</em>{k=1}^{K} \ln \pi_{k}^{\alpha_{0}-1}\right] \
&amp;=\ln C(\boldsymbol{\alpha}<em>{0})+\mathbb{E}\left[\sum</em>{k=1}^{K}\left(\alpha_{0}-1\right) \ln \pi_{k}\right] \
&amp;=\ln C(\boldsymbol{\alpha}<em>{0})+\left(\alpha</em>{0}-1\right) \sum_{k=1}^{K} \mathbb{E}[\ln \pi_{k}] \
&amp;=\ln C(\boldsymbol{\alpha}<em>{0})+\left(\alpha</em>{0}-1\right) \sum_{k=1}^{K} \ln \tilde{\pi}_{k}
\end{aligned}
$$</p>
<p>以上で$(10.73)$式が求まった。</p>
<hr>
<p>$(10.40)$で導入したガウス–ウィシャート事前分布
$$
p(\boldsymbol{\mu}, \mathbf{\Lambda}) = \prod_{k=1}^{K} \mathcal{N}\left(\boldsymbol{\mu}<em>{k} \mid \mathbf{m}</em>{0},\left(\beta_{0} \mathbf{\Lambda}<em>{k}\right)^{-1}\right) \mathcal{W}\left(\mathbf{\Lambda}</em>{k} \mid \mathbf{W}<em>{0}, \nu</em>{0}\right) \tag{10.40}
$$
を用いて</p>
<p>$$
\begin{aligned}
\mathbb{E}[\ln p(\boldsymbol{\mu}, \mathbf{\Lambda})] &amp;=\mathbb{E}\left[\ln \left[\prod_{k=1}^{K} \mathcal{N}\left(\boldsymbol{\mu}<em>{k} \mid \mathbf{m}</em>{0},\left(\beta_{0} \mathbf{\Lambda}<em>{k}\right)^{-1}\right) \mathcal{W}\left(\mathbf{\Lambda}</em>{k} \mid \mathbf{W}<em>{0}, \nu</em>{0}\right)\right]\right] \
&amp;=\mathbb{E}\left[\sum_{k=1}^{K} \ln \mathcal{N}\left(\boldsymbol{\mu}<em>{k} \mid \mathbf{m}</em>{0},\left(\beta_{0} \mathbf{\Lambda}<em>{k}\right)^{-1}\right)\right]+\mathbb{E}\left[\sum</em>{k=1}^{K} \ln \mathcal{W}\left(\mathbf{\Lambda}<em>{k} \mid \mathbf{W}</em>{0}, \nu_{0}\right)\right] \
&amp;=\sum_{k=1}^{K} \mathbb{E}\left[-\frac{D}{2} \ln (2 \pi)+\frac{1}{2} \ln \left|\beta_{0} \mathbf{\Lambda}<em>{k}\right|-\frac{1}{2}\left(\boldsymbol{\mu}</em>{k}-\mathbf{m}<em>{0}\right)^{\mathrm T}\left(\beta</em>{0} \mathbf{\Lambda}<em>{k}\right)\left(\boldsymbol{\mu}</em>{k}-\mathbf{m}<em>{0}\right)\right] \
&amp;+\sum</em>{k=1}^{K} \mathbb{E}\left[\ln B\left(\mathbf{W}<em>{0}, \nu</em>{0}\right)+\frac{\nu_{0}-D-1}{2} \ln \left|\mathbf{\Lambda}<em>{k}\right|-\frac{1}{2} \operatorname{Tr}\left[\mathbf{W}</em>{0}^{-1} \mathbf{\Lambda}<em>{k}\right]\right] \
&amp;=\frac{1}{2}\left{\sum</em>{k=1}^{K} D \ln \left( \frac{\beta_{0}}{2 \pi} \right)+\mathbb{E}\left[\ln \left|\mathbf{\Lambda}<em>{k}\right|\right]-\mathbb{E}\left[\sum</em>{k=1}^{K}\left(\boldsymbol{\mu}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm T}\left(\beta_{0} \mathbf{\Lambda}<em>k\right)\left(\boldsymbol{\mu}</em>{k}-\mathbf{m}<em>{0}\right)\right]\right} \
&amp;+K \ln B\left(\mathbf{W}</em>{0}, \nu_{0}\right)+\frac{\nu_{0}-D-1}{2} \sum_{k=1}^{K} \mathbb{E}\left[\ln | \mathbf{\Lambda}<em>{k} | \right]-\frac{1}{2} \sum</em>{k=1}^{K} \mathbb{E}\left[\operatorname{Tr}\left[\mathbf{W}<em>{0}^{-1} \mathbf{\Lambda}</em>{k}\right]\right] \
&amp;=\frac{1}{2}\left{\sum_{k=1}^{K} D \ln \left( \frac{\beta_{0}}{2 \pi} \right)+\ln \tilde{\Lambda}<em>{k}-\mathbb{E}\left[\sum</em>{k=1}^{K}\left(\boldsymbol{\mu}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm T}\left(\beta_{0} \mathbf{\Lambda}<em>k\right)\left(\boldsymbol{\mu}</em>{k}-\mathbf{m}<em>{0}\right)\right]\right} \
&amp;+K \ln B\left(\mathbf{W}</em>{0}, \nu_{0}\right)+\frac{\nu_{0}-D-1}{2} \sum_{k=1}^{K} \ln \tilde{\Lambda}<em>{k}-\frac{1}{2} \sum</em>{k=1}^{K} \mathbb{E}\left[\operatorname{Tr}\left[\mathbf{W}<em>{0}^{-1} \mathbf{\Lambda}</em>{k}\right]\right] \
\end{aligned}
$$</p>
<p>$(10.74)$との係数を比較して、</p>
<p>$$
\mathbb{E}\left[\sum_{k=1}^{K}\left(\boldsymbol{\mu}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm T}\left(\beta_{0} \mathbf{\Lambda}<em>{k}\right)\left(\boldsymbol{\mu}</em>{k}-\mathbf{m}<em>{0}\right)\right]=\sum</em>{k=1}^{K}\left{\frac{D \beta_{0}}{\beta_{k}}+\beta_{0} \nu_{k}\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm T} \mathbf{W}<em>{k}\left(\mathbf{m}</em>{k}-\mathbf{m}<em>{0}\right)\right} \tag{*}$$
および
$$\sum</em>{k=1}^{K} \mathbb{E}<em>{\mathbf{\Lambda}</em>{k}}\left[\operatorname{Tr}\left[\mathbf{W}<em>{0}^{-1} \mathbf{\Lambda}</em>{k}\right]\right]=\sum_{k=1}^{K} \nu_{k} \operatorname{Tr}\left[\mathbf{W}<em>{0}^{-1} \mathbf{W}</em>{k}\right] \tag{**}
$$
であることを示せば良い。まず$(*)$について</p>
<p>$$
\begin{aligned}
\sum_{k=1}^{K} \mathbb{E}\left{\left(\boldsymbol{\mu}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm T}\left(\beta_{0} \mathbf{\Lambda}<em>{k}\right)\left(\boldsymbol{\mu}</em>{k}-\mathbf{m}<em>{0}\right)\right} &amp;=\beta</em>{0} \sum_{k=1}^{K} \mathbb{E}\left{\operatorname{Tr}\left[\mathbf{\Lambda}<em>{k} \cdot\left(\boldsymbol{\mu}</em>{k}-\mathbf{m}<em>{0}\right)\left(\boldsymbol{\mu}</em>{k}-\mathbf{m}<em>{0}\right)^{\mathrm T}\right]\right} \
&amp;=\beta</em>{0} \sum_{k=1}^{K} \mathbb{E}<em>{\boldsymbol{\mu}</em>{k}, \mathbf{\Lambda}<em>{k}}\left{\operatorname{Tr}\left[\mathbf{\Lambda}</em>{k} \cdot\left(\boldsymbol{\mu}<em>{k} \boldsymbol{\mu}</em>{k}^{\mathrm T}-2 \boldsymbol{\mu}<em>{k} \mathbf{m}</em>{0}^{\mathrm T}+\mathbf{m}<em>{0} \mathbf{m}</em>{0}^{\mathrm T}\right)\right]\right} \
&amp;=\beta_{0} \sum_{k=1}^{K} \mathbb{E}<em>{\mathbf{\Lambda}</em>{k}}\left{\operatorname{Tr}\left[\mathbf{\Lambda}<em>{k} \cdot\left(\mathbf{m}</em>{k} \mathbf{m}<em>{k}^{\mathrm T}+\beta</em>{k}^{-1} \mathbf{\Lambda}<em>{k}^{-1}-2 \mathbf{m}</em>{k} \mathbf{m}<em>{0}^{\mathrm T}+\mathbf{m}</em>{0} \mathbf{m}<em>{0}^{\mathrm T}\right)\right]\right} \
&amp;=\beta</em>{0} \sum_{k=1}^{K} \mathbb{E}<em>{\mathbf{\Lambda}</em>{k}}\left{\operatorname{Tr}\left[\beta_{k}^{-1} \mathbf{I}+\mathbf{\Lambda}<em>{k} \cdot\left(\mathbf{m}</em>{k} \mathbf{m}<em>{k}^{\mathrm T}-2 \mathbf{m}</em>{k} \mathbf{m}<em>{0}^{\mathrm T}+\mathbf{m}</em>{0} \mathbf{m}<em>{0}^{\mathrm T}\right)\right]\right} \
&amp;=\beta</em>{0} \sum_{k=1}^{K} \mathbb{E}<em>{\mathbf{\Lambda}</em>{k}}\left{D \cdot \beta_{k}^{-1}+\operatorname{Tr}\left[\mathbf{\Lambda}<em>{k} \cdot\left(\mathbf{m}</em>{k}-\mathbf{m}<em>{0}\right)\left(\mathbf{m}</em>{k}-\mathbf{m}<em>{0}\right)^{\mathrm T}\right]\right} \
&amp;=\beta</em>{0} \sum_{k=1}^{K} \left{\frac{D}{\beta_{k}}+\mathbb{E}<em>{\mathbf{\Lambda}</em>{k}}\operatorname{Tr}\left[\mathbf{\Lambda}<em>{k} \cdot\left(\mathbf{m}</em>{k}-\mathbf{m}<em>{0}\right)\left(\mathbf{m}</em>{k}-\mathbf{m}<em>{0}\right)^{\mathrm T}\right]\right} \
&amp;=\beta</em>{0} \sum_{k=1}^{K} \left{\frac{D}{\beta_{k}}+\operatorname{Tr}\left[\mathbb{E}<em>{\mathbf{\Lambda}</em>{k}}\left[\mathbf{\Lambda}<em>{k}\right] \cdot\left(\mathbf{m}</em>{k}-\mathbf{m}<em>{0}\right)\left(\mathbf{m}</em>{k}-\mathbf{m}<em>{0}\right)^{\mathrm T}\right]\right} \
&amp;=\beta</em>{0} \sum_{k=1}^{K} \left{\frac{D}{\beta_{k}}+\operatorname{Tr}\left[\nu_{k}\mathbf{W}<em>{k} \left(\mathbf{m}</em>{k}-\mathbf{m}<em>{0}\right)\left(\mathbf{m}</em>{k}-\mathbf{m}<em>{0}\right)^{\mathrm T}\right]\right} \quad (\because (B.80))\
&amp;=\beta</em>{0} \sum_{k=1}^{K} \left{\frac{D}{\beta_{k}}+\nu_{k}\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm T} \mathbf{W}<em>{k} \left(\mathbf{m}</em>{k}-\mathbf{m}<em>{0}\right)\right} \
&amp;=\sum</em>{k=1}^{K}\left{\frac{D \beta_{0}}{\beta_{k}}+\beta_{0} \nu_{k}\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm T} \mathbf{W}<em>{k}\left(\mathbf{m}</em>{k}-\mathbf{m}_{0}\right)\right}
\end{aligned}
$$</p>
<p>$(**)$について、</p>
<p>$$
\begin{aligned}
\mathbb{E}<em>{\mathbf{\Lambda}</em>{k}}\left[\operatorname{Tr}\left[\mathbf{W}<em>{0}^{-1} \mathbf{\Lambda}</em>{k}\right]\right] &amp;=\operatorname{Tr}\left[\mathbf{W}<em>{0}^{-1} \cdot \mathbb{E}</em>{\mathbf{\Lambda}<em>{k}}[\mathbf{\Lambda}</em>{k}]\right] \
&amp;=\operatorname{Tr}\left[\mathbf{W}<em>{0}^{-1} \cdot \nu</em>{k} \mathbf{W}<em>{k}\right]\quad (\because (B .80)) \
&amp;=\nu</em>{k} \operatorname{Tr}\left[\mathbf{W}<em>{0}^{-1} \mathbf{W}</em>{k}\right]
\end{aligned}
$$</p>
<p>以上で$(10.74)$が示された。</p>
<hr>
<p>$$
\begin{aligned} \mathbb{E}\left[\ln q^{\star}(\mathbf{Z})\right] &amp;=\mathbb{E}<em>{\mathbf{Z}}\left[\ln \left(\prod</em>{n=1}^{N} \prod_{k=1}^{K} r_{n k}^{z_{n k}}\right)\right] \quad(\because(10.48)) \
&amp;=\mathbb{E}<em>{\mathbf{Z}}\left[\sum</em>{n=1}^{N} \sum_{k=1}^{K} z_{n k} \ln r_{n k}\right] \
&amp;=\sum_{n=1}^{N} \sum_{k=1}^{K} \mathbb{E}<em>{\mathbf{Z}}\left[\mathbf{Z}</em>{n k}\right] \mathbb{E}<em>{z}\left[\ln r</em>{n k}\right] \
&amp;=\sum_{n=1}^{N} \sum_{k=1}^{K} r_{n k} \ln r_{n k}
\end{aligned}
$$</p>
<hr>
<p>$$
\begin{aligned} \mathbb{E}\left[\ln q^{\star}(\pi)\right] &amp;=\mathbb{E}<em>{\pi}[\ln (\operatorname{Dir}(\boldsymbol{\pi} \mid \boldsymbol{\alpha}))] \quad(\because(10.57)) \
&amp;=\mathbb{E}</em>{\pi}\left[\ln C(\boldsymbol{\alpha}) \prod_{k=1}^{K} \pi_{k}^{\alpha_{k}-1}\right](\because(B. 16)) \
&amp;=\mathbb{E}<em>{\pi}[\ln C(\boldsymbol{\alpha})]+\mathbb{E}</em>{\pi}\left[\sum_{k=1}^{K}\left(\alpha_{k}-1\right) \ln \pi_{k}\right] \
&amp;=\sum_{k=1}^{K}\left(\alpha_{k}-1\right) \ln \tilde{\pi}_{k}+\ln C(\boldsymbol{\alpha}) \end{aligned}
$$</p>
<hr>
<p>$$
\begin{aligned} \mathbb{E}\left[\ln q^{\star}(\boldsymbol{\mu}, \mathbf{\Lambda})\right]
&amp;=\mathbb{E}<em>{\boldsymbol{\mu}<em>k, \mathbf{\Lambda}<em>k}\left[\ln \left[\prod</em>{k=1}^{K} q^{\star}\left(\boldsymbol{\mu}</em>{k}, \mathbf{\Lambda}</em>{k}\right)\right]\right] \quad(\because(10.55)) \
&amp;=\mathbb{E}<em>{\boldsymbol{\mu}<em>k, \mathbf{\Lambda}<em>k}\left[\sum</em>{k=1}^{K} \ln q^{\star}\left(\boldsymbol{\mu}</em>{k}, \mathbf{\Lambda}</em>{k}\right)\right] \
&amp; =\sum_{k=1}^{K} \mathbb{E}<em>{\boldsymbol{\mu}<em>k, \mathbf{\Lambda}<em>k}\left[\ln \mathcal{N}\left(\boldsymbol{\mu}</em>{k} \mid \mathbf{m}</em>{k},\left(\beta</em>{k} \mathbf{\Lambda}<em>{k}\right)^{-1}\right)\right]+\sum</em>{k=1}^{K} \mathbb{E}<em>{\boldsymbol{\mu}<em>k, \mathbf{\Lambda}<em>k}\left[\ln \mathcal{W}\left(\mathbf{\Lambda}</em>{k} \mid \mathbf{W}</em>{k}, \nu</em>{k}\right)\right] \
&amp;= \sum_{k=1}^{K} \mathbb{E}<em>{\boldsymbol{\mu}<em>k, \mathbf{\Lambda}<em>k}\left{-\frac{D}{2} \ln 2 \pi+\frac{D}{2} \ln \beta</em>{k}+\frac{1}{2} \ln \left|\mathbf{\Lambda}</em>{k}\right|-\frac{1}{2}\left(\boldsymbol{\mu}</em>{k}-\mathbf{m}<em>{k}\right)^{\mathrm T}\left(\beta</em>{k} \mathbf{\Lambda}<em>{k}\right)\left(\boldsymbol{\mu}</em>{k}-\mathbf{m}<em>{k}\right)\right} \
&amp;+\sum</em>{k=1}^{K} \mathbb{E}<em>{\boldsymbol{\mu}<em>k, \mathbf{\Lambda}<em>k}\left{\ln B\left(\mathbf{W}</em>{k}, \nu</em>{k}\right)+\frac{\nu</em>{k}-D-1}{2} \ln \left|\mathbf{\Lambda}<em>{k}\right|-\frac{1}{2} \operatorname{Tr}\left[\mathbf{W}</em>{k}^{-1} \mathbf{\Lambda}<em>{k}\right]\right} \
&amp;= \sum</em>{k=1}^{K} \left{ \frac{1}{2} \mathbb{E}<em>{\boldsymbol{\mu}<em>k, \mathbf{\Lambda}<em>k} \left[ \ln \left|\mathbf{\Lambda}</em>{k}\right| \right] + \frac{D}{2} \ln \left(\frac{\beta</em>{k}}{2\pi}\right) -\frac{1}{2} \operatorname{Tr}\left[\left(\beta</em>{k} \mathbf{\Lambda}<em>{k}\right)\left(\beta</em>{k} \mathbf{\Lambda}<em>{k}\right)^{-1}\right] \right} \
&amp;+\sum</em>{k=1}^{K} \mathbb{E}<em>{\boldsymbol{\mu}<em>k, \mathbf{\Lambda}<em>k}\left{\ln B\left(\mathbf{W}</em>{k}, \nu</em>{k}\right)+\frac{\nu</em>{k}-D-1}{2} \ln \left|\mathbf{\Lambda}<em>{k}\right|-\frac{1}{2} \nu</em>{k} \operatorname{Tr}\left[\mathbf{W}<em>{k}^{-1} \mathbf{W}</em>{k}\right]\right} \quad (\because 先述の(**)を利用) \
&amp;= \sum_{k=1}^{K} \left{ \frac{1}{2} \ln \tilde{\Lambda}<em>{k} + \frac{D}{2} \ln \left(\frac{\beta</em>{k}}{2\pi}\right) -\frac{D}{2} \right} +\sum_{k=1}^{K} \left{\ln B\left(\mathbf{W}<em>{k}, \nu</em>{k}\right)+\frac{\nu_{k}-D-1}{2} \mathbb{E}<em>{\mathbf{\Lambda}<em>k}\ln \left|\mathbf{\Lambda}</em>{k}\right|-\frac{\nu</em>{k} D}{2}\right}  \
\end{aligned}
$$</p>
<p>途中でMatrix Cookbook (380)の公式
$$
\mathbb{E}_{\mathbf{x} \sim \mathcal{N}(\mathbf{x}\mid \mathbf{m}, \mathbf{\Sigma})}\left[\left(\mathbf{x}-\mathbf{m}^{\prime}\right)^{\mathrm T} \mathbf{A}\left(\mathbf{x}-\mathbf{m}^{\prime}\right)\right]=\left(\mathbf{m}-\mathbf{m}^{\prime}\right)^{\mathrm T} \mathbf{A}\left(\mathbf{m}-\mathbf{m}^{\prime}\right)+\operatorname{Tr}(\mathbf{A} \mathbf{\Sigma})
$$
を用いた。</p>
<p>$(\textrm{B}.82)$からウィシャート分布$\mathcal{W}(\mathbf{\Lambda} \mid \mathbf{W}, \nu)$のエントロピーは</p>
<p>$$
-\ln B(\mathbf{W}, \nu)-\frac{(\nu-D-1)}{2} \mathbb{E}[\ln |\mathbf{\Lambda}|]+\frac{\nu D}{2} \tag{B.82}
$$</p>
<p>であり、これを教科書P.196では$\mathrm{H}\left[q\left(\mathbf{\Lambda}_{k}\right)\right]$とおいているので、</p>
<p>$$
\mathbb{E}\left[\ln q^{\star}(\boldsymbol{\mu}, \mathbf{\Lambda})\right] =
\sum_{k=1}^{K} \left{ \frac{1}{2} \ln \tilde{\Lambda}<em>{k} + \frac{D}{2} \ln \left(\frac{\beta</em>{k}}{2\pi}\right) -\frac{D}{2} - \mathrm{H}\left[q\left(\mathbf{\Lambda}_{k}\right)\right]\right} \tag{10.77}
$$</p>
<p>となり$(10.77)$を得た。</p>
<h2 id="演習-1018"><a class="header" href="#演習-1018">演習 10.18</a></h2>
<div class="panel-primary">
<p>この演習問題では，ガウス混合モデルでの変分ベイズ法の再推定を行う方程式を，下界を直接微分することで導出する．これを行うため，変分事後分布が
$$
q(\mathbf{Z}, \boldsymbol{\pi}, \boldsymbol{\mu}, \mathbf{\Lambda})=q(\mathbf{Z}) q(\boldsymbol{\pi}, \boldsymbol{\mu}, \mathbf{\Lambda}) \tag{10.42}
$$
と
$$
q(\boldsymbol{\pi}, \boldsymbol{\mu}, \mathbf{\Lambda})=q(\boldsymbol{\pi}) \prod_{k=1}^{K} q\left(\boldsymbol{\mu}<em>{k}, \mathbf{\Lambda}</em>{k}\right) \tag{10.55}
$$
で定義されるように分解され，各因子が
$$
q^{\star}(\mathbf{Z})=\prod_{n=1}^{N} \prod_{k=1}^{K} r_{n k}^{z_{n k}} \tag{10.48}
$$
$$
q^{\star}(\boldsymbol{\pi})=\operatorname{Dir}(\boldsymbol{\pi} \mid \boldsymbol{\alpha}) \tag{10.57}
$$
$$
q^{\star}\left(\boldsymbol{\mu}<em>{k}, \mathbf{\Lambda}</em>{k}\right)=\mathcal{N}\left(\boldsymbol{\mu}<em>{k} \mid \mathbf{m}</em>{k},\left(\beta_{k} \mathbf{\Lambda}<em>{k}\right)^{-1}\right) \mathcal{W}\left(\mathbf{\Lambda}</em>{k} \mid \mathbf{W}<em>{k}, \nu</em>{k}\right) \tag{10.59}
$$
で与えられることを仮定する．これらを
$$
\begin{aligned} \mathcal{L} &amp;= \mathbb{E}[\ln p(\mathbf{X} \mid \mathbf{Z}, \boldsymbol{\mu}, \mathbf{\Lambda})]+\mathbb{E}[\ln p(\mathbf{Z} \mid \boldsymbol{\pi})]+\mathbb{E}[\ln p(\boldsymbol{\pi})]+\mathbb{E}[\ln p(\boldsymbol{\mu}, \mathbf{\Lambda})] \ &amp;-\mathbb{E}[\ln q(\mathbf{Z})]-\mathbb{E}[\ln q(\boldsymbol{\pi})]-\mathbb{E}[\ln q(\boldsymbol{\mu}, \mathbf{\Lambda})] \end{aligned} \tag{10.70}
$$
に代入し，下界を変分事後分布の持つパラメータの関数として与えよ．次にこの下界をパラメータに関して最大化することで，変分事後分布の因子を再推定する方程式を導出しこれらが10.2.1節で得たものと一致することを示せ．</p>
</div>
<p>※やろうとすることは変分ベイズ法の再推定式$(10.58)$とか$(10.60)–(10.63)$を変分下界$(10.70)$を用いることでも求められるということを示せばいい……のだが非常に計算が多い。</p>
<p>各因子が$(10.48),(10.57),(10.59)$のように表せる場合、10.2.2節で得た$(10.71)–(10.77)$の変分下界をまず$\mathcal{L}$に代入すると</p>
<p>$$
\begin{aligned} \mathcal{L} &amp;= \mathbb{E}[\ln p(\mathbf{X} \mid \mathbf{Z}, \boldsymbol{\mu}, \mathbf{\Lambda})]+\mathbb{E}[\ln p(\mathbf{Z} \mid \boldsymbol{\pi})]+\mathbb{E}[\ln p(\boldsymbol{\pi})]+\mathbb{E}[\ln p(\boldsymbol{\mu}, \mathbf{\Lambda})] \ &amp;-\mathbb{E}[\ln q(\mathbf{Z})]-\mathbb{E}[\ln q(\boldsymbol{\pi})]-\mathbb{E}[\ln q(\boldsymbol{\mu}, \mathbf{\Lambda})] \
&amp;=\frac{1}{2} \sum_{k=1}^{K} N_{k}\left{\ln \widetilde{\Lambda}<em>{k}-D \beta</em>{k}^{-1}-\nu_{k} \operatorname{Tr}\left(\mathbf{S}<em>{k} \mathbf{W}</em>{k}\right)\right. \ &amp;-\left.\nu_{k}\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm{T}} \mathbf{W}<em>{k}\left(\overline{\mathbf{x}}</em>{k}-\mathbf{m}<em>{k}\right)-D \ln (2 \pi)\right} \
&amp;+\sum</em>{n=1}^{N} \sum_{k=1}^{K} r_{n k} \ln \widetilde{\pi}<em>{k} + \ln C\left(\boldsymbol{\alpha}</em>{0}\right)+\left(\alpha_{0}-1\right) \sum_{k=1}^{K} \ln \widetilde{\pi}<em>{k} \
&amp;+ \frac{1}{2} \sum</em>{k=1}^{K}\left{D \ln \left(\beta_{0} / 2 \pi\right)+\ln \widetilde{\Lambda}<em>{k}-\frac{D \beta</em>{0}}{\beta_{k}}-\beta_{0} \nu_{k}\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm{T}} \mathbf{W}<em>{k}\left(\mathbf{m}</em>{k}-\mathbf{m}<em>{0}\right)\right} \
&amp;+K \ln B\left(\mathbf{W}</em>{0}, \nu_{0}\right) +\frac{\left(\nu_{0}-D-1\right)}{2} \sum_{k=1}^{K} \ln \widetilde{\Lambda}<em>{k}-\frac{1}{2} \sum</em>{k=1}^{K} \nu_{k} \operatorname{Tr}\left(\mathbf{W}<em>{0}^{-1} \mathbf{W}</em>{k}\right) \
&amp;-\sum_{n=1}^{N} \sum_{k=1}^{K} r_{n k} \ln r_{n k} - \sum_{k=1}^{K}\left(\alpha_{k}-1\right) \ln \tilde{\pi}<em>{k} - \ln C(\boldsymbol{\alpha}) \
&amp;-\sum</em>{k=1}^{K}\left{\frac{1}{2} \ln \widetilde{\Lambda}<em>{k}+\frac{D}{2} \ln \left(\frac{\beta</em>{k}}{2 \pi}\right)-\frac{D}{2}-\mathrm{H}\left[q\left(\boldsymbol{\Lambda}_{k}\right)\right]\right}
\end{aligned}
$$</p>
<p>$\mathcal{L}$を整理する。$\ln\widetilde{\Lambda}_k$,$\ln\widetilde{\pi}_k$,$\beta_k$,$\nu_k$の項に分ける。</p>
<p>$$
\begin{aligned}
\mathcal{L}=&amp;\ \frac{1}{2} \sum_{k=1}^K N_{k}\left{\ln \tilde{\Lambda}<em>{k}-D \beta</em>{k}^{-1}-\nu_{k} \operatorname{Tr}\left(\mathbf{S}<em>{k} \mathbf{W}</em>{k}\right)-\nu_{k}\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm T} \mathbf{W}<em>{k}\left(\overline{\mathbf{x}}</em>{k}-\mathbf{m}<em>{k}\right)-D \ln (2 \pi)\right} \
&amp;+\sum</em>{n=1}^{N} \sum_{k=1}^{K} r_{n k} \ln \tilde{\pi}<em>{k}+\ln C\left(\boldsymbol{\alpha}</em>{0}\right)+\left(\alpha_{0}-1\right) \sum_{k=1}^{K} \ln \tilde{\pi}<em>{k} \
&amp;+\frac{1}{2} \sum</em>{k=1}^{K}\left{D \ln \left(\frac{\beta_{0}}{2 \pi}\right)+\ln \tilde{\Lambda}<em>{k}-\frac{D \beta_0}{\beta</em>{k}}-\beta_{0} \nu_{k}\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm T} \mathbf{W}<em>{k}\left(\mathbf{m}</em>{k}-\mathbf{m}<em>{0}\right)\right} \
&amp;+K \ln B\left(\mathbf{W}</em>{0}, \nu_{0}\right)+\frac{\nu_{0}-D-1}{2} \sum_{k=1}^{K} \ln \tilde{\Lambda}<em>{k}-\frac{1}{2} \sum</em>{k=1}^{K} \nu_{k} \operatorname{Tr}\left(\mathbf{W}<em>{0}^{-1} \mathbf{W}</em>{k}\right) \
&amp;-\sum_{n=1}^{N} \sum_{k=1}^{K} r_{n k} \ln r_{n k}-\sum_{k=1}^{K}\left(\alpha_{k}-1\right) \ln \widetilde{\pi}<em>{k}-\ln C(\boldsymbol{\alpha}) \
&amp;-\sum</em>{k=1}^{K}\left{\frac{1}{2} \ln \tilde{\Lambda}<em>{k}+\frac{D}{2} \ln \left(\frac{\beta</em>{k}}{2 \pi}\right)-\frac{D}{2}+\ln B\left(\mathbf{W}<em>{k}, \nu</em>{k}\right)+\frac{\nu_{k}-D-1}{2} \ln \tilde{\Lambda}<em>{k}-\frac{\nu</em>{k} D}{2}\right} \
=&amp;\ \frac{1}{2} \sum_{k=1}^{K} \ln \tilde{\Lambda}<em>{k}\left{N</em>{k}+1+\left(\nu_{0}-D-1\right)-1-\left(\nu_{k}-D-1\right)\right} \
&amp;+\sum_{k=1}^{K} \ln \tilde{\pi}<em>{k}\left{\sum</em>{n=1}^{N} r_{n k}+\left(\alpha_{0}-1\right)-\left(\alpha_{k}-1\right)\right} \
&amp;+\frac{1}{2} \sum_{k=1}^{K}\left{\beta_{k}^{-1}\left(-N_{k} D-D \beta_{0}\right)-D \ln \left(\frac{\beta_{k}}{2 \pi}\right)\right} \
&amp;+\frac{1}{2} \sum_{k=1}^{K} N_{k}\left{-\nu_{k} \operatorname{Tr}\left(\mathbf{S}<em>{k} \mathbf{W}</em>{k}\right)-\nu_{k}\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm T} \mathbf{W}<em>{k}\left(\overline{\mathbf{x}}</em>{k}-\mathbf{m}<em>{k}\right)\right} \
&amp;+\frac{1}{2} \sum</em>{k=1}^{K}\left{-\nu_{k} \operatorname{Tr}\left(\mathbf{W}<em>{0}^{-1} \mathbf{W}</em>{k}\right)-\beta_{0} \nu_{k}\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm T} \mathbf{W}<em>{k}\left(\mathbf{m}</em>{k}-\mathbf{m}<em>{0}\right)\right} \
&amp;-\sum</em>{k=1}^{K}\left{\ln B\left(\mathbf{W}<em>{k}, \nu</em>{k}\right)-\frac{\nu_{k} D}{2}\right} \
&amp;-\frac{1}{2} \sum_{k=1}^{K} N_{k} D \ln (2 \pi)+\ln C\left(\boldsymbol{\alpha}<em>{0}\right)+\frac{1}{2} \sum</em>{k=1}^{K} D \ln \left(\frac{\beta_{0}}{2 \pi}\right)+K \ln B\left(\mathbf{W}<em>{0}, \nu</em>{0}\right) \
&amp;-\sum_{n=1}^{N} \sum_{k=1}^{K} r_{n k} \ln r_{n k}-\ln C(\boldsymbol{\alpha})-\sum_{k=1}^{K}\left(-\frac{D}{2}\right) \
=&amp;\ \frac{1}{2} \sum_{k=1}^{K} \ln \tilde{\Lambda}<em>{k}\left(N</em>{k}+\nu_{0}-\nu_{k}\right) \
&amp;+\sum_{k=1}^{K} \ln \tilde{\pi}<em>{k}\left(N</em>{k}+\alpha_{0}-\alpha_{k}\right) \
&amp;-\frac{D}{2} \sum_{k=1}^{K}\left{\beta_{k}^{-1}\left(N_{k}+\beta_{0}\right)+\ln \left(\frac{\beta_{k}}{2 \pi}\right)\right} \
&amp;-\frac{1}{2} \sum_{k=1}^{K} N_{k} \nu_{k}\left{\operatorname{Tr}\left(\mathbf{S}<em>{k} \mathbf{W}</em>{k}\right)+\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm T} \mathbf{W}<em>{k}\left(\overline{\mathbf{x}}</em>{k}-\mathbf{m}<em>{k}\right)\right} \
&amp;-\frac{1}{2} \sum</em>{k=1}^{K} \nu_{k}\left{\operatorname{Tr}\left(\mathbf{W}<em>{0}^{-1} \mathbf{W}</em>{k}\right)+\beta_{0}\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm T} \mathbf{W}<em>{k}\left(\mathbf{m}</em>{k}-\mathbf{m}<em>{0}\right)\right} \
&amp;-\sum</em>{k=1}^{K}\left{\ln B\left(\mathbf{W}<em>{k}, \nu</em>{k}\right)-\frac{\nu_{k} D}{2}\right} \
&amp;-\sum_{n=1}^{N} \sum_{k=1}^{K} r_{n k} \ln r_{n k}-\ln C(\boldsymbol{\alpha}) \
&amp;-\frac{1}{2} N D \ln (2 \pi)+\ln C\left(\boldsymbol{\alpha}<em>{0}\right)+\frac{1}{2} K D \ln \left(\frac{B</em>{0}}{2 \pi}\right)+K \ln B\left(\mathbf{W}<em>{0}, \nu</em>{0}\right)+\frac{D K}{2}
\end{aligned}
$$</p>
<p>$\mathcal{L}$の停留条件からパラメータの更新式を得る。パラメータはEステップで決めるパラメータ$r_{nk}$とMステップで決める$\alpha_k, \beta_k, \mathbf{m}_k, \mathbf{W}<em>k,\nu</em>{k}$。</p>
<p>$\alpha_k$について、$\mathcal{L}$の$\alpha_k$についての停留条件から更新式$(10.58)$を得ることを示す。</p>
<p>$$
\begin{aligned}
\frac{\partial \alpha}{\partial \alpha_{k}}
&amp;=\frac{\partial}{\partial \alpha_{k}}\left{\sum_{k=1}^{K} \ln \tilde{\pi}<em>{k}\left(N</em>{k}+\alpha_{0}-\alpha_{t}\right)-\ln C(\boldsymbol{\alpha})\right} \
&amp;=\frac{\partial}{\partial \alpha_{k}}\left{\sum_{k=1}^{K}(\underbrace{\psi\left(\alpha_{k}\right)-\psi(\hat{\alpha})}<em>{(10.66)})\left(N</em>{k}+\alpha_{0}-\alpha_{k}\right)-\underbrace{\ln \Gamma(\hat{\alpha})+\sum_{k=1}^{K} \ln \Gamma\left(\alpha_{k}\right)}_{(B .23)}\right}
\end{aligned}
$$</p>
<p>$(B.24)$にあるように、$\displaystyle \widehat{\alpha} = \sum_{k=1}^{K}\alpha_k$である。$\displaystyle \frac{\partial \mathcal{L}}{\partial \alpha_k}=0$のとき</p>
<p>$$
\begin{aligned}
0 =&amp;\ \left{\frac{\partial}{\partial \alpha_{k}} \psi\left(\alpha_{k}\right)\left(N_{k}+\alpha_{0}-\alpha_{k}\right)-\psi\left(\alpha_{k}\right)-\frac{\partial \hat{\alpha}}{\partial \alpha_{k}} \frac{\partial \psi(\hat{\alpha})}{\partial \hat{\alpha}}\left(N_{k}+\alpha_{0}-\alpha_{k}\right)+\psi(\hat{\alpha})\right} \
&amp;-\frac{\partial \hat{\alpha}}{\partial \alpha_{k}} \frac{\partial}{\partial \hat{\alpha}} \ln \Gamma(\hat{\alpha})+\frac{\partial}{\partial \alpha_{k}} \ln \Gamma\left(\alpha_{k}\right) \
=&amp;\ \frac{\partial \psi\left(\alpha_{k}\right)}{\partial \alpha_{k}}\left(N_{k}+\alpha_{0}-\alpha_{k}\right)-\frac{\partial \psi(\hat{\alpha})}{\partial \hat{\alpha}}\left(N_{k}+\alpha_{0}-\alpha_{k}\right)-\psi\left(\alpha_{k}\right)+\psi(\hat{\alpha}) \
&amp;-\psi(\hat{\alpha})+\psi\left(\alpha_{k}\right) \
=&amp;\ \left(N_{k}+\alpha_{0}-\alpha_{k}\right)\left(\frac{\partial \psi\left(\alpha_{k}\right)}{\partial \alpha_{k}}-\frac{\partial \psi(\hat{\alpha})}{\partial \hat{\alpha}}\right)
\end{aligned}
$$</p>
<p>よって停留条件は$N_k+\alpha_0-\alpha_k = 0$、すなわち</p>
<p>$$
\alpha_k = \alpha_0 + N_k \tag{10.58}
$$</p>
<p>である。</p>
<hr />
<p>$\beta_{k}$について停留条件を求める。</p>
<p>$$
\begin{aligned} \frac{\partial \mathcal{L}}{\partial \beta_{k}}
&amp;=-\frac{D}{2} \frac{\partial}{\partial \beta_{k}}\left{\beta_{k}^{-1}\left(N_{k}+\beta_{0}\right)+\ln \beta_{k}-\ln (2 \pi)\right} \
&amp;=-\frac{D}{2}\left(-\frac{N_{k}+\beta_{0}}{\beta_{k}^{2}}+\frac{1}{\beta_{k}}\right) \ &amp;=\frac{D}{2 \beta_{k}^{2}}\left(N_{k}+\beta_{0}-\beta_{k}\right)=0
\end{aligned}
$$</p>
<p>以上から</p>
<p>$$
\beta_{k} =\beta_{0} +N_{k} \tag{10.60}
$$</p>
<p>のとき停留する。</p>
<hr />
<p>$\mathbf{m}_{k}$について停留条件を求める。</p>
<p>$$
\begin{aligned} \frac{\partial \mathcal{L}}{\partial \mathbf{m}<em>{k}}
=&amp;\ \frac{\partial}{\partial \mathbf{m}</em>{k}}\left{-\frac{1}{2} N_{k} \nu_{k}\left(\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm T} \mathbf{W}<em>{k}\left(\overline{\mathbf{x}}</em>{k}-\mathbf{m}<em>{k}\right)\right)\right.\ &amp;\left.-\frac{1}{2} \nu</em>{k} \beta_{0}\left(\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm T} \mathbf{W}<em>{k}\left(\mathbf{m}</em>{k}-\mathbf{m}<em>{0}\right)\right)\right} \
=&amp;\ N</em>{k} \nu_{k} \mathbf{W}<em>{k}\left(\overline{\mathbf{x}}</em>{k}-\mathbf{m}<em>{k}\right)-\nu</em>{k} \beta_{0} \mathbf{W}<em>{k}\left(\mathbf{m}</em>{k}-\mathbf{m}<em>{0}\right) \
=&amp;\ \nu</em>{k} \mathbf{W}<em>{k}\left{N</em>{k} \overline{\mathbf{x}}<em>{k}+\beta</em>{0} \mathbf{m}<em>{0}-\left(N</em>{k}+\beta_{0}\right) \mathbf{m}_{k}\right}=0
\end{aligned}
$$</p>
<p>以上から</p>
<p>$$
\mathbf{m}<em>{k}=\frac{N</em>{k} \overline{\mathbf{x}}<em>{k}+\beta</em>{0} \mathbf{m}<em>{0}}{N</em>{k}+\beta_{0}}=\frac{N_{k} \overline{\mathbf{x}}<em>{k}+\beta</em>{0} \mathbf{m}<em>{0}}{\beta</em>{k}} \tag{10.61}
$$</p>
<p>のとき停留する。</p>
<hr />
<p>$\nu_k$について、</p>
<p>$$
\begin{aligned}
\mathcal{L}=&amp;\ \frac{1}{2} \sum_{k=1}^{K}\left{\sum_{i=1}^{D}\psi\left(\frac{\nu_{k}+1-i}{2}\right)+D \ln 2+\ln \left|\mathbf{W}<em>{k}\right|\right}\left(N</em>{k}+\nu_{0}-\nu_{k}\right) \
&amp;-\frac{1}{2} \sum_{k=1}^{K} N_{k} \nu_{k}\left{\operatorname{Tr}\left(\mathbf{S}<em>{k} \mathbf{W}</em>{k}\right)+\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm{T}} \mathbf{W}<em>{k}\left(\overline{\mathbf{x}}</em>{k}-\mathbf{m}<em>{k}\right)\right} \
&amp;-\frac{1}{2} \sum</em>{k=1}^{K} \nu_{k}\left{\operatorname{Tr}\left(\mathbf{W}<em>{0}^{-1} \mathbf{W}</em>{k}\right)+\beta_{0}\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm{T}} \mathbf{W}<em>{k}\left(\mathbf{m}</em>{k}-\mathbf{m}<em>{0}\right)\right} \
&amp;-\sum</em>{k=1}^{K} \left{ \ln B\left(\mathbf{W}<em>{k}, \nu</em>{k}\right) - \frac{\nu_{k}D}{2} \right}+\text {const.} \
=&amp;\ \frac{1}{2} \sum_{k=1}^{K}\left{\sum_{i=1}^{D}\psi\left(\frac{\nu_{k}+1-i}{2}\right)+D \ln 2+\ln \left|\mathbf{W}<em>{k}\right|\right}\left(N</em>{k}+\nu_{0}-\nu_{k}\right) \
&amp;-\frac{1}{2} \sum_{k=1}^{K} N_{k} \nu_{k}\left{\operatorname{Tr}\left(\mathbf{S}<em>{k} \mathbf{W}</em>{k}\right)+\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm{T}} \mathbf{W}<em>{k}\left(\overline{\mathbf{x}}</em>{k}-\mathbf{m}<em>{k}\right)\right} \
&amp;-\frac{1}{2} \sum</em>{k=1}^{K} \nu_{k}\left{\operatorname{Tr}\left(\mathbf{W}<em>{0}^{-1} \mathbf{W}</em>{k}\right)+\beta_{0}\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm{T}} \mathbf{W}<em>{k}\left(\mathbf{m}</em>{k}-\mathbf{m}<em>{0}\right)\right} \
&amp;-\sum</em>{k=1}^{K} \left{ \ln |\mathbf{W}<em>{k}|^{-\nu</em>{k} / 2} - \ln \left(2^{\frac{\nu_{k} D}{2}} \pi^{D(D-1) / 4} \prod_{i=1}^{D} \Gamma\left(\frac{\nu_{k}+1-i}{2}\right)\right) - \frac{\nu_{k}D}{2}\right}+\text {const.}
\end{aligned}
$$</p>
<p>停留条件は</p>
<p>$$
\begin{aligned} \frac{\partial \mathcal{L}}{\partial \nu_{k}}=&amp;\ \frac{1}{2}\left{\sum_{i=1}^{D} \frac{\partial}{\partial \nu_{k}} \psi\left(\frac{\nu_{k}+1-i}{2}\right)\right}\left(N_{k}+\nu_{0}-\nu_{k}\right) \ &amp;-\frac{1}{2}\left{\sum_{i=1}^{D} \psi\left(\frac{\nu_{k}+1-i}{2}\right)+D \ln 2+\ln \left|\mathbf{W}<em>{k}\right|\right} \
&amp;-\frac{1}{2} N</em>{k}\left(\operatorname{Tr}\left(\mathbf{S}<em>{k} \mathbf{W}</em>{k}\right)+\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm{T}} \mathbf{W}<em>{k}\left(\overline{\mathbf{x}}</em>{k}-\mathbf{m}<em>{k}\right)\right) \
&amp;-\frac{1}{2}\left{\operatorname{Tr}\left(\mathbf{W}</em>{0}^{-1} \mathbf{W}<em>{k}\right)+\beta</em>{0}\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm{T}} \mathbf{W}<em>{k}\left(\mathbf{m}</em>{k}-\mathbf{m}<em>{0}\right)\right} \
&amp;-\left{-\frac{1}{2} \ln \left|\mathbf{W}</em>{k}\right|-\frac{D}{2} \ln 2-\sum_{i=1}^{D} \underbrace{\frac{1}{2} \psi\left(\frac{\nu_{k}+1-i}{2}\right)}<em>{(B .25)}\right}+\frac{D}{2} \
=&amp;\ \frac{1}{2} \sum</em>{i=1}^{D} \frac{\partial}{\partial \nu_{k}} \psi\left(\frac{\nu_{k}+1-i}{2}\right)\left(N_{k}+\nu_{0}-\nu_{k}\right) \
&amp;-\frac{1}{2} N_{k}\left(\operatorname{Tr}\left(\mathbf{S}<em>{k} \mathbf{W}</em>{k}\right)+\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm{T}} \mathbf{W}<em>{k}\left(\overline{\mathbf{x}}</em>{k}-\mathbf{m}<em>{k}\right)\right) \
&amp;-\frac{1}{2}\left{\operatorname{Tr}\left(\mathbf{W}</em>{0}^{-1} \mathbf{W}<em>{k}\right)+\beta</em>{0}\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm{T}} \mathbf{W}<em>{k}\left(\mathbf{m}</em>{k}-\mathbf{m}_{0}\right)\right}+\frac{D}{2}
\end{aligned}
$$</p>
<p>$\displaystyle \frac{\partial \mathcal{L}}{\partial \nu_{k}} = 0$は</p>
<p>$$
\begin{aligned}
&amp; \sum_{i=1}^{D} \frac{\partial}{\partial \nu_{k}} \psi\left(\frac{\nu_{k}+1-i}{2}\right)\left(N_{k}+\nu_{0}-\nu_{k}\right) \
&amp;-\operatorname{Tr}\left{\left(N_{k} \mathbf{S}<em>{k}+N</em>{k}\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm{T}}+\mathbf{W}<em>{0}^{-1}+\beta</em>{0}\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm{T}}\right) \mathbf{W}_{k}\right} \
&amp;+D=0\end{aligned}
$$</p>
<p>のときに成立する。よってこれを簡単にしていく。$\operatorname{Tr}()$の中について</p>
<p>$$
\begin{aligned}
&amp; N_{k} \mathbf{S}<em>{k}+N</em>{k}\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm{T}}+\mathbf{W}<em>{0}^{-1}+\beta</em>{0}\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm{T}} \
=&amp;\ N_{k} \mathbf{S}<em>{k}+N</em>{k} \overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm{T}}-N_{k} \overline{\mathbf{x}}<em>{k} \mathbf{m}</em>{k}^{\mathrm{T}}-N_{k} \mathbf{m}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm{T}}+N_{k} \mathbf{m}<em>{k} \mathbf{m}</em>{k}^{\mathrm{T}}+w_{0}^{-1}+\beta_{0} \mathbf{m}<em>{k} \mathbf{m}</em>{k}^{\mathrm{T}}-\beta_{0} \mathbf{m}<em>{k} \mathbf{m}</em>{0}^{\mathrm{T}}-\beta_{0} \mathbf{m}<em>{0} \mathbf{m}</em>{k}^{\mathrm{T}}+\beta_{0} \mathbf{m}<em>{0} \mathbf{m}</em>{0}^{\mathrm{T}} \
=&amp;\ \mathbf{W}<em>{0}^{-1}+N</em>{k} \mathbf{S}<em>{k}+N</em>{k} \overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm{T}}-\left(N_{k} \overline{\mathbf{x}}<em>{k}+\beta</em>{0} \mathbf{m}<em>{0}\right) \mathbf{m}</em>{k}^{\mathrm{T}}-\mathbf{m}<em>{k}\left(N</em>{k} \overline{\mathbf{x}}<em>{k}+\beta</em>{0} \mathbf{m}<em>{0}\right)^{\mathrm{T}}+\left(N</em>{k}+\beta_{0}\right) \mathbf{m}<em>{k} \mathbf{m}</em>{k}^{\mathrm{T}}+\beta_{0} \mathbf{m}<em>{0} \mathbf{m}</em>{0}^{\mathrm{T}}\
=&amp;\ \mathbf{W}<em>{0}^{-1}+N</em>{k} \mathbf{S}<em>{k}+N</em>{k} \overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm{T}}-\frac{1}{N_{k}+\beta_{0}}\left(N_{k} \overline{\mathbf{x}}<em>{k}+\beta</em>{0} \mathbf{m}<em>{0}\right)\left(N</em>{k} \overline{\mathbf{x}}<em>{k}+\beta</em>{0} \mathbf{m}<em>{0}\right)^{\mathrm T}+\beta</em>{0} \mathbf{m}<em>{0} \mathbf{m}</em>{0}^{\mathrm{T}} \left(\because \mathbf{m}<em>{k}=\frac{1}{N</em>{k}+\beta_{0}}\left(N_{k} \overline{\mathbf{x}}<em>{k}+\beta</em>{0} \mathbf{m}<em>{0}\right) \right) \
=&amp;\ \mathbf{W}</em>{0}^{-1}+N_{k} \mathbf{S}<em>{k}+\frac{1}{N</em>{k}+\beta_{0}}\left{\left(N_{k}+\beta_{0}\right) N_{k} \overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm{T}}-N_{k}^{2} \overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm{T}}-N_{k} \beta_{0} \overline{\mathbf{x}}<em>{k} \mathbf{m}</em>{0}^{\mathrm{T}}-\beta_{0} N_{k} \mathbf{m}<em>{0} \overline{\mathbf{x}}</em>{k}^{\mathrm{T}}-\beta_{0}^{2} \mathbf{m}<em>{0} \mathbf{m}</em>{0}^{\mathrm{T}}+\left(N_{k}+\beta_{0}\right) \beta_{0} \mathbf{m}<em>{0} \mathbf{m}</em>{0}^{\mathrm{T}}\right} \
=&amp;\ \mathbf{W}<em>{0}^{-1}+N</em>{k} \mathbf{S}<em>{k}+\frac{N</em>{k} \beta_{0}}{N_{k}+\beta_{0}}\left(\overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm{T}}-\overline{\mathbf{x}}<em>{k} \mathbf{m}</em>{0}^{\mathrm{T}}-\mathbf{m}<em>{0} \overline{\mathbf{x}}</em>{k}^{\mathrm{T}}+\mathbf{m}<em>{0} \mathbf{m}</em>{0}^{\mathrm{T}}\right) \
=&amp;\ \mathbf{W}<em>{0}^{-1}+N</em>{k} \mathbf{S}<em>{k}+\frac{N</em>{k} \beta_{0}}{N_{k}+\beta_{0}}\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{0}\right)\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm{T}} \
=&amp;\ \frac{N_{k}+\nu_{0}}{\nu_{k}}\mathbf{W}_{k}^{-1}
\end{aligned}
$$</p>
<p>これより停留条件を書き直すと</p>
<p>$$
\begin{aligned}
\sum_{i=1}^{D} \frac{\partial}{\partial \nu_{k}} \psi\left(\frac{\nu_{k}+1-i}{2}\right)\left(N_{k}+\nu_{0}-\nu_{k}\right) - \operatorname{Tr}\left{ \frac{N_{k}+\nu_{0}}{\nu_{k}}\mathbf{W}<em>{k}^{-1}\mathbf{W}</em>{k} \right} + D = 0 \
\sum_{i=1}^{D} \frac{\partial}{\partial \nu_{k}} \psi\left(\frac{\nu_{k}+1-i}{2}\right)\left(N_{k}+\nu_{0}-\nu_{k}\right) - \frac{N_{k}+\nu_{0}}{\nu_{k}} D + D = 0 \
\sum_{i=1}^{D} \frac{\partial}{\partial \nu_{k}} \psi\left(\frac{\nu_{k}+1-i}{2}\right)\left(N_{k}+\nu_{0}-\nu_{k}\right) - \frac{D}{\nu_{k}}\left( N_{k} + \nu_{0} - \nu_{k} \right) = 0 \
\end{aligned}
$$</p>
<p>以上から</p>
<p>$$
\nu_{k} = \nu_{0} + N_{k} \tag{10.63}
$$</p>
<p>のとき停留する。</p>
<hr />
<p>$\mathbf{W}_k$について、</p>
<p>$$
\ln \widetilde{\Lambda}<em>{k} \equiv \mathbb{E}\left[\ln \left|\mathbf{\Lambda}</em>{k}\right|\right]=\sum_{i=1}^{D} \psi\left(\frac{\nu_{k}+1-i}{2}\right)+D \ln 2+\ln \left|\mathbf{W}_{k}\right| \tag{10.65}
$$</p>
<p>を用いて計算する</p>
<p>$$
\begin{aligned}
\mathcal{L}=&amp;\ \frac{1}{2} \sum_{k=1}^{K}\left{\sum_{i=1}^{D}\psi\left(\frac{\nu_{k}+1-i}{2}\right)+D \ln 2+\ln \left|\mathbf{W}<em>{k}\right|\right}\left(N</em>{k}+\nu_{0}-\nu_{k}\right) \
&amp;-\frac{1}{2} \sum_{k=1}^{K} N_{k} \nu_{k}\left{\operatorname{Tr}\left(\mathbf{S}<em>{k} \mathbf{W}</em>{k}\right)+\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm{T}} \mathbf{W}<em>{k}\left(\overline{\mathbf{x}}</em>{k}-\mathbf{m}<em>{k}\right)\right} \
&amp;-\frac{1}{2} \sum</em>{k=1}^{K} \nu_{k}\left{\operatorname{Tr}\left(\mathbf{W}<em>{0}^{-1} \mathbf{W}</em>{k}\right)+\beta_{0}\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm{T}} \mathbf{W}<em>{k}\left(\mathbf{m}</em>{k}-\mathbf{m}<em>{0}\right)\right} \
&amp;-\sum</em>{k=1}^{K} \ln B\left(\mathbf{W}<em>{k}, \nu</em>{k}\right)+\text {const.}
\end{aligned}
$$</p>
<p>停留条件は、$\mathbf{S}<em>{k}^{\mathrm T} = \mathbf{S}</em>{k}, \mathbf{W}<em>{k}^{\mathrm T} = \mathbf{W}</em>{k}$である（対称行列）ことに注意して</p>
<p>$$
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial \mathbf{W}<em>{k}}
=&amp;\ \frac{1}{2} \underbrace{\mathbf{W}</em>{k}^{-1}}<em>{(C.28)} \left(N</em>{k}+\nu_{0}-\nu_{k}\right) \
&amp;-\frac{1}{2} N_{k} \nu_{k}\left{\mathbf{S}<em>{k}+\left(\overline{\mathbf{x}}</em>{k}-\mathbf{m}<em>{k}\right)\left(\overline{\mathbf{x}}</em>{k}-\mathbf{m}<em>{k}\right)^{\mathrm{T}}\right} \
&amp;-\frac{1}{2} \nu</em>{k}\left{\mathbf{W}<em>{0}^{-1}+\beta</em>{0}\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm{T}}\right} \
&amp;+\frac{\nu_{k}}{2}\mathbf{W}_{k}^{-1} \
=&amp;\ 0
\end{aligned}
$$</p>
<p>これより</p>
<p>$$
\begin{aligned}
&amp;\ \mathbf{W}<em>{k}^{-1}\left(N</em>{k}+\nu_{0}\right)-N_{k} \nu_{k}\left{\mathbf{S}<em>{k} +\left(\overline{\mathbf{x}}</em>{k}-\mathbf{m}<em>{k}\right)\left(\overline{\mathbf{x}}</em>{k}-\mathbf{m}<em>{k}\right)^{\mathrm T}\right} \
&amp;-\nu</em>{k}\left{w_{0}^{-1}+\beta_{0}\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm T}\right} = 0
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
\therefore \mathbf{W}<em>{k}^{-1}
&amp;=\frac{\nu</em>{k}}{N_{k}+\nu_{0}} \mathbf{W}<em>{0}^{-1}+\frac{N</em>{k} \nu_{k}}{N_{k}+\nu_{0}} \mathbf{S}<em>{k}+\frac{N</em>{k} \nu_{k}}{N_{k}+\nu_{0}}\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm T}+\frac{\nu_{k} \beta_{0}}{N_{k}+\nu_{0}}\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm T} \
&amp;=\frac{\nu_{k}}{N_{k}+\nu_{0}}\left{\mathbf{W}<em>{0}^{-1}+N</em>{k} \mathbf{S}<em>{k}+N</em>{k}\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm T}+\beta_{0}\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm T}\right} \
&amp;=\mathbf{W}<em>{0}^{-1}+N</em>{k} \mathbf{S}<em>{k}+N</em>{k}\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm T}+\beta_{0}\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm T}\ (\because \nu_{k}の停留条件 \nu_{k} = \nu_{0} + N_{k}) \
&amp;=\mathbf{W}<em>{0}^{-1}+N</em>{k} \mathbf{S}<em>{k}+\frac{N</em>{k}\beta_{0}}{\beta_{0}+N_{k}}\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{0}\right)\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm{T}}\hspace{1em}(10.62)
\end{aligned}
$$</p>
<p>となり、$\mathbf{W}_{k}^{-1}$の更新式を得た。ただし最後の変形は</p>
<p>$$
\begin{aligned}
&amp;\ N_{k}\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{k}\right)^{\mathrm{T}}+\beta_{0}\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)\left(\mathbf{m}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm{T}} \
=&amp;\ N_{k} \overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm{T}}-N_{k} \overline{\mathbf{x}}<em>{k} \frac{\left(\beta</em>{0} \mathbf{m}<em>{0}+N</em>{k} \overline{\mathbf{x}}<em>{k}\right)^{\mathrm{T}}}{\beta</em>{0}+N_{k}}-\frac{\beta_{0} \mathbf{m}<em>{0}+N</em>{k} \overline{\mathbf{x}}<em>{k}}{\beta</em>{0}+N_{k}} N_{k} \overline{\mathbf{x}}<em>{k}^{\mathrm{T}} \
&amp; +\frac{N</em>{k}\left(\beta_{0} \mathbf{m}<em>{0}+N</em>{k} \overline{\mathbf{x}}<em>{k}\right)\left(\beta</em>{0} \mathbf{m}<em>{0}+N</em>{k} \overline{\mathbf{x}}<em>{k}\right)^{\mathrm{T}}}{\left(\beta</em>{0}+N_{k}\right)^{2}}+\frac{\beta_{0}\left(\beta_{0} \mathbf{m}<em>{0}+N</em>{k} \overline{\mathbf{x}}<em>{k}\right)\left(\beta</em>{0} \mathbf{m}<em>{0}+N</em>{k} \overline{\mathbf{x}}<em>{k}\right)^{\mathrm{T}}}{\left(\beta</em>{0}+N_{k}\right)^{2}} \
&amp; -\beta_{0} \mathbf{m}<em>{0} \frac{\left(\beta</em>{0} \mathbf{m}<em>{0}+N</em>{k} \overline{\mathbf{x}}<em>{k}\right)^{\mathrm{T}}}{\beta</em>{0}+N_{k}}-\frac{\beta_{0} \mathbf{m}<em>{0}+N</em>{k} \overline{\mathbf{x}}<em>{k}}{\beta</em>{0}+N_{k}} \beta_{0}\mathbf{m}<em>{0}^{\mathrm{T}}+\beta</em>{0} \mathbf{m}<em>{0} \mathbf{m}</em>{0}^{\mathrm{T}} \
=&amp; \left( N_{k}-\frac{N_{k}^{2}}{\beta_{0}+N_{k}}-\frac{N_{k}^{2}}{\beta_{0}+N_{k}}+\frac{N_{k}^{3}}{\left(\beta_{0}+N_{k}\right)^{2}}+\frac{\beta_{0} N_{k}^{2}}{\left(\beta_{0}+N_{k}\right)^{2}} \right)\overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm{T}} \
&amp; +\left( -\frac{N_{k} \beta_{0}}{\beta_{0}+N_{k}}+\frac{\beta_{0} N_{k}^{2}}{\left(\beta_{0}+N_{k}\right)^{2}}+\frac{\beta_{0}^{2} N_{k}}{\left(\beta_{0}+N_{k}\right)^{2}}-\frac{N_{k} \beta_{0}}{\beta_{0}+N_{k}} \right)\overline{\mathbf{x}}<em>{k}\mathbf{m}</em>{0}^{\mathrm{T}} \
&amp; +\left( -\frac{N_{k} \beta_{0}}{\beta_{0}+N_{k}}+\frac{\beta_{0} N_{k}^{2}}{\left(\beta_{0}+N_{k}\right)^{2}}+\frac{\beta_{0}^{2} N_{k}}{\left(\beta_{0}+N_{k}\right)^{2}}-\frac{N_{k} \beta_{0}}{\beta_{0}+N_{k}} \right)\mathbf{m}<em>{0}\overline{\mathbf{x}}</em>{k}^{\mathrm{T}} \
&amp; +\left( \frac{N_{k} \beta_{0}^{2}}{\left(\beta_{0}+N_{k}\right)^{2}}+\frac{\beta_{0}^{3}}{\left(\beta_{0}+N_{k}\right)^{2}}-\frac{2 \beta_{0}^{2}}{\beta_{0}+N_{k}}+\beta_{0} \right)\mathbf{m}<em>{0} \mathbf{m}</em>{0}^{\mathrm{T}} \
=&amp;\ \frac{N_{k}\beta_{0}}{\beta_{0}+N_{k}}\overline{\mathbf{x}}<em>{k} \overline{\mathbf{x}}</em>{k}^{\mathrm{T}} - \frac{N_{k}\beta_{0}}{\beta_{0}+N_{k}}\overline{\mathbf{x}}<em>{k}\mathbf{m}</em>{0}^{\mathrm{T}} - \frac{N_{k}\beta_{0}}{\beta_{0}+N_{k}}\mathbf{m}<em>{0}\overline{\mathbf{x}}</em>{k}^{\mathrm{T}} + \frac{N_{k}\beta_{0}}{\beta_{0}+N_{k}}\mathbf{m}<em>{0} \mathbf{m}</em>{0}^{\mathrm{T}} \
=&amp;\ \frac{N_{k}\beta_{0}}{\beta_{0}+N_{k}}\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{0}\right)\left(\overline{\mathbf{x}}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm{T}}
\end{aligned}
$$</p>
<p>を用いた。</p>
<h2 id="演習-1019"><a class="header" href="#演習-1019">演習 10.19</a></h2>
<div class="panel-primary">
<p>ベイズ混合ガウスモデルの変分ベイズ法における予測分布
$$p(\widehat{\mathrm{x}} \mid \mathbf{X}) \simeq \frac{1}{\widehat{\alpha}} \sum_{k=1}^{K} \alpha_{k} \operatorname{St}\left(\widehat{\mathbf{x}} \mid \mathbf{m}<em>{k}, \mathbf{L}</em>{k}, \nu_{k}+1-D\right) \tag{10.81}$$
を導出せよ．</p>
</div>
<p>P.197の$(10.81)$式の導出を行うためにこの節での手順を一から踏む。</p>
<p>$$
p(\mathbf{Z} \mid \pi)=\prod_{n=1}^{N} \prod_{k=1}^{K} \pi_{k}^{z_{n k}} \tag{10.37}
$$
$$
p(\mathbf{X} \mid \mathbf{Z}, \boldsymbol{\mu}, \boldsymbol{\Lambda})=\prod_{n=1}^{N} \prod_{k=1}^{K} \mathcal{N}\left(\mathbf{x}<em>{n} \mid \boldsymbol{\mu}</em>{k}, \boldsymbol{\Lambda}<em>{k}^{-1}\right)^{z</em>{n k}} \tag{10.38}
$$</p>
<p>データセット$\mathbf{X}$に対する新しい観測値$\mathbf{\hat{x}}$の予測分布についてこれに対応する潜在分布$\mathbf{\hat{z}}$が存在し、よって予測分布はしたがって以下で与えられる（純粋なベイズの定理・周辺化を用いた）。</p>
<p>$$
p(\widehat{\mathbf{x}} \mid \mathbf{X})=\sum_{\widehat{\mathbf{z}}} \iiint p(\widehat{\mathbf{x}} \mid \widehat{\mathbf{z}}, \boldsymbol{\mu}, \boldsymbol{\Lambda}) p(\widehat{\mathbf{z}} \mid \pi) p(\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Lambda} \mid \mathbf{X}) \mathrm{d} \boldsymbol{\pi} \mathrm{d} \boldsymbol{\mu} \mathrm{d} \boldsymbol{\Lambda} \tag{10.78}
$$</p>
<p>$(10.37)$と$(10.38)$を代入して、</p>
<p>$$
p(\widehat{\mathbf{x}} \mid \mathbf{X})=\sum_{k=1}^{K} \iiint \pi_{k} \mathcal{N}\left(\widehat{\mathbf{x}} \mid \boldsymbol{\mu}<em>{k}, \boldsymbol{\Lambda}</em>{k}^{-1}\right) p(\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Lambda} \mid \mathbf{X}) \mathrm{d} \boldsymbol{\pi} \mathrm{d} \boldsymbol{\mu} \mathrm{d} \boldsymbol{\Lambda} \tag{10.79}
$$</p>
<p>となる。真の事後分布$p(\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Lambda} \mid \mathbf{X})$を変分近似で置き換える。このとき</p>
<p>$$
q(\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Lambda})=q(\boldsymbol{\pi}) \prod_{j=1}^{K} q\left(\boldsymbol{\mu}<em>{j}, \boldsymbol{\Lambda}</em>{j}\right) \tag{10.55}
$$</p>
<p>を用いて、和$\displaystyle \sum_{k=1}^{K}$のうち1つの項に注目する（＝$k$を固定する）。$j\neq k$であるような$j$についての$\displaystyle \int d\mu_{j}\int d\Lambda_{j}$を考えると、積分の中身は$\displaystyle \int q(\boldsymbol{\mu}<em>{j}, \boldsymbol{\Lambda}</em>{j})d\boldsymbol{\mu}<em>{j}d\boldsymbol{\Lambda}</em>{j} = 1$（確率の定義より）となるので、$k$番目の積分$\displaystyle \int d\mu_{k}\int d\Lambda_{k}$しか残らない。これより</p>
<p>$$
\begin{aligned}
p(\widehat{\mathbf{x}} \mid \mathbf{X}) &amp;=\sum_{k=1}^{K} \iiint \pi_{k} \mathcal{N}\left(\widehat{\mathbf{x}} \mid \boldsymbol{\mu}<em>{k}, \boldsymbol{\Lambda}</em>{k}^{-1}\right) p(\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Lambda} \mid \mathbf{X}) \mathrm{d} \boldsymbol{\pi} \mathrm{d} \boldsymbol{\mu} \mathrm{d} \boldsymbol{\Lambda} \
&amp;\simeq \sum_{k=1}^{K} \iiint \pi_{k} \mathcal{N}\left(\widehat{\mathbf{x}} \mid \boldsymbol{\mu}<em>{k}, \boldsymbol{\Lambda}</em>{k}^{-1}\right) q(\boldsymbol{\pi}) q\left(\boldsymbol{\mu}<em>{k}, \boldsymbol{\Lambda}</em>{k}\right) \mathrm{d} \boldsymbol{\pi} \mathrm{d} \boldsymbol{\mu}<em>{k} \mathrm{~d} \boldsymbol{\Lambda}</em>{k} \
&amp;=\sum_{k=1}^{K} \iiint \pi_{k} \mathcal{N}\left(\widehat{\mathbf{x}} \mid \boldsymbol{\mu}<em>{k}, \boldsymbol{\Lambda}</em>{k}^{-1}\right) \underbrace{\operatorname{Dir}(\boldsymbol{\pi}\mid \boldsymbol{\alpha})}<em>{(10.57)} \underbrace{\mathcal{N}\left(\boldsymbol{\mu}</em>{k} \mid \mathbf{m}<em>{k},\left(\beta</em>{k} \boldsymbol{\Lambda}<em>{k}\right)^{-1}\right) \mathcal{W}\left(\boldsymbol{\Lambda}</em>{k} \mid \mathbf{W}<em>{k}, \boldsymbol{\nu}</em>{k}\right)}<em>{(10.59)} \mathrm{d} \boldsymbol{\pi} \mathrm{d} \boldsymbol{\mu}</em>{k} \mathrm{d} \boldsymbol{\Lambda}<em>{k} \
&amp;=\sum</em>{k=1}^{K} \int\pi_{k} \operatorname{Dir}(\boldsymbol{\pi}\mid \boldsymbol{\alpha}) \mathrm{d} \boldsymbol{\pi} \int \left[ \int \mathcal{N}\left(\widehat{\mathbf{x}} \mid \boldsymbol{\mu}<em>{k}, \boldsymbol{\Lambda}</em>{k}^{-1}\right) \mathcal{N}\left(\boldsymbol{\mu}<em>{k} \mid \mathbf{m}</em>{k},\left(\beta_{k} \boldsymbol{\Lambda}<em>{k}\right)^{-1}\right) \mathrm{d} \boldsymbol{\mu}</em>{k} \right]\mathcal{W}\left(\boldsymbol{\Lambda}<em>{k} \mid \mathbf{W}</em>{k}, \boldsymbol{\nu}<em>{k}\right) \mathrm{d} \boldsymbol{\Lambda}</em>{k} \cdots (\textrm{A})\
\end{aligned}
$$</p>
<p>$\boldsymbol{\pi}$の積分に関係するのは$\pi_{k}\operatorname{Dir}(\boldsymbol{\pi}\mid \boldsymbol{\alpha})$のみで、$\int\pi_{k} \operatorname{Dir}(\boldsymbol{\pi}\mid \boldsymbol{\alpha}) \mathrm{d} \boldsymbol{\pi}$はディリクレ分布以下での$\pi_{k}$の期待値であるから</p>
<p>$$
\int\pi_{k} \operatorname{Dir}(\boldsymbol{\pi}\mid \boldsymbol{\alpha}) \mathrm{d} \boldsymbol{\pi} = \frac{\alpha_{k}}{\widehat{\alpha}}\ (\because{(B.17)})
$$</p>
<p>次に$\displaystyle \int \mathcal{N}\left(\widehat{\mathbf{x}} \mid \boldsymbol{\mu}<em>{k}, \boldsymbol{\Lambda}</em>{k}^{-1}\right) \mathcal{N}\left(\boldsymbol{\mu}<em>{k} \mid \mathbf{m}</em>{k},\left(\beta_{k} \boldsymbol{\Lambda}<em>{k}\right)^{-1}\right) \mathrm{d} \boldsymbol{\mu}</em>{k}$について、これを</p>
<p>$$
\begin{aligned}
p\left(\boldsymbol{\mu}<em>{k}\right)&amp;=\mathcal{N}\left(\boldsymbol{\mu}</em>{k} \mid \mathbf{m}<em>{k},\left(\beta</em>{k} \boldsymbol{\Lambda}<em>{k}\right)^{-1}\right) \
p\left(\widehat{\mathbf{x}} \mid \boldsymbol{\mu}</em>{k}\right)&amp;=\mathcal{N}\left(\widehat{\mathbf{x}} \mid \boldsymbol{\mu}<em>{k}, \boldsymbol{\Lambda}</em>{k}^{-1}\right)
\end{aligned}
$$</p>
<p>とみなして$(2.115)$の公式を用いると</p>
<p>$$
\begin{aligned}
&amp; \int \mathcal{N}\left(\widehat{\mathbf{x}} \mid \boldsymbol{\mu}<em>{k}, \boldsymbol{\Lambda}</em>{k}^{-1}\right) \mathcal{N}\left(\boldsymbol{\mu}<em>{k} \mid \mathbf{m}</em>{k},\left(\beta_{k} \boldsymbol{\Lambda}<em>{k}\right)^{-1}\right) d \boldsymbol{\mu}</em>{k} \
=&amp;\ \mathcal{N}\left(\widehat{\mathbf{x}} \mid \mathbf{m}<em>{k},\left(\boldsymbol{\Lambda}</em>{k}^{-1}+\beta_{k}^{-1} \boldsymbol{\Lambda}<em>{k}^{-1}\right)\right) \
=&amp;\ \mathcal{N}\left(\widehat{\mathbf{x}} \mid \mathbf{m}</em>{k},\left(1+\beta_{k}^{-1}\right) \boldsymbol{\Lambda}_{k}^{-1}\right) \end{aligned}
$$</p>
<p>となる。以上から$(\textrm{A})$式に戻ると</p>
<p>$$
\begin{aligned} p(\widehat{\mathbf{x}} \mid \mathbf{X})\simeq &amp; \sum_{k=1}^{K} \frac{\alpha_{k}}{\widehat{\alpha}} \int \mathcal{N}\left(\widehat{\mathbf{x}} \mid \mathbf{m}<em>{k},\left(1+\beta</em>{k}^{-1}\right) \boldsymbol{\Lambda}<em>{k}^{-1}\right) \mathcal{W}\left(\boldsymbol{\Lambda}</em>{k} \mid \mathbf{W}<em>{k}, \nu</em>{k}\right) \mathrm{d} \boldsymbol{\Lambda}<em>{k} \
=&amp;\ \sum</em>{k=1}^{K} \frac{\alpha_{k}}{\widehat{\alpha}} \int \frac{1}{(2 \pi)^{D / 2}} \frac{\left|\boldsymbol{\Lambda}<em>{k}\right|^{1 / 2}}{\left(1+\beta</em>{k}^{-1}\right)^{D / 2}} \exp \left{-\frac{\left(\widehat{\mathbf{x}}-\mathbf{m}<em>{k}\right)^{\mathrm{T}} \boldsymbol{\Lambda}</em>{k}\left(\widehat{\mathbf{x}}-\mathbf{m}<em>{k}\right)}{2\left(1+\beta</em>{k}^{-1}\right)}\right} \ &amp; B\left(\mathbf{W}<em>{k}, \nu</em>{k}\right)\left|\boldsymbol{\Lambda}<em>{k}\right|^{\left(\nu</em>{k}-D-1\right) / 2} \exp \left{-\frac{1}{2} \operatorname{Tr}\left[\mathbf{W}<em>{k}^{-1} \boldsymbol{\Lambda}</em>{k}\right]\right} \mathrm{d} \boldsymbol{\Lambda}<em>{k} \
=&amp;\ \sum</em>{k=1}^{K} \frac{\alpha_{k}}{\widehat{\alpha}} \int \frac{B\left(\mathbf{W}<em>{k}, \nu</em>{k}\right)}{(2 \pi)^{D / 2}} \frac{\left|\boldsymbol{\Lambda}<em>{k}\right|^{\left((\nu</em>{k}+1)-D-1\right) / 2}}{\left(1+\beta_{k}^{-1}\right)^{D / 2}} \exp \left{-\frac{\left(\widehat{\mathbf{x}}-\mathbf{m}<em>{k}\right)^{\mathrm{T}} \boldsymbol{\Lambda}</em>{k}\left(\widehat{\mathbf{x}}-\mathbf{m}<em>{k}\right)}{2\left(1+\beta</em>{k}^{-1}\right)}\right}  \exp \left{-\frac{1}{2} \operatorname{Tr}\left[\mathbf{W}<em>{k}^{-1} \boldsymbol{\Lambda}</em>{k}\right]\right} \mathrm{d} \boldsymbol{\Lambda}<em>{k} \
=&amp;\ \sum</em>{k=1}^{K} \frac{\alpha_{k}}{\widehat{\alpha}} \int \frac{B\left(\mathbf{W}<em>{k}, \nu</em>{k}\right)}{(2 \pi)^{D / 2}} \frac{\left|\boldsymbol{\Lambda}<em>{k}\right|^{\left((\nu</em>{k}+1)-D-1\right) / 2}}{\left(1+\beta_{k}^{-1}\right)^{D / 2}} \exp \left{-\frac{1}{2} \operatorname{Tr}\left[\left(\frac{\left(\widehat{\mathbf{x}}-\mathbf{m}<em>{k}\right)\left(\widehat{\mathbf{x}}-\mathbf{m}</em>{k}\right)^{\mathrm{T}}}{1+\beta_{k}^{-1}}+\mathbf{W}<em>{k}^{-1}\right) \boldsymbol{\Lambda}</em>{k}\right]\right} \mathrm{d} \boldsymbol{\Lambda}<em>{k} \
=&amp;\ \sum</em>{k=1}^{K} \frac{\alpha_{k}}{\widehat{\alpha}} \frac{B\left(\mathbf{W}<em>{k}, \nu</em>{k}\right)}{(2 \pi)^{D / 2} \left(1+\beta_{k}^{-1}\right)^{D / 2}}
\int \left|\boldsymbol{\Lambda}<em>{k}\right|^{\left((\nu</em>{k}+1)-D-1\right) / 2} \exp \left{-\frac{1}{2} \operatorname{Tr}\left[\left(\frac{\left(\widehat{\mathbf{x}}-\mathbf{m}<em>{k}\right)\left(\widehat{\mathbf{x}}-\mathbf{m}</em>{k}\right)^{\mathrm{T}}}{1+\beta_{k}^{-1}}+\mathbf{W}<em>{k}^{-1}\right) \boldsymbol{\Lambda}</em>{k}\right]\right} \mathrm{d} \boldsymbol{\Lambda}_{k}
\end{aligned}
$$</p>
<p>ここで、$\int$の中身は</p>
<p>$$
\begin{aligned}
\mathbf{W^{\prime}}<em>{k}^{-1} &amp;=\left(1+\beta</em>{k}^{-1}\right)^{-1}\left(\widehat{\mathbf{x}}-\mathbf{m}<em>{k}\right)\left(\widehat{\mathbf{x}}-\mathbf{m}</em>{k}\right)^{\mathrm{T}} + \mathbf{W}<em>{k}^{-1} \
{\nu^{\prime}}</em>{k} &amp;= \nu_{k}+1
\end{aligned}
$$</p>
<p>としたときのウィシャート分布$\mathcal{W}(\boldsymbol{\Lambda}<em>{k}\mid \mathbf{W^{\prime}}</em>{k}, {\nu^{\prime}}<em>{k})$となっているので、この積分結果は正規化定数である$B(\mathbf{W^{\prime}}</em>{k}, {\nu^{\prime}}_{k})$の逆数になることがわかる。すなわち</p>
<p>$$
p(\widehat{\mathbf{x}} \mid \mathbf{X}) \simeq \sum_{k=1}^{K} \frac{\alpha_{k}}{\widehat{\alpha}} \frac{1}{(2 \pi)^{D / 2} \left(1+\beta_{k}^{-1}\right)^{D / 2}}\frac{B\left(\mathbf{W}<em>{k}, \nu</em>{k}\right)}{B(\mathbf{W^{\prime}}<em>{k}, {\nu^{\prime}}</em>{k})} \tag{B}
$$</p>
<p>となる。この正規化定数部分をさらに展開していく。</p>
<p>$$
\begin{aligned}
\frac{B(\mathbf{W}<em>k,\nu_k)}{B(\mathbf{W^{\prime}}</em>{k},\nu_k+1)}
&amp;=\frac{\left|\mathbf{W}<em>{k}\right|^{-\frac{\nu</em>{k}}{2}}\left(2^{\frac{\nu_{k} D}{2}} \pi^{\frac{D(D-1)}{4}} \prod_{i=1}^{D} \Gamma\left(\frac{\nu_{k}+1-i}{2}\right)\right)^{-1}}{\left|\mathbf{W}^{\prime}<em>{k}\right|^{-\frac{\nu</em>{k}+1}{2}}\left(2^{\frac{\left(\nu_{k}+1\right) D}{2}} \pi^{\frac{D(D-1)}{4}} \prod_{i=1}^{D} \Gamma\left(\frac{\nu_{k}+2-i}{2}\right)\right)^{-1}} ~~~(\because\ (B.79)) \
&amp;=\frac{\left|\mathbf{W}<em>{k}\right|^{-\frac{\nu</em>{k}}{2}}}{\left|\mathbf{W}^{\prime}<em>{k}\right|^{-\frac{\nu</em>{k}+1}{2}}} 2^{\frac{D}{2}} \frac{\prod_{i=1}^{D} \Gamma\left(\frac{\nu_{k}+2-i}{2}\right)}{\prod_{i=1}^{D} \Gamma\left(\frac{\nu_{k}+1-i}{2}\right)} \
&amp;=2^{D/2}\frac{\left|\mathbf{W}<em>{k}\right|^{-\frac{\nu</em>{k}}{2}}}{\left|\left{\mathbf{W}<em>{k}^{-1}+\left(1+\beta</em>{k}^{-1}\right)^{-1}\left(\widehat{\mathbf{x}}-\mathbf{m}<em>{k}\right)\left(\widehat{\mathbf{x}}-\mathbf{m}</em>{k}\right)^{\mathrm{T}}\right}^{-1}\right|^{-\frac{\nu_{k}}{2}}} \
&amp;~~~~\frac{\Gamma\left(\frac{\nu_{k}+1}{2}\right) \Gamma\left(\frac{\nu k}{2}\right) \Gamma\left(\frac{\nu_{k}-1}{2}\right) \cdots \Gamma\left(\frac{\nu_{k}+2-D}{2}\right)}{\Gamma\left(\frac{\nu k}{2}\right) \Gamma\left(\frac{\nu_{k}-1}{2}\right) \cdots \Gamma\left(\frac{\nu_{k}+2-D}{2}\right) \Gamma\left(\frac{\nu_{k}+1-D}{2}\right)} \
&amp;=2^{D/2}\left|\mathbf{W}<em>{k}\right|^{-\frac{\nu</em>{k}}{2}}\left|\mathbf{W}<em>{k}^{-1}\left{\mathbf{I}+\mathbf{W}</em>{k}\left(1+\beta_{k}^{-1}\right)^{-1}\left(\widehat{\mathbf{x}}-\mathbf{m}<em>{k}\right)\left(\widehat{\mathbf{x}}-\mathbf{m}</em>{k}\right)^{\mathrm{T}}\right}\right|^{-\frac{\nu_{k}+1}{2}}\frac{\Gamma\left(\frac{\nu_{k}+1}{2}\right)}{\Gamma\left(\frac{\nu_{k}+1-D}{2}\right)} \
&amp;=2^{D/2}\left|\mathbf{W}<em>{k}\right|^{1/2}\left|\mathbf{I}+\mathbf{W}</em>{k}\left(1+\beta_{k}^{-1}\right)^{-1}\left(\widehat{\mathbf{x}}-\mathbf{m}<em>{k}\right)\left(\widehat{\mathbf{x}}-\mathbf{m}</em>{k}\right)^{\mathrm{T}}\right|^{-\frac{\nu_{k}+1}{2}}\frac{\Gamma\left(\frac{\nu_{k}+1}{2}\right)}{\Gamma\left(\frac{\nu_{k}+1-D}{2}\right)} \
&amp;=2^{D/2}\left|\mathbf{W}<em>{k}\right|^{1/2}\left[1+\left{\mathbf{W}</em>{k}\left(1+\beta_{k}^{-1}\right)^{-1}\left(\widehat{\mathbf{x}}-\mathbf{m}<em>{k}\right)\right}^{\mathrm{T}}\left(\widehat{\mathbf{x}}-\mathbf{m}</em>{k}\right)\right]^{-\frac{\nu_{k}+1}{2}}\frac{\Gamma\left(\frac{\nu_{k}+1}{2}\right)}{\Gamma\left(\frac{\nu_{k}+1-D}{2}\right)} ~~ (\because (\textrm{C}.15))\
&amp;=2^{D/2}\left|\mathbf{W}<em>{k}\right|^{1/2}\left{1+\left(1+\beta</em>{k}^{-1}\right)^{-1}\left(\widehat{\mathbf{x}}-\mathbf{m}<em>{k}\right)^{\mathrm{T}} \mathbf{W}</em>{k}\left(\widehat{\mathbf{x}}-\mathbf{m}<em>{k}\right)\right}^{-\frac{\nu</em>{k}+1}{2}}\frac{\Gamma\left(\frac{\nu_{k}+1}{2}\right)}{\Gamma\left(\frac{\nu_{k}+1-D}{2}\right)}~~ (\because \mathbf{W}<em>{k}^{\mathrm T} = \mathbf{W}</em>{k})
\end{aligned}
$$</p>
<p>これを$(\textrm{B})$に代入して</p>
<p>$$
\begin{aligned}
p(\widehat{\mathbf{x}} \mid \mathbf{X}) &amp;\simeq \sum_{k=1}^{K} \frac{\alpha_{k}}{\widehat{\alpha}} \frac{\Gamma\left(\frac{\nu_{k}+1}{2}\right)}{\Gamma\left(\frac{\nu_{k}+1-D}{2}\right)}\frac{\left|\mathbf{W}<em>{k}\right|^{1/2}}{\pi^{D / 2} \left(1+\beta</em>{k}^{-1}\right)^{D / 2}}\left{1+\left(1+\beta_{k}^{-1}\right)^{-1}\left(\widehat{\mathbf{x}}-\mathbf{m}<em>{k}\right)^{\mathrm{T}} \mathbf{W}</em>{k}\left(\widehat{\mathbf{x}}-\mathbf{m}<em>{k}\right)\right}^{-\frac{\nu</em>{k}+1}{2}} \
&amp;= \sum_{k=1}^{K} \frac{\alpha_{k}}{\widehat{\alpha}} \frac{\Gamma\left(\frac{\nu_{k}+1-D}{2} + \frac{D}{2}\right)}{\Gamma\left(\frac{\nu_{k}+1-D}{2}\right)}
\frac{\left|\frac{\nu_{k}+1-D}{1+\beta_{k}^{-1}}\mathbf{W}<em>{k}\right|^{1/2}}{\pi^{D / 2} \left(\nu</em>{k}+1-D\right)^{D / 2}} \
&amp;~~~~\left{1+\left(\widehat{\mathbf{x}}-\mathbf{m}<em>{k}\right)^{\mathrm{T}} \left( \frac{1}{\nu</em>{k}+1-D}\frac{\nu_{k}+1-D}{1+\beta_{k}^{-1}}\mathbf{W}<em>{k} \right)\left(\widehat{\mathbf{x}}-\mathbf{m}</em>{k}\right)\right}^{-\frac{\nu_{k}+1-D}{2} - \frac{D}{2}} \
&amp;= \sum_{k=1}^{K} \frac{\alpha_{k}}{\widehat{\alpha}} \frac{\Gamma\left(\frac{\nu_{k}+1-D}{2} + \frac{D}{2}\right)}{\Gamma\left(\frac{\nu_{k}+1-D}{2}\right)} \frac{\left|\mathbf{L}<em>{k}\right|^{1/2}}{\left{\pi (\nu</em>{k}+1-D)\right}^{D/2}}\left( 1 + \frac{\Delta^{2}}{\nu_{k}+1-D}\right)^{-\frac{\nu_{k}+1-D}{2} - \frac{D}{2}} \
&amp;= \frac{1}{\widehat{\alpha}}\sum_{k=1}^{K}\alpha_{k}\operatorname{St} \left( \widehat{\mathbf{x}} \mid \mathbf{m}<em>{k}, \mathbf{L}</em>{k}, \nu_{k}+1-D \right) ~~ (\because (\textrm{B}.68))
\end{aligned}
$$</p>
<p>となる。ここで、
$$
\mathbf{L}<em>{k} =\frac{\nu</em>{k}+1-D}{1+\beta_{k}^{-1}} \mathbf{W}<em>{k} = \frac{(\nu</em>{k}+1-D)\beta_{k}}{(1+\beta_{k})} \mathbf{W}<em>{k} \tag{10.82}
$$
$$
\Delta^{2} =\left(\widehat{\mathbf{x}}-\mathbf{m}</em>{k}\right)^{\mathrm{T}} \mathbf{L}<em>{k}\left(\widehat{\mathbf{x}}-\mathbf{m}</em>{k}\right)
$$</p>
<p>とした。これより$(10.81)$を得た。</p>
<h2 id="演習-1020"><a class="header" href="#演習-1020">演習 10.20</a></h2>
<div class="panel-primary">
<p>この演習問題では，データ集合のサイズ$N$が大きくなった場合の混合ガウスモデルの変分ベイズ法による解を考え，これが(期待通り)9章のEMアルゴリズムに基づく最尤推定の解に近づくことを示す．この演習問題を解くには，付録Bの結果が有用であろう．最初に，精度の事後分布$q^{\star}(\mathbf{\Lambda}_k)$が最尤推定値の周囲に鋭い分布を持つことを示せ．平均の事後分布$q^{\star}(\boldsymbol{\mu}_k \mid \mathbf{\Lambda}_k)$についても同様のことを示せ．次に，混合比の事後分布$q^{\star}(\boldsymbol{\pi})$について考え，これも最尤推定値の周囲に鋭く分布することを示せ．同様に，大きな$N$については負担率は対応する最尤推定値と等しくなることを，大きな$x$についてのディガンマ関数の次の漸近的な結果</p>
<p>$$
\psi(x)=\ln x+O(1 / x)
$$</p>
<p>を利用して示せ．最後に</p>
<p>$$
p(\widehat{\mathbf{x}} \mid \mathbf{X}) \simeq \sum_{k=1}^{K} \iiint \pi_{k} \mathcal{N}\left(\widehat{\mathbf{x}} \mid \boldsymbol{\mu}<em>{k}, \mathbf{\Lambda}</em>{k}^{-1}\right) q(\boldsymbol{\pi}) q\left(\boldsymbol{\mu}<em>{k}, \mathbf{\Lambda}</em>{k}\right) \mathrm{d} \boldsymbol{\pi} \mathrm{d} \boldsymbol{\mu}<em>{k} \mathrm{~d} \mathbf{\Lambda}</em>{k} \tag{10.80}
$$</p>
<p>を用いて，大きな$N$については予測分布は混合ガウス分布になることを示せ．</p>
</div>
<p>(10.59)式の導出を考えると、(演習10.13より)
$$
q^{\star}\left(\mathbf{\Lambda}<em>{k}\right)=\mathcal{W}\left(\mathbf{\Lambda}</em>{k} \mid \mathbf{W}<em>{k}, \nu</em>{k}\right)\
q^{\star}\left(\boldsymbol{\mu}<em>{k} \mid \boldsymbol{\Lambda}</em>{k}\right)=\mathcal{N}\left(\boldsymbol{\mu}<em>{k} \mid \mathbf{m}</em>{k}, \beta_{k} \boldsymbol{\Lambda}<em>{k}\right)
$$
となる。
これらの分布について、$N \rightarrow \infty$のとき、
$N</em>{k} \rightarrow \infty$であり、
(10.60)~(10.63)式より、
$$
\beta_{k} \rightarrow N_{k}\
\mathbf{m}<em>{k} \rightarrow \overline{\mathrm{x}}</em>{k}\
\mathbf{W}<em>{k} \rightarrow N</em>{k}^{-1} \mathbf{S}<em>{k}^{-1}\
\nu</em>{k} \rightarrow N_{k}
$$
である。</p>
<p>これらと(B.79)~(B.81)式より,
$$
\mathrm{E}\left[\boldsymbol{\Lambda}<em>{k}\right]=\nu</em>{k} \mathbf{W}<em>{k} \rightarrow \mathbf{S}</em>{k}^{-1}
$$</p>
<p>$$
\begin{aligned}
-\ln B\left(\mathbf{W}<em>{k}, \nu</em>{k}\right)&amp;=-\ln (|\mathbf{W}<em>{k}|^{-\nu</em>{k} / 2}\left(2^{\nu_{k} D / 2} \pi^{D(D-1) / 4} \prod_{i=1}^{D} \Gamma\left(\frac{\nu_{k}+1-i}{2}\right)\right)^{-1})\
&amp;\rightarrow-\frac{N_{k}}{2}\left(D \ln N_{k}+\ln \left|\mathbf{S}<em>{k}\right|-D \ln 2\right)+\sum</em>{i=1}^{D} \ln \Gamma\left(\frac{N_{k}+1-i}{2}\right)\
&amp;\rightarrow-\frac{N_{k}}{2}\left(D \ln N_{k}+\ln \left|\mathbf{S}<em>{k}\right|-D \ln 2\right)+\sum</em>{i=1}^{D} \frac{N_{k}}{2}\left(\ln N_{k}-\ln 2-1\right)~~ (\because (\textrm1.146))\
&amp; \rightarrow-\frac{N_{k} D}{2}\left(\ln N_{k}-\ln 2-\ln N_{k}+\ln 2+1\right)-\frac{N_{k}}{2} \ln \left|\mathbf{S}<em>{k}\right| \
&amp;=-\frac{N</em>{k}}{2}\left(\ln \left|\mathbf{S}_{k}\right|+D\right)
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
\mathbb{E}[\ln |\boldsymbol{\Lambda}<em>{k}|] &amp;=\sum</em>{i=1}^{D} \psi\left(\frac{\nu_{k}+1-i}{2}\right)+D \ln 2+\ln |\mathbf{W}<em>{k}|\
&amp; \rightarrow D \ln \frac{N</em>{k}}{2}+D \ln 2-D \ln N_{k}-\ln \left|\mathbf{S}<em>{k}\right| \
&amp;=-\ln \left|\mathbf{S}</em>{k}\right|
\end{aligned}
$$
ただし、$\psi(\cdot)$は(10.241)式:
$$\psi(x)=\ln x+O(1 / x)$$
のディガンマ分布。</p>
<p>よって(B.82)式より
$$
\begin{aligned}
\mathrm{H}[\boldsymbol{\Lambda}<em>{k}]&amp;=-\ln B(\mathbf{W}</em>{k}, \nu_{k})-\frac{(\nu_{k}-D-1)}{2} \mathbb{E}[\ln |\boldsymbol{\Lambda}<em>{k}|]+\frac{\nu</em>{k} D}{2}\
&amp; \rightarrow 0
\end{aligned}
$$</p>
<p>これにより $q^{\star}\left(\boldsymbol{\Lambda}_{k}\right)$ については示された。</p>
<p>また、
$$
\mathbf{m}<em>{k} \rightarrow \overline{\mathrm{x}}</em>{k}\
\beta_{k} \mathbf{\Lambda}<em>{k} \rightarrow \beta</em>{k} \nu_{k} \mathbf{W}<em>{k} \rightarrow N</em>{k} \mathbf{S}<em>{k}^{-1}
$$
より $q^{\star}\left(\boldsymbol{\mu}</em>{k} \mid \boldsymbol{\Lambda}_{k}\right)$ についても示された。</p>
<p>$q^{\star}(\pi)$については、
(10.56),(10.57)式にある通り
$$
q^{\star}(\pi)=\operatorname{Dir}(\pi \mid \alpha)\
\alpha_{k}=\alpha_{0}+N_{k}
$$
であり、$\alpha_{k} \rightarrow N_{k}$である。
(B.17),(B.19)式より、
$$
\begin{aligned}
\mathbb{E}\left[\pi_{k}\right]&amp;=\frac{\alpha_{k}}{\overline{\alpha}}\
&amp;\rightarrow \frac{N_{k}}{N}
\end{aligned}
$$
$$
\begin{aligned}
\operatorname{cov}\left[\mu_{j} \mu_{k}\right]&amp;=-\frac{\alpha_{j} \alpha_{k}}{\widehat{\alpha}^{2}(\widehat{\alpha}+1)}\
&amp;\rightarrow 0
\end{aligned}
$$
よって$q^{\star}(\pi)$についても示された。</p>
<p>最後に(10.80)式より、
$$
\begin{aligned}
p(\widehat{\mathbf{x}} \mid \mathbf{X}) &amp;\simeq \sum_{k=1}^{K} \iiint \pi_{k} \mathcal{N}\left(\widehat{\mathbf{x}} \mid \boldsymbol{\mu}<em>{k}, \mathbf{\Lambda}</em>{k}^{-1}\right) q(\boldsymbol{\pi}) q\left(\boldsymbol{\mu}<em>{k}, \mathbf{\Lambda}</em>{k}\right) \mathrm{d} \boldsymbol{\pi} \mathrm{d} \boldsymbol{\mu}<em>{k} \mathrm{~d} \mathbf{\Lambda}</em>{k}\
&amp;\rightarrow \sum_{k=1}^{K} \frac{\alpha_{k}}{\bar{\alpha}} \iint \mathcal{N}\left(\widehat{\mathbf{x}} \mid \boldsymbol{\mu}<em>{k}, \boldsymbol{\Lambda}</em>{k}\right) q\left(\boldsymbol{\mu}<em>{k}, \boldsymbol{\Lambda}</em>{k}\right) \mathrm{d} \boldsymbol{\mu}<em>{k} \mathrm{~d} \boldsymbol{\Lambda}</em>{k}\
&amp;\rightarrow \sum_{k=1}^{K} \frac{N_{k}}{N} \mathcal{N}\left(\widehat{\mathbf{x}} \mid \overline{\mathbf{x}}<em>{k}, \mathbf{W}</em>{k}\right)
\end{aligned}
$$
ただし最後の行は$q^{\star}\left(\boldsymbol{\Lambda}<em>{k}\right)$と$q^{\star}\left(\boldsymbol{\mu}</em>{k} \mid \boldsymbol{\Lambda}_{k}\right)$が特定の位置についてのデルタ関数と近似した。</p>
<p>これにより示された。</p>
<h2 id="演習-1021"><a class="header" href="#演習-1021">演習 10.21</a></h2>
<div class="panel-primary">
<p>$K$個の混合要素を持つ混合モデルにおいて，混合要素の入れ替えについての対称性から得られる，同値なパラメータ設定の数は$K!$であることを示せ．</p>
</div>
<p>P.197によれば</p>
<blockquote>
<p>例として，一つの観測値$x$についての二混合のガウス混合分布を考えよう．パラメータの値は$\pi_{1} = a$，$\pi_{2} = b$，$\pi_{3} = c$，$\pi_{4} = d$，$\pi_{5} = e$，$\pi_{6} = f$とする．このとき，二つの混合要素を入れ替えた別の設定$\pi_{1} = b$，$\pi_{2} = a$，$\pi_{3} = d$，$\pi_{4} = c$，$\pi_{5} = f$，$\pi_{6} = e$も，対称性から同じ$p(x)$を与える．</p>
</blockquote>
<p>とあるように、もし$K$個の混合要素が存在する場合は、それらを入れ替えることで同値なパラメータ設定が可能なので、一般に$K!$個存在することは明らかである。</p>
<h2 id="演習-1022"><a class="header" href="#演習-1022">演習 10.22</a></h2>
<div class="panel-primary">
<p>これまでにガウス混合モデルの事後分布の持つそれぞれの峰は，$K!$個ある同値な峰の一つであることを見てきた．変分ベイズ推論のアルゴリズムを実行した結果，近似事後分布$q$がどれかの峰の周りに局所化して得られたとしよう．このとき，完全な事後分布はこうした分布$q$の$K!$個の混合分布となり，各混合要素が峰となって同じ混合係数を持つ．この混合分布$q$の混合要素の間の重なりが無視できる程度だと仮定すると，結果として得られる全体の下界は，$q$の一つの混合要素の下界に項$\ln K!$を加えたものになることを示せ．</p>
</div>
<p>今、p166並びに、演習9.24より
$$
\begin{aligned}
\ln  p(\mathbf{X}) = L(q) + KL(q||p)
\end{aligned}
$$
が成り立つ。</p>
<p>この時、$KL(q||p)$はKLダイバージェンスであり、1.6.1の議論から、$KL(q||p) \geq 0$である。よって、$\ln  p(\mathbf{X}) \geq L(q)$であり、$L(q)$は、$\ln  p(\mathbf{X})$の下界である。よって、本題は、求めたい真の分布を$p(\mathbf{Z}|\mathbf{X})$として、$L(p(\mathbf{Z}|\mathbf{X}))$を求めれば良い。</p>
<p>まず、各峰は、pの真の各峰を$r_i$ $(i \in {1, 2... K!})$とおくと、pは単純に各$r_i$の平均で表すことができる。</p>
<p>$$
\begin{aligned}
p(\mathbf{Z}|\mathbf{X}) \simeq \sum_i^{K!} \frac{1}{K!} r_i(\mathbf{Z}|\mathbf{X})
\end{aligned}
$$</p>
<p>また、各峰の重なりが無視できるという仮定から、$r_k \neq 0 \rightarrow r_{i \neq k = 0}=0$が成り立つ。</p>
<p>ここで、ある真の峰$r_k$の近似の峰をqとおく。すなわち、この問題では、$p$を混合要素$r_i$を重ね合わせたものと見なし、その近似であるqによって下界を表すことを目指す。</p>
<p>すると、10.3式から、
$$
\begin{aligned}
L(p(\mathbf{Z}|\mathbf{X})) &amp;= \int p(\mathbf{Z}|\mathbf{X}) \ln {\frac{p(\mathbf{Z}, \mathbf{X})}{p(\mathbf{Z}|\mathbf{X})} } d\mathbf{Z} \
&amp;= \int\sum_i^{K!} \frac{1}{K!} r_i(\mathbf{Z}|\mathbf{X}) \ln {\frac{p(\mathbf{Z}, \mathbf{X})}{\sum_i^{K!} \frac{1}{K!} r_i(\mathbf{Z}|\mathbf{X})} } d\mathbf{Z} \
&amp;= \frac{1}{K!}  \int r_1(\mathbf{Z}|\mathbf{X}) \ln {\frac{p(\mathbf{Z}, \mathbf{X})}{\sum_i^{K!} \frac{1}{K!} r_i(\mathbf{Z}|\mathbf{X})} } d\mathbf{Z} +
\frac{1}{K!}  \int r_2(\mathbf{Z}|\mathbf{X}) \ln {\frac{p(\mathbf{Z}, \mathbf{X})}{\sum_i^{K!} \frac{1}{K!} r_i(\mathbf{Z}|\mathbf{X})} } d\mathbf{Z} + \cdots
\frac{1}{K!}  \int r_{K!}(\mathbf{Z}|\mathbf{X}) \ln {\frac{p(\mathbf{Z}, \mathbf{X})}{\sum_i^{K!} \frac{1}{K!} r_i(\mathbf{Z}|\mathbf{X})} } d\mathbf{Z} \
&amp;= \frac{1}{K!}  \int r_1(\mathbf{Z}|\mathbf{X}) \ln {\frac{p(\mathbf{Z}, \mathbf{X})}{\frac{1}{K!} r_1(\mathbf{Z}|\mathbf{X})} } d\mathbf{Z} +
\frac{1}{K!}  \int r_2(\mathbf{Z}|\mathbf{X}) \ln {\frac{p(\mathbf{Z}, \mathbf{X})}{ \frac{1}{K!} r_2(\mathbf{Z}|\mathbf{X})} } d\mathbf{Z} + \cdots
\frac{1}{K!}  \int r_{K!}(\mathbf{Z}|\mathbf{X}) \ln {\frac{p(\mathbf{Z}, \mathbf{X})}{\frac{1}{K!} r_i{K!}\mathbf{Z}|\mathbf{X})} } d\mathbf{Z} &amp;\because r_k \neq 0 \rightarrow r_{i \neq k = 0}=0 \
&amp;= \frac{1}{K!} \sum_i^{K!} \int r_i(\mathbf{Z}|\mathbf{X}) \ln {\frac{p(\mathbf{Z}, \mathbf{X})}{\frac{1}{K!} r_i(\mathbf{Z}|\mathbf{X})} } d\mathbf{Z} \
&amp;= \frac{1}{K!} \sum_i^{K!} \int r_i(\mathbf{Z}|\mathbf{X}) { \ln \frac{p(\mathbf{Z}, \mathbf{X})}{ r_i(\mathbf{Z}|\mathbf{X})} + \ln K!} d\mathbf{Z} \
&amp;= \frac{1}{K!} { \sum_i^{K!}  \int r_i(\mathbf{Z}|\mathbf{X}) \ln \frac{p(\mathbf{Z}, \mathbf{X})}{ r_i(\mathbf{Z}|\mathbf{X})}  d\mathbf{Z} +
\ln K! \sum_i^{K!} \int r_i(\mathbf{Z}|\mathbf{X})  d\mathbf{Z}}
\
&amp;= \frac{1}{K!} \sum_i^{K!} \int r_i(\mathbf{Z}|\mathbf{X}) \ln \frac{p(\mathbf{Z}, \mathbf{X})}{ r_i(\mathbf{Z}|\mathbf{X})}  d\mathbf{Z} +
\ln K! &amp;\because \int r_i(\mathbf{Z}|\mathbf{X})  d\mathbf{Z} = r_i(\mathbf{X}|\mathbf{X})  = 1\
&amp;=\frac{1}{K!} { \int r_1 (\mathbf{Z}|\mathbf{X}) \ln \frac{p(\mathbf{Z}, \mathbf{X})}{ r_1(\mathbf{Z}|\mathbf{X})}  d\mathbf{Z}+
\int r_2 (\mathbf{Z}|\mathbf{X}) \ln \frac{p(\mathbf{Z}, \mathbf{X})}{ r_2(\mathbf{Z}|\mathbf{X})}  d\mathbf{Z}
}+\cdots +
\int r_k (\mathbf{Z}|\mathbf{X}) \ln \frac{p(\mathbf{Z}, \mathbf{X})}{ r_k(\mathbf{Z}|\mathbf{X})}  d\mathbf{Z}
}+\cdots +
\int r_{K!} (\mathbf{Z}|\mathbf{X}) \ln \frac{p(\mathbf{Z}, \mathbf{X})}{ r_{K!}(\mathbf{Z}|\mathbf{X})}  d\mathbf{Z}
}+
\ln K! \
&amp;=\frac{1}{K!} K! \int q(\mathbf{Z}|\mathbf{X}) \ln \frac{p(\mathbf{Z}, \mathbf{X})}{ q(\mathbf{Z}|\mathbf{X})}  d\mathbf{Z} +
\ln K! &amp;\because r_i\text{はそれぞれ同値であり、積分は同じ。詳細は最後　}\
&amp;= L(q) + \ln K! &amp;\because (10.3)
\end{aligned}
$$</p>
<p>今、$L(q)$は一つの混合要素の下界なので、題意は満たされた。</p>
<p>最後から2番目の式変形について、まず、自明に、$\int r_k (\mathbf{Z}|\mathbf{X}) \ln \frac{p(\mathbf{Z}, \mathbf{X})}{ r_k(\mathbf{Z}|\mathbf{X})}  d\mathbf{Z} =\int q(\mathbf{Z}|\mathbf{X}) \ln \frac{p(\mathbf{Z}, \mathbf{X})}{ q(\mathbf{Z}|\mathbf{X})}  d\mathbf{Z}$が成り立つ。</p>
<p>そして、$i \neq k$について、
$$
\begin{aligned}
\int r_i (\mathbf{Z}|\mathbf{X}) \ln \frac{p(\mathbf{Z}, \mathbf{X})}{ r_i(\mathbf{Z}|\mathbf{X})}  d\mathbf{Z}
&amp;= \int r_i (\mathbf{Z}|\mathbf{X}) \ln p(\mathbf{Z}| \mathbf{X})d\mathbf{Z}
+\int r_i (\mathbf{Z}|\mathbf{X}) \ln p(\mathbf{X})  d\mathbf{Z}
-\int r_i (\mathbf{Z}|\mathbf{X}) \ln r_i (\mathbf{Z}|\mathbf{X}) d\mathbf{Z} \
&amp;= \int r_i (\mathbf{Z}|\mathbf{X}) \ln \frac{r_i(\mathbf{Z}| \mathbf{X})}{K!} d\mathbf{Z}
+\int r_i (\mathbf{Z}|\mathbf{X}) \ln p(\mathbf{X})  d\mathbf{Z}
-\int r_i (\mathbf{Z}|\mathbf{X}) \ln r_i (\mathbf{Z}|\mathbf{X}) d\mathbf{Z} &amp;\because r_i(\mathbf{Z} \notin \mathbf{Z}_i |\mathbf{X}) = 0\
&amp;= \int r_k (\mathbf{Z}|\mathbf{X}) \ln \frac{r_k(\mathbf{Z}| \mathbf{X})}{K!} d\mathbf{Z}
+\int r_k (\mathbf{Z}|\mathbf{X}) \ln p(\mathbf{X})  d\mathbf{Z}
-\int r_k (\mathbf{Z}|\mathbf{X}) \ln r_k (\mathbf{Z}|\mathbf{X}) d\mathbf{Z} &amp;\because r_i\text{はそれぞれ同値であり、積分は同じ}\
&amp;= \int r_k (\mathbf{Z}|\mathbf{X}) \ln p(\mathbf{Z}| \mathbf{X})d\mathbf{Z}
+\int r_k (\mathbf{Z}|\mathbf{X}) \ln p(\mathbf{X})  d\mathbf{Z}
-\int r_k (\mathbf{Z}|\mathbf{X}) \ln r_k (\mathbf{Z}|\mathbf{X}) d\mathbf{Z} &amp;\because r_k(\mathbf{Z} \notin \mathbf{Z}_k |\mathbf{X}) = 0\
&amp;= \int r_k (\mathbf{Z}|\mathbf{X}) \ln \frac{p(\mathbf{Z},  \mathbf{X})}{r_k (\mathbf{Z}|\mathbf{X})} d\mathbf{Z}\
&amp;= \int q(\mathbf{Z}|\mathbf{X}) \ln \frac{p(\mathbf{Z}, \mathbf{X})}{ q(\mathbf{Z}|\mathbf{X})}  d\mathbf{Z}
\end{aligned}
$$</p>
<p>最後は冗長かもしれないのでその時はご教示ください。</p>
<h2 id="演習-1023"><a class="header" href="#演習-1023">演習 10.23</a></h2>
<div class="panel-primary">
<p>混合係数${ \pi_k }$に事前分布を与えない変分ベイズガウス混合モデルを考えよう．代わりに混合係数はパラメータとして扱い，対数周辺尤度の下界を最大化する際に値を求める．ラグランジュ乗数法を用いて，混合係数の和が$1$になる制約条件の下でこの下界を混合係数について最大化すると，再推定式
$$\pi_{k}=\frac{1}{N} \sum_{n=1}^{N} r_{n k} \tag{10.83}$$
の結果が得られることを示せ．この際，下界のすべての項を考える必要はなく，${\pi_k}$に依存する項だけを考えればよいことに注意せよ．</p>
</div>
<p>変分ベイズガウス混合モデルでは、下界は(10.70)式で与えられる。
本問では、<strong>混合係数${ \pi_k }$に事前分布を与えない</strong>パラメータとして扱うため、対数周辺尤度として第２項のみを考えれば良い。つまり、</p>
<p>$$
\mathscr{L} \propto \mathbb{E}[\ln p(\mathbf{Z} \mid \pi)]=\sum_{n=1}^{N} \sum_{k=1}^{K} r_{n k} \ln \pi_{k} \tag{10.72}
$$</p>
<p>$$
L=\sum_{n=1}^{N} \sum_{k=1}^{K} r_{n k} \ln \pi_{k}+\lambda \cdot\left(\sum_{k=1}^{K} \pi_{k}-1\right)
$$</p>
<p>上式のLagrangianについて、$\pi_k$について微分し、=0とおくと、
$$
\frac{\partial L}{\partial \pi_{k}}=\frac{\sum_{n=1}^{N}r_{nk}}{\pi_{k}}+\lambda=\frac{N_{k}}{\pi_{k}}+\lambda=0 \tag{A}
$$</p>
<p>(A)の両辺に$\pi_k$をかけ、$\sum_{k=1}^{K}$をとると、
$$
\sum_{k=1}^{K} N_{k}+\lambda \sum_{k=1}^{K}\pi_{k}=0
$$</p>
<p>$\sum_{k=1}^{K} N_{k}=N$, $\sum_{k=1}^{K}\pi_{k}=1$より、
$$
\lambda=-N \tag{A}
$$</p>
<p>(A)に代入し、$\pi_{k}$について解くと、
$$
\pi_{k}=\frac{N_{k}}{N}=\underline{\frac{1}{N} \sum_{n=1}^{N} r_{n k}}
$$</p>
<h2 id="演習-1024"><a class="header" href="#演習-1024">演習 10.24</a></h2>
<div class="panel-primary">
<p>10.2節でガウス混合モデルを最尤推定で扱う際に現れる特異性は，ベイズ的な解では現れないことを見た．こうした特異性は，ベイズモデルを最大事後機率(MAP)推定を使って解く際には現れるかどうか議論せよ．</p>
</div>
<p>最尤推定で現れる特異性とは、9.2.1節で議論した$\left|\boldsymbol{\Lambda}_{k}\right| \rightarrow \infty$に発散してしまうことを意味している。ベイズモデルではこのようなことが起きないことを示す。</p>
<p>混合ガウス分布の事後確率は、(10.9),(10.38),(10.40),(10.50)を利用すれば以下となる。</p>
<p>$$
\begin{aligned}
\mathbb{E}<em>{q(\mathbf{Z})} &amp;[\ln p(\mathbf{X} \mid \mathbf{Z}, \boldsymbol{\mu}, \boldsymbol{\Lambda}) p(\boldsymbol{\mu}, \mathbf{\Lambda})] \
=&amp; \frac{1}{2} \sum</em>{n=1}^{N} r_{k n}\left(\ln \left|\boldsymbol{\Lambda}<em>{k}\right|-\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}<em>{k}\right)^{\mathrm{T}} \boldsymbol{\Lambda}</em>{k}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{k}\right)\right) \
&amp;+\ln \left|\boldsymbol{\Lambda}<em>{k}\right|-\beta</em>{0}\left(\boldsymbol{\mu}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm{T}} \boldsymbol{\Lambda}<em>{k}\left(\boldsymbol{\mu}</em>{k}-\mathbf{m}<em>{0}\right) \
&amp;+\left(\nu</em>{0}-D-1\right) \ln \left|\boldsymbol{\Lambda}<em>{k}\right|-\operatorname{Tr}\left[\mathbf{W}</em>{0}{ }^{1} \boldsymbol{\Lambda}_{k}\right]+\text { const. }
\end{aligned}
$$</p>
<p>これを（10.51)-(10.53)を利用して$\mathbf{\Lambda}<em>{k}$について整理すると（$\mathbf{\Lambda}</em>{k}$と無関係な項は無視）</p>
<p>$$
\left(\nu_{0}+N_{k}-D\right) \ln \left|\boldsymbol{\Lambda}<em>{k}\right|-\operatorname{Tr}\left[\left(\mathbf{W}</em>{0}^{-1}+\beta_{0}\left(\boldsymbol{\mu}<em>{k}-\mathbf{m}</em>{0}\right)\left(\boldsymbol{\mu}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm{T}}+N_{k} \mathbf{S}<em>{k}\right) \boldsymbol{\Lambda}</em>{k}\right]
$$</p>
<p>(C.24),(C.28)を使用して$\mathbf{\Lambda}_{k}$で微分してゼロとおくと以下になる。</p>
<p>$$
\mathbf{\Lambda}<em>{k}^{-1}=\frac{1}{\nu</em>{0}+N_{k}-D}\left(\mathbf{W}<em>{0}^{-1}+\beta</em>{0}\left(\boldsymbol{\mu}<em>{k}-\mathbf{m}</em>{0}\right)\left(\boldsymbol{\mu}<em>{k}-\mathbf{m}</em>{0}\right)^{\mathrm{T}}+N_{k} \mathbf{S}_{k}\right)
$$</p>
<p>ウィシャート分布の正式より、$\left|\boldsymbol{\Lambda}_{k}^{-1}\right|$はゼロになることはない。</p>
<p>よって、ベイズモデルでは、$\left|\boldsymbol{\Lambda}_{k}\right| \rightarrow \infty$に発散することはないことが示された。</p>
<h2 id="演習-1025"><a class="header" href="#演習-1025">演習 10.25</a></h2>
<div class="panel-primary">
<p>10.2節で議論したベイズ混合ガウス分布の変分ベイス法による解では，事後分布について分解した近似
$$q(\mathbf{Z})=\prod_{i=1}^{M} q_{i}\left(\mathbf{Z}_{i}\right) \tag{10.5}$$
を用いた図10.2で見たように，こうした分解の仮定はパラメータ空間で、の事後分布の特定の方向の分散を過小評価してしまう．この影響がモデルエピデンスの変分近似に及ぼす影響について質的に議論せよ．さらに，この影響が混合モデルの混合要素数に関してどう変わるか述べよ．これから，変分ガウス混合モデルが最適な混合要素数を過小評価しがちか，過大評価しがちか説明せよ．</p>
</div>
<p>混合成分の数が増えると、相関している可能性のある変数の数も増える一方、平均場近似の式(10.5)を用いるとそれらの相関を表現することができない(図10.2,3)。その結果、KLダイバージェンスの最小化を行うときに複数の山をつぶして近似してしまうことが考えられるため、過小評価する。</p>
<h2 id="演習-1026"><a class="header" href="#演習-1026">演習 10.26</a></h2>
<div class="panel-primary">
<p>ベイズ線形回帰モデルの変分ベイズ法による解法を拡張し，$\beta$についてガンマ超事前分布$\textrm{Gam}(\beta\mid c_0, d_0)$を導入して，分解された変分事後分布$q(\mathbf{w}) q(\alpha) q(\beta)$を仮定して変分ベイズ法によって解け．変分事後分布の三つの因子の更新式を導出し，さらに下界および予測分布の式を求めよ．</p>
</div>
<p>$\beta$を含めた全ての変数の同時分布は</p>
<p>$$
p(\mathbf{t}, \mathbf{w}, \alpha, \beta)=p(\mathbf{t}|\mathbf{w}, \beta)p(\mathbf{w}|\alpha)p(\alpha)p(\beta)
$$</p>
<p>と書くことができる．本文中の議論をなぞって，$\mathbf{w}, \alpha, \beta$の尤度関数と事前分布を</p>
<p>$$
\begin{aligned}
&amp;p(\mathbf{t} \mid \mathbf{w}, \beta, \mathbf{X})=\prod_{n=1}^{N} N\left(\mathbf{t}<em>{n} \mid \mathbf{w}^{\mathrm{T}} \phi</em>{n}, \beta^{-1}\right) \
&amp;p(\mathbf{w} \mid \alpha)=N\left(\mathbf{w} \mid 0, \alpha^{-1} \mathbf{I}\right) \
&amp;p(\alpha)=\operatorname{Gam}\left(\alpha \mid a_{0}, b_{0}\right) \
&amp;p(\beta)=\operatorname{Gam}\left(\beta \mid c_{0}, d_{0}\right)
\end{aligned}
$$</p>
<p>と書くことができる．</p>
<p>ここで変分推論の枠組みで考え，問題中の設定から変分事後分布は</p>
<p>$$
q(\mathbf{w}, \alpha, \beta) = q(\mathbf{w})q(\alpha)q(\beta)
$$</p>
<p>と分解できるとする．</p>
<p>$q(\mathbf{w}),q(\alpha),q(\beta)$の更新式を求める．まず$q(\alpha)$から10.1節で導出した一般的な結果(10.9)を用いて</p>
<p>$$
\begin{aligned}
\ln q^*(\alpha)&amp;=\mathbb{E}<em>{\mathbf{w}, \beta}[\ln p(\mathbf{t}, \mathbf{w}, \alpha, \beta \mid \mathbf{X})]\
&amp;=\mathbb{E}</em>{\mathbf{w}, \beta}[\ln p(\mathbf{t} \mid \mathbf{w}, \beta, \mathbf{X}) p(\mathbf{w} \mid \alpha) p(\alpha) p(\beta)]\
&amp;=\mathbb{E}<em>{\mathbf{w}, \beta}[\ln p(\mathbf{t} \mid \mathbf{w}, \beta, \mathbf{X})]+\mathbb{E}</em>{\mathbf{w}}[\ln \beta(\mathbf{w} \mid \alpha)]+\ln p(\alpha)+\mathbb{E}<em>{\beta}[\ln p(\beta)]\
&amp;=\mathbb{E}</em>{\mathbf{w}}\left[\ln N\left(\mathbf{w} \mid 0, \alpha^{-1} \mathbf{I}\right)\right]+\ln \operatorname{Gam}\left(\alpha \mid a_{0}, b_{0}\right)+\textrm{const}\
&amp;=\mathbb{E}<em>{\mathbf{w}}\left[\ln \frac{1}{(2 \pi)^{\frac{M}{2}}} \frac{1}{\left(\alpha^{-1}\right)^{\frac{1}{2}}} \operatorname{exp}\left(-\frac{\alpha}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w}\right)\right]+\ln \frac{1}{\Gamma\left(a</em>{0}\right)} b_{0}^{a_{0}} \alpha^{a_{0}-1} e^{-b_{0} \alpha}+ \textrm{const.}\
&amp;=\frac{M}{2} \ln \alpha-\frac{\alpha}{2} \mathbb{E}\left[\mathbf{w}^{\mathrm{T}} \mathbf{w}\right]+\left(a_{0}-1\right) \ln \alpha-b_{0} \alpha+ \textrm{const.}\
&amp;=\left(\frac{M}{2}+a_{0}-1\right) \ln \alpha-\left(\frac{1}{2} \mathbb{E}\left[\mathbf{w}^{\mathrm{T}} \mathbf{w}\right]+b_{0}\right) \alpha+ \textrm{const.}
\end{aligned}
$$</p>
<p>ここで$\beta$を導入した場合にも$\alpha$に依存しない項はconstに押し込んで計算することができるため，(10.92)-(10.95)式までの議論をそのまま用いることができる．</p>
<p>$$
q^*(\alpha)=\operatorname{Gam}\left(\alpha \mid a_{N}, b_{N}\right) , a_N=\frac{M}{2} + a_0, b_N=\frac{1}{2}\mathbb{E}\left[\mathbf{w}^{\mathrm{T}} \mathbf{w}\right]+b_0
$$</p>
<p>を得る．次に$q(\mathbf{w})$について(10.9)より</p>
<p>$$
\begin{aligned}
\ln q^*(\mathbf{w})&amp;=\mathbb{E}<em>{\alpha, \beta}[\ln \beta(\mathbf{t}, \mathbf{w}, \alpha, \beta \mid x)]\
&amp;=\mathbb{E}</em>{\alpha, \beta}[\ln p(\mathbf{t} \mid \mathbf{w}, \beta, X) p(\mathbf{w} \mid \alpha) \gamma(\alpha) p(\beta)]\
&amp;=\mathbb{E}<em>{\beta}\left[\ln \prod</em>{n=1}^{N} N\left(\mathbf{t}<em>{n} \mid \mathbf{w}^{\mathrm{T}} \phi</em>{n}, \beta^{-1}\right)\right]+\mathbb{E}<em>{\alpha}\left[\ln N\left(w \mid 0, \alpha^{-1} I\right)\right]+ \textrm{const.}\
&amp;=\mathbb{E}</em>{\beta}\left[\sum_{n=1}^N\ln \frac{1}{(2 \pi)^{\frac{M}{2}}} \frac{1}{\left(\beta^{-1}\right)^{\frac{1}{2}}} \operatorname{exp}\left{-\frac{\beta}{2}(\mathbf{t}<em>n-\mathbf{w}^{\mathrm{T}}\phi</em>{n})^2 \right}\right]+\mathbb{E}<em>{\alpha}\left[\ln \frac{1}{(2 \pi)^{\frac{M}{2}}} \frac{1}{\left(\alpha^{-1}\right)^{\frac{1}{2}}} \operatorname{exp}\left(-\frac{\alpha}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w}\right)\right]+ \textrm{const.}\
&amp;=\mathbb{E}</em>{\beta}\left[\beta\right]\left(\mathbf{w}^{\mathrm{T}}\Phi^{\mathrm{T}}\mathbf{t}-\frac{1}{2}\mathbf{w}^{\mathrm{T}}\Phi^{\mathrm{T}}\Phi\mathbf{w}\right)-\frac{1}{2}\mathbb{E}<em>{\alpha}\left[\alpha\right]\mathbf{w}^{\mathrm{T}}\mathbf{w}+ \textrm{const.}\
&amp;=-\frac{1}{2}\mathbf{w}^{\mathrm{T}}\left(\mathbb{E}</em>{\beta}[\beta] \Phi^{\mathrm{T}} \Phi+\mathbb{E}<em>{\alpha}[\alpha]\mathbf{I}\right) \mathbf{w}+\mathbb{E}</em>{\beta}[\beta] \mathbf{w}^{\mathrm{T}} \Phi^{\mathrm{T}} \mathbf{t}+ \textrm{const.}\
\end{aligned}
$$</p>
<p>これは$\mathbf{w}$に関して2次形式なのでガウス分布になり，平方完成すると</p>
<p>$$
q^*(\mathbf{w})=\mathcal{N}(\mathbf{w}\mid \mathbf{m}_N, \mathbf{S}_N)
$$</p>
<p>$$
\mathbf{m}<em>N=\mathbb{E}</em>{\beta}[\beta]\mathbf{S}_N\mathbf{\Phi}^{\mathrm{T}}\mathbf{t}
$$</p>
<p>$$
\mathbf{S}<em>N=\mathbb{E}</em>{\alpha}[\alpha]\mathbf{I}+\mathbb{E}_{\beta}[\beta]\mathbf{\Phi}^{\mathrm{T}}\mathbf{\Phi}
$$</p>
<p>を得る．最後に$q(\beta)$について(10.9)より</p>
<p>$$
\begin{aligned}
\ln q^{\star}(\beta) &amp;=\mathbb{E}<em>{\mathbf{w}, \alpha}[\ln p(\mathbf{t}, \mathbf{w}, \alpha, \beta \mid \mathbf{X})]\
&amp;=\mathbb{E}</em>{\mathbf{w}, \alpha}[\ln p(\mathbf{t} \mid \mathbf{w}, \beta, \mathbf{X}) p(\mathbf{w} \mid \alpha) p(\alpha) p(\beta)]\
&amp;=\mathbb{E}<em>{\mathbf{w}}[\ln p(\mathbf{t} \mid \mathbf{w}, \beta)]+\ln p(\beta)+\text { const } \
&amp;= \frac{N}{2} \cdot \ln \beta-\frac{\beta}{2} \cdot \mathbb{E}\left[\sum</em>{n=1}^{N}\left(t_{n}-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}<em>{n}\right)^{2}\right]+\left(c</em>{0}-1\right) \ln \beta-d_{0} \beta +\text { const }\
&amp;=\left(\frac{N}{2}+c_{0}-1\right) \cdot \ln \beta-\frac{\beta}{2} \cdot \mathbb{E}\left[\left|\mathbf{\Phi}<em>{\mathbf{w}}-\mathbf{t}\right|^{2}\right]-d</em>{0} \beta +\text { const }\
&amp;=\left(\frac{N}{2}+c_{0}-1\right) \cdot \ln \beta-\beta \cdot\left{\frac{1}{2} \cdot \mathbb{E}\left[|\mathbf{\Phi} \mathbf{w}-\mathbf{t}|^{2}\right]+d_{0}\right} +\text { const }\
&amp;=\left(\frac{N}{2}+c_{0}-1\right) \cdot \ln \beta-\beta \cdot\left{\frac{1}{2} \cdot \mathbb{E}\left[\mathbf{w}^{\mathrm{T}} \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}<em>{\mathbf{w}}-2 \mathbf{t}^{\mathrm{T}} \mathbf{\Phi}</em>{\mathbf{w}}+\mathbf{t}^{\mathrm{T}} \mathbf{t}\right]+d_{0}\right} +\text { const }\
&amp;=\left(\frac{N}{2}+c_{0}-1\right) \cdot \ln \beta-\beta \cdot\left{\frac{1}{2} \cdot \operatorname{Tr}\left[\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \mathbb{E}\left[\mathbf{w} \mathbf{w}^{\mathrm{T}}\right]\right]-\mathbf{t}^{\mathrm{T}} \mathbf{\Phi} \mathbb{E}[\mathbf{w}]+\frac{1}{2} \mathbf{t}^{\mathrm{T}} \mathbf{t}+d_{0}\right} +\text { const }\
&amp;=\left(\frac{N}{2}+c_{0}-1\right) \cdot \ln \beta-\beta \cdot\left{\frac{1}{2} \cdot \operatorname{Tr}\left[\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\left(\mathbf{m}<em>{N} \mathbf{m}</em>{N}^{\mathrm{T}}+\mathbf{S}<em>{N}\right)\right]-\mathbf{t}^{\mathrm{T}} \mathbf{\Phi} \mathbf{m}</em>{N}+\frac{1}{2} \mathbf{t}^{\mathrm{T}} \mathbf{t}+d_{0}\right} +\text { const }\
&amp;=\left(\frac{N}{2}+c_{0}-1\right) \cdot \ln \beta-\beta \cdot\left{\frac{1}{2} \operatorname{Tr}\left[\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \mathbf{S}<em>{N}\right]+\frac{1}{2} \mathbf{m}</em>{N}^{\mathrm{T}} \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \mathbf{m}<em>{N}-\mathbf{t}^{\mathrm{T}} \mathbf{\Phi} \mathbf{m}</em>{N}+\frac{1}{2} \mathbf{t}^{\mathrm{T}} \mathbf{t}+d_{0}\right} +\text { const }\
&amp;=\left(\frac{N}{2}+c_{0}-1\right) \cdot \ln \beta-\beta \cdot \frac{1}{2}\left{\operatorname{Tr}\left[\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \mathbf{S}<em>{N}\right]+\left|\mathbf{\Phi} \mathbf{m}</em>{N}-\mathbf{t}\right|^{2}+2 d_{0}\right}+\text { const }
\end{aligned}
$$</p>
<p>これより</p>
<p>$$
q^{\star}(\beta)=\operatorname{Gam}\left(\beta \mid c_{N}, d_{N}\right)
$$</p>
<p>$$
c_N=\frac{N}{2}+c_0
$$</p>
<p>$$
d_N=d_{0}+\frac{1}{2}\left{\operatorname{Tr}\left[\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \mathbf{S}<em>{N}\right]+\left|\mathbf{\Phi} \mathbf{m}</em>{N}-\mathbf{t}\right|^{2}\right}
$$</p>
<p>以上から各因子の更新式が得られた．</p>
<p>次に変分下界を求める．変分下界は本文中の式(10.107)を$\beta$を考慮した形に修正すれば得られ，考えるべき項は$\mathbb{E}\left[\ln p(\beta)\right], -\mathbb{E}\left[\ln q^*(\beta)\right]$の二つであるので，それぞれの計算をして，ディガンマ関数$\varphi(a)=\frac{d}{da}\ln\Gamma(a)$として</p>
<p>$$
\begin{aligned}
\mathbb{E}[\ln p(\beta)] &amp;=\left(c_{0}-1\right) \mathbb{E}[\ln \beta]-d_{0} \mathbb{E}[\beta]+c_{0} \ln d_{0}-\ln \Gamma\left(c_{0}\right) \
&amp;=\left(c_{0}-1\right) \cdot\left(\varphi\left(c_{N}\right)-\ln d_{N}\right)-d_{0} \frac{c_{N}}{d_{N}}+c_{0} \ln d_{0}-\ln \Gamma\left(c_{0}\right)
\end{aligned}
$$</p>
<p>ここで(B.26)(ガンマ分布の関数形についての定義式),(B.30)(ガンマ分布に従う確率変数の自然対数の期待値がディガンマ関数に紐づけられる式)をそれぞれ用いた．また</p>
<p>$$
-\mathbb{E}\left[\ln q^{\star}(\beta)\right]=\left(c_{N}-1\right) \cdot \varphi\left(c_{N}\right)-c_{N}+\ln d_{N}-\ln \Gamma\left(c_{N}\right)
$$</p>
<p>ここでガンマ分布に従う確率変数のエントロピーについての式(B.31)を用いた．(10.107)-(10.112)の式を修正することで$\beta$を考慮に入れた変分下界を得る．</p>
<p>最後に予測分布を考える．</p>
<p>これも本文中の議論を$\beta$を考慮したものに修正して得ることができて(10.105),(10.106)から</p>
<p>$$
\begin{aligned}
p(t \mid \mathbf{x}, \mathbf{t}) &amp;=\int p(t \mid \mathbf{x}, \mathbf{w}) p(\mathbf{w} \mid \mathbf{t}) \mathrm{d} \mathbf{w} \
&amp; \simeq \int p(t \mid \mathbf{x}, \mathbf{w}) q(\mathbf{w}) \mathrm{d} \mathbf{w} \
&amp;=\int \mathcal{N}\left(t \mid \mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x}), \beta^{-1}\right) \mathcal{N}\left(\mathbf{w} \mid \mathbf{m}<em>{N}, \mathbf{S}</em>{N}\right) \mathrm{d} \mathbf{w} \
&amp;=\mathcal{N}\left(t \mid \mathbf{m}_{N}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x}), \sigma^{2}(\mathbf{x})\right)
\end{aligned}
$$</p>
<p>ここで分散は</p>
<p>$$
\sigma^{2}(\mathbf{x})=\frac{1}{\mathbb{E}\left[\beta\right]}+\boldsymbol{\phi}(\mathbf{x})^{\mathrm{T}} \mathbf{S}_{N} \boldsymbol{\phi}(\mathbf{x})
$$</p>
<p>である．</p>
<h2 id="演習-1027"><a class="header" href="#演習-1027">演習 10.27</a></h2>
<div class="panel-primary">
<p>付録Bで与えられている公式を用いて， 線形基底関数回帰モデルの変分下界は
$$
\begin{aligned} \mathcal{L}(q)&amp;= \mathbb{E}[\ln p(\mathbf{w}, \alpha, \mathbf{t})]-\mathbb{E}[\ln q(\mathbf{w}, \alpha)] \
&amp;= \mathbb{E}<em>{\mathbf{w}}[\ln p(\mathbf{t} \mid \mathbf{w})]+\mathbb{E}</em>{\mathbf{w}, \alpha}[\ln p(\mathbf{w} \mid \alpha)]+\mathbb{E}<em>{\alpha}[\ln p(\alpha)] \
&amp;-\mathbb{E}</em>{\alpha}[\ln q(\mathbf{w})]_{\mathbf{w}}-\mathbb{E}[\ln q(\alpha)] \end{aligned} \tag{10.107}
$$
の形で書け，その各項は</p>
<p>$$\begin{aligned} \mathbb{E}[\ln p(\mathbf{t} \mid \mathbf{w})]<em>{\mathbf{w}}=&amp; \frac{N}{2} \ln \left(\frac{\beta}{2 \pi}\right)-\frac{\beta}{2} \mathbf{t}^{\mathrm{T}} \mathbf{t}+\beta \mathbf{m}</em>{N}^{\mathrm{T}} \mathbf{\Phi}^{\mathrm{T}} \mathbf{t} \
&amp;-\frac{\beta}{2} \operatorname{Tr}\left[\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\left(\mathbf{m}<em>{N} \mathbf{m}</em>{N}^{\mathrm{T}}+\mathbf{S}<em>{N}\right)\right] \end{aligned} \tag{10.108}$$
$$\begin{aligned} \mathbb{E}[\ln p(\mathbf{w} \mid \alpha)]</em>{\mathbf{w}, \alpha}=&amp;-\frac{M}{2} \ln (2 \pi)+\frac{M}{2}\left(\psi\left(a_{N}\right)-\ln b_{N}\right) \
&amp;-\frac{a_{N}}{2 b_{N}}\left[\mathbf{m}<em>{N}^{\mathrm{T}} \mathbf{m}</em>{N}+\operatorname{Tr}\left(\mathbf{S}<em>{N}\right)\right] \end{aligned}\tag{10.109}$$
$$\begin{aligned} \mathbb{E}[\ln p(\alpha)]</em>{\alpha}=&amp;\ a_{0} \ln b_{0}+\left(a_{0}-1\right)\left[\psi\left(a_{N}\right)-\ln b_{N}\right] \
&amp;-b_{0} \frac{a_{N}}{b_{N}}-\ln \Gamma\left(a_{0}\right) \end{aligned} \tag{10.110}$$
$$-\mathbb{E}[\ln q(\mathbf{w})]<em>{\mathbf{w}}=\frac{1}{2} \ln \left|\mathbf{S}</em>{N}\right|+\frac{M}{2}[1+\ln (2 \pi)] \tag{10.111}$$
$$-\mathbb{E}[\ln q(\alpha)]<em>{\alpha}=\ln \Gamma\left(a</em>{N}\right)-\left(a_{N}-1\right) \psi\left(a_{N}\right)-\ln b_{N}+a_{N} \tag{10.112}$$</p>
<p>となることを示せ．</p>
</div>
<p>※演習10.16, 10.17のように各項の確率分布に適切なものを当てはめて計算していくだけ。</p>
<p>$$
\begin{aligned}
\mathbb{E}<em>{\mathbf{w}}[\ln p(\mathbf{t} \mid \mathbf{w})] &amp;=\mathbb{E}</em>{\mathbf{w}}\left[\ln \prod_{n=1}^{N} \mathcal{N}\left(t_{n} \mid \mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}<em>{n}, \beta^{-1}\right)\right]\hspace{1em}(\because(B.87))\
&amp;=\mathbb{E}</em>{\mathbf{w}}\left[\sum_{n=1}^{N} \ln \mathcal{N}\left(t_{n} \mid \mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}<em>{n}, \beta^{-1}\right)\right] \
&amp;=\mathbb{E}</em>{\mathbf{w}}\left[\sum_{n=1}^{N} \ln \left{\left(\frac{\beta}{2 \pi}\right)^{\frac{1}{2}} \exp \left{-\frac{\beta}{2}\left(t_{n}-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}<em>{n}\right)^{2}\right}\right.\right.\
&amp;=\frac{N}{2} \ln \left(\frac{\beta}{2 \pi}\right)-\frac{\beta}{2} \mathbb{E}</em>{\mathbf{w}}\left[\sum_{n=1}^{N}\left(t_{n}-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}<em>{n}\right)^{2}\right] \
&amp;=\frac{N}{2} \ln \left(\frac{\beta}{2 \pi}\right)-\frac{\beta}{2} \mathbb{E}</em>{\mathbf{w}}\left[(\mathbf{t}-\mathbf{\Phi} \mathbf{w})^{\mathrm{T}}(\mathbf{t}-\mathbf{\Phi} \mathbf{w})\right] \
&amp;=\frac{N}{2} \ln \left(\frac{\beta}{2 \pi}\right)-\frac{\beta}{2} \mathbf{t}^{\mathrm{T}} \mathbf{t}+\beta \mathbb{E}<em>{\mathbf{w}}\left[\mathbf{w}^{\mathrm{T}}\right] \mathbf{\Phi}^{\mathrm{T}} \mathbf{t}-\frac{\beta}{2} \mathbb{E}</em>{\mathbf{w}}\left[\mathbf{w}^{\mathrm{T}} \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \mathbf{w}\right] \
&amp;=\frac{N}{2} \ln \left(\frac{\beta}{2 \pi}\right)-\frac{\beta}{2} \mathbf{t}^{\mathrm{T}} \mathbf{t}+\beta \mathbf{m}<em>{N}^{\mathrm{T}} \mathbf{\Phi}^{\mathrm{T}} \mathbf{t}-\frac{\beta}{2} \operatorname{Tr}\left[\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \mathbb{E}</em>{\mathbf{w}}\left[\mathbf{ww}^{\mathrm{T}}\right]\right] \
&amp;=\frac{N}{2} \ln \left(\frac{\beta}{2 \pi}\right)-\frac{\beta}{2} \mathbf{t}^{\mathrm{T}} \mathbf{t}+\beta_{m N}^{\mathrm{T}} \mathbf{\Phi} \mathbf{t}-\frac{\beta}{2} \operatorname{Tr}\left[\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\left(\mathbf{m}<em>{N} \mathbf{m}</em>{N}^{\mathrm{T}}+\mathbf{S}_{N}\right)\right]
\end{aligned}
$$</p>
<hr />
<p>$$
\begin{aligned} \mathbb{E}<em>{\mathbf{w}, \alpha}[\ln p(\mathbf{w} \mid \alpha)] &amp;=\mathbb{E}</em>{\mathbf{w}, \alpha}\left[\ln \mathcal{N}\left(\mathbf{w} \mid \mathbf{0}, \alpha^{-1} \mathbf{I}\right)\right] \
&amp;=\mathbb{E}<em>{\mathbf{w}, \alpha}\left[\ln \left{\left(\frac{\alpha}{2 \pi}\right)^{M / 2} \exp \left{-\frac{\alpha}{2} \mathbf{w}^{\mathrm{T}} w\right}\right]\right.\
&amp;=\mathbb{E}</em>{\mathbf{w}, \alpha}\left[\frac{M}{2} \ln \left(\frac{\alpha}{2 \pi}\right)\right]-\frac{1}{2} \mathbb{E}<em>{\mathbf{w}, \alpha}\left[\alpha \mathbf{w}^{\mathrm{T}} \mathbf{w}\right] \
&amp;=-\frac{M}{2} \ln (2 \pi)+\frac{M}{2} \mathbb{E}</em>{\alpha}[\ln \alpha]-\frac{\mathbb{E}<em>{\alpha}[\alpha]}{2} \mathbb{E}</em>{\mathbf{w}}\left[\mathbf{w}^{\mathrm{T}} \mathbf{w}\right] \
&amp;=-\frac{M}{2} \ln (2 \pi)+\frac{M}{2} \underbrace{\left(\psi\left(a_{N}\right)-\ln b_{N}\right)}<em>{(B.30)} - \underbrace{\frac{a</em>{N}}{2 b_{N}}}<em>{(B.27)} \mathbb{E}</em>{\mathbf{w}}\left[\operatorname{Tr}\left(\mathbf{w} \mathbf{w}^{\mathrm{T}}\right)\right] \
&amp;=-\frac{M}{2} \ln (2 \pi)+\frac{M}{2} \left(\psi\left(a_{N}\right)-\ln b_{N}\right) - \frac{a_{N}}{2 b_{N}} \left[\mathbf{m}<em>{N}^{\mathrm{T}} \mathbf{m}</em>{N}+\operatorname{Tr}\left(\mathbf{S}_{N}\right)\right]
\end{aligned}
$$</p>
<p>ここで$\mathbb{E}<em>{\mathbf{w}}\left[\operatorname{Tr}\left(\mathbf{w} \mathbf{w}^{\mathrm{T}}\right)\right]$の変形についてはトレースと期待値の交換性と
$$
\begin{aligned} &amp; \operatorname{Tr}\left[\mathbb{E}</em>{\mathbf{w}}\left[\mathbf{w} \mathbf{w}^{\mathrm{T}}\right]\right] \
= &amp; \operatorname{Tr}\left[\operatorname{cov}[\mathbf{w}]+\mathbb{E}<em>{\mathbf{w}}[\mathbf{w}] \mathbb{E}</em>{\mathbf{w}}\left[\mathbf{w}^{\mathrm{T}}\right]\right] \quad(\because(1.42)) \
= &amp; \operatorname{Tr}\left[\mathbf{S}<em>{N}+\mathbf{m}</em>{N} \mathbf{m}<em>{N}^{\mathrm{T}}\right] \
= &amp;\ \mathbf{m}</em>{N}^{\mathrm{T}} \mathbf{m}<em>{N}+\operatorname{Tr}\left(\mathbf{S}</em>{N}\right)
\end{aligned}
$$</p>
<p>を用いた。</p>
<hr />
<p>$$
\begin{aligned} \mathbb{E}<em>{\alpha}[\ln p(\alpha)] &amp;=\mathbb{E}</em>{\alpha \sim q(\alpha)}\left[\ln \operatorname{Gam}\left(\alpha \mid a_{0}, b_{0}\right)\right] \
&amp;=\mathbb{E}<em>{\alpha \sim q(\alpha)}\left[\ln \left{\frac{1}{\Gamma\left(a</em>{0}\right)} b_{0}^{a_{0}} \alpha^{a_{0}-1} e^{-b_{0} \alpha}\right}\right] \
&amp;=\mathbb{E}<em>{\alpha \sim q(\alpha)}\left[-\ln \Gamma\left(a</em>{0}\right)+a_{0} \ln b_{0}+\left(a_{0}-1\right) \ln \alpha-b_{0} \alpha\right] \
&amp;=a_{0} \ln b_{0}+\left(a_{0}-1\right) \mathbb{E}<em>{\alpha}[\ln \alpha]-b</em>{0} \mathbb{E}<em>{\alpha}[\alpha]-\ln \Gamma\left(a</em>{0}\right) \
&amp;=a_{0} \ln b_{0}+\left(a_{0}-1\right)\left(\psi\left(a_{N}\right)-b_{N}\right)-b_{0} \frac{a_{N}}{b_{N}}-\ln \Gamma\left(a_{0}\right) \end{aligned}
$$</p>
<hr />
<p>$$
\begin{aligned}
-\mathbb{E}<em>{\mathbf{w}}\left[\ln q(\mathbf{w})\right] &amp;=-\mathbb{E}</em>{\mathbf{w} \sim q(\mathbf{w})}\left[\ln \mathcal{N}\left(\mathbf{w} \mid \mathbf{m}<em>{N}, \mathbf{S}</em>{N}\right)\right] \
&amp;=-\mathbb{E}<em>{\mathbf{w} \sim q(\mathbf{w})}\left[\ln \left{\left(\frac{1}{2 \pi}\right)^{\frac{M}{2}} \frac{1}{\left|\mathbf{S}</em>{N}\right|^{\frac{1}{2}}} \exp \left{-\frac{1}{2}\left(\mathbf{w}-\mathbf{m}<em>{N}\right)^{\mathrm{T}} \mathbf{S}</em>{N}^{-1}\left(\mathbf{w}-\mathbf{m}<em>{N}\right)\right}\right]\right.\
&amp;=\frac{M}{2} \ln (2 \pi)+\frac{1}{2} \ln \left|\mathbf{S}</em>{N}\right|+\frac{1}{2} \operatorname{Tr}\left[\mathbb{E}<em>{\mathbf{w}}\left[\left(\mathbf{w}-\mathbf{m}</em>{N}\right)\left(\mathbf{w}-\mathbf{m}<em>{N}\right)^{\mathrm{T}}\right] \mathbf{S}</em>{N}^{-1}\right] \
&amp;=\frac{M}{2} \ln (2 \pi)+\frac{1}{2} \ln \left|\mathbf{S}<em>{N}\right|+\frac{1}{2} \operatorname{Tr}\left[\operatorname{cov}[\mathbf{w}] \mathbf{S}</em>{N}^{-1}\right] \
&amp;=\frac{M}{2} \ln (2 \pi)+\frac{1}{2} \ln \left|\mathbf{S}<em>{N}\right|+\frac{1}{2} M \
&amp;=\frac{1}{2} \ln \left|\mathbf{S}</em>{N}\right|+\frac{M}{2}[1+\ln (2 \pi)]
\end{aligned}
$$</p>
<hr />
<p>$$
\begin{aligned}-\mathbb{E}<em>{\alpha}[\ln q(\alpha)] &amp;=-\mathbb{E}</em>{\alpha \sim q(\alpha)}\left[\ln \operatorname{Gam}\left(\alpha \mid a_{N}, b_{N}\right)\right] \
&amp;=-\mathbb{E}<em>{\alpha \sim q(\alpha)}\left[-\ln \Gamma\left(a</em>{N}\right)+a_{N} \ln b_{N}+\left(a_{N}-1\right) \ln \alpha-b_{N} \alpha\right] \
&amp;=\ln \Gamma\left(a_{N}\right)-a_{N} \ln b_{N}-(a_N - 1)\mathbb{E}<em>{\alpha \sim q(\alpha)}[\ln \alpha]+b</em>{N} \mathbb{E}<em>{\alpha \sim q(\alpha)}[\alpha] \
&amp;=\ln \Gamma\left(a</em>{N}\right)-a_{N} \ln b_{N}-\left(a_{N}-1\right)\left(\psi\left(a_{N}\right)-\ln b_{N}\right)+b_{N} \frac{a_{N}}{b_{N}} \
&amp;=\ln \Gamma\left(a_{N}\right)-\left(a_{N}-1\right) \psi\left(a_{N}\right)-\ln b_{N}+a_{N} \end{aligned}
$$</p>
<h2 id="演習-1028"><a class="header" href="#演習-1028">演習 10.28</a></h2>
<div class="panel-primary">
<p>10.2節で導入したベイズ混合ガウスモデルを10.4節で議論した指数分布族とその共役事前分布のモデルとして書き換えよ．すなわち，一般的な結果
$$\begin{aligned} \ln q^{\star}(\mathbf{Z}) &amp;=\mathbb{E}<em>{\eta}[\ln p(\mathbf{X}, \mathbf{Z} \mid \eta)]+\text { const } \ &amp;=\sum</em>{n=1}^{N}\left{\ln h\left(\mathbf{x}<em>{n}, \mathbf{z}</em>{n}\right)+\mathbb{E}\left[\boldsymbol{\eta}^{\mathrm{T}}\right] \mathbf{u}\left(\mathbf{x}<em>{n}, \mathbf{z}</em>{n}\right)\right}+\text { const } \end{aligned} \tag{10.115}$$
$$q^{\star}(\eta)=f\left(\nu_{N}, \boldsymbol{\chi}<em>{N}\right) g(\eta)^{\nu</em>{N}} \exp \left{\nu_{N} \eta^{\mathrm{T}} \boldsymbol{\chi}<em>{N}\right} \tag{10.119}$$
を用いて，特定の場合の結果
$$q^{\star}(\mathbf{Z})=\prod</em>{n=1}^{N} \prod_{k=1}^{K} r_{n k}^{z_{n k}} \tag{10.48}$$
$$q^{\star}(\boldsymbol{\pi})=\operatorname{Dir}(\boldsymbol{\pi} \mid \boldsymbol{\alpha}) \tag{10.57}$$
$$q^{\star}\left(\boldsymbol{\mu}<em>{k}, \mathbf{\Lambda}</em>{k}\right)=\mathcal{N}\left(\boldsymbol{\mu}<em>{k} \mid \mathbf{m}</em>{k},\left(\beta_{k} \mathbf{\Lambda}<em>{k}\right)^{-1}\right) \mathcal{W}\left(\mathbf{\Lambda}</em>{k} \mid \mathbf{W}<em>{k}, \nu</em>{k}\right) \tag{10.59}$$
を導け．</p>
</div>
<p>ベイズ混合ガウス分布における、指数型分布族のそれぞれの関数形を導出して、一般的な結果(10.115),(10.119)に代入していく。</p>
<p>１変数ガウス分布と、指数型分布族の標準形との対応関係は、演習2.57により(2.220)〜(2.223)のとおり導出済み。
多変数ガウス分布（混合分布ではない）との対応関係は、これを拡張して、$p(\mathbf{x}|\boldsymbol{\eta})=h(\mathbf{x})g(\boldsymbol{\eta})\exp [\boldsymbol{\eta}^T \mathbf{u}(\mathbf{x})]$において、</p>
<p>$$
\begin{aligned}
\boldsymbol{\eta}
:=\left[\begin{array}{c}
\boldsymbol{\eta}_1 \
\boldsymbol{\eta}_2
\end{array}\right]
&amp;\leftrightarrow
\left[\begin{array}{c}
\Lambda \boldsymbol{\mu} \
-\frac{1}{2}\vec{\Lambda}
\end{array}\right] \
\mathbf{u}(\mathbf{x}) &amp;\leftrightarrow \left[\begin{array}{c}
\mathbf{x} \
\mathbf{x}\mathbf{x}^T
\end{array}\right] \
h(\mathbf{x})&amp;\leftrightarrow \frac{1}{(2\pi)^{D/2}}\
g(\boldsymbol{\eta})&amp;\leftrightarrow |-2\boldsymbol\eta _2|^{1/2} \exp \left( \frac{1}{4}\boldsymbol{\eta}_1^T \boldsymbol\eta _2 ^{-1}\boldsymbol\eta _1\right)
\end{aligned}
$$</p>
<p>と書ける。ただし、行列の上に矢印（$\rightarrow$）が書かれているのは、行列の各要素を並べた$D \times D$次元のベクトルを意味する。後の式変形の都合で、$g(\boldsymbol{\eta})$の要素を$\boldsymbol{\eta}$と$\mathbf{u}(\mathbf{x})$に押し込めて、</p>
<p>$$
\begin{aligned}
\boldsymbol{\eta}
&amp;\leftrightarrow
\left[\begin{array}{c}
\Lambda \boldsymbol{\mu} \
-\frac{1}{2}\vec{\Lambda}\
\boldsymbol{\mu}^T \boldsymbol\Lambda \boldsymbol\mu \
\ln |\boldsymbol{\Lambda} |
\end{array}\right] \
\mathbf{u}(\mathbf{x}) &amp;\leftrightarrow \left[\begin{array}{c}
\mathbf{x} \
\mathbf{x}\mathbf{x}^T\
-\frac{1}{2}\
\frac{1}{2}
\end{array}\right] \
h(\mathbf{x})&amp;\leftrightarrow \frac{1}{(2\pi)^{D/2}}\
g(\boldsymbol{\eta})&amp;\leftrightarrow 1
\end{aligned}
$$</p>
<p>と書き直す。</p>
<p>今考えているベイズ混合ガウスモデル:</p>
<p>$$
\begin{aligned}
p(\mathbf{X}, \mathbf{Z}, \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol\Lambda )
&amp;=p(\mathbf{X}| \mathbf{Z}, \boldsymbol{\mu}, \boldsymbol\Lambda )
p(\mathbf{Z}| \boldsymbol{\pi} )
p(\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol\Lambda )\
&amp;=\left(\prod_{n=1}^N \prod_{k=1}^K \pi_k^{z_{nk}}\right)
\left(\prod_{n=1}^N \prod_{k=1}^K \mathcal{N} (\mathbf{x}<em>n|\boldsymbol\mu <em>k,\Lambda_k^{-1})^{z</em>{nk}} \right)
p(\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol\Lambda )\
&amp;=\left(\prod</em>{n=1}^N \prod_{k=1}^K \left{ \pi_k
\mathcal{N} (\mathbf{x}_n|\boldsymbol\mu <em>k,\Lambda_k^{-1})\right}^{z</em>{nk}} \right)
p(\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol\Lambda )\
\end{aligned}
の形に徐々に近づけていく。まずは$\pi_k \mathcal{N} (\mathbf{x}_n|\boldsymbol\mu _k,\Lambda_k^{-1})$を指数型分布族の標準形に対応づけるには、
\begin{aligned}
\boldsymbol{\eta}
&amp;\leftrightarrow
\left[\begin{array}{c}
\Lambda_k \boldsymbol{\mu}_k \
-\frac{1}{2}\vec\Lambda_k\
\boldsymbol{\mu}_k^T \boldsymbol\Lambda_k \boldsymbol\mu_k \
\ln |\boldsymbol{\Lambda}_k|\
\ln\pi_k
\end{array}\right] \
\mathbf{u}(\mathbf{x}_n) &amp;\leftrightarrow \left[\begin{array}{c}
\mathbf{x}_n \
\overrightarrow{\mathbf{x}_n\mathbf{x}_n^T}\
-\frac{1}{2}\
\frac{1}{2}\
1
\end{array}\right] \
h(\mathbf{x}_n)&amp;\leftrightarrow \frac{1}{(2\pi)^{D/2}}\
g(\boldsymbol{\eta})&amp;\leftrightarrow 1
\end{aligned}
次に、$\left{ \pi_k \mathcal{N} (\mathbf{x}_n|\boldsymbol\mu <em>k,\Lambda_k^{-1})\right}^{z</em>{nk}}$を指数型分布族の標準形に対応づけるには、
\begin{aligned}
\boldsymbol{\eta}
&amp;\leftrightarrow
\left[\begin{array}{c}
\Lambda_k \boldsymbol{\mu}_k \
-\frac{1}{2}\vec\Lambda_k\
\boldsymbol{\mu}_k^T \boldsymbol\Lambda_k \boldsymbol\mu_k \
\ln |\boldsymbol{\Lambda}<em>k|\
\ln\pi_k
\end{array}\right] \
\mathbf{u} (\mathbf{x}<em>n,z</em>{nk})&amp;\leftrightarrow z</em>{nk} \left[\begin{array}{c}
\mathbf{x}_n \
\overrightarrow{\mathbf{x}_n\mathbf{x}<em>n^T}\
-\frac{1}{2}\
\frac{1}{2}\
1
\end{array}\right] \
h(\mathbf{x}<em>n,z</em>{nk})&amp;\leftrightarrow \left( \frac{1}{(2\pi)^{D/2}} \right)^{z</em>{nk}}\
g(\boldsymbol{\eta})&amp;\leftrightarrow 1
\end{aligned}
$$</p>
<p>最後に、$\prod_{k=1}^K \left{ \pi_k \mathcal{N} (\mathbf{x}_n|\boldsymbol\mu <em>k,\Lambda_k^{-1})\right}^{z</em>{nk}}$を指数型分布族の標準形に対応づけるには、</p>
<p>$$
\begin{aligned}
\boldsymbol{\eta}
&amp;\leftrightarrow
\left[\begin{array}{c}
\Lambda_k \boldsymbol{\mu}_k \
-\frac{1}{2}\vec\Lambda_k\
\boldsymbol{\mu}_k^T \boldsymbol\Lambda_k \boldsymbol\mu_k \
\ln |\boldsymbol{\Lambda}_k|\
\ln\pi_k
\end{array}\right] <em>{k=1,\cdots,K}\
\mathbf{u} (\mathbf{x}<em>n,\mathbf{z}</em>{n})&amp;\leftrightarrow
\left[ z</em>{nk} \left[\begin{array}{c}
\mathbf{x}_n \
\overrightarrow{\mathbf{x}_n\mathbf{x}_n^T}\
-\frac{1}{2}\
\frac{1}{2}\
1
\end{array}
\right] \right] _{k=1,\cdots,K}\
h(\mathbf{x}<em>n,\mathbf{z}<em>n)&amp;\leftrightarrow \prod</em>{k=1}^K \left( \frac{1}{(2\pi)^{D/2}} \right)^{z</em>{nk}}\
g(\boldsymbol{\eta})&amp;\leftrightarrow 1
\end{aligned}
$$</p>
<p>ここで、$[\ \ \ ]_{k=1,\cdots,K}$とは、$k=1$に対応するベクトル、$k=2$に対応するベクトル$\cdots$と順に並べてできる長いベクトルを表す。</p>
<p>今得られた対応関係を(10.116)式に代入して、</p>
<p>$$
\begin{aligned}
q^\star (\mathbf{z}_n)
&amp;= h(\mathbf{x}_n, \mathbf{z}<em>n) g(\mathbb{E}[\boldsymbol\eta])\exp {\mathbb{E} [\boldsymbol{\eta}^T] \mathbf{u}(\mathbf{x}<em>n , \mathbf{z}<em>n )}\
&amp;= \left{ \prod</em>{k=1}^K \left(\frac{1}{(2\pi)^{D/2}}\right)^{z</em>{nk}}\right}
\cdot 1 \cdot
\exp \left{ \sum</em>{k=1}^K
\left( \mathbb{E}\left[\begin{array}{c}
\Lambda_k \boldsymbol{\mu}<em>k \
-\frac{1}{2}\vec\Lambda_k\
\boldsymbol{\mu}<em>k^T \boldsymbol\Lambda_k \boldsymbol\mu_k \
\ln |\boldsymbol{\Lambda}<em>k|\
\ln\pi_k
\end{array}\right]
\right)^T \left( z</em>{nk} \left[\begin{array}{c}
\mathbf{x}<em>n \
\overrightarrow{\mathbf{x}<em>n\mathbf{x}<em>n^T}\
-\frac{1}{2}\
\frac{1}{2}\
1
\end{array}
\right]
\right)
\right}\
&amp;= \left{ \prod</em>{k=1}^K \left(\frac{1}{(2\pi)^{D/2}}\right)^{z</em>{nk}}\right}
\exp \left{ \sum</em>{k=1}^K z</em>{nk}
\cdot\mathbb{E}</em>{\boldsymbol\mu_k , \boldsymbol\Lambda_k}\left[
\boldsymbol{\mu}_k^T \Lambda_k
\mathbf{x}_n
-\frac{1}{2}\mathbf{x}_n^T\Lambda_k
\mathbf{x}_n
-\frac{1}{2}
\boldsymbol{\mu}<em>k^T \boldsymbol\Lambda_k \boldsymbol\mu_k
+\frac{1}{2}
\ln |\boldsymbol{\Lambda}<em>k|
+\ln\pi_k
\right]
\right}\
&amp;= \prod</em>{k=1}^K \left(\frac{1}{(2\pi)^{D/2}}
\exp \left{ -\frac{1}{2}
\mathbb{E}</em>{\boldsymbol\mu_k , \boldsymbol\Lambda_k}\left[
(\mathbf{x}_n-\boldsymbol{\mu}_k)^T \Lambda_k  (\mathbf{x}<em>n-\boldsymbol{\mu}<em>k)\right]
+\frac{1}{2}
\mathbb{E}[\ln |\boldsymbol{\Lambda}<em>k|]
+\mathbb{E}[\ln\pi_k]
\right}
\right)^{z</em>{nk}}\
&amp;= \prod</em>{k=1}^K \left(
\exp \left{ - \frac{D}{2}\ln (2\pi)-\frac{1}{2}
\mathbb{E}</em>{\boldsymbol\mu_k , \boldsymbol\Lambda_k}\left[
(\mathbf{x}_n-\boldsymbol{\mu}_k)^T \Lambda_k  (\mathbf{x}_n-\boldsymbol{\mu}<em>k)\right]
+\frac{1}{2}
\mathbb{E}[\ln |\boldsymbol{\Lambda}<em>k|]
+\mathbb{E}[\ln\pi_k]
\right}
\right)^{z</em>{nk}}\
&amp;=\prod</em>{k=1}^K \rho <em>{nk}^{z</em>{nk}}
\end{aligned}
$$</p>
<p>を得る。（最後の式変形は、(10.46)式の$\rho _{nk}$の定義より。）以上より、</p>
<p>$$
\begin{aligned}
q^\star (\mathbf{Z})
&amp;= \prod_{n=1}^{N} q^\star (\mathbf{z}<em>n)\
&amp;= \prod</em>{n=1}^{N}\prod_{k=1}^K \rho <em>{nk}^{z</em>{nk}}
\end{aligned}
$$</p>
<p>と(10.48)式を得る。</p>
<hr />
<p>次に、(10.119)式を用いて(10.57)式と(10.59)式を導く。</p>
<p>$$
\begin{aligned}
q^\star (\boldsymbol\eta )
&amp;\propto g(\boldsymbol\eta)^{\nu_N} \exp \left[ \nu <em>N \boldsymbol \eta^T \boldsymbol{\chi} <em>N \right]\
&amp;= 1 \cdot \exp \left[ \boldsymbol\eta^T \left( \nu_0\boldsymbol\chi_0+\sum</em>{n=1}^N \mathbb{E}</em>{z_n}[\mathbf{u}(\mathbf{x}<em>n,\mathbf{z}<em>n)]\right)\right]\
&amp;= \exp \left(
\nu_0\boldsymbol\eta^T \boldsymbol\chi_0
+\sum</em>{n=1}^N\sum</em>{k=1}^K \mathbb{E}[z_{nk}] \left[\begin{array}{c}
\Lambda_k \boldsymbol{\mu}_k \
-\frac{1}{2}\vec\Lambda_k\
\boldsymbol{\mu}_k^T \boldsymbol\Lambda_k \boldsymbol\mu_k \
\ln |\boldsymbol{\Lambda}_k|\
\ln\pi_k
\end{array}\right]
^T \left[\begin{array}{c}
\mathbf{x}<em>n \
\overrightarrow{\mathbf{x}<em>n\mathbf{x}<em>n^T}\
-\frac{1}{2}\
\frac{1}{2}\
1
\end{array}
\right]
\right)\
&amp;= \exp \left[
\nu_0\boldsymbol\eta^T \boldsymbol\chi_0
+\sum</em>{n=1}^N\sum</em>{k=1}^K r</em>{nk} \left(
\boldsymbol{\mu}_k^T \Lambda_k
\mathbf{x}_n
-\frac{1}{2}\mathbf{x}_n^T\Lambda_k
\mathbf{x}<em>n
-\frac{1}{2}
\boldsymbol{\mu}<em>k^T \boldsymbol\Lambda_k \boldsymbol\mu_k
+\frac{1}{2}
\ln |\boldsymbol{\Lambda}<em>k|
+\ln\pi_k
\right)
\right]\
&amp;= \exp \left[
\nu_0\boldsymbol\eta^T \boldsymbol\chi_0
+\sum</em>{n=1}^N\sum</em>{k=1}^K r</em>{nk} \left( -\frac{1}{2}
(\mathbf{x}_n-\boldsymbol{\mu}_k)^T \Lambda_k  (\mathbf{x}_n-\boldsymbol{\mu}_k)
+\frac{1}{2}
\ln |\boldsymbol{\Lambda}_k|
+\ln\pi_k
\right)
\right]
\end{aligned}
$$</p>
<p>事前確率分布の項$\exp\left[\nu_0\boldsymbol\eta^T\boldsymbol\chi_0\right]$は、図10.5の事前確率分布</p>
<p>$$
\begin{aligned}
p(\boldsymbol\pi,\boldsymbol\mu,\boldsymbol\Lambda)
&amp;= p(\boldsymbol\pi)p(\boldsymbol\mu|\boldsymbol\Lambda)p(\boldsymbol\Lambda)\
&amp;\propto \prod_{k=1}^K \pi_k ^{\alpha_0-1}
|\boldsymbol\Lambda_k |^{1/2}\exp \left[-\frac{1}{2}
(\boldsymbol\mu_k-\mathbf{m}_0)^T (\beta_0\boldsymbol\Lambda_k)
(\boldsymbol\mu_k-\mathbf{m}_0)\right]
|\boldsymbol\Lambda_k|^{(\nu_0-D-1)/2}\exp\left(-\frac{1}{2}{\rm Tr}(\mathbf{W}<em>0^{-1}\boldsymbol\Lambda_k) \right)\
&amp;=
\exp \left[\sum</em>{k=1}^K\left{
(\alpha_0-1)\ln \pi_k
-\frac{1}{2}
(\boldsymbol\mu_k-\mathbf{m}_0)^T (\beta_0\boldsymbol\Lambda_k)
(\boldsymbol\mu_k-\mathbf{m}_0)
+\frac{\nu_0-D}{2}
\ln
|\boldsymbol\Lambda_k|
-\frac{1}{2}{\rm Tr}(\mathbf{W}_0^{-1}\boldsymbol\Lambda_k) \right}\right]
\end{aligned}
$$</p>
<p>に一致するように$\nu_0\boldsymbol\chi_0$を選ぶことは可能（指数型分布族の事前共役分布の形を所与とすれば、この事実は証明不要な気もするが、念のため本問の解答の最後で$\nu_0\boldsymbol\chi_0$の具体的な形を構築する）。</p>
<p>$$
\begin{aligned}
q^
\star (\boldsymbol\eta )
\propto &amp;
\exp \left[\sum_{k=1}^K\left{
(\alpha_0-1)\ln \pi_k
-\frac{1}{2}
(\boldsymbol\mu_k-\mathbf{m}<em>0)^T (\beta_0\boldsymbol\Lambda_k)
(\boldsymbol\mu_k-\mathbf{m}<em>0)
+\frac{\nu_0-D}{2}
\ln
|\boldsymbol\Lambda_k|
-\frac{1}{2}{\rm Tr}(\mathbf{W}<em>0^{-1}\boldsymbol\Lambda_k) \right}\right]\
&amp;\cdot \exp \left[
\sum</em>{n=1}^N\sum</em>{k=1}^K r</em>{nk} \left( -\frac{1}{2}
(\mathbf{x}_n-\boldsymbol{\mu}_k)^T \Lambda_k  (\mathbf{x}_n-\boldsymbol{\mu}_k)
+\frac{1}{2}
\ln |\boldsymbol{\Lambda}_k|
+\ln\pi_k
\right)
\right]
\end{aligned}
$$</p>
<p>この同時確率分布は$\boldsymbol\pi$に依存する項だけ積の形でくくり出せて、</p>
<p>$$
\begin{aligned}
q^\star (\boldsymbol\eta )
&amp;\propto
\exp \left[\sum_{k=1}^K\left{
(\alpha_0-1)\ln \pi_k
+\sum_{n=1}^N r_{nk}
\ln\pi_k
\right}
\right]\
&amp;=\exp \left[\sum_{k=1}^K\left{
(\alpha_0+N_k-1)\ln \pi_k
\right}
\right]\
&amp;=\prod_{k=1}^K \pi_k^{\alpha_0+N_k-1}
\end{aligned}
$$
であり、(10.57)式のディリクレ分布が導かれた。</p>
<p>残りの項のうち、$\boldsymbol\mu_k$に依存する項のみをくくり出すと、
$$
\begin{aligned}
q^\star (\boldsymbol\eta )
\propto \ &amp;
\exp \left[
-\frac{1}{2}\left{ \sum_{k=1}^K \boldsymbol\mu_k^T \left(\beta_0 \boldsymbol\Lambda_k + \sum_{n=1}^N r_{nk}\boldsymbol\Lambda_k\right)\boldsymbol\mu_k
-2\left(\boldsymbol\mu_k^T\beta_0\boldsymbol\Lambda_k \mathbf{m}<em>0 + \sum</em>{n=1}^N r_{nk} \boldsymbol\mu_k^T \Lambda_k \mathbf{x}<em>n
\right)
\right}
\right]\
=:\ &amp;\exp \left[
-\frac{1}{2}\left{ \sum</em>{k=1}^K \boldsymbol\mu_k^T \left((\beta_0+N_k) \boldsymbol\Lambda_k \right)\boldsymbol\mu_k
-2\boldsymbol\mu_k^T\left(\boldsymbol\Lambda_k (\beta_0 \mathbf{m}<em>0 + N</em>{k} \overline{\mathbf{x}<em>k}
) \right)
\right}
\right]\
= \ &amp;|\boldsymbol\Lambda_k|^{1/2} \exp \left[
-\frac{1}{2} \sum</em>{k=1}^K \left( \boldsymbol\mu_k  - \frac{\beta_0\mathbf{m}_0+N_k\overline{\mathbf{x}_k}}{\beta_0+N_k}\right)^T \left((\beta_0+N_k) \boldsymbol\Lambda_k \right)\left( \boldsymbol\mu_k  - \frac{\beta_0\mathbf{m}_0+N_k\overline{\mathbf{x}_k}}{\beta_0+N_k}\right)
\right]
\
&amp; \cdot |\boldsymbol\Lambda_k|^{-1/2}\exp \left[ \frac{1}{2} \frac{1}{\beta_0+N_k} (\beta_0\mathbf{m}_0+N_k\overline{\mathbf{x}_k})^T\Lambda_k(\beta_0\mathbf{m}_0+N_k\overline{\mathbf{x}_k})\right]\
\end{aligned}
$$</p>
<p>１行目は、$\boldsymbol\mu_k$が(10.59)式のガウス分布に従うことを表す。</p>
<p>最後に、その他の項をくくり出すと、
$$
\begin{aligned}
q^\star (\boldsymbol\eta )
\propto &amp;
\exp \left[\sum_{k=1}^K\left{
-\frac{1}{2}
\mathbf{m}_0^T \beta_0\boldsymbol\Lambda_k
\mathbf{m}<em>0
+\frac{\nu_0-D}{2}
\ln
|\boldsymbol\Lambda_k|
-\frac{1}{2}{\rm Tr}(\mathbf{W}<em>0^{-1}\boldsymbol\Lambda_k)
+\sum</em>{n=1}^N r</em>{nk} \left( -\frac{1}{2}
\mathbf{x}_n^T \Lambda_k  \mathbf{x}_n
+\frac{1}{2}
\ln |\boldsymbol{\Lambda}_k|
\right)
\right}
\right]
\ \cdot &amp; |\boldsymbol\Lambda_k|^{-1/2}\exp \left[ \frac{1}{2} \frac{1}{\beta_0+N_k} (\beta_0\mathbf{m}_0+N_k\overline{\mathbf{x}_k})^T\Lambda_k(\beta_0\mathbf{m}_0+N_k\overline{\mathbf{x}<em>k})\right]\
=&amp; \prod</em>{k=1}^K
|\boldsymbol\Lambda_k|^{\frac{\nu_0+N_k-D-1}{2}}
\exp \left[
-\frac{1}{2}
\left(
\mathbf{m}_0^T \beta_0\boldsymbol\Lambda_k
\mathbf{m}<em>0
+{\rm Tr}(\mathbf{W}<em>0^{-1}\boldsymbol\Lambda_k)
+\sum</em>{n=1}^N r</em>{nk}
\mathbf{x}_n^T \Lambda_k  \mathbf{x}_n
-\frac{1}{\beta_0+N_k} (\beta_0\mathbf{m}_0+N_k\overline{\mathbf{x}_k})^T\Lambda_k(\beta_0\mathbf{m}_0+N_k\overline{\mathbf{x}<em>k})
\right)
\right]\
=&amp; \prod</em>{k=1}^K
|\boldsymbol\Lambda_k|^{\frac{\nu_0+N_k-D-1}{2}}
\exp \left[
-\frac{1}{2}
\left{
{\rm Tr} \left( \mathbf{m}_0 \mathbf{m}<em>0^T \beta_0\boldsymbol\Lambda_k
\right)
+{\rm Tr}\left(\mathbf{W}<em>0^{-1}\boldsymbol\Lambda_k\right)
+{\rm Tr} \left( \sum</em>{n=1}^N r</em>{nk}
\mathbf{x}_n \mathbf{x}_n^T \Lambda_k  \right)
-\frac{1}{\beta_0+N_k} {\rm Tr}\left(
(\beta_0\mathbf{m}_0+N_k\overline{\mathbf{x}_k})
(\beta_0\mathbf{m}_0+N_k\overline{\mathbf{x}<em>k})^T\Lambda_k
\right)
\right}
\right]\
=&amp; \prod</em>{k=1}^K
|\boldsymbol\Lambda_k|^{\frac{\nu_0+N_k-D-1}{2}}
\exp \left[
-\frac{1}{2}
{\rm Tr}\left{ \left( \beta_0 \mathbf{m}_0 \mathbf{m}<em>0^T
+\mathbf{W}<em>0^{-1} + \sum</em>{n=1}^N r</em>{nk}
\mathbf{x}_n \mathbf{x}_n^T -\frac{1}{\beta_0+N_k}
(\beta_0\mathbf{m}_0+N_k\overline{\mathbf{x}_k})
(\beta_0\mathbf{m}_0+N_k\overline{\mathbf{x}_k})^T
\right)\Lambda_k
\right}
\right]
\
\end{aligned}
$$
となる。最後の式は、$|\Lambda|^{<em>}\exp [-\frac{1}{2}$Tr$(</em>\Lambda)]$という形をしており、(10.59)式のウィシャート分布を得る。（一見すると(10.62)式の形と異なるように見えるが、$\mathbf{S}_k$の定義に戻って計算すると、上式と一致することが示せる。</p>
<hr />
<p><strong>おまけ：</strong>
${\nu}_0\boldsymbol\chi_0$の具体的な形状は、以下の通り。
$$
\begin{aligned}
{\nu}_0\boldsymbol\chi_0
&amp;=
\left[\begin{array}{c}
\beta_0 \mathbf{m}_0 \
\beta_0 \overrightarrow{\mathbf{m}_0 \mathbf{m}_0^T}+\overrightarrow{\mathbf{W}_0^{-1}}\
-\frac{1}{2}\beta_0\
\frac{\nu_0-D-1}{2}\
\alpha_0-1
\end{array}\right] _{k=1,\cdots,K}
\end{aligned}
$$
念のため、上の式を再現できることを確認しておく。
$$
\begin{aligned}
{\nu}_0 \boldsymbol{\eta} ^T
\boldsymbol\chi_0
&amp;=
\left[\begin{array}{c}
\Lambda_k \boldsymbol{\mu}_k \
-\frac{1}{2}\vec\Lambda_k\
\boldsymbol{\mu}_k^T \boldsymbol\Lambda_k \boldsymbol\mu_k \
\ln |\boldsymbol{\Lambda}_k|\
\ln\pi_k
\end{array}\right] _{k=1,\cdots,K}^T
\left[\begin{array}{c}
\beta_0 \mathbf{m}_0 \
\beta_0 \overrightarrow{\mathbf{m}_0 \mathbf{m}_0^T}+\overrightarrow{\mathbf{W}_0^{-1}}\
-\frac{1}{2}\beta_0\
\frac{\nu_0-D-1}{2}\
\alpha_0-1
\end{array}\right] <em>{k=1,\cdots,K}\
&amp;=
\sum</em>{k=1}^K\left{
(\alpha_0-1)\ln \pi_k
-\frac{1}{2}
(\boldsymbol\mu_k-\mathbf{m}_0)^T (\beta_0\boldsymbol\Lambda_k)
(\boldsymbol\mu_k-\mathbf{m}_0)
+\frac{\nu_0-D-1}{2}
\ln
|\boldsymbol\Lambda_k|
-\frac{1}{2}{\rm Tr}(\mathbf{W}_0^{-1}\boldsymbol\Lambda_k) \right}
\end{aligned}
$$
となる。なお、最後のTr$(\mathbf{W}_0^{-1}\boldsymbol\Lambda_k)$の項への変形にあたって、一般の正方行列$A,B$について</p>
<p>$$
\begin{aligned}
{\rm Tr}[AB]={\rm Tr}\left[\left( \sum_j a_{ij}b_{jk}\right)<em>{ik} \right]
=\sum</em>{i, j}a_{ij}b_{ji}
\end{aligned}
$$</p>
<p>が成り立つこと、および$B$が対称行列であれば最右辺が$\sum_{i,j} a_{ij}b_{ij}$に一致することを用いた。</p>
<h2 id="演習-1029"><a class="header" href="#演習-1029">演習 10.29</a></h2>
<div class="panel-primary">
<p>二階微分を計算することで，関数$f(x) = \ln (x)$は$0 \lt x \lt \infty$で上に凸であることを示せ．
$$g(\eta)=\min _{x}{\eta x-f(x)} \tag{10.133}$$
で定義される双対な関数$g(\eta)$の形を求め，
$$f(x)=\min _{\eta}{\eta x-g(\eta)} \tag{10.132}$$
に従って$\eta x - g(\eta)$を$\eta$について最小化すると，実際に関数$\ln(x)$が得られることを確かめよ．</p>
</div>
<p>※P.209のように$f(x)$と$g(\eta)$が双対の働きとなっていることを示す。</p>
<p>$f(x) = \ln(x)$の2階微分は$\displaystyle f''(x) = - \frac{1}{x^{2}}$である。これは$0 \lt x \lt \infty$で$f''(x) &lt; 0$となるので上に凸である。</p>
<p>$(10.133)$式に代入して</p>
<p>$$
g(\eta)=\min _{x}{\eta x-\ln(x)}
$$</p>
<p>最小値を求めるため$x$について微分すると</p>
<p>$$
\frac{dg}{dx} = \eta - \frac{1}{x}
$$</p>
<p>$\displaystyle x=\frac{1}{\eta}$のとき最小となり、このとき$g(\eta) = 1+\ln(\eta)$となる。</p>
<p>今度はこれを$(10.132)$式に代入して$\displaystyle f(x) = \min _{\eta}{\eta x-1-\ln(\eta)}$となる。これも同様に$\eta$について微分すると</p>
<p>$$
\frac{df}{d\eta} = x - \frac{1}{\eta}
$$</p>
<p>これは$\displaystyle \eta = \frac{1}{x}$のとき最小となり、このとき</p>
<p>$$
f(x) = 1-\left( 1+\ln\left(\frac{1}{x}\right) \right) = \ln(x)
$$</p>
<p>となり題意通り$\ln(x)$が得られた。</p>
<h2 id="演習-1030"><a class="header" href="#演習-1030">演習 10.30</a></h2>
<div class="panel-primary">
<p>二階微分を計算することで，対数ロジスティック関数$f(x)=-\ln \left(1+e^{-x}\right)$が上に凸であることを示せ．対数ロジスティック関数を点$x = \xi$のまわりで一次までテイラー展開することで，変分上界
$$\sigma(x) \leqslant \exp (\eta x-g(\eta)) \tag{10.137}$$
を直接導出せよ．</p>
</div>
<p>後の問題設定のため、対数ロジスティック関数である$f(x)$を</p>
<p>$$
\sigma(x) = \frac{1}{1+e^{-x}} \tag{10.134}
$$</p>
<p>を用いて</p>
<p>$$
f(x)=-\ln \left(1+e^{-x}\right)=\ln \left(\frac{1}{1+e^{-x}}\right)=\ln \sigma(x)
$$</p>
<p>と表しておく。これより</p>
<p>$$
\begin{aligned}
\frac{d \sigma}{d x} &amp;=\frac{e^{-x}}{\left(1+e^{-x}\right)^{2}} = (\sigma(x))^2e^{-x} \
&amp;=\frac{1}{1+e^{-x}}-\frac{1}{\left(1+e^{-x}\right)^2} \
&amp;=\sigma(x)-(\sigma(x))^{2} \
&amp;=\sigma(x)(1-\sigma(x))
\end{aligned}
$$</p>
<p>である。これから$f(x)$が上に凸であること、すなわち$f''(x) \lt 0$を示す。</p>
<p>$$
f^{\prime}(x)=\frac{d}{d x} \ln \sigma(x)=\frac{1}{\sigma(x)} \frac{d}{d x} \sigma(x)=\frac{1}{\sigma(x)}(\sigma(x))^{2} e^{-x}=\sigma(x) e^{-x}
$$</p>
<p>$$
\begin{aligned} f^{\prime \prime}(x) &amp;=\frac{d}{d x}\left(\sigma(x) e^{-x}\right) \
&amp;=\frac{d}{d x} \sigma(x) \cdot e^{-x}-\sigma(x) e^{-x} \
&amp;={\sigma(x)(1-\sigma(x))-\sigma(x)} e^{-x} \
&amp;=-{\sigma(x)}^{2} e^{-x} \lt 0 \end{aligned}
$$</p>
<p>これより$f(x)$は上に凸であることが示された。</p>
<p>次に$\ln \sigma(x)$を$x=\xi$の周りで一次のテイラー展開を行うと</p>
<p>$$
\begin{aligned} \ln \sigma(x) &amp;=\ln \sigma(\xi)+\left.\frac{d\ln \sigma(x)}{d x}\right|_{x=\xi}(x-\xi)+O\left(\xi^{2}\right) \
&amp;\simeq \ln \sigma(\xi)+\sigma(\xi) e^{-\xi}(x-\xi) \end{aligned}
$$</p>
<p>$\ln \sigma(x)$は上に凸（concave）な関数なので、このテイラー展開は</p>
<p>$$
\ln \sigma(x) \leq \ln \sigma(\xi)+\sigma(\xi) e^{-\xi}(x-\xi) \tag{A}
$$</p>
<p>となる。</p>
<p>変分上界を求めるために、10.5節の議論のように（$(10.127)$あたりの変形）$\eta = \sigma(\xi)e^{-\xi}$とおいたときの$\sigma(\xi)$と$\xi$を$\eta$で表すことを目指す。</p>
<p>$$
\begin{aligned} \eta &amp;=\left(1+e^{-\xi}\right)^{-1} e^{-\xi} \
&amp;=\frac{e^{-\xi}}{1+e^{-\xi}} \
&amp;=1-\frac{1}{1+e^{-\xi}} \
&amp;=1-\sigma(\xi) \end{aligned}
$$</p>
<p>これより$\sigma(\xi) = 1 - \eta$となる。また、$\eta = \sigma(\xi)e^{-\xi}$の両辺の対数をとって</p>
<p>$$
\begin{aligned} \ln \eta &amp;=\ln \sigma(\xi)-\xi \ \xi &amp;=\ln \sigma(\xi)-\ln \eta \
&amp;=\ln (1-\eta)-\ln \eta \end{aligned}
$$</p>
<p>となる。これらを$(\textrm{A})$に代入して</p>
<p>$$
\begin{aligned} \ln \sigma(x) &amp; \leq \ln (1-\eta)+\eta(x-\ln (1-\eta)+\ln \eta) \
&amp;=\eta x+(1-n) \ln (1-\eta)+\eta \ln \eta \
&amp;=\eta x-g(\eta)\hspace{1em}(\because(10.136))
\end{aligned}
$$</p>
<p>これは変分上界</p>
<p>$$
\sigma(x) \leqslant \exp (\eta x-g(\eta)) \tag{10.137}
$$</p>
<p>と等価である。すなわち、題意の通りテイラー展開から直接$(10.137)$式が導出された。</p>
<h2 id="演習-1031"><a class="header" href="#演習-1031">演習 10.31</a></h2>
<div class="panel-primary">
<p>$x$について二階微分を求めることで，関数$f(x) = -\ln(e^{x/2}+e^{-x/2})$は$x$の上に凸な関数であることを示せ．次に，変数$x^2$についての二階微分を考え，これが$x^2$については下に凸な関数で、あることを示せ．$f(x)$のグラフを$x$および$x^2$について描いてみよ．関数$f(x)$を変数$x^2$について$\xi^2$を中心として一次までテイラー展開することにより，ロジスティックシグモイド関数の下界
$$\sigma(x) \geqslant \sigma(\xi) \exp \left{(x-\xi) / 2-\lambda(\xi)\left(x^{2}-\xi^{2}\right)\right} \tag{10.144}$$
を直接導出せよ．</p>
</div>
<p>微分を計算する時の係数が煩わしいので、$x/2$を$x$と定義し直す。（証明すべき凸性に影響はない。最後の計算結果で元に戻す。）</p>
<ul>
<li>
<p>$x$に関して上に凸であることの証明
$$
\begin{aligned}
f(x) &amp;= -\ln(e^{x}+e^{-x})\
f'(x) &amp;= -\frac{e^{x}-e^{-x}}{e^x+e^{-x}}\
f''(x) &amp;= -\frac{(e^{x}+e^{-x})^2-(e^{x}-e^{-x})^2}{(e^{x}+e^{-x})^2}\
&amp;= -\frac{(2e^x)\cdot (2e^{-x})}{(e^{x}+e^{-x})^2}\
&amp;= -\frac{4}{(e^{x}+e^{-x})^2} &lt; 0
\end{aligned}
$$</p>
</li>
<li>
<p>$x^2$に関して下に凸であることの証明
$y=x^2$とおく。$f(y)=-\ln (e^{\sqrt{y}}+e^{-\sqrt{y}})$を真面目に$y$で2階微分するのは面倒なので、以下のように解く。
$$
\begin{aligned}
\frac{d}{dy}f(x) &amp;= \frac{dx}{dy}\frac{df(x)}{dx}\
\frac{d^2}{dy^2}f(x) &amp;= \frac{d}{dy}\left( \frac{dx}{dy} \frac{df(x)}{dx} \right)\
&amp;= \frac{d^2x}{dy^2}\frac{df(x)}{dx} + \frac{dx}{dy} \frac{d}{dy} \left( \frac{df(x)}{dx} \right)\
&amp;= \frac{d^2x}{dy^2}\frac{df(x)}{dx} + \left( \frac{dx}{dy} \right)^2 \cdot \frac{d^2 f(x)}{dx^2}\
\end{aligned}
$$</p>
</li>
</ul>
<p>ここで、</p>
<p>$$
\begin{aligned}
\frac{dx}{dy} &amp;= \left( \frac{dy}{dx} \right)^{-1} = \left( \frac{d}{dx} x^2 \right)^{-1} = (2x)^{-1} = \frac{1}{2x} \
\frac{d^2x}{dy^2} &amp;= \frac{d}{dy} \left( \frac{dx}{dy} \right) = \frac{d}{dy} \left( \frac{1}{2x} \right)= \frac{dx}{dy}\frac{d}{dx} \left( \frac{1}{2x} \right)
= \frac{1}{2x} \left( -\frac{1}{2x^2}\right) = -\frac{1}{4x^3}\
\end{aligned}
$$</p>
<p>であるから、</p>
<p>$$
\begin{aligned}
\frac{d^2}{dy^2}f(x) &amp;=  \frac{d^2x}{dy^2}\frac{df(x)}{dx} + \left( \frac{dx}{dy} \right)^2 \cdot \frac{d^2 f(x)}{dx^2}\
&amp;= \left(-\frac{1}{4x^3} \right)\left(-\frac{e^x-e^{-x}}{e^x+e^{-x}} \right) + \left( \frac{1}{2x}\right)^2 \left( - \frac{4}{(e^x+e^{-x})^2}\right)\
&amp;=\frac{(e^x-e^{-x})(e^x+e^{-x})-4x}{4x^3 (e^x+e^{-x})^2}\
&amp;=\frac{e^{2x}-e^{-2x}-4x}{4x^3 (e^x+e^{-x})^2}
\end{aligned}
$$
ここで、分子が$x&gt;0$のとき正、$x&lt;0$のとき負であることは、微分して普通に示しても良いが、</p>
<p>$$
\begin{aligned}
e^{2x} &amp;= 1+(2x)+ \frac{1}{2!}(2x)^2+\frac{1}{3!}(2x)^3+\frac{1}{4!}(2x)^4 +\frac{1}{5!}(2x)^5 +\cdots\
e^{-2x} &amp;= 1-(2x)+ \frac{1}{2!}(2x)^2-\frac{1}{3!}(2x)^3+\frac{1}{4!}(2x)^4 -\frac{1}{5!}(2x)^5 +\cdots\
e^{2x}-e^{-2x} &amp;= 4x+ 2\cdot \left( \frac{1}{3!}(2x)^3+\frac{1}{5!}(2x)^5\cdots \right)\
\end{aligned}
$$
に注目すれば明らか。よって、右辺全体は$x&gt;0$でも$x&lt;0$でも正なので、$\frac{d^2}{dy^2}f(x)&gt;0$と言える。</p>
<ul>
<li>
<p>グラフを書いてみよ
$f(x)$のグラフを$x$について描く
<img src="https://i.imgur.com/K697SEc.jpg" alt="" />
$f(x)$のグラフを$x^2$について描く
<img src="https://i.imgur.com/Kg2aNfs.jpg" alt="" /></p>
</li>
<li>
<p>下界(10.44)を直接導出する（$x$の定義は元に戻す）
$$
\begin{aligned}
f(x)&amp;=f(x)|<em>{x=\xi} + (x^2-\xi^2) \left( \left. \frac{df(x)}{dy} \right|</em>{x=\xi} \right)+ \cdots\
&amp;\geq f(x)|<em>{x=\xi} + (x^2-\xi^2) \left(\left. \frac{df(x)}{dy} \right|</em>{x=\xi}\right)\
&amp;= f(\xi) + (x^2-\xi^2)\left( \frac{dx}{dy} \left.\frac{f(x)}{dx}\right|_{x=\xi}\right)\
&amp;= f(\xi) + (x^2-\xi^2)\left( -\frac{1}{4\xi} \tanh(\xi) \right)\
&amp;\equiv f(\xi) - \lambda(\xi) (x^2-\xi^2)\
\end{aligned}
$$</p>
</li>
</ul>
<p>となる（不等号は、$x^2$に関する凸性より）。従って、</p>
<p>$$
\begin{aligned}
\ln \sigma(x) &amp;= \frac{x}{2} + f(x)\
&amp;\geq \frac{x}{2} + f(\xi) - \lambda(\xi) (x^2-\xi^2)\
&amp;= \frac{x-\xi}{2} + \left( \frac{\xi}{2}+f(\xi)\right) - \lambda(\xi) (x^2-\xi^2)\
&amp;= \frac{x-\xi}{2} + \ln \sigma (\xi) - \lambda(\xi) (x^2-\xi^2)\
\end{aligned}
$$
両辺のexpをとって、(10.144)式を得る。</p>
<h2 id="演習-1032"><a class="header" href="#演習-1032">演習 10.32</a></h2>
<div class="panel-primary">
<p>ロジスティック回帰の変分ベイズ推定を時系列に従って学習し，データ点が一回に一つだけ到着し，次のデータ点が到着するまでに処理して廃棄しなければならない場合について考える．このとき，事後分布のガウス近似は下界
$$p(t \mid \mathbf{w})=e^{a t} \sigma(-a) \geqslant e^{a t} \sigma(\xi) \exp \left{-(a+\xi) / 2-\lambda(\xi)\left(a^{2}-\xi^{2}\right)\right} \tag{10.151}$$
を用いることで常に保持できることを示せ．この場合，分布は事前分布で初期化され，データ点が取り込まれるごとに。対応する変分パラメータ$\xi_n$が最適化される．</p>
</div>
<p>※(方針)
(10.151)式を利用しガウス変分事後分布の逐次的な更新式が得られることを確認する</p>
<p>データN個のときの変分事後分布を</p>
<p>$$
q_N(\mathbf{w})=\mathcal{N}(\mathbf{w}\mid\mathbf{m}_N,\mathbf{S}_N^{-1})
$$</p>
<p>とする．N+1個めのデータが与えられたとき(10.151)より</p>
<p>$$
p(t_{N+1}|,\mathbf{w})=e^{\mathbf{w}^{\top} \phi_{N+1} t_{N+1}} \sigma\left(-\mathbf{w}^{\top} \phi_{N+1}\right) \geq e^{\mathbf{w}^{\top} \phi_{N+1} t_{N+1}} \sigma\left(\xi_{N+1}\right) \exp \left[-\left(\mathbf{w}^{\top} \phi_{N+1}+\xi_{N+1}\right) / 2-\lambda\left(\xi_{N+1}\right)\left{\left(\mathbf{w}^{\top} \phi_{N+1}\right)^{2}-\xi_{N+1}^{2}\right}\right].
$$</p>
<p>この式から</p>
<p>$$
p(t_{N+1},\mathbf{w})=p(t_{N+1}\mid\mathbf{w})p(\mathbf{w})\geq p(\mathbf{w})e^{\mathbf{w}^{\top} \phi_{N+1} t_{N+1}} \sigma\left(\xi_{N+1}\right) \exp \left[-\left(\mathbf{w}^{\top} \phi_{N+1}+\xi_{N+1}\right) / 2-\lambda\left(\xi_{N+1}\right)\left{\left(\mathbf{w}^{\top} \phi_{N+1}\right)^{2}-\xi_{N+1}^{2}\right}\right].
$$</p>
<p>を得る両辺の対数をとって</p>
<p>$$
\begin{aligned}
\ln p(t_{N+1}\mid\mathbf{w})p(\mathbf{w})&amp;\geq\ln p(\mathbf{w})+\mathbf{w}^{\top} \phi_{N+1} t_{N+1}+\ln \sigma\left(\xi_{N+1}\right)-\left(\mathbf{w}^{\top} \phi_{N+1}+\xi_{N+1}\right) / 2-\lambda\left(\xi_{N+1}\right)\left{\left(\mathbf{w}^{\top} \phi_{N+1}\right)^{2}-\xi_{N+1}^{2}\right}\
&amp;\simeq -\frac{1}{2}(\mathbf{w}-\mathbf{m}<em>N)^{\top}\mathbf{S}<em>N^{-1}(\mathbf{w}-\mathbf{m}<em>N)+\mathbf{w}^{\top} \phi</em>{N+1}\left(t</em>{N+1}-\frac{1}{2}\right)-\lambda\left(\xi</em>{N+1}\right) \mathbf{w}^{\top} \phi_{N+1} \phi_{N+1}^{\top} \mathbf{w}+Const
\end{aligned}
$$</p>
<p>これは$\mathbf{w}$の二次形式になっているため，N+1個目のデータが与えられたときの変分事後分布を$q_{N+1}(\mathbf{w})$とするとガウス分布になり，平方完成することで</p>
<p>$$
q_{N+1}(\mathbf{w})=\mathcal{N}(\mathbf{w}\mid\mathbf{m}<em>{N+1},\mathbf{S}</em>{N+1}^{-1})
$$</p>
<p>$$
\mathbf{m}<em>{N+1}＝\mathbf{S}</em>{N+1}\left[\mathbf{S}<em>{N}^{-1} \mathbf{m}</em>{N}+\left(t_{N+1}-1 / 2\right) \boldsymbol{\phi}_{N+1}\right]
$$</p>
<p>$$
\mathbf{S}<em>{N+1}^{-1}＝2 \lambda\left(\xi</em>{N+1}\right) \boldsymbol{\phi}<em>{N+1} \boldsymbol{\phi}</em>{N+1}^{T}+\mathbf{S}_{N}^{-1}
$$</p>
<p>が得られる．</p>
<h2 id="演習-1033"><a class="header" href="#演習-1033">演習 10.33</a></h2>
<div class="panel-primary">
<p>$$Q\left(\boldsymbol{\xi}, \boldsymbol{\xi}^{\text {old }}\right)=\sum_{n=1}^{N}\left{\ln \sigma\left(\xi_{n}\right)-\xi_{n} / 2-\lambda\left(\xi_{n}\right)\left(\boldsymbol{\phi}<em>{n}^{\mathrm{T}} \mathbb{E}\left[\mathbf{w} \mathbf{w}^{\mathrm{T}}\right] \boldsymbol{\phi}</em>{n}-\xi_{n}^{2}\right)\right}+\mathrm{const} . \tag{10.161}$$
で定義した量$Q\left(\boldsymbol{\xi}, \boldsymbol{\xi}^{\text {old }}\right)$を変分パラメータ$\xi_n$について微分することで，
$$\left(\xi_{n}^{\text {new }}\right)^{2}=\boldsymbol{\phi}<em>{n}^{\mathrm{T}} \mathbb{E}\left[\mathbf{w} \mathbf{w}^{\mathrm{T}}\right] \boldsymbol{\phi}</em>{n}=\boldsymbol{\phi}<em>{n}^{\mathrm{T}}\left(\mathbf{S}</em>{N}+\mathbf{m}<em>{N} \mathbf{m}</em>{N}^{\mathrm{T}}\right) \boldsymbol{\phi}_{n} \tag{10.163}$$
で与えられるベイズロジスティック回帰モデルの$\xi_n$の更新式を示せ．</p>
</div>
<h3 id="preparation"><a class="header" href="#preparation">Preparation</a></h3>
<ul>
<li>
<p>$\sigma(x)$の微分
$$
\begin{aligned}
\frac{\partial \sigma(x)}{\partial x} &amp;= \frac{e^{-x}}{(1 + e^{-x})^2} \
&amp;= \sigma(x)(1 - \sigma(x)) \notag
\end{aligned}
$$</p>
</li>
<li>
<p>$\log(\sigma(x))$の微分
$$
\begin{aligned}
\frac{\partial \log(\sigma(x))}{\partial x} &amp;= (1 + e^{-x})\frac{e^{-x}}{(1 + e^{-x})^2} \
&amp;= \frac{e^{-x}}{1 + e^{-x}} = 1 - \sigma(x) \notag
\end{aligned}
$$</p>
</li>
<li>
<p>$\lambda(\xi)$
$$
\lambda(\xi) = \frac{1}{2\xi}\left[\sigma(\xi) - \frac{1}{2}\right] \tag{10.150}
$$</p>
</li>
<li>
<p>分散の公式</p>
</li>
</ul>
<p>$$
\begin{aligned}
\mathbf{S}_N &amp;= \mathbb{E}[\mathbf{ww}^{\mathrm{T}}] - \mathbb{E}[\mathbf{w}]\mathbb{E}[\mathbf{w}^{\mathrm{T}}] \
&amp;= \mathbb{E}[\mathbf{ww}^{\mathrm{T}}] - \mathbf{m}_N\mathbf{m}^{\mathrm{T}}_N
\end{aligned}
$$</p>
<h3 id="solution"><a class="header" href="#solution">Solution</a></h3>
<p>$$
\begin{aligned}
\frac{\partial Q}{\partial \xi_n} &amp;= (1 - \sigma(\xi_n)) - \frac{1}{2} + 2\xi_n\lambda(\xi_n) - \lambda'(\xi_n)(\mathbf{\phi}^{\mathrm{T}}_n\mathbb{E}\left[\mathbf{ww}^{\mathrm{T}}\right]\mathbf{\phi}_n - \xi^2_n) \
&amp;= -2\xi_n\lambda(\xi_n) + 2\xi_n\lambda(\xi_n) - \lambda'(\xi_n)(\mathbf{\phi}^{\mathrm{T}}_n\mathbb{E}\left[\mathbf{ww}^{\mathrm{T}}\right]\mathbf{\phi}_n - \xi^2_n) \
&amp;= 0
\end{aligned}
$$
よって、
$$
(\xi^{\mathrm{new}}_n)^2 = \mathbf{\phi}^{\mathrm{T}}_n\mathbb{E}\left[\mathbf{ww}^{\mathrm{T}}\right]\mathbf{\phi}_n = \mathbf{\phi}^{\mathrm{T}}_n\left(\mathbf{S}_N + \mathbf{m}_N\mathbf{m}^{\mathrm{T}}_N\right)\mathbf{\phi}_n
$$</p>
<h2 id="演習-1034"><a class="header" href="#演習-1034">演習 10.34</a></h2>
<div class="panel-primary">
<p>この演習問題では，4.5節のベイスロジスティック回帰モデルの変分パラメータ$\boldsymbol{\xi}$の更新式を，
$$\begin{aligned} \mathcal{L}(\boldsymbol{\xi})&amp;= \frac{1}{2} \ln \frac{\left|\mathbf{S}<em>{N}\right|}{\left|\mathbf{S}</em>{0}\right|}+\frac{1}{2} \mathbf{m}<em>{N}^{\mathrm{T}} \mathbf{S}</em>{N}^{-1} \mathbf{m}<em>{N}-\frac{1}{2} \mathbf{m}</em>{0}^{\mathrm{T}} \mathbf{S}<em>{0}^{-1} \mathbf{m}</em>{0} \ &amp;+\sum_{n=1}^{N}\left{\ln \sigma\left(\xi_{n}\right)-\frac{1}{2} \xi_{n}+\lambda\left(\xi_{n}\right) \xi_{n}^{2}\right} \end{aligned} \tag{10.164}$$
で与えられる下界を直接最大化することで導出する．これには$\mathcal{L}(\boldsymbol{\xi})$の$\xi_n$に附する微分を0としてみよ．行列式の対数の微分には結果
$$\frac{d}{d \alpha} \ln |\mathbf{A}|=\operatorname{Tr}\left(\mathbf{A}^{-1} \frac{d}{d \alpha} \mathbf{A}\right) \tag{3.117}$$
を，変分事後分布$q(\mathbf{w})$の平均と分散には
$$\mathbf{m}<em>{N}=\mathbf{S}</em>{N}\left(\mathbf{S}<em>{0}^{-1} \mathbf{m}</em>{0}+\sum_{n=1}^{N}\left(t_{n}-1 / 2\right) \phi_{n}\right) \tag{10.157}$$
$$\mathbf{S}<em>{N}^{-1}=\mathbf{S}</em>{0}^{-1}+2 \sum_{n=1}^{N} \lambda\left(\xi_{n}\right) \phi_{n} \phi_{n}^{\mathrm{T}} \tag{10.158}$$
の式を利用せよ．</p>
</div>
<h3 id="preparation-1"><a class="header" href="#preparation-1">Preparation</a></h3>
<ul>
<li>公式
$$
\frac{\partial}{\partial x}(\mathbf{A}^{-1}) = -\mathbf{A}^{-1}\frac{\partial\mathbf{A}}{\partial x}\mathbf{A}^{-1} \tag{C.21}
$$</li>
<li>後で使う式変形
$$
\begin{aligned}
\mathbf{m}^{\mathrm{T}}<em>N\mathbf{S}^{-1}</em>{N}\mathbf{m}<em>N &amp;= [\mathbf{S}</em>{N}\mathbf{S}^{-1}<em>{N}\mathbf{m}<em>N]^{\mathrm{T}}\mathbf{S}^{-1}</em>{N}[\mathbf{S}</em>{N}\mathbf{S}^{-1}<em>{N}\mathbf{m}<em>N] \
&amp;= [\mathbf{S}</em>{N}\mathbf{a}<em>N]^{\mathrm{T}}\mathbf{S}^{-1}</em>{N}[\mathbf{S}</em>{N}\mathbf{a}_N] \
&amp;= \mathbf{a}^{\mathrm{T}}<em>N\mathbf{S}^{-1}</em>{N}\mathbf{a}_N.
\end{aligned}
$$</li>
</ul>
<p>ただし、
$$
\mathbf{a}<em>N = \mathbf{S}^{-1}</em>{N}\mathbf{m}_N.
$$</p>
<h3 id="solution-1"><a class="header" href="#solution-1">Solution</a></h3>
<p>$$
\begin{aligned}
\frac{\partial \mathcal{L}(\mathbf{\xi})}{\partial \xi_n} &amp;= \frac{1}{2}\mathrm{Tr}\left(\mathbf{S}^{-1}<em>{N}\frac{\partial \mathbf{S}</em>{N}}{\partial \xi_n}\right) + \frac{1}{2} \mathrm{Tr}\left(\mathbf{a}<em>N\mathbf{a}^{\mathrm{T}}<em>N \frac{\partial \mathbf{S}</em>{N}}{\partial \xi_n}\right) + \lambda'(\xi_n)\xi^2_n \
&amp;= \frac{1}{2}\mathrm{Tr}\left((\mathbf{S}^{-1}</em>{N} + \mathbf{a}<em>N\mathbf{a}^{\mathrm{T}}<em>N)\frac{\partial \mathbf{S}</em>{N}}{\partial \xi_n}\right) + \lambda'(\xi_n)\xi^2_n \
&amp;= -\frac{1}{2}\mathrm{Tr}\left((\mathbf{S}^{-1}</em>{N} + \mathbf{a}_N\mathbf{a}^{\mathrm{T}}_N)\mathbf{S}_N[2\lambda'(\xi_n)\mathbf{\phi}_N\mathbf{\phi}^{\mathrm{T}}_N]\mathbf{S}<em>N\right) + \lambda'(\xi_n)\xi^2_n \
&amp;= -\lambda'(\xi_n)\left{\mathrm{Tr}\left((\mathbf{S}^{-1}</em>{N} + \mathbf{a}_N\mathbf{a}^{\mathrm{T}}_N)\mathbf{S}_N\mathbf{\phi}_N\mathbf{\phi}^{\mathrm{T}}_N\mathbf{S}_N\right) - \xi^2_n\right} \
&amp;= 0.
\end{aligned}
$$</p>
<p>よって、</p>
<p>$$
\begin{aligned}
\xi^2_n &amp;= \mathrm{Tr}\left((\mathbf{S}^{-1}_{N} + \mathbf{a}_N\mathbf{a}^{\mathrm{T}}_N)\mathbf{S}_N\mathbf{\phi}_N\mathbf{\phi}^{\mathrm{T}}_N\mathbf{S}_N\right) \
&amp;= (\mathbf{S}_N\mathbf{\phi}<em>n)^{\mathrm{T}}(\mathbf{S}^{-1}</em>{N} + \mathbf{a}_N\mathbf{a}^{\mathrm{T}}_N)(\mathbf{S}_N\mathbf{\phi}_n) \
&amp;= \mathbf{\phi}^{\mathrm{T}}_n(\mathbf{S}_N + \mathbf{S}_N\mathbf{a}_N\mathbf{a}^{\mathrm{T}}_N\mathbf{S}_N)\mathbf{\phi}_n \
&amp;= \mathbf{\phi}^{\mathrm{T}}_n(\mathbf{S}_N + \mathbf{m}_N\mathbf{m}^{\mathrm{T}}_N)\mathbf{\phi}_n
\end{aligned}
$$</p>
<h2 id="演習-1035"><a class="header" href="#演習-1035">演習 10.35</a></h2>
<div class="panel-primary">
<p>変分ベイズロジスティック回帰モデルの下界$\mathcal{L}(\boldsymbol{\xi})$についての結果
$$\begin{aligned} \mathcal{L}(\boldsymbol{\xi})&amp;= \frac{1}{2} \ln \frac{\left|\mathbf{S}<em>{N}\right|}{\left|\mathbf{S}</em>{0}\right|}+\frac{1}{2} \mathbf{m}<em>{N}^{\mathrm{T}} \mathbf{S}</em>{N}^{-1} \mathbf{m}<em>{N}-\frac{1}{2} \mathbf{m}</em>{0}^{\mathrm{T}} \mathbf{S}<em>{0}^{-1} \mathbf{m}</em>{0} \ &amp;+\sum_{n=1}^{N}\left{\ln \sigma\left(\xi_{n}\right)-\frac{1}{2} \xi_{n}+\lambda\left(\xi_{n}\right) \xi_{n}^{2}\right} \end{aligned} \tag{10.164}$$
を導出せよ．これには$\mathcal{L}(\boldsymbol{\xi})$を定める積分
$$\ln p(\mathbf{t})=\ln \int p(\mathbf{t} \mid \mathbf{w}) p(\mathbf{w}) \mathrm{d} \mathbf{w} \geqslant \ln \int h(\mathbf{w}, \boldsymbol{\xi}) p(\mathbf{w}) \mathrm{d} \mathbf{w}=\mathcal{L}(\boldsymbol{\xi}) \tag{10.159}$$
の中で，ガウス事前分布の式に$q(\mathbf{w}) = \mathcal{N}(\mathbf{w}\mid \mathbf{m}_0, \mathbf{\Sigma}_0)$を，尤度関数に下界$h(\mathbf{w}, \boldsymbol{\xi})$を代入するのが最も簡単である．次に指数関数の中で$\mathbf{w}$に依存する項をまとめ，平方完成することでガウス分布の積分を導出せよ．多次元ガウス分布の正規化項についての標準的な結果を使ってこれを計算し，最後に対数をとって$(10.164)$を求めよ．</p>
</div>
<ul>
<li>
<p>$h(\mathbf{w}, \xi)p(\mathbf{w})$を計算する
$$
\begin{aligned}
h(\mathbf{w}, \xi)p(\mathbf{w}) &amp;= \mathrm{N}(\mathbf{w}|\mathbf{m}_0, \mathbf{S}_0)\prod^N \sigma(\xi_n)\exp{\mathbf{w}^{\mathrm{T}}\mathbf{\phi}_nt_n - (\mathbf{w}^{\mathrm{T}}\mathbf{\phi}_n + \xi_n)/2 - \lambda(\xi_n)([\mathbf{w}^{\mathrm{T}}\mathbf{\phi}]^2 - \xi^2_n)} \
&amp;= {(2\pi)^{-W/2} \cdot |\mathbf{S}_0|^{-1/2} \cdot \prod^N \sigma(\xi_n)} \cdot \exp{-\frac{1}{2}(\mathbf{w} - \mathbf{m}_0)^{\mathrm{T}}\mathbf{S}^{-1}_0(\mathbf{w} - \mathbf{m}_0)} \
&amp; \ \ \ \cdot \prod^N \exp{\mathbf{w}^{\mathrm{T}}\mathbf{\phi}_nt_n - (\mathbf{w}^{\mathrm{T}}\mathbf{\phi}_n + \xi_n)/2 - \lambda(\xi_n)([\mathbf{w}^{\mathrm{T}}\mathbf{\phi}]^2 - \xi^2_n)} \
&amp;= \left{(2\pi)^{-W/2} \cdot |\mathbf{S}_0|^{-1/2} \cdot \prod^N \sigma(\xi_n) \cdot \exp\left(-\frac{1}{2}\mathbf{m}^{\mathrm{T}}_0\mathbf{S}^{-1}_0\mathbf{m}_0 - \sum^N \frac{\xi_n}{2} + \sum^N \lambda(\xi_n)\xi^2_n\right)\right} \
&amp; \ \ \ \cdot \exp\left{-\frac{1}{2}\mathbf{w}^{\mathrm{T}}\left(\mathbf{S}^{-1}_0 + 2\sum^N\lambda(\xi_n)\mathbf{\phi}_n\mathbf{\phi}^{\mathrm{T}}_n\right)\mathbf{w} + \mathbf{w}^{\mathrm{T}}\left(\mathbf{S}^{-1}_0\mathbf{m}_0 + \sum^N \mathbf{\phi}_n\left(t_n - \frac{1}{2}\right)\right) \right} \
&amp;= \left{(2\pi)^{-W/2} \cdot |\mathbf{S}_0|^{-1/2} \cdot \prod^N \sigma(\xi_n) \cdot \exp\left(-\frac{1}{2}\mathbf{m}^{\mathrm{T}}_0\mathbf{S}^{-1}_0\mathbf{m}_0 - \sum^N \frac{\xi_n}{2} + \sum^N \lambda(\xi_n)\xi^2_n\right)\right} \
&amp; \ \ \ \cdot \exp\left{\frac{1}{2}\mathbf{w}^{\mathrm{T}}\mathrm{S}^{-1}_N\mathbf{w} + \mathbf{w}^{\mathrm{T}}\mathrm{S}^{-1}_N\mathbf{m}_N\right} \ (\because (10.157)-(10.158)) \
&amp;= \left{(2\pi)^{-W/2} \cdot |\mathbf{S}_0|^{-1/2} \cdot \prod^N \sigma(\xi_n) \cdot \exp\left(-\frac{1}{2}\mathbf{m}^{\mathrm{T}}_0\mathbf{S}^{-1}_0\mathbf{m}_0 - \sum^N \frac{\xi_n}{2} + \sum^N \lambda(\xi_n)\xi^2_n\right) + \frac{1}{2}\mathbf{m}^{\mathrm{T}}_N\mathbf{S}^{-1}_N\mathbf{m}_N\right} \
&amp; \ \ \ \cdot \exp\left{-\frac{1}{2}(\mathbf{w} - \mathbf{m}_N)^{\mathrm{T}}\mathrm{S}^{-1}_N(\mathbf{w} - \mathbf{m}_N)\right}
\end{aligned}
$$</p>
</li>
<li>
<p>$\mathbf{w}$で積分する
$$
\begin{aligned}
\int h(\mathbf{w}, \xi)p(\mathbf{w})d\mathbf{w} &amp;= (2\pi)^{-W/2} \cdot |\mathbf{S}_0|^{-1/2} \
&amp; \ \ \ \cdot \prod^N \sigma(\xi_n) \cdot \exp\left(-\frac{1}{2}\mathbf{m}^{\mathrm{T}}_0\mathbf{S}^{-1}_0\mathbf{m}_0 - \sum^N \frac{\xi_n}{2} + \sum^N \lambda(\xi_n)\xi^2_n\right) + \frac{1}{2}\mathbf{m}^{\mathrm{T}}_N\mathbf{S}^{-1}_N\mathbf{m}_N \
&amp; \ \ \ \cdot (2\pi)^{W/2} \cdot |\mathbf{S}_N|^{1/2} \
&amp;= \left(\frac{|\mathbf{S}_N|}{|\mathbf{S}_0|}\right)^{1/2} \prod^N \sigma(\xi_n) \cdot \exp\left(-\frac{1}{2}\mathbf{m}^{\mathrm{T}}_0\mathbf{S}^{-1}_0\mathbf{m}_0 - \sum^N \frac{\xi_n}{2} + \sum^N \lambda(\xi_n)\xi^2_n\right) + \frac{1}{2}\mathbf{m}^{\mathrm{T}}_N\mathbf{S}^{-1}_N\mathbf{m}_N
\end{aligned}
$$</p>
</li>
<li>
<p>対数をとる</p>
</li>
</ul>
<p>$$
\mathcal{L}(\xi) = \frac{1}{2}\log\frac{|\mathbf{S}_N|}{|\mathbf{S}_0|} - \frac{1}{2}\mathbf{m}^{\mathrm{T}}_0\mathbf{S}^{-1}_0\mathbf{m}_0 + \frac{1}{2} \mathbf{m}^{\mathrm{T}}_N\mathbf{S}^{-1}_N\mathbf{m}_N + \sum^N \left{\log\sigma(\xi_n) - \frac{\xi_n}{2} + \lambda(\xi_n)\xi^2_n \right}
$$</p>
<h2 id="演習-1036"><a class="header" href="#演習-1036">演習 10.36</a></h2>
<div class="panel-primary">
<p>10.7節で議論したADFの近似法を考える．このとき.因子$f_j(\boldsymbol{\theta})$を含めることで，モデルエビデンスを</p>
<p>$$p_{j}(\mathcal{D}) \simeq p_{j-1}(\mathcal{D}) Z_{j} \tag{10.242}$$</p>
<p>のように更新できることを示せ．ここで$Z_j$は正規化定数であり
$$Z_{j}=\int f_{j}(\theta) q^{\backslash j}(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta} \tag{10.197}$$
で与えられる$p_0({\mathcal{D}} ) = 1$と初期化して上の結果を再帰的に適用することで，</p>
<p>$$p(\mathcal{D}) \simeq \prod_{j} Z_{j} \tag{10.243}$$</p>
<p>を導け．</p>
</div>
<p>※</p>
<h2 id="演習-1037"><a class="header" href="#演習-1037">演習 10.37</a></h2>
<div class="panel-primary">
<p>10.7節のEP法のアルゴリスムについて考え，定義
$$p(\mathcal{D}, \theta)=\prod_{i} f_{i}(\boldsymbol{\theta}) \tag{10.188}$$
における因子の一つ$f_0(\boldsymbol{\theta})$が，近似分布$q(\boldsymbol{\theta})$と同じ形の指数分布族になっていると仮定する．このとき因子$\tilde{f}_0(\boldsymbol{\theta})$を$f_0(\boldsymbol{\theta})$で初期化すれば，EP法での$\tilde{f}_0(\boldsymbol{\theta})$の更新は$\tilde{f}_0(\boldsymbol{\theta})$を変えないことを示せ．典型的には，これは因子の一つが事前分布$p(\boldsymbol{\theta})$の場合に起こり，事前分布に対応する因子は一度だけ厳密な形で取り込まれ，以降は更新する必要はないことがわかる．</p>
</div>
<p>$q(\boldsymbol{\theta})$の初期値は
$$
q_{\mathrm{init}}(\boldsymbol{\theta}) = \tilde{f_0}(\boldsymbol{\theta})\prod_{i \neq 0}\tilde{f_i}(\boldsymbol{\theta})
$$
と表される。ここで、
$$
q^{\setminus 0}(\boldsymbol{\theta}) = \prod_{i \neq 0}\tilde{f_i}(\boldsymbol{\theta}).
$$
$q^{new}(\boldsymbol{\theta})$は、モーメント一致法を用いて、
$$
q^{new}(\boldsymbol{\theta}) = q^{\setminus 0}(\boldsymbol{\theta})f_0(\boldsymbol{\theta}).
$$
問題の設定より、$\tilde{f_0}(\boldsymbol{\theta})$の初期値は$f_0(\boldsymbol{\theta})$なので、
$$
q^{new}(\boldsymbol{\theta}) = q_{\mathrm{init}}(\boldsymbol{\theta}).
$$
また、
$$
Z_0 = \int q^{\setminus 0}(\boldsymbol{\theta})f_0(\boldsymbol{\theta})d\boldsymbol{\theta} = \int q_{\mathrm{init}}(\boldsymbol{\theta})d\boldsymbol{\theta} = 1.
$$
従って、(10.207)の更新式は、
$$
\tilde{f_0}(\boldsymbol{\theta}) = Z_0\frac{q^{new}(\boldsymbol{\theta})}{q^{\setminus 0}(\boldsymbol{\theta})} = f_0(\boldsymbol{\theta})
$$
となる。</p>
<h2 id="演習-1038"><a class="header" href="#演習-1038">演習 10.38</a></h2>
<div class="panel-primary">
<p>この演習問題と次の演習問題では，雑音データ問題でのEP法の結果$(10.214)-(10.224)$を確かめる．除算を行う公式
$$q^{\backslash j}(\boldsymbol{\theta})=\frac{q(\boldsymbol{\theta})}{\widetilde{f}<em>{j}(\boldsymbol{\theta})} \tag{10.205}$$
から始めて，
$$\mathbf{m}^{\backslash n}=\mathbf{m}+v^{\backslash n} v</em>{n}^{-1}\left(\mathbf{m}-\mathbf{m}<em>{n}\right) \tag{10.214}$$
$$\left(v^{\backslash n}\right)^{-1}=v^{-1}-v</em>{n}^{-1}\tag{10.215}$$
を，指数関数の中を平方完成して平均と分散を見出すことで導け．また
$$Z_{j}=\int q^{\backslash j}(\boldsymbol{\theta}) f_{j}(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta} \tag{10.206}$$
で定義される正規化定数$Z_n$は，雑音データ問題については
$$Z_{n}=(1-w) \mathcal{N}\left(\mathbf{x}<em>{n} \mid \mathbf{m}^{\backslash n},\left(v^{\backslash n}+1\right) \mathbf{I}\right)+w \mathcal{N}\left(\mathbf{x}</em>{n} \mid \mathbf{0}, a \mathbf{I}\right) \tag{10.216}$$
で与えられることを示せ．これには一般的な結果
$$p(\mathbf{y})=\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \boldsymbol{\mu}+\mathbf{b}, \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}\right) \tag{2.115}$$
を利用すればよい．</p>
</div>
<p>(10.205)、(10.212)、(10.213)式より、
$$
\begin{aligned}
q^{\setminus j}(\boldsymbol{\theta}) &amp;= \frac{q(\boldsymbol{\theta})}{\tilde{f_j}(\boldsymbol{\theta})} = \frac{\mathcal{N}(\boldsymbol{\theta}|\mathbf{m}, v\mathbf{I})}{s_n \mathcal{N}(\boldsymbol{\theta}|\mathbf{m}_n, v_n\mathbf{I})} \
&amp;\propto \frac{\exp\left{-\frac{1}{2}(\boldsymbol{\theta} - \mathrm{m})^{\mathrm{T}}(v\mathbf{I})^{-1}(\boldsymbol{\theta} - \mathrm{m})\right}}{\exp\left{-\frac{1}{2}(\boldsymbol{\theta} - \mathrm{m}_n)^{\mathrm{T}}(v_n\mathbf{I})^{-1}(\boldsymbol{\theta} - \mathrm{m}_n)\right}} \
&amp;\propto \exp\left{-\frac{1}{2}(\boldsymbol{\theta} - \mathrm{m})^{\mathrm{T}}(v\mathbf{I})^{-1}(\boldsymbol{\theta} - \mathrm{m}) + \frac{1}{2}(\boldsymbol{\theta} - \mathrm{m}_n)^{\mathrm{T}}(v_n\mathbf{I})^{-1}(\boldsymbol{\theta} - \mathrm{m}_n)\right} \
&amp;= \exp\left{-\frac{1}{2}(\boldsymbol{\theta}^{\mathrm{T}}\mathbf{A}\boldsymbol{\theta} + \boldsymbol{\theta}^{\mathrm{T}}\mathbf{B})\right} + \mathrm{const}
\end{aligned}
$$
まず、</p>
<p>$$
\begin{aligned}
(\boldsymbol{\Sigma}^{\setminus n})^{-1} &amp;= \mathbf{A} = (v\mathbf{I})^{-1} - (v_n\mathbf{I})^{-1} \
&amp;= (v^{-1} - v^{-1}_n)\mathbf{I}^{-1}
\end{aligned}
$$
また、
$$
-2(\boldsymbol{\Sigma}^{\setminus n})^{-1}\mathbf{m}^{\setminus n} = \mathbf{B} = 2\left(-(v\mathbf{I})^{-1}\mathbf{m} + (v_n\mathbf{I})^{-1}\mathbf{m}_n\right)
$$
より、
$$
\begin{aligned}
\mathbf{m}^{\setminus n} &amp;= -(\boldsymbol{\Sigma}^{\setminus n})^{-1}\left(-(v\mathbf{I})^{-1}\mathbf{m} + (v_n\mathbf{I})^{-1}\mathbf{m}_n\right) \
&amp;= -(v^{\setminus n})\left(-v^{-1}\mathbf{m} + (v_n^{-1}\mathbf{m}_n\right) \
&amp;= v^{\setminus n}v^{-1}\mathbf{m} - \frac{v^{\setminus n}}{v_n}\mathbf{m}_n \
&amp;= v^{\setminus n}((v^{\setminus n})^{-1} - v^{-1}_n)\mathbf{m} - \frac{v^{\setminus n}}{v_n}\mathbf{m}_n \
&amp;= \mathbf{m} + \frac{v^{\setminus n}}{v_n}(\mathbf{m} - \mathbf{m}_n)
\end{aligned}
$$
次に、(10.206)、(10.209)式より、
$$
\begin{aligned}
Z_n &amp;= \int q^{\setminus n}(\boldsymbol{\theta})f_n(\boldsymbol{\theta})d\boldsymbol{\theta} \
&amp;= \int \mathcal{N}(\boldsymbol{\theta}|\mathbf{m}^{\setminus n}, v^{\setminus n}\mathbf{I})\left{(1 - w)\mathcal{N}(\mathbf{x}_n|\boldsymbol{\theta}, \mathbf{I}) + w\mathcal{N}(\mathbf{x}_n|\mathbf{0}, a\mathbf{I})\right} \
&amp;= (1 - w)\int \mathcal{N}(\boldsymbol{\theta}|\mathbf{m}^{\setminus n}, v^{\setminus n}\mathbf{I})\mathcal{N}(\mathbf{x}_n|\boldsymbol{\theta}, \mathbf{I})d\boldsymbol{\theta} + w\int \mathcal{N}(\boldsymbol{\theta}|\mathbf{m}^{\setminus n}, v^{\setminus n}\mathbf{I})\mathcal{N}(\mathbf{x}_n|\mathbf{0}, a\mathbf{I})d\boldsymbol{\theta} \
&amp;= (1 - w) \mathcal{N}(\mathbf{x}_n|\mathbf{m}^{\setminus n}, (v^{\setminus n} + 1)\mathbf{I}) + w\mathcal{N}(\mathbf{x}_n|\mathbf{0}, a\mathbf{I}). (\because (2.113)-(2.115))
\end{aligned}
$$</p>
<h2 id="演習-1039"><a class="header" href="#演習-1039">演習 10.39</a></h2>
<div class="panel-primary">
<p>雑音データ問題でのEP法における$q^{\text {new }}(\boldsymbol{\theta})$の平均と分散は
$$\mathbf{m}^{\text {new }}=\mathbf{m}^{\backslash n}+\rho_{n} \frac{v^{\backslash n}}{v^{\backslash n}+1}\left(\mathbf{x}<em>{n}-\mathbf{m}^{\backslash n}\right) \tag{10.217}$$
$$v^{\text {new }}=v^{\backslash n}-\rho</em>{n} \frac{\left(v^{\backslash n}\right)^{2}}{v^{\backslash n}+1}+\rho_{n}\left(1-\rho_{n}\right) \frac{\left(v^{\backslash n}\right)^{2}\left|\mathbf{x}_{n}-\mathbf{m}^{\backslash n}\right|^{2}}{D\left(v^{\backslash n}+1\right)^{2}} \tag{10.218}$$
で与えられることを示せ．このためには，最初に$q^{\text {new }}(\boldsymbol{\theta})$の下での$\boldsymbol{\theta}$と$\boldsymbol{\theta}\boldsymbol{\theta}^{\mathrm T}$の期待値が</p>
<p>$$\begin{aligned} \mathbb{E}[\boldsymbol{\theta}] &amp;=\mathbf{m}^{\backslash n}+v^{\backslash n} \nabla_{\mathbf{m} \backslash n} \ln Z_{n} \ \mathbb{E}\left[\boldsymbol{\theta}^{\mathrm{T}} \boldsymbol{\theta}\right] &amp;=2\left(v^{\backslash n}\right)^{2} \nabla_{v^{\backslash n}} \ln Z_{n}+2 \mathbb{E}[\boldsymbol{\theta}]^{\mathrm{T}} \mathbf{m}^{\backslash n}-\left|\mathrm{m}^{\backslash n}\right|^{2}+v^{\backslash n} D \end{aligned}$$</p>
<p>となることを証明し，$Z_n$についての結果
$$Z_{n}=(1-w) \mathcal{N}\left(\mathbf{x}<em>{n} \mid \mathbf{m}^{\backslash n},\left(v^{\backslash n}+1\right) \mathbf{I}\right)+w \mathcal{N}\left(\mathbf{x}</em>{n} \mid \mathbf{0}, a \mathbf{I}\right) \tag{10.216}$$
を用いる．次に，
$$\widetilde{f}<em>{j}(\boldsymbol{\theta})=Z</em>{j} \frac{q^{\text {new }}(\boldsymbol{\theta})}{q \backslash j(\theta)} \tag{10.207}$$
を用いて指数関数の中を平方完成することで$(10.220)-(10.222)$の結果を証明せよ．最後に，$(10.208)$を用いて$(10.223)$の結果を導け．</p>
</div>
<h3 id="preparation-2"><a class="header" href="#preparation-2">Preparation</a></h3>
<ul>
<li>正規分布の微分</li>
</ul>
<p>$$
\begin{aligned}
\frac{\partial \mathcal{N}(x|\mu, \sigma^2)}{\partial \mu} &amp;= \mathcal{N}(x|\mu, \sigma^2)\cdot\frac{x - \mu}{\sigma^2} \
\frac{\partial \mathcal{N}(x|\mu, \sigma^2)}{\partial \sigma^2} &amp;= \mathcal{N}(x|\mu, \sigma^2)\cdot\left(\frac{(x - \mu)^2}{(\sigma^2)^2} - \frac{1}{\sigma^2}\right) \
\end{aligned}
$$</p>
<ul>
<li>$\rho_n$</li>
</ul>
<p>$$
\begin{aligned}
\rho_n &amp;= \frac{1}{Z_n}(1 - w)\mathcal{N}(\mathbf{x}_n|\mathbf{m}^{\setminus n}, (v^{\setminus n} + 1)\mathbf{I}) \
&amp;= \frac{1}{Z_n}(1 - w)\frac{Z_n - w\mathcal{N}(\mathbf{x}_n|\mathbf{0}, a\mathbf{I})}{1 - w} \ (\because (10.216)) \
&amp;= 1 - \frac{w}{Z_n}\mathcal{N}(\mathbf{x}_n|\mathbf{0}, a\mathbf{I}).
\end{aligned}
$$</p>
<h3 id="solution-2"><a class="header" href="#solution-2">Solution</a></h3>
<p>まず、</p>
<p>$$
\begin{aligned}
\nabla_{\mathbf{m}^{\setminus n}} \ln Z_n &amp;= \frac{1}{Z_n}\nabla_{\mathbf{m}^{\setminus n}} \int q^{\setminus n}(\boldsymbol{\theta}) f_n(\boldsymbol{\theta}) d\boldsymbol{\theta} \
&amp;= \frac{1}{Z_n}\int q^{\setminus n}(\boldsymbol{\theta}) f_n(\boldsymbol{\theta})\left{-\frac{1}{v^{\setminus n}}(\mathbf{m}^{\setminus n} - \boldsymbol{\theta})\right} d\boldsymbol{\theta} \
&amp;= -\frac{\mathbf{m}^{\setminus n}}{v^{\setminus n}} + \frac{\mathbb{E}(\boldsymbol{\theta})}{v^{\setminus n}} \ (\because (10.203)???)
\end{aligned}
$$</p>
<p>上式を整理すると、</p>
<p>$$
\begin{aligned}
\mathbb{E}(\boldsymbol{\theta}) &amp;= \mathbf{m}^{\setminus n} + v^{\setminus n}\nabla_{\mathbf{m}^{\setminus n}} \ln Z_n \
&amp;= \mathbf{m}^{\setminus n} + v^{\setminus n} \frac{1}{Z_n} (1 - w)\mathcal{N}(\mathbf{x}_n|\mathbf{m}^{\setminus n}, (v^{\setminus n} + 1)\mathbf{I})\cdot\frac{\mathbf{x}_n - \mathbf{m}^{\setminus n}}{v^{\setminus n} + 1} \
&amp;= \mathbf{m}^{\setminus n} + v^{\setminus n}\rho_n\frac{\mathbf{x}_n - \mathbf{m}^{\setminus n}}{v^{\setminus n} + 1}
\end{aligned}
$$</p>
<p>同様に、</p>
<p>$$
\begin{aligned}
\nabla_{v^{\setminus n}} \ln Z_n &amp;= \frac{1}{Z_n}\nabla_{v^{\setminus n}} \int q^{\setminus n}(\boldsymbol{\theta}) f_n(\boldsymbol{\theta}) d\boldsymbol{\theta} \
&amp;= \frac{1}{Z_n} \int q^{\setminus n}(\boldsymbol{\theta}) f_n(\boldsymbol{\theta})\left{\frac{1}{2(v^{\setminus n})^2}(\mathbf{m}^{\setminus n} - \boldsymbol{\theta})^{\mathrm{T}}(\mathbf{m}^{\setminus n} - \boldsymbol{\theta}) - \frac{D}{2v^{\setminus n}}\right} \
&amp;= \frac{1}{2(v^{\setminus n})^2}\left{\mathbb{E}(\boldsymbol{\theta}^{\mathrm{T}}\boldsymbol{\theta}) - 2\mathbb{E}(\boldsymbol{\theta}^{\mathrm{T}})\mathbf{m}^{\setminus n} + |\mathbf{m}^{\setminus n}|^2 \right} - \frac{D}{2v^{\setminus n}}.
\end{aligned}
$$</p>
<p>整理すると、</p>
<p>$$
\mathbb{E}(\boldsymbol{\theta}^{\mathrm{T}}\boldsymbol{\theta}) = 2(v^{\setminus n})^2 \nabla_{v^{\setminus n}} \ln Z_n + 2\mathbb{E}(\boldsymbol{\theta}^{\mathrm{T}})\mathbf{m}^{\setminus n} - |\mathbf{m}^{\setminus n}|^2 + Dv^{\setminus n}
$$
となる。また、</p>
<p>$$
\begin{aligned}
\nabla_{v^{\setminus n}} \ln Z_n &amp;= \frac{1}{Z_n} (1 - w)\mathcal{N}(\mathbf{x}_n|\mathbf{m}^{\setminus n}, (v^{\setminus n} + 1)\mathbf{I}) \left(\frac{1}{2(v^{\setminus n} + 1)^2}|\mathbf{x}_n - \mathbf{m}^{\setminus n}|^2 - \frac{D}{2(v^{\setminus n} + 1)}\right) \
&amp;= \rho_n \left(\frac{1}{2(v^{\setminus n} + 1)^2}|\mathbf{x}_n - \mathbf{m}^{\setminus n}|^2 - \frac{D}{2(v^{\setminus n} + 1)}\right) \
\end{aligned}
$$</p>
<p>以上の結果と、
$$
v\mathbf{I} = \mathbb{E}(\boldsymbol{\theta}\boldsymbol{\theta}^{\mathrm{T}}) - \mathbb{E}(\boldsymbol{\theta})\mathbb{E}(\boldsymbol{\theta^{\mathrm{T}}})
$$
を組み合わせることで、(10.218)式を得る。</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="prml第11章演習問題解答"><a class="header" href="#prml第11章演習問題解答">PRML第11章演習問題解答</a></h1>
<head>
<style>
  div.panel-primary {
	border: 1px solid #000;
    margin: 10px 5px;
    padding: 16px 10px 0px;
  }
</style>
</head>
<h2 id="演習-111-1"><a class="header" href="#演習-111-1">演習 11.1</a></h2>
<div class="panel-primary">
<p>$$
\widehat{f}=\frac{1}{L} \sum_{l=1}^{L} f\left(\mathbf{z}^{(l)}\right) \tag{11.2}
$$</p>
<p>で定義される有限のサンプルによる推定量$f$は$E[f]$に等しい平均と</p>
<p>$$
\operatorname{var}[\widehat{f}]=\frac{1}{L} \mathbb{E}\left[(f-\mathbb{E}[f])^{2}\right] \tag{11.3}
$$</p>
<p>で与えられる分散を持つことを示せ．</p>
</div>
<p>※
「有限和で近似した期待値」の期待値が元の期待値と一致することを示す．定義から</p>
<p>$$
\begin{aligned}
\mathbb{E}\left[\widehat{f}\right]&amp;=\mathbb{E}\left[\frac{1}{L} \sum_{l=1}^{L} f\left(\mathbf{z}^{(l)}\right)\right]\
&amp;=\frac{1}{L}\sum_{l=1}^{L}\mathbb{E}\left[ f\left(\mathbf{z}^{(l)}\right)\right]\
&amp;=\frac{1}{L}L\mathbb{E}\left[ f\left(\mathbf{z}^{(l)}\right)\right]\
&amp;=\mathbb{E}[ f]
\end{aligned}
$$</p>
<p>確率変数$\mathbf{z}^{(l)}$は確率分布$p(\mathbf{z})$からサンプリングされるため$\mathbb{E}[f]=\mathbb{E}[f(\mathbf{z}^{(l)})]$となることを用いた．</p>
<p>次に分散(11.3)を示す．分散と期待値の二乗を紐付ける式から</p>
<p>$$
\begin{aligned} \operatorname{var}[\widehat{f}] &amp;=\mathbb{E}\left[(\widehat{f}-\mathbb{E}[\widehat{f}])^{2}\right]=\mathbb{E}\left[\hat{f}^{2}\right]-\mathbb{E}[\widehat{f}]^{2}=\mathbb{E}\left[\hat{f}^{2}\right]-\mathbb{E}[f]^{2} \ &amp;=\mathbb{E}\left[\left(\frac{1}{L} \sum_{l=1}^{L} f\left(\mathbf{z}^{(l)}\right)\right)^{2}\right]-\mathbb{E}[f]^{2} \ &amp;=\frac{1}{L^{2}} \mathbb{E}\left[\left(\sum_{l=1}^{L} f\left(\mathbf{z}^{(l)}\right)\right)^{2}\right]-\mathbb{E}[f]^{2} \ &amp;=\frac{1}{L^{2}} \mathbb{E}\left[\sum_{l=1}^{L} f^{2}\left(\mathbf{z}^{(l)}\right)+\sum_{i, j=1, i \neq j}^{L} f\left(\mathbf{z}^{(i)}\right) f\left(\mathbf{z}^{(j)}\right)\right]-\mathbb{E}[f]^{2} \ &amp;=\frac{1}{L^{2}} \mathbb{E}\left[\sum_{l=1}^{L} f^{2}\left(\mathbf{z}^{(l)}\right)\right]+\frac{L^{2}-L}{L^{2}} \mathbb{E}[f]^{2}-\mathbb{E}[f]^{2} \ &amp;=\frac{1}{L^{2}} \sum_{l=1}^{L} \mathbb{E}\left[f^{2}\left(\mathbf{z}^{(l)}\right)\right]-\frac{1}{L} \mathbb{E}[f]^{2} \ &amp;=\frac{1}{L^{2}} \cdot L \cdot \mathbb{E}\left[f^{2}\right]-\frac{1}{L} \mathbb{E}[f]^{2} \ &amp;=\frac{1}{L} \mathbb{E}\left[f^{2}\right]-\frac{1}{L} \mathbb{E}[f]^{2}=\frac{1}{L} \mathbb{E}\left[(f-\mathbb{E}[f])^{2}\right] \end{aligned}
$$</p>
<p>以上により示された．</p>
<h2 id="演習-112-1"><a class="header" href="#演習-112-1">演習 11.2</a></h2>
<div class="panel-primary">
<p>$z$は区間$(0,1)$上の一様分布を持つ確率変数で， $z$を$y=h^{-1}(z)$で変換することを考えることで，$h(y)$は
$$
z=h(y) \equiv \int_{-\infty}^{y} p(\widehat{y}) \mathrm{d} \widehat{y} \tag{11.6}
$$
で与えられる$y$が分布$p(y)$を持つことを示せ．</p>
</div>
<p>※
yの分布を$p^\star(y)$とおくと(11.5)式の$p(y)=p(z)\left|\frac{\mathrm{d} z}{\mathrm{~d} y}\right|$と(11.6)式および$p(z)=1$を用いて</p>
<p>$$
p^{\star}(y)=p(z) \cdot\left|\frac{d z}{d y}\right|=1 \cdot h^{\prime}(y)=\frac{d}{d y} \int_{-\infty}^{y} p(\widehat{y}) d \widehat{y}=p(y)
$$</p>
<p>となりyが(11.6)式で与えられる分布p(y)を持つことがわかる．</p>
<h2 id="演習-113-1"><a class="header" href="#演習-113-1">演習 11.3</a></h2>
<div class="panel-primary">
<p>区間$(0,1)$上で一様分布する確率変数$z$が与えられたとき，$y$が
$$
p(y)=\frac{1}{\pi} \frac{1}{1+y^{2}} \tag{11.8}
$$
で与えられるコーシ一分布を持つようにする変換$y=f(z)$を求めよ．</p>
</div>
<p>※</p>
<p>(11.8)より</p>
<p>$$
p(y) = \frac{1}{\pi}\frac{1}{1+y^2}
$$</p>
<p>(11.5)式よりこれを積分することで$z=h(y)$が得られて</p>
<p>$$
\begin{aligned}
z&amp;=h(y)\
&amp;=\int_{-\infty}^yp(\widehat{y})d\widehat{y}\
&amp;=\int_{-\infty}^y\frac{1}{\pi}\frac{1}{1+\widehat{y}^2}d\widehat{y}\
&amp;=\left[\frac{1}{\pi}\tan^{-1}\widehat{y}\right]_{-\infty}^y\
&amp;=\frac{1}{\pi}\tan^{-1}y-\frac{1}{\pi}(-\frac{\pi}{2})\
&amp;=\frac{1}{\pi}\tan^{-1}y+\frac{1}{2}
\end{aligned}
$$</p>
<p>これをyについて解くと</p>
<p>$$
y = \tan\left{\pi(z-\frac{1}{2})\right}
$$</p>
<p>が得られる</p>
<h2 id="演習-114-1"><a class="header" href="#演習-114-1">演習 11.4</a></h2>
<div class="panel-primary">
<p>図11.3に示すように，$z_1$と$z_2$が単位円上で一様分布し，</p>
<p>$$
y_{1}=z_{1}\left(\frac{-2 \ln r^{2}}{r^{2}}\right)^{1 / 2} \tag{11.10}
$$</p>
<p>および
$$
y_{2}=z_{2}\left(\frac{-2 \ln r^{2}}{r^{2}}\right)^{1 / 2} \tag{11.11}
$$
で与えられる変数変換を行うとする.$(y_1,y_2)$が
$$
\begin{aligned} p\left(y_{1}, y_{2}\right) &amp;=p\left(z_{1}, z_{2}\right)\left|\frac{\partial\left(z_{1}, z_{2}\right)}{\partial\left(y_{1}, y_{2}\right)}\right| \ &amp;=\left[\frac{1}{\sqrt{2 \pi}} \exp \left(-y_{1}^{2} / 2\right)\right]\left[\frac{1}{\sqrt{2 \pi}} \exp \left(-y_{2}^{2} / 2\right)\right] \end{aligned} \tag{11.12}
$$
に従って分布することを示せ．</p>
</div>
<p>※
方針：計算を楽にするため極座標で表してヤコビアンの計算を以下のように行う．</p>
<p>$$
\begin{aligned}
\left|\frac{\partial\left(z_{1}, z_{2}\right)}{\partial\left(y_{1}, y_{2}\right)}\right| &amp;=\left|\frac{\partial\left(z_{1}, z_{2}\right)}{\partial(r, \theta)} \cdot \frac{\partial(r, \theta)}{\partial\left(y_{1}, y_{2}\right)}\right| \
\end{aligned}
$$</p>
<p>$r^2=z_1^2+z_2^2$であるから，</p>
<p>$$
z_1=r\cos\theta
$$</p>
<p>$$
z_2=r\sin\theta
$$</p>
<p>とすると</p>
<p>$$
\frac{\partial\left(z_{1}, z_{2}\right)}{\partial(r, \theta)}=\left[\begin{array}{ll}
\partial z_{1} / \partial r &amp; \partial z_{1} / \partial \theta \
\partial z_{2} / \partial r &amp; \partial z_{2} / \partial \theta
\end{array}\right]=\left[\begin{array}{cc}
\cos \theta &amp; -r \sin \theta \
\sin \theta &amp; r \cos \theta
\end{array}\right]
$$</p>
<p>であるから，行列式の値は</p>
<p>$$
\left|\frac{\partial\left(z_{1}, z_{2}\right)}{\partial(r, \theta)}\right|=r\left(\cos ^{2} \theta+\sin ^{2} \theta\right)=r
$$</p>
<p>となる．つぎに$(y_1,y_2)$について考えると</p>
<p>$$
y_{1}=r \cos \theta\left(\frac{-2 \ln r^{2}}{r^{2}}\right)^{1 / 2}=\cos \theta\left(-2 \ln r^{2}\right)^{1 / 2}
$$</p>
<p>同様に</p>
<p>$$
y_{2}=\sin \theta\left(-2 \ln r^{2}\right)^{1 / 2}
$$</p>
<p>が求まるので，</p>
<p>$$
\frac{\partial\left(y_{1}, y_{2}\right)}{\partial(r, \theta)}=\left[\begin{array}{ll}
\partial y_{1} / \partial r &amp; \partial y_{1} / \partial \theta \
\partial y_{2} / \partial r &amp; \partial y_{2} / \partial \theta
\end{array}\right]=\left[\begin{array}{cc}
-2 \cos \theta\left(-2 \ln r^{2}\right)^{-1 / 2} \cdot r^{-1} &amp; -\sin \theta\left(-2 \ln r^{2}\right)^{1 / 2} \
-2 \sin \theta\left(-2 \ln r^{2}\right)^{-1 / 2} \cdot r^{-1} &amp; \cos \theta\left(-2 \ln r^{2}\right)^{1 / 2}
\end{array}\right]
$$</p>
<p>となる．したがって行列式の値は</p>
<p>$$
\left|\frac{\partial\left(y_{1}, y_{2}\right)}{\partial(r, \theta)}\right|=\left(-2 r^{-1}\left(\cos ^{2} \theta+\sin ^{2} \theta\right)\right)=-2 r^{-1}
$$</p>
<p>となる．以上により求めたかったヤコビアンを計算することができて，</p>
<p>$$
\begin{aligned}
\left|\frac{\partial\left(z_{1}, z_{2}\right)}{\partial\left(y_{1}, y_{2}\right)}\right| &amp;=\left|\frac{\partial\left(z_{1}, z_{2}\right)}{\partial(r, \theta)} \cdot \frac{\partial(r, \theta)}{\partial\left(y_{1}, y_{2}\right)}\right| \
&amp;=\left|\frac{\partial\left(z_{1}, z_{2}\right)}{\partial(r, \theta)}\right| \cdot\left|\frac{\partial(r, \theta)}{\partial\left(y_{1}, y_{2}\right)}\right| \
&amp;=\left|\frac{\partial\left(z_{1}, z_{2}\right)}{\partial(r, \theta)}\right| \cdot\left|\frac{\partial\left(y_{1}, y_{2}\right)}{\partial(r, \theta)}\right|^{-1} \
&amp;=r \cdot\left(-2 r^{-1}\right)^{-1}=-\frac{r^{2}}{2}
\end{aligned}
$$</p>
<p>が得られる．次に$r^2$を$(y_1,y_2)$で表すと(11.10)，(11.11) 及び $r^2=z_1^2+z_2^2$ から</p>
<p>$$
y_1^2+y_2^2=-2\ln r^2
$$</p>
<p>$$
r^2=\exp\left{-\frac{y_1^2+y_2^2}{2}\right}
$$</p>
<p>いま$p(z_1, z_2)=\frac{1}{\pi}$であるので</p>
<p>$$
\begin{aligned}
p\left(y_{1}, y_{2}\right) &amp;=p\left(z_{1}, z_{2}\right)\left|\frac{\partial\left(z_{1}, z_{2}\right)}{\partial\left(y_{1}, y_{2}\right)}\right|\
&amp;=\frac{1}{\pi}\mid-\frac{r^{2}}{2}\mid\
&amp;=\frac{1}{2\pi}\exp\left{-\frac{y_1^2+y_2^2}{2}\right}\
&amp;=\left[\frac{1}{\sqrt{2 \pi}} \exp \left(-y_{1}^{2} / 2\right)\right]\left[\frac{1}{\sqrt{2 \pi}} \exp \left(-y_{2}^{2} / 2\right)\right]
\end{aligned}
$$</p>
<h2 id="演習-115"><a class="header" href="#演習-115">演習 11.5</a></h2>
<div class="panel-primary">
<p>$z$を平均ゼロと単位行列の共分散行列を持つ$D$次元のガウス分布に従う確率変数とし正定値対称行列$\mathbf{\Sigma}$がコレスキー分解$\mathbf{\Sigma} = \mathbf{LL}^{\mathrm T}$を持つとする。ここで，$\mathbf{L}$は下三角行列(すなわち，対角成分より上側がゼロになる行列)である。変数$\mathbf{y} = \boldsymbol{\mu}+\mathbf{Lz}$が平均$\boldsymbol{\mu}$，共分散行列$\mathbf{\Sigma}$であるガウス分布に従うことを示せ．これは，平均$0$分散$1$の$1$変数ガウス分布からのサンプルを用いて， 一般の多変量ガウス分布からのサンプルを生成する技術を提供する．</p>
</div>
<p>ガウス分布に従う確率変数を線形変換して得られる確率変数はガウス分布に従うので
$\mathbf{y}$の平均と分散を計算すれば良い.$\mathbf{z}\backsim\mathcal{N}(\mathbf{0},\mathbf{I})$ であるから</p>
<p>$$
\begin{aligned}
\mathbb{E}\left[\mathbf{y}\right]&amp;=\mathbb{E}\left[\boldsymbol{\mu}+\mathbf{Lz}\right]\
&amp;=\boldsymbol{\mu}+\mathbf{L}\mathbb{E}\left[\mathbf{z}\right]\
&amp;=\boldsymbol{\mu}
\end{aligned}
$$</p>
<p>次に分散共分散行列について$\operatorname{cov}[\mathbf{z}]=\mathbb{E}\left[\mathbf{z z}^{\mathrm{T}}\right]-\mathbb{E}[\mathbf{z}] \mathbb{E}\left[\mathbf{z}^{\mathrm{T}}\right]=\mathbb{E}\left[\mathbf{z z}^{\mathrm{T}}\right]=\mathbf{I}$, を用いて</p>
<p>$$
\begin{aligned}
\operatorname{cov}[\mathbf{y}] &amp;=\mathbb{E}\left[\mathbf{y} \mathbf{y}^{\mathrm{T}}\right]-\mathbb{E}[\mathbf{y}] \mathbb{E}\left[\mathbf{y}^{\mathrm{T}}\right] \
&amp;=\mathbb{E}\left[(\boldsymbol{\mu}+\mathbf{L} \mathbf{z}) \cdot(\boldsymbol{\mu}+\mathbf{L z})^{\mathrm{T}}\right]-\boldsymbol{\mu} \boldsymbol{\mu}^{\mathrm{T}} \
&amp;=\mathbb{E}\left[\boldsymbol{\mu} \boldsymbol{\mu}^{\mathrm{T}}+2 \boldsymbol{\mu} \cdot(\mathbf{L z})^{\mathrm{T}}+(\mathbf{L z}) \cdot(\mathbf{L z})^{\mathrm{T}}\right]-\boldsymbol{\mu} \boldsymbol{\mu}^{\mathrm{T}} \
&amp;=2 \boldsymbol{\mu} \cdot \mathbb{E}\left[\mathbf{z}^{\mathrm{T}}\right] \cdot \mathbf{L}^{\mathrm{T}}+\mathbb{E}\left[\mathbf{L z z}^{\mathrm{T}} \mathbf{L}^{\mathrm{T}}\right] \
&amp;=\mathbf{L} \cdot \mathbb{E}\left[\mathbf{z z}^{\mathrm{T}}\right] \cdot \mathbf{L}^{\mathrm{T}}=\mathbf{L} \cdot \mathbf{I} \cdot \mathbf{L}^{\mathrm{T}} \
&amp;=\mathbf{\Sigma}
\end{aligned}
$$</p>
<p>以上から.$\mathbf{y}\backsim\mathcal{N}(\boldsymbol{\mu},\mathbf{\Sigma})$がわかる</p>
<h2 id="演習-116"><a class="header" href="#演習-116">演習 11.6</a></h2>
<div class="panel-primary">
<p>この練習問題では，棄却サンプリングが求めたい分布$p(\mathbf{z})$から実際にサンプルを抽出することをより注意深く示す。提案分布を$q(\mathbf{z})$とし，サンプル値$\mathbf{z}$が受理される確率が$\widetilde{p}(\mathbf{z}) / k q(\mathbf{z})$であることを示せ．ここで，$\tilde{p}$は$p(\mathbf{z})$に比例する任意の正規化されていない分布であり，定数$k$は$k q(\mathbf{z}) \geqslant \widetilde{p}(\mathbf{z})$をすべての$\mathbf{z}$の値に対して保証する最小値に設定される．値$\mathbf{z}$を抽出する確率はその値を$q(\mathbf{z})$から抽出する確率とその値が抽出されたときにそれが受理される確率の積であることに注意せよ．この事実と確率の和と積の規則を共に用いて，$\mathbf{z}$上の分布を正規化された形に書き下し，それが$p(\mathbf{z})$に等しいことを示せ．</p>
</div>
<p>※まずP.242の流れをしっかりと確認する。</p>
<p>$p(\mathbf{z})$から直接サンプリングすることは困難であるが、任意の与えられた$\mathbf{z}$の値について$p(\mathbf{z})$を求めることは、正規化定数を除いて容易だとする。すなわち、</p>
<p>$$
p(\mathbf{z}) = \frac{1}{Z_{p}}\tilde{p}(\mathbf{z})
$$</p>
<p>において$\tilde{p}(\mathbf{z})$はすぐに求まるが、$Z_{p}$はわからないとする。</p>
<p>ここから教科書の流れと少し変わる。値$\mathbf{z}$が与えられている時に、それが受理される確率は$p(\textrm{acceptance}\mid \mathbf{z})$と書けて、かつ図11.4のように、区間$[0,kq(\mathbf{z})]$の一様分布から得られたサンプル$u$が$0\le u \le \tilde{p}(\mathbf{z})$となったときに受理(acceptance)され、サンプル値$\mathbf{z}$として保持されるので</p>
<p>$$
p(\textrm{acceptance}\mid \mathbf{z}) = \int_{0}^{\tilde{p}(\mathbf{z})}\frac{1}{kq(\mathbf{z})}du = \frac{\tilde{p}(\mathbf{z})}{kq(\mathbf{z})}
$$</p>
<p>これがサンプル値$\mathbf{z}$の受理確率である。</p>
<p>次に受理されたサンプル値$\mathbf{z}$の確率密度分布は数式上で$p(\mathbf{z} \mid \textrm{acceptance})$とかけ、これが$p(\mathbf{z})$となることを示せば良い。</p>
<p>ベイズの定理から</p>
<p>$$
p(\mathbf{z} \mid \textrm{acceptance}) = \frac{p(\mathbf{z},\textrm{acceptance})}{p(\textrm{acceptance})}
$$</p>
<p>となり、分子$p(\mathbf{z},\textrm{acceptance})$は任意の$\mathbf{z}$が受理されるときの同時確率を表すので、これは$q(\mathbf{z})$と$p(\textrm{acceptance}\mid \mathbf{z})$の積になり、</p>
<p>$$
\begin{aligned}
p(\mathbf{z},\textrm{acceptance}) &amp;= q(\mathbf{z}) p(\textrm{acceptance} \mid \mathbf{z}) \
&amp;= q(\mathbf{z}) \frac{\widetilde{p}(\mathbf{z})}{k q(\mathbf{z})} \
&amp;=\frac{\tilde{p}(\mathbf{z})}{k} \
&amp;=\frac{Z_{p}}{k}p(\mathbf{z}) \quad (\because (11.13))
\end{aligned}
$$</p>
<p>分母$p(\textrm{acceptance})$はP.243の$(11.14)$の導出と同様で</p>
<p>$$
\begin{aligned}
p(\textrm{acceptance}) &amp;=\int q(\mathbf{z}) \frac{\widetilde{p}(\mathbf{z})}{k q(\mathbf{z})} \mathrm{d} \mathbf{z} \quad (\because (11.14))\
&amp;=\frac{1}{k} \int \widetilde{p}(\mathbf{z}) \mathrm{d} \mathbf{z}\
&amp;=\frac{Z_{p}}{k}\int p(\mathbf{z})  \mathrm{d} \mathbf{z}\quad (\because (11.13)) \
&amp;= \frac{Z_{p}}{k} \quad \left(\because \int p(\mathbf{z}) \mathrm{d} \mathbf{z} = 1 \right)
\end{aligned}
$$</p>
<p>以上から、</p>
<p>$$
p(\mathbf{z} \mid \textrm{acceptance}) = \frac{p(\mathbf{z},\textrm{acceptance})}{p(\textrm{acceptance})} = p(\mathbf{z})
$$</p>
<p>となり、題意が示された。</p>
<h2 id="演習-117-1"><a class="header" href="#演習-117-1">演習 11.7</a></h2>
<div class="panel-primary">
<p>$y$が区間$[0,1]$上の一様分布に従うとせよ．変数$z = b\tan y + c$が
$$
q(z)=\frac{k}{1+(z-c)^{2} / b^{2}} \tag{11.16}
$$
で与えられるコーシ一分布に従うことを示せ．</p>
</div>
<p>※ $(11.5)$式の変換を用いる。教科書11.1.1節における説明に使われている変数の組$y,z$の役割がこの問題では逆になっていることに注意する。</p>
<p>$y$を$[0,1]$の一様分布とし、$z=f(y)=b\tan y +c$の関数$z$の値が求めたい特定の分布$q(z)$に従うようになっていることを示す。ここでは$p(y)=1$となっていることに注意する。</p>
<p>$\displaystyle y = \tan^{-1}\left( \frac{z-c}{b} \right)$と書けるので、</p>
<p>$$
q(z) = p(y)\left| \frac{dy}{dz} \right| \tag{11.5}
$$</p>
<p>に当てはめるために$dy/dz$の値を求めると、$\displaystyle \frac{d}{dx} \tan^{-1} x = \frac{1}{1+x^2}$であることを用いて</p>
<p>$$
\begin{aligned} \frac{d y}{d z} &amp;=\frac{1}{1+\left(\frac{z-c}{b}\right)^{2}} \frac{d}{d z}\left(\frac{z-c}{b}\right) \ &amp;=\frac{1}{b} \frac{1}{1+\left(\frac{z-c}{b}\right)^{2}} \end{aligned}
$$</p>
<p>$(11.5)$式に当てはめて</p>
<p>$$
\begin{aligned}
q(z) &amp;=p(y)\left|\frac{d y}{d z}\right| \
&amp;=1 \times \frac{1}{b} \frac{1}{1+\left(\frac{z-c}{b}\right)^{2}}
\end{aligned}
$$</p>
<p>$k = 1/b$とみなせば、これは$(11.16)$式と同型になる。</p>
<p>※しかし、そもそも教科書の$(11.16)$式は、提案分布$q(z)$ではなく比較関数$kq(z)$と呼ぶのが適切なのではないかという意見。。</p>
<h2 id="演習-118-1"><a class="header" href="#演習-118-1">演習 11.8</a></h2>
<div class="panel-primary">
<p>連続性と正規化の条件を用いて適応的棄却サンプリングの包絡分布</p>
<p>$$
q(z)=k_{i} \lambda_{i} \exp \left{-\lambda_{i}\left(z-z_{i}\right)\right} \quad \widehat{z}<em>{i-1, i}&lt;z \leqslant \widehat{z}</em>{i, i+1} \tag{11.17}
$$</p>
<p>の係数$k_i$の式を決定せよ．</p>
</div>
<p>$f(z)=l_{i} \lambda_{i} \exp \left{-\lambda_{i}\left(z-z_{i}\right)\right}, \hat{z}<em>{i-1, i}&lt;z \leqq \hat{z}</em>{i, i+1}$ とする。</p>
<p>対数を取ると$\ln f(z)=\ln l_{i}+\ln \lambda_{i}-\lambda_{i}\left(z-z_{i}\right)$</p>
<p>これが、$z_{i}$において、$\ln p(z_{i})$と接する。傾きが同じになるので</p>
<p>$$\lambda_{i}=(\ln p(z_{i}))^{\prime}=\frac{p\left(z_{i}\right)^{\prime}}{p\left(z_{i}\right)}$$</p>
<p>また値が同じになるので</p>
<p>$$
\ln p\left(z_{i}\right)=\ln \ln l_{i}+\ln \lambda_{i}=\ln \ln l_{i}+\ln \frac{p^{\prime}\left(z_{i}\right)}{p\left(z_{i}\right)}
$$</p>
<p>これより</p>
<p>$$
p\left(z_{i}\right)=l_{i} \frac{p^{\prime}\left(z_{i}\right)}{p\left(z_{i}\right)}
$$</p>
<p>$$
\therefore l_{i}=\frac{p\left(z_{i}\right)^{2}}{p\left(z_{i}\right)^{\prime}}
$$</p>
<p>よって</p>
<p>$$
f(z)=l_{i} \lambda_{i} \exp \left{-\lambda_{i}\left(z-z_{i}\right)\right}, \lambda_{i}=\frac{p^{\prime}\left(z_{i}\right)}{p\left(z_{i}\right)}, l_{i}=\frac{p\left(z_{i}\right)^{2}}{p\left(z_{i}\right)^{\prime}}
$$</p>
<p>$f(z)$の正規化係数は</p>
<p>$$
Z_{q}=\int f(z) d z=\sum_{i=1}^{N} \int_{\hat{z}<em>{i-1, i}}^{\hat{z}</em>{i, i+1}} f(z) d z
$$</p>
<p>$N$はグリッドの個数、$\hat{z}<em>{i-1, i}$は接線の交点の$z$座標。$\hat{z}</em>{0,1}=-\infty, \hat{z}<em>{N, N</em>{2} 1}=\infty$</p>
<p>ここで</p>
<p>$$
\begin{aligned}
\int_{\hat{z}<em>{i-1}, i}^{\hat{z}, i, i+1} f(z) d z &amp;=\int</em>{\hat{z}<em>{i-1, i}}^{\hat{z}</em>{i, i+1}} l_{i} \lambda_{i} \exp \left{-\lambda_{i}\left(z-z_{i}\right)\right} d z \
&amp;=l_{i} \lambda_{i} \int_{\hat{z}<em>{i-1, i}}^{\hat{z}</em>{i, i+1}} \exp \left{-\lambda_{i}\left(z-z_{i}\right)\right} d z \
&amp;=l_{i} \lambda_{i}\left[-\frac{1}{\lambda_{i}} \exp \left{-\lambda_{i}\left(z-z_{i}\right)\right}\right]<em>{\hat{z}</em>{i-1, i}}^{\hat{z}<em>{i, i+1}} \
&amp;=l</em>{i}\left[\exp \left{-\lambda_{i}\left(\hat{z}<em>{i-1, i}-z</em>{i}\right)\right}-\exp \left{-\lambda_{i}\left(\hat{z}_{i, i+1}-z\right)\right}\right]
\end{aligned}
$$</p>
<p>よって正規化係数は以下になる。
$$
Z_{q}=\sum_{i=1}^{N} l_{i}\left[\exp \left{-\lambda_{i}\left(\hat{z}<em>{i-1, i}-z</em>{i}\right)\right}-\exp \left{-\lambda_{i}\left(\hat{z}<em>{i, i+1}-z</em>{i}\right)\right}\right.
$$</p>
<p>ここで$(11.17)$は以下でも表現できる。
$$
q(z)=\frac{1}{z_{z}} f(z)
$$</p>
<p>$$
q(z)=\frac{1}{Z_{q}} l_{i} \lambda_{i} \exp \left{-\lambda_{i}\left(z-z_{i}\right)\right}
$$</p>
<h2 id="演習-119-1"><a class="header" href="#演習-119-1">演習 11.9</a></h2>
<div class="panel-primary">
<p>11.1.1節で述べた，単一の指数分布からサンプリングする技術を用いて
$$
q(z)=k_{i} \lambda_{i} \exp \left{-\lambda_{i}\left(z-z_{i}\right)\right} \quad \widehat{z}<em>{i-1, i} \lt z \leqslant \widehat{z}</em>{i, i+1} \tag{11.17}
$$
で定義される区分的な指数分布からサンプリングするアルゴリズムを考案せよ．</p>
</div>
<p>区分的な指数分布からのサンプリング確率を知る必要があるが、(11.17)の$q(z)$は正規化されていない。従って、まず初めに正規化定数$Z_{q}$を求める。</p>
<p>$$
\begin{aligned}
Z_{q} &amp;=\int_{\tilde{z}<em>{0,1}}^{\tilde{z}</em>{N, N+1}} q(z) d z=\sum_{i=1}^{N} \int_{\tilde{z}<em>{i-1, i}}^{\tilde{z}</em>{i, i+1}} q_{i}\left(z_{i}\right) d z_{i} \
&amp;=\sum_{i=1}^{N} \int_{\tilde{z}<em>{i-1, i}}^{\tilde{z}</em>{i, i+1}} k_{i} \lambda_{i} \exp \left{-\lambda_{i}\left(z-z_{i}\right)\right} d z_{i} \
&amp;=\sum_{i=1}^{N}-\left.k_{i} \exp \left{-\lambda_{i}\left(z-z_{i}\right)\right}\right|<em>{\tilde{z}</em>{i-1, i}} ^{\tilde{z}<em>{i, i+1}} \
&amp;=\sum</em>{i=1}^{N}-k_{i}\left[\exp \left{-\lambda_{i}\left(\widetilde{z}<em>{i, i+1}-z</em>{i}\right)\right}-\exp \left{-\lambda_{i}\left(\widetilde{z}<em>{i-1, i}-z</em>{i}\right)\right}\right]=\sum_{i=1}^{N} \widehat{k}<em>{i}
\end{aligned}
$$
$\widehat{k}</em>{i}$は以下のように定義する。</p>
<p>$$
\widehat{k}<em>{i}=-k</em>{i}\left[\exp \left{-\lambda_{i}\left(\widetilde{z}<em>{i, i+1}-z</em>{i}\right)\right}-\exp \left{-\lambda_{i}\left(\widetilde{z}<em>{i-1, i}-z</em>{i}\right)\right}\right]
\tag{A}
$$</p>
<p>この導出から、$i$番目の区分からサンプリングする確率は、$Z_{q}=\sum_{i=1}^{N} \widehat{k}<em>{i}$とすると、 $\widehat{k}</em>{i} / Z_{q}$で与えられる。 そこで今度は区間$[0,1]$で一様な補助的な確率変数$\eta$を定義する。
$$
i=j \quad \text { if } \eta \in\left[\frac{1}{Z_{q}} \sum_{m=0}^{j-1} \widehat{k}<em>{m}, \frac{1}{Z</em>{q}} \sum_{m=0}^{j} \widehat{k}<em>{m}\right], \quad j=1,2, \ldots, N
\tag{B}
$$
ここで便宜上、$\widehat{k}</em>{0}=0$と定義し、ここまでで選択された$i$番目の区分を決定した。
次に、11.1.1節の手法を用いて、i番目の指数分布からサンプリングする。(11.6)より、次のように書くことができる。</p>
<p>$$
\begin{aligned}
h_{i}(z) &amp;=\int_{\tilde{z}<em>{i-1, i}}^{z} \frac{q</em>{i}\left(z_{i}\right)}{\widehat{k}<em>{i}} d z</em>{i} \
&amp;=\frac{1}{\widehat{k}<em>{i}} \int</em>{\tilde{z}<em>{i-1, i}}^{z} k</em>{i} \lambda_{i} \exp \left{-\lambda_{i}\left(z-z_{i}\right)\right} d z_{i} \
&amp;=\left.\frac{-k_{i}}{\widehat{k}<em>{i}} \exp \left{-\lambda</em>{i}\left(z-z_{i}\right)\right}\right|<em>{\tilde{z}</em>{i-1, i}} ^{z} \
&amp;=\frac{-k_{i}}{\widehat{k}<em>{i}}\left[\exp \left{-\lambda</em>{i}\left(z-z_{i}\right)\right}-\exp \left{-\lambda_{i}\left(\widetilde{z}<em>{i-1, i}-z</em>{i}\right)\right}\right] \
&amp;=\frac{k_{i}}{\widehat{k}<em>{i}}\exp \left(\lambda</em>{i} z_{i}\right)\left[\exp \left{-\lambda_{i} \widetilde{z}<em>{i-1, i}\right}-\exp \left{-\lambda</em>{i} z\right}\right]
\end{aligned}
$$
なお、$q_{i}(z)$は正しく正規化されておらず、$q_{i}(z)/\widehat{k}<em>{i}$が正しい正規化された形になる。
いくつか並べ替えを行うと次のようになる。
$$
\begin{aligned}
h</em>{i}^{-1}(\xi) &amp;=\frac{1}{-\lambda_{i}} \ln \left[\exp \left{-\lambda_{i} \tilde{z}<em>{i-1, i}\right}-\frac{\xi}{\frac{k</em>{i}}{\hat{k}<em>{i}} \exp \left(\lambda</em>{i} z_{i}\right)}\right] \
&amp;=\frac{1}{-\lambda_{i}} \frac{\ln \left[\exp \left{-\lambda_{i} \tilde{z}<em>{i-1, i}\right]\right.}{\ln \frac{\widehat{\hat{k}</em>{i} \xi}}{k_{i} \cdot \exp \left(\lambda_{i} z_{i}\right)}} \
&amp;=\frac{\tilde{z}<em>{i-1, i}}{\ln \xi+\ln \frac{\widehat{k}</em>{i}}{k_{i}}-\lambda_{i} z_{i}}
\end{aligned}
$$</p>
<p>結論として、まず、区間$[0,1]$に一様なランダム変数$\eta$を生成し、(B)に従って値$i$を求め、次に、同じく区間$[0,1]$に一様なランダム変数$x_i$を生成し、$z=h_{i}^{-1}(\xi)$を用いて$z$に変換している。</p>
<p>ここで，$z_{1}, z_{2}, \ldots, z_{N}$の格子点が与えられれば，$\lambda_{i}, \widetilde{z}<em>{i, i+1}$と$k</em>{i}$が得られることに注意。詳細は前問を参照。これらの変数が得られた後、(A)を用いて$\widehat{k}<em>{i}$も決定することができるため、$h</em>{i}^{-1}(\xi)$を決定することができる。</p>
<h2 id="演習-1110"><a class="header" href="#演習-1110">演習 11.10</a></h2>
<div class="panel-primary">
<p>$$
p\left(z^{(\tau+1)}=z^{(\tau)}\right)=0.5 \tag{11.34}
$$
$$
p\left(z^{(\tau+1)}=z^{(\tau)}+1\right)=0.25 \tag{11.35}
$$
および
$$
p\left(z^{(\tau+1)}=z^{(\tau)}-1\right)=0.25 \tag{11.36}
$$
で定義される整数上の単純なランダムウォークが$\mathbb{E}\left[\left(z^{(\tau)}\right)^{2}\right]=\mathbb{E}\left[\left(z^{(\tau-1)}\right)^{2}\right]+1 / 2$という性質を持ち，よって帰納法により$\mathbb{E}[(z^{(r)})^2] = \tau / 2$であることを示せ．</p>
</div>
<p>仮定より、
\begin{align}
\mathbb{E}[(z^{(r)})^2] &amp;= \sum (z^{(\tau)})^2 p(z^{(\tau)}) \
&amp;=  0.5 \cdot \mathbb{E}[(z^{(\tau-1)})^2]+ 0.25 \cdot \mathbb{E}[(z^{(\tau-1)}-1)^2]+ 0.25 \cdot \mathbb{E}[(z^{(\tau-1)}+1)^2] \
&amp;= \mathbb{E}\left[\left(z^{(\tau-1)}\right)^{2}\right]+1 / 2
\end{align}
である。今、$\mathbb{E}[(z^{(0)})^2] = 0$で、$\mathbb{E}\left[\left(z^{(\tau)}\right)^{2}\right] - \mathbb{E}\left[\left(z^{(\tau-1)}\right)^{2}\right] = 1/2$の等差数列と見做せるので、$\mathbb{E}[(z^{(r)})^2] = \tau / 2$である</p>
<h2 id="演習-1111"><a class="header" href="#演習-1111">演習 11.11</a></h2>
<div class="panel-primary">
<p>11.3節で述べたギブスサンプリングアルゴリズムは，
$$
p^{\star}(\mathbf{z}) T\left(\mathbf{z}, \mathbf{z}^{\prime}\right)=p^{\star}\left(\mathbf{z}^{\prime}\right) T\left(\mathbf{z}^{\prime}, \mathbf{z}\right) \tag{11.40}
$$
で定義される詳細釣り合い条件を満たすことを示せ．</p>
</div>
<p>ギブスサンプリングではある時刻$\tau$において$\mathbf{z}^{(\tau)}$の全変数のうち$z_k$を除く変数すべてを固定して$z_k^{(\tau + 1)}$をサンプリングする。したがってこの一操作では$k$を除く残りすべての${z_{i}}<em>{i\ne k}$は不変である。つまり${{z</em>{i}}^{\prime}}<em>{i\ne k} = {z</em>{i}}_{i\ne k}$である。</p>
<p>P.254の説明から遷移確率$T\left(\mathbf{z}, \mathbf{z}^{\prime}\right) \equiv p\left(\mathbf{z}^{\prime} \mid \mathbf{z}\right)$であるが、ここでは一操作あたり$T\left(\mathbf{z}, \mathbf{z}^{\prime}\right) \equiv p\left({z_{k}}^{\prime} \mid {z_{i}}_{i\ne k}\right)$と書ける(下の式では$p^{\star}$としているが同じである)。</p>
<p>以上を用いると</p>
<p>$$
\begin{aligned}
p^{\star}(\mathbf{z}) T\left(\mathbf{z}, \mathbf{z}^{\prime}\right) &amp;=p^{\star}\left(z_{k},\left{z_{i}\right}<em>{i \neq k}\right) p^{\star}\left(z</em>{k}^{\prime} \mid\left{z_{i}\right}<em>{i \neq k}\right) \
&amp;=p^{\star}\left(z</em>{k} \mid\left{z_{i}\right}<em>{i \neq k}\right) p^{\star}\left(\left{z</em>{i}\right}<em>{i \neq k}\right) p^{\star}\left(z</em>{k}^{\prime} \mid\left{z_{i}\right}<em>{i \neq k}\right) \
&amp;=p^{\star}\left(z</em>{k} \mid\left{z_{i}^{\prime}\right}<em>{i \neq k}\right) p^{\star}\left(\left{z</em>{i}^{\prime}\right}<em>{i \neq k}\right) p^{\star}\left(z</em>{k}^{\prime} \mid\left{z_{i}^{\prime}\right}<em>{i \neq k}\right)\quad \left(\because {{z</em>{i}}^{\prime}}<em>{i\ne k} = {z</em>{i}}<em>{i\ne k} \right)\
&amp;=p^{\star}\left(z</em>{k} \mid\left{z_{i}^{\prime}\right}<em>{i \neq k}\right) p^{\star}\left(z</em>{k}^{\prime},\left{z_{i}^{\prime}\right}_{i \neq k}\right) \
&amp;=T\left(\mathbf{z}^{\prime}, \mathbf{z}\right)p^{\star}\left(\mathbf{z}^{\prime}\right) \
&amp;=p^{\star}\left(\mathbf{z}^{\prime}\right)T\left(\mathbf{z}^{\prime}, \mathbf{z}\right)
\end{aligned}
$$</p>
<p>以上で$(11.40)$式が示された。</p>
<h2 id="演習-1112"><a class="header" href="#演習-1112">演習 11.12</a></h2>
<div class="panel-primary">
<p><img src="https://i.imgur.com/TC56byx.png" alt="" /></p>
<p>図11.15に示す分布を考えよ．この分布に対する標準的なギブスサンプリングの手続きがエルゴード的かどうか，よって，この分布から正しくサンプリングするかどうか，について論ぜよ．</p>
</div>
<p>エルゴード的でない。なぜなら、$z_1$軸と$z_2$軸、どちらに射影してみても、影がつけられた二つの領域は重ならず、それゆえ片方の領域から最初の点としてサンプリングを行うと、ギブスサンプリングにより他方の領域に到達することはなく、初期サンプリングがどちらの領域に所属するかによって分布の収束先が変わってしまうからである。</p>
<h2 id="演習-1113"><a class="header" href="#演習-1113">演習 11.13</a></h2>
<div class="panel-primary">
<p><img src="https://i.imgur.com/4NkevK3.png" alt="" /></p>
<p>図11.16に示す単純な3ノードのグラフで, 観測ノード$x$が平均$\mu$，精度$\tau$のガウス分布$\mathcal{N}(x\mid \mu，\tau^{-1})$で与えられるものを考えよ．平均と精度の周辺分布が$\mathcal{N}(\mu \mid \mu_{0}，s_{0})$および$\operatorname{Gam}(\tau \mid a，b)$で与えられるとせよ．ここで，$\operatorname{Gam}(\cdot\mid\cdot,\cdot)$はガンマ分布を表す事後分布$p(\mu，\tau \mid x)$にギブスサンプリングを適用するために必要となる条件付き分布$p(\mu\mid x，\tau)$ と$p(\tau \mid x，\mu)$の式を書き下せ．</p>
</div>
<p>※</p>
<h2 id="演習-1114"><a class="header" href="#演習-1114">演習 11.14</a></h2>
<div class="panel-primary">
<p>$z_i$が平均$\mu_i$，分散$\sigma_{i}^{2}$を持ち，$\nu$が平均$0$，分散$1$を持つとき，過剰緩和の更新式
$$
z_{i}^{\prime}=\mu_{i}+\alpha\left(z_{i}-\mu_{i}\right)+\sigma_{i}\left(1-\alpha^{2}\right)^{1 / 2} \nu \tag{11.50}
$$
が平均$\mu_i$，分散$\sigma_i^{2}$である値$z_{i}^{\prime}$を与えることを示せ．</p>
</div>
<p>※期待値と分散の定義にしたがって計算していけば良い。</p>
<p>$z_{i}^{\prime}$の期待値は</p>
<p>$$
\begin{aligned} \mathbb{E}\left[z_{i}^{\prime}\right] &amp;=\mathbb{E}\left[\mu_{i}+\alpha\left(z_{i}-\mu_{i}\right)+\sigma_{i}\left(1-\alpha^{2}\right)^{1 / 2}{ }<em>{\nu}\right] \ &amp;=\mu</em>{i}+\alpha\left(\mathbb{E}\left[z_{i}\right]-\mu_{i}\right)+\sigma_{i}\left(1-\alpha^{2}\right)^{1 / 2} \mathbb{E}[\nu] \
&amp;=\mu_{i}+0+0 \
&amp;=\mu_{i} \end{aligned}
$$</p>
<p>$z_{i}^{\prime}$の分散は</p>
<p>$$
\begin{aligned} \operatorname{var}\left[z_{i}^{\prime}\right] &amp;=\mathbb{E}\left[\left(z_{i}^{\prime}\right)^{2}\right]-\mathbb{E}\left[z_{i}^{\prime}\right]^{2} \ &amp;=\mathbb{E}\left[\left(\mu_{i}+\alpha\left(z_{i}-\mu_{i}\right)+\sigma_{i}\left(1-\alpha^{2}\right)^{1 / 2} \nu\right)^{2}\right]-\mu_{i}^{2} \
&amp;=\alpha^{2} \mathbb{E}\left[\left(z_{i}-\mu_{i}\right)^{2}\right]+\sigma_{i}^{2}\left(1-\alpha^{2}\right) \mathbb{E}\left[\nu^{2}\right] \
&amp;=\alpha^{2} \sigma_{i}^{2}+\sigma_{i}^{2}\left(1-\alpha^{2}\right)\
&amp;=\sigma_{i}^{2} \end{aligned}
$$</p>
<h2 id="演習-1115"><a class="header" href="#演習-1115">演習 11.15</a></h2>
<div class="panel-primary">
<p>$$
K(\mathbf{r})=\frac{1}{2}|\mathbf{r}|^{2}=\frac{1}{2} \sum_{i} r_{i}^{2} \tag{11.56}
$$
$$
H(\mathbf{z}, \mathbf{r})=E(\mathbf{z})+K(\mathbf{r}) \tag{11.57}
$$
を用いて，ハミルトン方程式
$$
\frac{\mathrm{d} z_{i}}{\mathrm{~d} \tau}=\frac{\partial H}{\partial r_{i}} \tag{11.58}
$$
が
$$
r_{i}=\frac{\mathrm{d} z_{i}}{\mathrm{~d} \tau} \tag{11.53}
$$
と等価であることを示せ.同様に，$(11.57)$を用いて，
$$
\frac{\mathrm{d} r_{i}}{\mathrm{~d} \tau}=-\frac{\partial H}{\partial z_{i}} \tag{11.59}
$$
が
$$
\frac{\mathrm{d} r_{i}}{\mathrm{~d} \tau}=-\frac{\partial E(\mathbf{z})}{\partial z_{i}} \tag{11.55}
$$
に等価であることを示せ．</p>
</div>
<p>(11.57)を$r_{i}$で偏微分すると、
$$
\frac{\partial H}{\partial r_{i}}=\frac{\partial K}{\partial r_{i}}=r_{i}
(\because 11.56)
$$
(11.53)と比較して(11.58)を得る。</p>
<p>同様に(11.57)を$z_{i}$で偏微分すると、
$$
\frac{\partial H}{\partial z_{i}}=\frac{\partial E}{\partial z_{i}}
(\because 11.56)
$$</p>
<p>を得る。
(11.55)と比較して(11.59)を得る。</p>
<h2 id="演習-1116"><a class="header" href="#演習-1116">演習 11.16</a></h2>
<div class="panel-primary">
<p>$$
K(\mathbf{r})=\frac{1}{2}|\mathbf{r}|^{2}=\frac{1}{2} \sum_{i} r_{i}^{2} \tag{11.56}
$$
$$
H(\mathbf{z}, \mathbf{r})=E(\mathbf{z})+K(\mathbf{r}) \tag{11.57}
$$
および
$$
p(\mathbf{z}, \mathbf{r})=\frac{1}{Z_{H}} \exp (-H(\mathbf{z}, \mathbf{r})) \tag{11.63}
$$
を用いて，条件付き分布$p(\mathbf{r}\mid \mathbf{z})$がガウス分布であることを示せ．</p>
</div>
<p>※
ベイズの定理と(11.54),(11.63)によれば、
$$
\begin{align}
p(\mathbf{r} \mid \mathbf{z})&amp;=\frac{p(\mathbf{z}, \mathbf{r})}{p(\mathbf{z})}\
&amp;= \frac{1 / Z_{H} \cdot \exp (-H(\mathbf{z}, \mathbf{r}))}{1 / Z_{p} \cdot \exp (-E(\mathbf{z}))}\
&amp;= \frac{Z_{p}}{Z_{H}} \cdot \exp (-K(\mathbf{r}))(\because 11.57)\
&amp;= \frac{Z_{p}}{Z_{H}} \cdot \exp (-\frac{1}{2} \sum_{i} r_{i}^{2})(\because 11.56)
\end{align}
$$</p>
<p>従って、$p(\mathbf{r} \mid \mathbf{z})$はガウス分布に従う。</p>
<h2 id="演習-1117"><a class="header" href="#演習-1117">演習 11.17</a></h2>
<div class="panel-primary">
<p>2つの確率
$$
\frac{1}{Z_{H}} \exp (-H(\mathcal{R})) \delta V \frac{1}{2} \min \left{1, \exp \left(H(\mathcal{R})-H\left(\mathcal{R}^{\prime}\right)\right)\right} \tag{11.68}
$$
$$
\frac{1}{Z_{H}} \exp \left(-H\left(\mathcal{R}^{\prime}\right)\right) \delta V \frac{1}{2} \min \left{1, \exp \left(H\left(\mathcal{R}^{\prime}\right)-H(\mathcal{R})\right)\right} \tag{11.69}
$$
が等しくよってハイブリッドモンテカルロアルゴリズムで詳細釣り合い条件が満たされることを確認せよ．</p>
</div>
<p>$$
\frac{1}{Z_{H}} \exp (-H(R)) \delta V \frac{1}{2} \min \left{1, \exp \left(H(R)-H\left(R^{\prime}\right)\right)\right}
\tag{11.68}
$$
$$
\frac{1}{Z_{H}} \exp \left(-H\left(R^{\prime}\right)\right) \delta V \frac{1}{2} \min \left{1, \exp \left(H\left(R^{\prime}\right)-H(R)\right)\right}
\tag{11.69}
$$</p>
<p>$H(R)=H\left(R^{\prime}\right)$の時、両者は明らかに等しい。
$H(R)&gt;H\left(R^{\prime}\right)$の時、(11.68)は
$$
\frac{1}{Z_{H}} \exp (-H(R)) \delta V \frac{1}{2}
$$
に減少する。
この時、$(11.69)$は</p>
<p>$$
\frac{1}{Z_{H}} \exp \left(-H\left(R^{\prime}\right)\right) \delta V \frac{1}{2} \exp \left(H\left(R^{\prime}\right)-H(R)\right)
=\frac{1}{Z_{H}} \exp (-H(R)) \delta V \frac{1}{2}
$$
ゆえに両者は同一である。</p>
<p>$H(R)&lt;H\left(R^{\prime}\right)$の時も同様である。</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="prml第12章演習問題解答1215"><a class="header" href="#prml第12章演習問題解答1215">PRML第12章演習問題解答（〜12.15）</a></h1>
<head>
<style>
  div.panel-primary {
	border: 1px solid #000;
    margin: 10px 5px;
    padding: 16px 10px 0px;
  }
</style>
</head>
<h2 id="演習-121-1"><a class="header" href="#演習-121-1">演習 12.1</a></h2>
<div class="panel-primary">
<p>この演習問題では，帰納法を使って，射影されたデータの分散を最大化するような$M$次元部分空間の上への線形写像がデータ共分散行列$\mathbf{S}$,</p>
<p>$$\mathbf{S}=\frac{1}{N} \sum_{n=1}^{N}\left(\mathbf{x}<em>{n}-\overline{\mathbf{x}}\right)\left(\mathbf{x}</em>{n}-\overline{\mathbf{x}}\right)^{\mathrm{T}} \tag{12.3}$$</p>
<p>の上位$M$個の固有値に属する$M$本の固有ベクトルにより定義されることを証明する．12.1節では，$M=1$に対してこの結果を証明した．今度は，ある一般的な値$M$に対してこの結果が成り立つと仮定して，その下で$M+1$次元に対しても成り立つことを示す．これを行うため，最初に，射影されたデータの分散の，ベクトル$\mathbf{u}<em>{M+1}$に対する微分を$0$とおく．$\mathbf{u}</em>{M+1}$はデータ空間における新しい方向を定義する．このとき，次の2つの制約を同時に満足しなければならない．ひとつは，$\mathbf{u}<em>{M+1}$がすでに求めたベクトル$\mathbf{u}</em>{1},\ldots,\mathbf{u}<em>{M}$と直交するという制約であり，もうひとつは単位長さに規格化しておかなければならないという制約である．この制約を取り込むためにラグランジュ乗数(付録E)を使ってみよ．そうして，新しいベクトル$\mathbf{u}</em>{M+1}$が$\mathbf{S}$の固有ベクトルであることを示すために，ベクトル$\mathbf{u}<em>{1},\ldots,\mathbf{u}</em>{M}$の正規直交性を利用せよ．最後に，固有値が大きい順に並べられているときに，その固有ベクトル$\mathbf{u}<em>{M+1}$を$\lambda</em>{M+1}$に対応する固有ベクトルに選べば，分散が最大化されることを示せ．</p>
</div>
<p>※P.278下部の「一般の場合として$M$次元の射影空間を考えればデータ分散行列$\mathbf{S}$の, 大きい順に$M$個の固有値$\lambda_{1},\ldots,\lambda_{M}$に対応する$M$個の固有ベクトル$\mathbf{u}<em>{1},\ldots,\mathbf{u}</em>{M}$により，射影されたデータの分散を最大にする最適な線形射影が得られる」ことを帰納法で示す。</p>
<p>(i) $M=1$のとき、P.278の$(12.4)-(12.6)$の手続きによって$\mathbf{Su}<em>{1} = \lambda</em>{1}\mathbf{u}<em>{1}$となり、このとき$\mathbf{u}</em>{1}^{\mathrm T}\mathbf{Su}<em>{1}$は$\lambda</em>{1}$で最大となることが示される（本文参照）。</p>
<p>(ii) $M$次元についてもP.278の議論が成立していると仮定する。すなわち、
$$
\mathbf{S}\mathbf{u}<em>{M} = \lambda</em>{M}\mathbf{u}<em>{M}
$$
である。この条件下で、$\mathbf{u}</em>{M+1}\mathbf{S}\mathbf{u}<em>{M+1}$を$\mathbf{u}</em>{M+1}$に対して最大化したとき、$\mathbf{S}\mathbf{u}<em>{M+1} = \lambda</em>{M+1}\mathbf{u}<em>{M+1}$が成立することを示す。（この式から$\lambda</em>{M+1}$は$\lambda_{1},\ldots,\lambda_{M}$に次ぐ最大の$\mathbf{S}$の固有値となり、$\mathbf{u}<em>{M+1}\mathbf{S}\mathbf{u}</em>{M+1} = \lambda_{M+1}$となることは示される。）</p>
<p>2つの制約をラグランジュ未定乗数法を用いて導入する。1つは$\mathbf{u}<em>{M+1}$が正規直交基底であることの
$$
\mathbf{u}</em>{M+1}^{\mathrm T}\mathbf{u}<em>{M+1} = 1
$$
であり、もう1つは$\mathbf{u}</em>{M+1}$がすでに求めたベクトル$\mathbf{u}<em>{1},\ldots,\mathbf{u}</em>{M}$と直交することの
$$
\mathbf{u}<em>{i}^{\mathrm T}\mathbf{u}</em>{M+1} = 0 \quad \textrm{for} \quad i=1,\ldots, M
$$
である。これらの制約をそれぞれ未定乗数$\lambda, \eta_{i}$を用いてラグランジュ関数にすると</p>
<p>$$
\mathbf{u}<em>{M+1}\mathbf{S}\mathbf{u}</em>{M+1} + \lambda_{M+1}\left( 1-\mathbf{u}<em>{M+1}^{\mathrm T}\mathbf{u}</em>{M+1} \right) + \sum_{i=1}^{M}\eta_{i}\mathbf{u}<em>{i}^{\mathrm T}\mathbf{u}</em>{M+1}
$$</p>
<p>この関数の停留点を求める。$\mathbf{u}_{M+1}$についてこのラグランジュ関数を微分すると</p>
<p>$$
2\mathbf{S}\mathbf{u}<em>{M+1} - 2\lambda</em>{M+1}\mathbf{u}<em>{M+1} + \sum</em>{i=1}^{M}\eta_{i}\mathbf{u}_{i} = 0
$$</p>
<p>この式に左から$\mathbf{u}_{j}^{\mathrm T}\ (j=1,\ldots, M)$をかけると、第2項は正規直交基底の性質から$0$となる。第1項については</p>
<p>$$
\begin{aligned}
\mathbf{u}<em>{j}^{\mathrm T}\mathbf{S}\mathbf{u}</em>{M+1} &amp;= \mathbf{u}<em>{M+1}^{\mathrm T}\mathbf{S}\mathbf{u}</em>{j} \quad (\because \textrm{scholar}) \
&amp;=\mathbf{u}<em>{M+1}^{\mathrm T}\lambda</em>{j}\mathbf{u}_{j} \quad (\because \textrm{仮定}) \
&amp;=0
\end{aligned}
$$</p>
<p>より$0$となる。よって第3項について、$i \neq j$では$0$, $i=j$では$\eta_{j}$となるので、</p>
<p>$$
\eta_{i} = 0 \quad \textrm{for} \quad i=1,\ldots, M
$$</p>
<p>となる。すなわち停留点は</p>
<p>$$
\mathbf{S}\mathbf{u}<em>{M+1} = \lambda</em>{M+1}\mathbf{u}_{M+1}
$$</p>
<p>のときに得られる。以上(i), (ii)から任意の$M$次元について$\mathbf{S}\mathbf{u}<em>{M} = \lambda</em>{M}\mathbf{u}<em>{M}$が成立するときに射影された分散$\mathbf{u}</em>{M}^{\mathrm T}\mathbf{S}\mathbf{u}_{M}$が最大化されることが帰納的に示された。</p>
<p>最後に、$\mathbf{u}_{M+1}^{\mathrm T}$を左からかければ</p>
<p>$$
\mathbf{u}<em>{M+1}^{\mathrm T}\mathbf{S}\mathbf{u}</em>{M+1} = \lambda_{M+1}
$$</p>
<p>がただちに得られ、$\mathbf{u}<em>{M+1}$ベクトルに対して射影されたデータの分散$\mathbf{u}</em>{M+1}^{\mathrm T}\mathbf{S}\mathbf{u}<em>{M+1}$は$\lambda</em>{M+1}$で最大値をとることが得られる。</p>
<h2 id="演習-122-1"><a class="header" href="#演習-122-1">演習 12.2</a></h2>
<div class="panel-primary">
<p>$$
J=\frac{1}{N} \sum_{n=1}^{N} \sum_{i=M+1}^{D}\left(\mathbf{x}<em>{n}^{\mathrm{T}} \mathbf{u}</em>{i}-\overline{\mathbf{x}}^{\mathrm{T}} \mathbf{u}<em>{i}\right)^{2}=\sum</em>{i=M+1}^{D} \mathbf{u}<em>{i}^{\mathrm{T}} \mathbf{S} \mathbf{u}</em>{i} \tag{12.15}
$$</p>
<p>で与えられる主成分分析の歪み尺度$J$の，正規直交条件</p>
<p>$$
\mathbf{u}<em>{i}^{\mathrm T}\mathbf{u}</em>{j} = \delta_{ij} \tag{12.7}
$$</p>
<p>の下での$\mathbf{u}<em>{i}$に対する最小値は，$\mathbf{u}</em>{i}$がデータ共分散行列$\mathbf{S}$の固有ベクトルであるときに得られることを示せ．これを行うために，ラグランジュ乗数の行列$\mathbf{H}$を導入し制約条件のそれぞれを取り込む．その結果，歪み尺度の式は修正を受け，行列形式で表すと</p>
<p>$$\tilde{J}=\operatorname{Tr}\left{\widehat{\mathbf{U}}^{\mathrm{T}} \mathbf{S} \widehat{\mathbf{U}}\right}+\operatorname{Tr}\left{\mathbf{H}\left(\mathbf{I}-\widehat{\mathbf{U}}^{\mathrm{T}} \widehat{\mathbf{U}}\right)\right} \tag{12.93}$$</p>
<p>のようになる．ここで，$\widehat{\mathbf{U}}$は$D \times (D - M)$行列で，その列ベクトルは$\mathbf{u}_{i}$で与えられる．$\widehat{\mathbf{U}}$についてこの$\tilde{J}$を最小化しその解が$\mathbf{S\widehat{U}}=\widehat{\mathbf{U}} \mathbf{H}$を満たすことを示せ．明らかに，可能なひとつの解は，$\widehat{\mathbf{U}}$の列ベクトルが$\mathbf{S}$の固有ベクトルとなっている場合である．その場合， $\mathbf{H}$は対応する固有値を持った対角行列となる．一般的な解を得るために，$\mathbf{H}$が対称行列と仮定できることを示し，その固有ベクトル展開を用いることで，$\mathbf{S\widehat{U}}=\widehat{\mathbf{U}} \mathbf{H}$の一般解が，$\mathbf{U}$の列ベクトルを$\mathbf{S}$の固有ベクトルに選ぶという特解と同じ$\tilde{J}$の値を与えることを示せ．これらの解は等価なので固有ベクトルの方の解を選んだ方が楽である．</p>
</div>
<p>※ P.280下部の議論は$M=1, D=2$の特別な場合であり、これを任意の$D$と$D&lt;M$条件下に拡張する。</p>
<p>（前半）
演習12.1のように、ラグランジュ未定乗数法を用いてラグランジュ関数$\tilde{J}$を定義する。</p>
<p>$$
\tilde{J} = \underbrace{\sum_{i=M+1}^D \mathbf{u}<em>{i} \mathbf{S} \mathbf{u}</em>{i}}<em>{歪み尺度J} +
\underbrace{\sum</em>{i=M+1}^D \lambda_i (1-\mathbf{u}<em>{i}^{\mathrm T} \mathbf{u}</em>{i})}<em>{\mathbf{u}</em>{i} が規格化されている条件} +
\underbrace{\sum_{i=M+1}^{D-1}\sum_{j=i+1}^{D}\mu_{ij}(-\mathbf{u}<em>{j}\mathbf{u}</em>{i})}<em>{\mathbf{u}</em>{j} と \mathbf{u}_{i} が直交している条件}
$$</p>
<p>これを展開していくと$(12.93)$が得られることを示す。問題文の設定から</p>
<p>$$
\widehat{\mathbf{U}} = \begin{pmatrix} \mathbf{u}<em>{M+1} &amp; \mathbf{u}</em>{M+2} &amp; \cdots &amp; \mathbf{u}_{D} \end{pmatrix}
$$</p>
<p>ラグランジュ乗数の行列$\mathbf{H}$を以下のように設定する。</p>
<p>$$
\mathbf{H}=\begin{pmatrix}
\lambda_{M+1} &amp; \frac{1}{2} \mu_{M+1, M+2} &amp; \ldots &amp; \frac{1}{2} \mu_{M+1, D} \
\frac{1}{2} \mu_{M+1, M+2} &amp; \lambda_{M+2} &amp; \ldots &amp; \vdots \
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \
\frac{1}{2} \mu_{M+1, D} &amp; \ldots &amp; \ldots &amp; \lambda_{D}
\end{pmatrix} \quad (\mathbf{H}は対称行列)
$$</p>
<p>まず$(12.93)$の第1項の歪み尺度$\displaystyle \sum_{i=M+1}^D \mathbf{u}<em>{i} \mathbf{S} \mathbf{u}</em>{i}$が$\operatorname{Tr}\left{\widehat{\mathbf{U}}^{\mathrm{T}} \mathbf{S} \widehat{\mathbf{U}}\right}$と一致することを示す。これは</p>
<p>$$
\begin{aligned}
\hat{\mathbf{U}}^{\mathrm T} \mathbf{S} \hat{\mathbf{U}} &amp;=\begin{pmatrix}
\mathbf{u}<em>{M+1}^{\mathrm T} \
\mathbf{u}</em>{M+2}^{\mathrm T} \
\vdots \
\mathbf{u}<em>{D}^{\mathrm T}
\end{pmatrix} \mathbf{S} \begin{pmatrix} \mathbf{u}</em>{M+1} &amp; \mathbf{u}<em>{M+2} &amp; \ldots &amp;\mathbf{u}</em>{D}
\end{pmatrix}\
&amp;=\begin{pmatrix}
\mathbf{u}<em>{M+1}^{\mathrm T} \
\mathbf{u}</em>{M+2}^{\mathrm T} \
\vdots \
\mathbf{u}<em>{D}^{\mathrm T}
\end{pmatrix}
\begin{pmatrix}
\mathbf{S} \mathbf{u}</em>{M+1} &amp; \mathbf{S} \mathbf{u}<em>{M+2} &amp; \ldots &amp; \mathbf{S} \mathbf{u}</em>{D}
\end{pmatrix}
\
&amp;=\begin{pmatrix}
\mathbf{u}<em>{M+1}^{\mathrm T} \mathbf{S u}</em>{M+1} &amp; \mathbf{u}<em>{M+1}^{\mathrm T} \mathbf{S u}</em>{M+2} &amp; \ldots &amp; \mathbf{u}<em>{M+1}^{\mathrm T} \mathbf{S u}</em>{D} \
\mathbf{u}<em>{M+2}^{\mathrm T} \mathbf{S u}</em>{M+1} &amp; \mathbf{u}<em>{M+2}^{\mathrm T} \mathbf{S u}</em>{M+2} &amp; \ldots &amp; \vdots \
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \
\mathbf{u}<em>{D}^{\mathrm T} \mathbf{S u}</em>{M+1} &amp; \ldots &amp; \ldots &amp; \mathbf{u}<em>{D}^{\mathrm T} \mathbf{S u}</em>{D}
\end{pmatrix}
\end{aligned}
$$</p>
<p>これより$\displaystyle \sum_{i=M+1}^D \mathbf{u}<em>{i} \mathbf{S} \mathbf{u}</em>{i} = \operatorname{Tr}\left{\widehat{\mathbf{U}}^{\mathrm{T}} \mathbf{S} \widehat{\mathbf{U}}\right}$が示された。</p>
<p>続いて残りを計算する</p>
<p>$$
\begin{aligned}
&amp;\mathbf{H}\left(\mathbf{I}-\widehat{\mathbf{U}}^{\mathrm T} \widehat{\mathbf{U}}\right) \
=\ &amp;\mathbf{H}\left(\mathbf{I}-\begin{pmatrix}
\mathbf{u}<em>{M+1}^{\mathrm T} \
\mathbf{u}</em>{M+2}^{\mathrm T} \
\vdots \
\mathbf{u}<em>{D}^{\mathrm T}
\end{pmatrix}\begin{pmatrix}
\mathbf{u}</em>{M+1} &amp; \mathbf{u}<em>{M+2} &amp; \ldots &amp; \mathbf{u}</em>{D}
\end{pmatrix}\right)\
=\ &amp; \mathbf{H}\left(\begin{pmatrix}
1 &amp; 0 &amp; \ldots &amp; 0 \
0 &amp; 1 &amp; \ldots &amp; \vdots \
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \
0 &amp; \ldots &amp; \ldots &amp; 1
\end{pmatrix}-\begin{pmatrix}
\mathbf{u}<em>{M+1}^{\mathrm T} \mathbf{u}</em>{M+1} &amp; \mathbf{u}<em>{M+1}^{\mathrm T} \mathbf{u}</em>{M+2} &amp; \ldots &amp; \mathbf{u}<em>{M+1}^{\mathrm T} \mathbf{u}</em>{D} \
\mathbf{u}<em>{M+2}^{\mathrm T} \mathbf{u}</em>{M+1} &amp; \mathbf{u}<em>{M+2}^{\mathrm T} \mathbf{u}</em>{M+2} &amp; \ldots &amp; \vdots \
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \
\mathbf{u}<em>{D}^{\mathrm T} \mathbf{u}</em>{M+1} &amp; \ldots &amp; \ldots &amp; \mathbf{u}<em>{D}^{\mathrm T} \mathbf{u}</em>{D}
\end{pmatrix}\right) \
=\ &amp;\begin{pmatrix}
\lambda_{M+1} &amp; \frac{1}{2} \mu_{M+1, M+2} &amp; \cdots &amp; \frac{1}{2} \mu_{M+1, D} \
\frac{1}{2} \mu_{M+1, M+2} &amp; \lambda_{M+2} &amp; \cdots &amp; \vdots \
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \
\frac{1}{2} \mu_{M+1, D} &amp; \cdots &amp; \cdots &amp; \lambda_{D}
\end{pmatrix}\begin{pmatrix}
1-\mathbf{u}<em>{M+1}^{\mathrm T} \mathbf{u}</em>{M+1} &amp; -\mathbf{u}<em>{M+1}^{\mathrm T} \mathbf{u}</em>{M+2} &amp; \cdots &amp; -\mathbf{u}<em>{M+1}^{\mathrm T} \mathbf{u}</em>{D} \
-\mathbf{u}<em>{M+2}^{\mathrm T} \mathbf{u}</em>{M+1} &amp; 1-\mathbf{u}<em>{M+2}^{\mathrm T} \mathbf{u}</em>{M+2} &amp; \cdots &amp; \vdots \
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \
-\mathbf{u}<em>{D}^{\mathrm T} \mathbf{u}</em>{M+1} &amp; \cdots &amp; \cdots &amp; 1-\mathbf{u}<em>{D}^{\mathrm T} \mathbf{u}</em>{D}
\end{pmatrix}
\end{aligned}
$$</p>
<p>これより</p>
<p>$$
\begin{aligned}
&amp; \operatorname{Tr}\left(\mathbf{H}\left(\mathbf{I}-\widehat{\mathbf{U}}^{\mathrm T} \widehat{\mathbf{U}}\right)\right) \
=\ &amp; \lambda_{M+1}\left(1-\mathbf{u}<em>{M+1}^{\mathrm T} \mathbf{u}</em>{M+1}\right)+\frac{1}{2} \mu_{M+1, M+2}\left(-\mathbf{u}<em>{M+2}^{\mathrm T} \mathbf{u}</em>{M+1}\right)+\ldots+\frac{1}{2} \mu_{M+1, D}\left(-\mathbf{u}<em>{D}^{\mathrm T} \mathbf{u}</em>{M+1}\right) \
&amp;+\frac{1}{2} \mu_{M+1, M+2}\left(-\mathbf{u}<em>{M+1}^{\mathrm T} \mathbf{u}</em>{M+2}\right)+\lambda_{M+2}\left(1-\mathbf{u}<em>{M+2}^{\mathrm T} \mathbf{u}</em>{M+2}\right)+\ldots \
&amp; \quad\quad\quad \vdots \
&amp;+\frac{1}{2} \mu_{M+1, D}\left(-\mathbf{u}<em>{M+1}^{\mathrm T} \mathbf{u}</em>{D}\right)+\ldots+\lambda_{D}\left(1-\mathbf{u}<em>{D}^{\mathrm T} \mathbf{u}</em>{D}\right) \
=\ &amp; \sum_{i=M+1}^{D} \lambda_{i}\left(1-\mathbf{u}<em>{i}^{\mathrm T} \mathbf{u}</em>{i}\right)+\sum_{i=M+1}^{D-1} \sum_{j=i+1}^{D} \mu_{i j}\left(-\mathbf{u}<em>{j}^{\mathrm T} \mathbf{u}</em>{i}\right)
\end{aligned}
$$</p>
<p>以上から$(12.93)$式が示された。</p>
<p>（2）「$\widehat{\mathbf{U}}$についてこの$\tilde{J}$を最小化しその解が$\mathbf{S\widehat{U}}=\widehat{\mathbf{U}} \mathbf{H}$を満たすことを示せ」</p>
<p>$(12.93)$式を$\widehat{\mathbf{U}}$で微分すると、Matrix Cookbookの公式(108),(112)を使って</p>
<p>$$
\begin{aligned}
\frac{\partial\tilde{J}}{\partial \widehat{\mathbf{U}}}&amp;=
\frac{\partial}{\partial \widehat{\mathbf{U}}}\operatorname{Tr}\left{\widehat{\mathbf{U}}^{\mathrm{T}} \mathbf{S} \widehat{\mathbf{U}}\right}+\frac{\partial}{\partial \widehat{\mathbf{U}}}\operatorname{Tr}\left{\mathbf{H}\left(\mathbf{I}-\widehat{\mathbf{U}}^{\mathrm{T}} \widehat{\mathbf{U}}\right)\right} \
&amp;=(\mathbf{S} \widehat{\mathbf{U}}+\mathbf{S}^{\mathrm T} \widehat{\mathbf{U}}) - (\widehat{\mathbf{U}}\mathbf{H}^{\mathrm T}+\widehat{\mathbf{U}}\mathbf{H}) \
&amp;=2\mathbf{S} \widehat{\mathbf{U}} - 2\widehat{\mathbf{U}}\mathbf{H}
\end{aligned}
$$</p>
<p>これを$0$とすれば、$\mathbf{S} \widehat{\mathbf{U}} = \widehat{\mathbf{U}}\mathbf{H}$を満たす$\widehat{\mathbf{U}}$が解となり、このとき$\tilde{J}$は最小値
$$
\begin{aligned} \tilde{J} &amp;=\operatorname{Tr}\left(\widehat{\mathbf{U}}^{\mathrm T} \mathbf{S} \widehat{\mathbf{U}}\right)+\operatorname{Tr}\left(\mathbf{H}\left(\mathbf{I}-\widehat{\mathbf{U}}^{\mathrm T} \widehat{\mathbf{U}}\right)\right) \ &amp;=\operatorname{Tr}\left(\widehat{\mathbf{U}}^{\mathrm T} \widehat{\mathbf{U}} \mathbf{H}\right)+\operatorname{Tr}\left(\mathbf{H}\left(\mathbf{I}-\widehat{\mathbf{U}}^{\mathrm T} \widehat{\mathbf{U}}\right)\right) \ &amp;=\operatorname{Tr}(\mathbf{H})=\sum_{i=M+1}^{D} \lambda_{i} \end{aligned}
$$
を得る。これはすなわち$\mathbf{S}$の固有値のうち大きい順から$M+1\sim D$番目の固有値となるときに最小となる。</p>
<p>(3) 一般的な解を得るために，$\mathbf{H}$が対称行列と仮定できることを示し，その固有ベクトル展開を用いることで，$\mathbf{S\widehat{U}}=\widehat{\mathbf{U}} \mathbf{H}$の一般解が，$\mathbf{U}$の列ベクトルを$\mathbf{S}$の固有ベクトルに選ぶという特解と同じ$\tilde{J}$の値を与えることを示せ．</p>
<p>$\mathbf{H}$の一般解を議論するために、すでに上で設定した対称行列$\mathbf{H}$を用いる。$\mathbf{\Phi}$を$(D-M)\times(D-M)$の対角行列、すなわち$\mathbf{\Phi}^{\mathrm T}\mathbf{\Phi} = \mathbf{\Phi}\mathbf{\Phi}^{\mathrm T}=\mathbf{I}$を満たす行列とすれば、<strong>任意の対称行列の異なる固有値に対応する固有ベクトルは直交する</strong>ので、</p>
<p>$$
\mathbf{H\Psi} = \mathbf{\Psi L}
$$</p>
<p>と書ける。ここで$\mathbf{L}$は対角成分が$\mathbf{H}$の固有値となっている対角行列である（※ここでは固有値は縮退している可能性がある）。</p>
<p>$\mathbf{S} \widehat{\mathbf{U}} = \widehat{\mathbf{U}}\mathbf{H}$に右から$\mathbf{\Psi}$をかけ、新たに$\widetilde{\mathbf{U}} = \widehat{\mathbf{U}}\mathbf{\Psi}$を定義すると</p>
<p>$$
\mathbf{SU\Psi} =\widehat{\mathbf{U}} \mathbf{H\Psi} =\widehat{\mathbf{U}} \mathbf{\Psi L} \
\mathbf{S}\widetilde{\mathbf{U}} = \widetilde{\mathbf{U}}\mathbf{L}
$$</p>
<p>となる。よって$\widetilde{\mathbf{U}}$の列ベクトルは$\mathbf{S}$の固有ベクトルになり、$\mathbf{L}$の対角成分が固有値となる。</p>
<p>最後に$\mathbf{S}\widetilde{\mathbf{U}} = \widetilde{\mathbf{U}}\mathbf{L}$を用いたときに$\tilde{J}$の最小値が$\mathbf{S} \widehat{\mathbf{U}} = \widehat{\mathbf{U}}\mathbf{H}$を用いたときと等価になることを示す。</p>
<p>$$
\begin{aligned}
\tilde{J} &amp;=\operatorname{Tr}\left(\widehat{\mathbf{U}}^{\mathrm T} \widehat{\mathbf{U}}\right)+\operatorname{Tr}\left(\mathbf{H}\left(\mathbf{I}-\widehat{\mathbf{U}}^{\mathrm T} \widehat{\mathbf{U}}\right)\right) \
&amp;=\operatorname{Tr}\left(\widehat{\mathbf{U}}^{\mathrm T} \mathbf{S} \widehat{\mathbf{U}} \mathbf{\Psi \Psi}^{\mathrm T}\right)+\operatorname{Tr}\left(\mathbf{H \Psi \Psi}^{\mathrm T}\right)-\operatorname{Tr}\left(\mathbf{H} \widehat{\mathbf{U}}^{\mathrm T} \widehat{\mathbf{U}} \mathbf{\Psi \Psi}^{\mathrm T}\right) \
&amp;=\operatorname{Tr}\left(\widetilde{\mathbf{U}}^{\mathrm T} \mathbf{S} \widetilde{\mathbf{U}}\right)+\operatorname{Tr}\left(\mathbf{\Psi S \Psi}^{\mathrm T}\right)-\operatorname{Tr}\left(\mathbf{L \Psi}^{\mathrm T} \widehat{\mathbf{U}}^{\mathrm T} \widetilde{\mathbf{U}}\right) \
&amp;=\operatorname{Tr}(\mathbf{L})+\operatorname{Tr}(\mathbf{L})-\operatorname{Tr}(\mathbf{L}) \
&amp;=\operatorname{Tr}(\mathbf{L}) \
&amp;=\sum_{i=1}^{D-m} \lambda_{i}
\end{aligned}
$$</p>
<p>これより、$\tilde{J}$を最小化するためには小さい順に$\mathbf{S}$の$D-m$個の固有値を選べば良い。この結果は一般的な対称行列$\mathbf{H}$を用いた場合の値と等価である。</p>
<h2 id="演習-123-1"><a class="header" href="#演習-123-1">演習 12.3</a></h2>
<div class="panel-primary">
<p>$$
\mathbf{u}<em>{i}=\frac{1}{\left(N \lambda</em>{i}\right)^{1 / 2}} \mathbf{X}^{\mathrm{T}} \mathbf{v}_{i} \tag{12.30}
$$</p>
<p>で定義される固有ベクトルが単位長さに規格化されていることを示せ．ただし固有ベクトル$\mathbf{v}_{i}$は単位長さを持っていると仮定する．</p>
</div>
<p>$| \mathbf{u}<em>{i}|^{2}=1$であることを示せば良い。P.286の$\mathbf{v}</em>{i} = \mathbf{Xu}_{i}$の定義と</p>
<p>$$
\frac{1}{N} \mathbf{X X}^{\mathrm{T}} \mathbf{v}<em>{i}=\lambda</em>{i} \mathbf{v}_{i} \tag{12.28}
$$</p>
<p>を利用する。</p>
<p>$$
\begin{aligned}
\mathbf{u}<em>{i}^{\mathrm T} \mathbf{u}</em>{i} &amp;=\left(\frac{1}{\left(N \lambda_{i}\right)^{1/2}}\right)^{2} \mathbf{v}<em>{i}^{\mathrm T} \mathbf{X} \mathbf{X}^{\mathrm T} \mathbf{v}</em>{i} \
&amp;=\frac{1}{N \lambda_{i}} \mathbf{v}<em>{i}^{\mathrm T}\left(N \lambda</em>{i}\right) \mathbf{v}<em>{i} \quad (\because(12.28)) \
&amp;=\mathbf{v}</em>{i}^{\mathrm T} \mathbf{v}<em>{i}=1 \quad \left(\because\left|\mathbf{v}</em>{i}\right|^{2}=1\right)
\end{aligned}
$$</p>
<p>以上で示された。</p>
<h2 id="演習-124-1"><a class="header" href="#演習-124-1">演習 12.4</a></h2>
<div class="panel-primary">
<p>確率的主成分分析モデルの潜在変数の空間分布
$$
p(\mathbf{z})=\mathcal{N}(\mathbf{z} \mid \mathbf{0}, \mathbf{I}) \tag{12.31}
$$
では，平均$0$で単位共分散のものが仮定されている．それを，より一般的なガウス分布$\mathcal{N}(\mathbf{z}\mid \mathbf{m},\mathbf{\Sigma})$で置き換えるものとしよう．モデルのパラメータの再定義により，$\mathbf{m}$と$\mathbf{\Sigma}$の任意の選択が，観測変数についての同ーの周辺分布$p(\mathbf{x})$を導くことを示せ．</p>
</div>
<p>$(2.113)-(2.115)$式より、</p>
<p>$$
\begin{aligned}
p(\mathbf{x}) &amp;= \mathcal{N}(\mathbf{Wm} + \boldsymbol{\mu}, \sigma^2\mathbf{I} + \mathbf{W\Sigma W}^T)
\end{aligned}
$$</p>
<p>ここで、</p>
<p>$$
\begin{aligned}
\tilde{\boldsymbol{\mu}} &amp;= \mathbf{Wm} + \boldsymbol{\mu} \
\tilde{\mathbf{W}} &amp;= \mathbf{W\Sigma}^{1/2}
\end{aligned}
$$</p>
<p>とおくと、$p(\mathbf{x})$は</p>
<p>$$
\begin{aligned}
p(\mathbf{x}) &amp;= \mathcal{N}(\tilde{\boldsymbol{\mu}}, \sigma^2\mathbf{I} + \tilde{\mathbf{W}}\tilde{\mathbf{W}}^{T})
\end{aligned}
$$
と表すことができ、これは</p>
<p>$$
p(\mathbf{x})=\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{C}) \tag{12.35}
$$</p>
<p>と同じ形である。</p>
<h2 id="演習-125-1"><a class="header" href="#演習-125-1">演習 12.5</a></h2>
<div class="panel-primary">
<p>$\mathbf{x}$を$D$次元のガウス分布$\mathcal{N}(\mathbf{x}\mid \boldsymbol{\mu},\mathbf{\Sigma})$に従う確率変数とし，$\mathbf{y}$を$M \times D$行列$\mathbf{A}$によって$\mathbf{y} = \mathbf{Ax} + \mathbf{b}$で定義される$M$次元確率変数とする．$\mathbf{y}$もまたガウス分布を持つことを示し，その平均と分散に対する表現を見出せ．$M \lt D$，$M = D$および$M \gt D$の各場合に対し，そのガウス分布の形がどうなるか考察せよ．</p>
</div>
<p>$\mathbf{A}$の「第$i$行、第$j$列」成分を$a_{ij}$、$\mathbf{\Sigma}$の「第$i$行、第$j$列」成分を$\sigma_{ij}$とおく。以降は、$\mathbf{A}$が正方行列ではないケースも包含した議論（のつもり）。</p>
<p>ガウス分布の線型性から、線型結合で得られる$\mathbf{y}$がガウス分布に従うことは明らか。
また、期待値に関する線型性から、
$$
\begin{aligned}
\mathbb{E}[\mathbf{A}\mathbf{x}+\mathbf{b}]
= \mathbf{A} \mathbb{E}[\mathbf{x} ] +\mathbf{b}
= \mathbf{A} \boldsymbol{\mu} +\mathbf{b}
\end{aligned}
$$</p>
<p>である。</p>
<p>共分散行列$\mathbb{V}[\mathbf{Ax}+\mathbf{b}]=\mathbb{V}[\mathbf{Ax}]$の「第$i$行、第$j$列」成分に関しては、</p>
<p>$$
\begin{aligned}
\big( \mathbb{V}[\mathbf{Ax}]\big)<em>{ij}
&amp;= {\rm E} \left[ \sum</em>{k=1}^D a_{ik}x_k \sum_{l=1}^D a_{jl}x_l \right] - {\rm E} \left[ \sum_{k=1}^D a_{ik}x_k \right] E\left[ \sum_{l=1}^D a_{jl}x_l \right] \
&amp;=  \sum_{k,l=1}^D a_{ik}a_{jl}{\rm E} \left[x_k x_l \right] -  \sum_{k,l=1}^D a_{ik} a_{jl}{\rm E} \left[ x_k \right]E\left[ x_l \right] \
&amp;=  \sum_{k,l=1}^D a_{ik}a_{jl} \left( {\rm E}\left[x_k x_l \right] -  {\rm E} \left[ x_k \right]E\left[ x_l \right]
\right)\
&amp;=  \sum_{k,l=1}^D a_{ik}a_{jl} \sigma_{kl}\
&amp;= \left( \mathbf{A}\boldsymbol{\Sigma} \mathbf{A}^{\rm T} \right)_{ij}
\end{aligned}
$$</p>
<p>であり、この式変形は$\mathbf{A}$が正方行列でなくても成立する・・・ような気がするのですが。</p>
<h2 id="演習-126-1"><a class="header" href="#演習-126-1">演習 12.6</a></h2>
<div class="panel-primary">
<p>確率的主成分分析モデルに対する有向確率グラフを描け．有向確率グラフについては12.2節に説明がある．観測変数$\mathbf{x}$の各要素は明示的に分離したノードとして示されるはずであり，したがって確率的主成分分析モデルでは，8.2.2節で議論したナイーブベイズモデルと同様の独立構造を持っていることを確かめよ．</p>
</div>
<p>確率的主成分分析モデルの有向確率グラフは図12.10のようになり，この図中のパラメータを省略し確率変数$z, x$のみに着目すると，8.2.2節の図8.24のナイーブベイズの有向グラフと同一の構造になることがわかる．このことから同様な独立構造を持っていることがわかる．
<img src="https://i.imgur.com/tcwfg97.png" alt="" /></p>
<h2 id="演習-127-1"><a class="header" href="#演習-127-1">演習 12.7</a></h2>
<div class="panel-primary">
<p>一般的な分布の平均と分散に対する</p>
<p>$$
\begin{aligned}
\mathbb{E}[x] &amp;=\mathbb{E}<em>{y}\left[\mathbb{E}</em>{x}[x \mid y]\right] &amp; (2.270)\ \operatorname{var}[x] &amp;=\mathbb{E}<em>{y}\left[\operatorname{var}</em>{x}[x \mid y]\right]+\operatorname{var}<em>{y}\left[\mathbb{E}</em>{x}[x \mid y]\right] &amp; (2.271)
\end{aligned}
$$</p>
<p>の結果を利用して。確率的主成分分析モデルの周辺分布$p(\mathbf{x})$に対する結果</p>
<p>$$
p(\mathbf{x})=\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{C}) \tag{12.35}
$$
を導け．</p>
</div>
<p>※
$$
\begin{aligned}
\mathbb{E}[\mathbf{x}] &amp;= \mathbb{E}_z[\mathbb{E}_x[\mathbf{x}|\mathbf{z}]] \notag \
&amp;= \mathbb{E}_z[\mathbf{Wz} + \mathbf{\mu}] \notag \
&amp;= \mathbf{\mu}. \notag
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
cov[\mathbf{x}] &amp;= \mathbb{E}_z[cov_x[\mathbf{x}|\mathbf{z}]] + cov_z[\mathbb{E}_x[\mathbf{x}|\mathbf{z}]] \notag \
&amp;= \mathbb{E}_z[\sigma^2\mathbf{I}] + cov_z[\mathbf{Wz} + \mathbf{\mu}] \notag \
&amp;= \sigma^2\mathbf{I} + \mathbb{E}_z[\mathbf{Wz}\mathbf{z}^T\mathbf{W}^T] \notag \
&amp;= \sigma^2\mathbf{I} + \mathbf{W}\mathbf{W}^T \notag
\end{aligned}
$$</p>
<h2 id="演習-128-1"><a class="header" href="#演習-128-1">演習 12.8</a></h2>
<div class="panel-primary">
<p>$$
p(\mathbf{x} \mid \mathbf{y})=\mathcal{N}\left(\mathbf{x} \mid \mathbf{\Sigma}\left{\mathbf{A}^{\mathrm{T}} \mathbf{L}(\mathbf{y}-\mathbf{b})+\mathbf{\Lambda} \boldsymbol{\mu}\right}, \mathbf{\Sigma}\right) \tag{2.116}
$$
の結果を利用して確率的主成分分析モデルで出てくる事後分布$p(\mathbf{z}\mid \mathbf{x})$が</p>
<p>$$
p(\mathbf{z} \mid \mathbf{x})=\mathcal{N}\left(\mathbf{z} \mid \mathbf{M}^{-1} \mathbf{~W}^{\mathbf{T}}(\mathbf{x}-\boldsymbol{\mu}), \sigma^{2} \mathbf{M}^{-1}\right) \tag{12.42}
$$</p>
<p>で与えられることを示せ．</p>
</div>
<p>※
(2.113), (2.114), (2.116), (2.117), (12.31), (12.32)式より、
$$
\begin{aligned}
p(\mathbf{z}|\mathbf{x}) &amp;= \mathcal{N}(\mathbf{z}|(\mathbf{I} + \sigma^{-2}\mathbf{W}^T\mathbf{W})^{-1}\mathbf{W}^T\sigma^{-2}\mathbf{I}(\mathbf{x} - \mathbf{\mu}), (\mathbf{I} + \sigma^{-2}\mathbf{W}^T\mathbf{W})^{-1}) \notag
\end{aligned}
$$</p>
<p>ここで、
$$
\begin{aligned}
(\mathbf{I} + \sigma^{-2}\mathbf{W}^T\mathbf{W})^{-1} &amp;= (\sigma^{-2}\mathbf{M})^{-1}\ \ (\because \mathrm{Eq}(12.41)) \notag \
&amp;= \sigma^2\mathbf{M}^{-1}, \notag
\end{aligned}
$$</p>
<p>であることから、
$$
\begin{aligned}
p(\mathbf{z}|\mathbf{x}) &amp;= \mathcal{N}(\mathbf{z}|\mathbf{M}^{-1}\mathbf{W}^T(\mathbf{x} - \mathbf{\mu}), \sigma^2\mathbf{M}^{-1}) \notag
\end{aligned}
$$</p>
<h2 id="演習-129-1"><a class="header" href="#演習-129-1">演習 12.9</a></h2>
<div class="panel-primary">
<p>確率的主成分分析モデルの対数尤度
$$
\begin{aligned}
\ln p\left(\mathbf{X} \mid \boldsymbol{\mu}, \mathbf{W}, \sigma^{2}\right) &amp;= \sum_{n=1}^{N} \ln p\left(\mathbf{x}<em>{n} \mid \mathbf{W}, \boldsymbol{\mu}, \sigma^{2}\right) \
&amp;=-\frac{N D}{2} \ln (2 \pi)-\frac{N}{2} \ln |\mathbf{C}|-\frac{1}{2} \sum</em>{n=1}^{N}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}\right)^{\mathrm{T}} \mathbf{C}^{-1}\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}\right)
\end{aligned}
\tag{12.43}
$$
をパラメータ$\boldsymbol{\mu}$に対して最大化すると，$\boldsymbol{\mu}_{\mathrm{ML}}=\overline{\mathbf{x}}$の結果になることを確かめよ．ただし$\overline{\mathbf{x}}$はデータベクトルの平均である．</p>
</div>
<p>対数尤度を$\ln L = \ln p(\mathbf{X}\mid \boldsymbol{\mu}, \mathbf{W}, \sigma^2)$とし、$\frac{\partial \ln L}{\partial \boldsymbol{\mu}} = 0$を求める。$(12.36)$の定義より$\mathbf{C}=\mathbf{W} \mathbf{W}^{\mathrm{T}}+\sigma^{2} \mathbf{I}$である。</p>
<p>$$
\begin{aligned}
\frac{\partial \ln L}{\partial \boldsymbol{\mu}} &amp;=-\frac{1}{2} \sum_{n=1}^{N} \frac{\partial}{\partial \boldsymbol{\mu}}\left(\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}\right)^{\mathrm T} \mathbf{C}^{-1}\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}\right)\right) \
&amp;=-\frac{1}{2} \sum_{n=1}^{N}\left(-2 \mathbf{C}^{-1}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}\right)\right) \
&amp;=-\sum</em>{n=1}^{N} \mathbf{C}^{-1} \boldsymbol{\mu}+\sum_{n=1}^{N} \mathbf{C}^{-1} \mathbf{x}<em>{n} \
&amp;= -N\mathbf{C}^{-1} \boldsymbol{\mu}+\sum</em>{n=1}^{N} \mathbf{C}^{-1} \mathbf{x}_{n}=0
\end{aligned}
$$</p>
<p>これを解くと</p>
<p>$$
\boldsymbol{\mu} = \frac{1}{N}\sum_{n=1}^{N}\mathbf{x}_{n} = \overline{\mathbf{x}}
$$</p>
<p>すなわちデータベクトルの平均が得られた。</p>
<h2 id="演習-1210"><a class="header" href="#演習-1210">演習 12.10</a></h2>
<div class="panel-primary">
<p>確率的主成分分析モデルの対数尤度関数
$$
\begin{aligned}
\ln p\left(\mathbf{X} \mid \boldsymbol{\mu}, \mathbf{W}, \sigma^{2}\right) &amp;= \sum_{n=1}^{N} \ln p\left(\mathbf{x}<em>{n} \mid \mathbf{W}, \boldsymbol{\mu}, \sigma^{2}\right) \
&amp;=-\frac{N D}{2} \ln (2 \pi)-\frac{N}{2} \ln |\mathbf{C}|-\frac{1}{2} \sum</em>{n=1}^{N}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}\right)^{\mathrm{T}} \mathbf{C}^{-1}\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}\right)
\end{aligned}
\tag{12.43}
$$
のパラメータ$\boldsymbol{\mu}$に対する2次微分を求めることにより，停留点$\boldsymbol{\mu}_{\mathrm{ML}}=\overline{\mathbf{x}}$が唯一の最大値を与える点となることを示せ．</p>
</div>
<p>演習問題12.9の続きで、もう一度$\boldsymbol{\mu}$で微分すると</p>
<p>$$
\frac{\partial}{\partial \boldsymbol{\mu}}\left(\frac{\partial \ln L}{\partial \boldsymbol{\mu}}\right) = -N\mathbf{C}^{-1}
$$</p>
<p>となる。ここで$\mathbf{C}$は正定値対称行列なので$-N\mathbf{C}^{-1}$は負定値対称行列になる。これはすなわち対数尤度関数が$\boldsymbol{\mu}$の値によらず上に凸であることを表すため、12.9で求めた点は極大かつ最大となる。</p>
<h2 id="演習-1211"><a class="header" href="#演習-1211">演習 12.11</a></h2>
<div class="panel-primary">
<p>$\sigma^{2}\to 0$の極限において，確率的主成分分析モデルの事後平均が，通常の主成分分析と同様に主部分空間の上への直交射影となることを示せ．</p>
</div>
<p>※
$\sigma^2 \rightarrow 0$のとき、 (12.41)式を(12.48)式に代入すると、
$$
\begin{aligned}
\mathbb{E}[\mathbf{z}|\mathbf{x}] = (\mathbf{W}<em>{ML}^T\mathbf{W}</em>{ML})^{-1}\mathbf{W}_{ML}^T (\mathbf{x} - \bar{\mathbf{x}}) \notag
\end{aligned}
$$</p>
<p>を得る。$\mathbf{R} = \mathbf{I}$として、(12.45)式を代入する。
$$
\begin{aligned}
(\mathbf{W}<em>{ML}^T\mathbf{W}</em>{ML})^{-1}\mathbf{W}_{ML}^T (\mathbf{x} - \bar{\mathbf{x}}) &amp;= \left[\left{(\mathbf{L}_M - \sigma^2\mathbf{I})^{1/2}\right}^T\mathbf{U}_M^T\mathbf{U}_M(\mathbf{L}_M - \sigma^2\mathbf{I})^{1/2}\right]\mathbf{W}^T (\mathbf{x} - \bar{\mathbf{x}}) \notag \
&amp;= (\mathbf{L}_M - \sigma^2\mathbf{I})^{-1}(\mathbf{L}_M - \sigma^2\mathbf{I})^{1/2}\mathbf{U}_M^T (\mathbf{x} - \bar{\mathbf{x}}) \notag \
&amp;= (\mathbf{L}_M - \sigma^2\mathbf{I})^{-1/2}\mathbf{U}_M^T (\mathbf{x} - \bar{\mathbf{x}}). \notag
\end{aligned}
$$</p>
<p>ここで1行目から2行目は$(\mathbf{L}_M - \sigma^2\mathbf{I})$が対称行列であること及び定義より$\mathbf{U}_M^T\mathbf{U}_M = \mathbf{I}$であることを用いた。$\sigma^2 \rightarrow 0$のとき上式は
$$
\begin{aligned}
\mathbf{L}_M^{-1/2}\mathbf{U}_M^T (\mathbf{x} - \bar{\mathbf{x}}) \notag
\end{aligned}
$$</p>
<p>となるが、これは非確率的な主成分分析について得られた結果 ((12.24)式) に等しい。</p>
<h2 id="演習-1212"><a class="header" href="#演習-1212">演習 12.12</a></h2>
<div class="panel-primary">
<p>$\sigma^{2}\gt 0$のとき，確率的主成分分析モデルの事後平均が，直交射影の結果と比べると，原点に向かってシフトしていることを示せ．</p>
</div>
<p>※
演習12.11の結果を用い、$\sigma^2 \rightarrow 0$のとき及び$\sigma^2 &gt; 0$のときの$\mathbf{z}$の期待値 ($\mathbf{z}_0$及び$\mathbf{z}_1$と表記する) のノルム (の2乗) を比較する。
$$
\begin{aligned}
|\mathbf{z}_0|^2 - |\mathbf{z}_1|^2 &amp;= (\mathbf{x} - \bar{\mathbf{x}})^T(\mathbf{U}_M\mathbf{L}<em>M^{-1}\mathbf{U}<em>M^T - \mathbf{W}</em>{ML}\mathbf{M}^{-1}\mathbf{M}^{-1}\mathbf{W}</em>{ML})(\mathbf{x} - \bar{\mathbf{x}}). \notag
\end{aligned}
$$</p>
<p>ここで、
$$
\begin{aligned}
\mathbf{M}^{-1} &amp;= (\mathbf{W}<em>{ML}^T\mathbf{W}</em>{ML} + \sigma^2\mathbf{I})^{-1} \notag \
&amp;= ((\mathbf{L}_M - \sigma^2\mathbf{I}) + \sigma^2\mathbf{I})^{-1} \notag \
&amp;= \mathbf{L}_M^{-1}. \notag
\end{aligned}
$$</p>
<p>よって、
$$
\begin{aligned}
|\mathbf{z}_0|^2 - |\mathbf{z}_1|^2 &amp;= (\mathbf{x} - \bar{\mathbf{x}})^T(\mathbf{U}_M\mathbf{L}_M^{-1}\mathbf{U}<em>M^T - \mathbf{W}</em>{ML}\mathbf{L}<em>M^{-2}\mathbf{W}</em>{ML})(\mathbf{x} - \bar{\mathbf{x}}) \notag \
&amp;= (\mathbf{x} - \bar{\mathbf{x}})^T\mathbf{U}_M(\mathbf{L}_M^{-1} - (\mathbf{L}_M - \sigma^2\mathbf{I})^{1/2}\mathbf{L}_M^{-2}(\mathbf{L}_M - \sigma^2\mathbf{I})^{1/2})\mathbf{U}_M^T(\mathbf{x} - \bar{\mathbf{x}}) \tag{*}
\end{aligned}
$$</p>
<p>となる。ここで、行列$(\mathbf{L}_M^{-1} - (\mathbf{L}_M - \sigma^2\mathbf{I})^{1/2}\mathbf{L}_M^{-2}(\mathbf{L}_M - \sigma^2\mathbf{I})^{1/2})$は、定義より$i$番目の対角要素が
$$
\begin{aligned}
\frac{1}{\lambda_i} - \frac{\lambda_i - \sigma^2}{\lambda_i^2} = \frac{\sigma^2}{\lambda_i^2} &gt; 0 \notag
\end{aligned}
$$</p>
<p>に等しい対角行列である。よって二次形式$(*)$は正。つまり、$|\mathbf{z}_0| &gt; |\mathbf{z}_1|$となり、$\sigma^{2}\gt 0$のときに事後平均が原点に向かってシフトしていることが示された。</p>
<h2 id="演習-1213"><a class="header" href="#演習-1213">演習 12.13</a></h2>
<div class="panel-primary">
<p>確率的主成分分析の下でもともとのデータ点を再現する際，通常の主成分分析の最小2乗射影のコスト関数を使うと，最適な再現点は以下で与えられることを示せ．</p>
<p>$$\widetilde{\mathbf{x}}=\mathbf{W}<em>{\mathrm{ML}}\left(\mathbf{W}</em>{\mathrm{ML}}^{\mathrm{T}}\mathbf{W}_{\mathrm{ML}}\right)^{-1} \mathbf{M} \mathbb{E}[\mathbf{z} \mid \mathbf{x}] \tag{12.94}$$</p>
</div>
<p>$$
\mathbb{E}[\mathbf{z} \mid \mathbf{x}]=\mathbf{M}^{-1} \mathbf{W}_{\mathrm{ML}}^{\mathrm{T}}(\mathbf{x}-\overline{\mathbf{x}}) \tag{12.48}
$$
の右辺を$(12.94)$に代入すると、次のようになる。</p>
<p>$$
\widetilde{\mathbf{x}}<em>{n}=\mathbf{W}</em>{\mathrm{ML}}\left(\mathbf{W}<em>{\mathrm{ML}}^{\mathrm{T}} \mathbf{W}</em>{\mathrm{ML}}\right)^{-1} \mathbf{W}<em>{\mathrm{ML}}^{\mathrm{T}}\left(\mathbf{x}</em>{n}-\overline{\mathbf{x}}\right)
$$</p>
<p>$$
\mathbf{W}<em>{\mathrm{ML}}=\mathbf{U}</em>{M}\left(\mathbf{L}_{M}-\sigma^{2} \mathbf{I}\right)^{1 / 2} \mathbf{R} \tag{12.45}
$$</p>
<p>より</p>
<p>$$
\left(\mathbf{W}<em>{\mathrm{ML}}^{\mathrm{T}} \mathbf{W}</em>{\mathrm{ML}}\right)^{-1}=\left(\mathbf{L}_{M}-\sigma^{2} \mathbf{I}\right)^{-1}
$$</p>
<p>そして</p>
<p>$$
\widetilde{\mathbf{x}}<em>{n}=\mathbf{U}</em>{M} \mathbf{U}<em>{M}^{\mathrm{T}}\left(\mathbf{x}</em>{n}-\overline{\mathbf{x}}\right)
$$</p>
<p>これは、M個の最大固有値に対応するM個の固有ベクトルを使って$x_n - x$を再構成したもので、12.1.2節から最小二乗射影コスト(12.11)を最小にすることがわかる。</p>
<h2 id="演習-1214"><a class="header" href="#演習-1214">演習 12.14</a></h2>
<div class="panel-primary">
<p>確率的主成分分析モデルの共分散行列の中にある独立なパラメータの数は，
$$
D M+1-M(M-1) / 2 \tag{12.51}
$$
で与えられる．ただし，潜在変数空間が$M$次元データ空間が$D$次元である．$M = D-1$の場合に，独立なパラメータの数が一般的な共分散行列を持つガウス分布と一致することを確かめよ．一方，$M=0$のときにそれが等方的な共分散を持つガウス分布と同じであることも確かめよ．</p>
</div>
<p>※
$M = D - 1$のとき、パラメータの数は
$$
\begin{aligned}
DM + 1 - \frac{M(M - 1)}{2} &amp;= D(D - 1) + 1 - \frac{(D - 1)(D - 2)}{2} \notag \
&amp;= \frac{1}{2}(D^2 + D) \notag \
&amp;= \frac{1}{2}(D^2 - D) + D. \notag
\end{aligned}
$$</p>
<p>となる。また、式(12.51)に$M = 0$を代入すると、このときパラメータの数は1となる (i.e., パラメータは$\sigma^2$のみ) ことがわかる。</p>
<h2 id="演習-1215"><a class="header" href="#演習-1215">演習 12.15</a></h2>
<div class="panel-primary">
<p>$$
\begin{aligned}\mathbb{E}\left[\ln p\left(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\mu}, \mathbf{W}, \sigma^{2}\right)\right]=&amp;-\sum_{n=1}^{N}\left{\frac{D}{2} \ln \left(2 \pi \sigma^{2}\right)+\frac{1}{2} \operatorname{Tr}\left(\mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm{T}}\right]\right)\right. \
&amp;+\frac{1}{2 \sigma^{2}}\left|\mathbf{x}<em>{n}-\boldsymbol{\mu}\right|^{2}-\frac{1}{\sigma^{2}} \mathbb{E}\left[\mathbf{z}</em>{n}\right]^{\mathrm{T}} \mathbf{W}^{\mathrm{T}}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}\right) \
&amp;+\frac{1}{2 \sigma^{2}} \operatorname{Tr}\left(\mathbb{E}\left[\mathbf{z}</em>{n} \mathbf{z}<em>{n}^{\mathrm{T}}\right] \mathbf{W}^{\mathrm{T}} \mathbf{W}\right)+\left.\frac{M}{2} \ln (2 \pi)\right}
\end{aligned} \tag{12.53}
$$
で与えられる完全データの対数尤度関数の期待値を最大化することにより、確率的主成分分析モデルのMステップの式
$$
\mathbf{W}</em>{\text {new }}=\left[\sum_{n=1}^{N}\left(\mathbf{x}<em>{n}-\overline{\mathbf{x}}\right) \mathbb{E}\left[\mathbf{z}</em>{n}\right]^{\mathrm{T}}\right]\left[\sum_{n=1}^{N} \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm{T}}\right]\right]^{-1} \tag{12.56}
$$
と
$$
\begin{aligned} \sigma_{\text {new }}^{2}=&amp; \frac{1}{N D} \sum_{n=1}^{N}\left{\left|\mathbf{x}<em>{n}-\overline{\mathbf{x}}\right|^{2}-2 \mathbb{E}\left[\mathbf{z}</em>{n}\right]^{\mathrm{T}} \mathbf{W}<em>{\text {new }}^{\mathrm{T}}\left(\mathbf{x}</em>{n}-\overline{\mathbf{x}}\right)\right.\ &amp;\left.+\operatorname{Tr}\left(\mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm{T}}\right] \mathbf{W}<em>{\text {new }}^{\mathrm{T}} \mathbf{W}</em>{\text {new }}\right)\right} \end{aligned} \tag{12.57}
$$
を導け．</p>
</div>
<p>※
Mステップの更新式を求めるためには、(12.53)式を$\mathbf{W}$及び$\sigma^2$について微分する必要がある。
まず、
$$
\begin{aligned}
\frac{\partial}{\partial \mathbf{W}}\mathbb{E}[\ln p (\mathbf{X}, \mathbf{Z}|\mathbf{\mu}, \mathbf{W}, \sigma^2)] &amp;= \sum_{n = 1}^N \left{\frac{1}{\sigma^2}(\mathbf{x}_n - \mathbf{\mu})\mathbb{E}[\mathbf{z}_n]^T - \frac{1}{\sigma^2}\mathbf{W}\mathbb{E}[\mathbf{z}_n\mathbf{z}_n^T]\right}. \notag
\end{aligned}
$$</p>
<p>ここでは<a href="https://www.math.uwaterloo.ca/%7Ehwolkowi/matrixcookbook.pdf">The Matrix Cookbook</a>の(71)式及びPRML(C.24)式を用いた。
また、
$$
\begin{aligned}
\frac{\partial}{\partial \mathbf{\sigma^2}}\mathbb{E}[\ln p (\mathbf{X}, \mathbf{Z}|\mathbf{\mu}, \mathbf{W}, \sigma^2)] &amp;= \sum_{n = 1}^N \left{\frac{D}{2\sigma^2} - \frac{1}{2\sigma^4}|\mathbf{x}_n - \mathbf{\mu}|^2 + \frac{1}{\sigma^4}\mathbb{E}[\mathbf{z}_n]\mathbf{W}^T(\mathbf{x}_n - \mathbf{\mu}) - \frac{1}{2\sigma^4}\mathrm{Tr}(\mathbb{E}[\mathbf{z}_n\mathbf{z}_n^T]\mathbf{W}^T\mathbf{W}) \right}. \notag
\end{aligned}
$$</p>
<p>これらの式を0とおき、$\mathbf{\mu} = \bar{\mathbf{x}}$を代入したうえで方程式を解くと(12.56)式及び(12.57)式を得る。</p>
<h2 id="演習-1216"><a class="header" href="#演習-1216">演習 12.16</a></h2>
<div class="panel-primary">
<p><img src="https://i.imgur.com/kXPibhJ.png" alt="" />
図12.11においていくつかのデータの値がランダムに欠損しているデータ集合に確率的主成分分析を適用する例を説明した．この状況において，確率的主成分分析モデルの尤度関数を最大化するEMアルゴリズムを導け．${\mathbf{z}_n}$に加えて，ベクトル${\mathbf{x}_n}$の要素である欠損データ値も潜在変数として扱う必要があることに注意せよ．すべてのデータ値が観測される特別な場合に，このアルゴリズムが12.2.2節で導かれた確率的主成分分析のEMアルゴリズムに帰着することを示せ．</p>
</div>
<p>まず、尤度を各データの各要素の和の形に分解する。</p>
<p>$$
\begin{aligned}
p(\mathbf{X}|\mathbf{\mu}, \mathbf{W}, \mathbf{\sigma^2}) &amp;= \int p(\mathbf{Z}) p(\mathbf{X}| \mathbf{Z}, \mathbf{\mu}, \mathbf{W}, \mathbf{\sigma^2}) d \mathbf{Z} \
&amp;= \prod_n^N \int p(\mathbf{z}_{n}) p(\mathbf{x}_n| \mathbf{z}<em>n, \mathbf{\mu}, \mathbf{W}, \mathbf{\sigma^2}) d \mathbf{z}<em>n \
&amp;= \prod_n^N \int p(\mathbf{z}</em>{n})\prod_i^D \mathcal{N}(x</em>{ni}| \mathbf{w}_i \mathbf{z}_n+ \mathbf{\mu}_i, \mathbf{\sigma^2}) d \mathbf{z}_n \
\end{aligned}
$$</p>
<p>今、$x_{ni}$は、$\mathbf{x}_n$の$i$番目の要素を示す。$\mathbf{\mu}_i$は$\mathbf{\mu}$の$i$番目の要素を、$\mathbf{w}_i$は、$\mathbf{W}$の$i$行目を指す。ランダム欠損であると仮定すると、上の式から欠損を積分消去できることによって尤度を求められる。よって、観察された変数、欠損した変数をそれぞれ$\mathbf{x}_n^o$、$\mathbf{x}_n^m$として書き直すと、</p>
<p>$$
\begin{aligned}
p(\mathbf{X}|\mathbf{\mu}, \mathbf{W}, \mathbf{\sigma^2}) &amp;=
\prod_n^N \int p(\mathbf{z}<em>{n})\prod</em>{x_{ni} \in \mathbf{x}<em>n^o} \mathcal{N}(x</em>{ni}| \mathbf{w}_i \mathbf{z}<em>n+ \mathbf{\mu}<em>i, \mathbf{\sigma^2}) \int \prod</em>{x</em>{ni} \in \mathbf{x}<em>n^m} \mathcal{N}(x</em>{ni}| \mathbf{w}_i \mathbf{z}_n+ \mathbf{\mu}<em>i, \mathbf{\sigma^2})
d \mathbf{x}<em>n^m d \mathbf{z}<em>n\
&amp;= \prod_n^N \int p(\mathbf{z}</em>{n})\prod</em>{x</em>{ni} \in \mathbf{x}<em>n^o} \mathcal{N}(x</em>{ni}| \mathbf{w}_i \mathbf{z}_n+ \mathbf{\mu}<em>i, \mathbf{\sigma^2})  d \mathbf{z}<em>n \
&amp;= \prod_n^N \int p(\mathbf{z}</em>{n}) \mathcal{N}(\mathbf{x</em>{n}^o}| \mathbf{z}_n, \mathbf{\mu}, \mathbf{W}, \mathbf{\sigma^2})  d \mathbf{z}_n \
&amp;= \prod_n p(\mathbf{x}_n^o|\mathbf{\mu}, \mathbf{W}, \mathbf{\sigma^2})
\end{aligned}
$$</p>
<p>すなわち、観察された$\mathbf{x}_n^o$から求められる。</p>
<p>ここで、観察/欠損を区別するために$\iota_{ni}$を導入する。これは$x_{ni}$が観察されたときに1を取り、欠損の時に0を取る変数である。よって、(12.32)式を書き直すと、</p>
<p>$$
\begin{aligned}
p(\mathbf{x}|\mathbf{z}) = \prod_i^D \mathcal{N}(x_{ni}|\mathbf{w}<em>i\mathbf{z}+ \mu_i, \sigma^2)^{\iota</em>{ni}}  \space \space (Ex12.16.1)
\end{aligned}
$$</p>
<p>になり、完全データ対数尤度は、これを用いて、
$$
\begin{aligned}
\ln p(\mathbf{X}, \mathbf{Z}|\mathbf{\mu}, \mathbf{W}, \mathbf{\sigma^2}) &amp;=  \sum_n^N { \ln p(\mathbf{z}<em>n) + \sum_i^D \ln \iota</em>{ni} \mathcal{N}(x_{ni}| \mathbf{w}_i\mathbf{z}+ \mu_i, \sigma^2) }
\end{aligned}
$$</p>
<p>と書ける。あとは、欠損がない場合(p. 295)の通り、$p(\mathbf{z})  = \mathcal{N}(\mathbf{z}|\mathbf{0}, \mathbf{I})(12.31)$ と(Ex.12.16.1)を用いて、完全データ対数尤度の期待値をとると、
$$
\begin{aligned}
E[\ln p(\mathbf{X}, \mathbf{Z}|\mathbf{\mu}, \mathbf{W}, \mathbf{\sigma^2}) ] = -\sum_n{\frac{M}{2} \ln (2\pi) + \frac{1}{2}Tr(E[\mathbf{z}<em>n\mathbf{z}<em>n^T]) + \sum_i^D \iota</em>{ni}{\ln (2\pi \sigma) + \frac{1}{2\sigma^2}(x</em>{ni}-\mu_{ni})^2 - \frac{1}{\sigma^2} E[\mathbf{z}<em>n]^T\mathbf{w}<em>i^T(x</em>{ni}-\mu</em>{ni})+ \frac{1}{2\sigma^2}Tr(E[\mathbf{z}\mathbf{z}^T]\mathbf{w}_i^T\mathbf{w}_i) }}
\end{aligned}
$$</p>
<p>あとはこの式を更新するEMアルゴリズムを求めていけば良い。教科書と同様に$\mu$で微分して0を取る$\mu$を求めると、
$$
\begin{aligned}
\mu_i^{new} = \frac{1}{\sum_m^M \iota_{mi}} \sum_n \iota_{ni} x_{ni}
\end{aligned}
$$</p>
<p>そしてEステップにおいて(12.54)、(12.55)相当を求めると、
$$
\begin{aligned}
E[\mathbf{z}_n] = \mathbf{M}^{-1}\mathbf{W}^T\mathbf{y}_n
\end{aligned}
$$</p>
<p>なお、$y_n$は、$\mathbf{x}_n - \mathbf{\bar{x}}$の観察された部分である。$\mathbf{W}_n$も$\mathbf{W}$の観察された$\mathbf{x}_n$に対応する。(12.41)に対応するのは、
$$
\begin{aligned}
\mathbf{M}_n = \mathbf{W}_n^T\mathbf{W}_n + \sigma^2 \mathbf{I}
\end{aligned}
$$</p>
<p>である。そして、(12.55)式も
$$
\begin{aligned}
E[\mathbf{z}_n \mathbf{z}_n^T ] = \sigma^2 \mathbf{M}_n^{-1} + E[\mathbf{z}_n]E[ \mathbf{z}_n]^T
\end{aligned}
$$</p>
<p>そして、Mステップも同様に、
$$
\begin{aligned}
\mathbf{W}_{new} = [\sum_n^N \mathbf{y}_n E[\mathbf{z}_n]^T][\sum_n^N E[\mathbf{z}_n \mathbf{z}_n^T] ]^{-1}
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
\sigma_{new}^2 =  \frac{1}{\sum_n^N \sum_i^D \iota_{ni}} \sum_n^N \sum_i^D \iota_{ni} { (x_{ni}-\mu_{ni}^{new})^2 - 2 E[\mathbf{z}<em>n]^T(\mathbf{w}<em>i^{new})^T(x</em>{ni}-\mu</em>{ni}^{new})+ Tr(E<a href="%5Cmathbf%7Bw%7D_i%5E%7Bnew%7D">\mathbf{z}\mathbf{z}^T</a>^T\mathbf{w}_i^{new}) }}
\end{aligned}
$$</p>
<p>なお、$\mathbf{w}<em>i^{bew}$は$\mathbf{W}</em>{new}$の$i$番目の行である。</p>
<p>そして、もし全て観察された場合には、$\iota_{ni}$は全て1であり、$\mathbf{y}_n = \mathbf{x}_n$であり、$\mathbf{W}_n = \mathbf{W}$であり、$\mu^{new} = \bar{\mathbf{x}}$である。この時、教科書12.54-12.57になる。</p>
<h2 id="演習-1217"><a class="header" href="#演習-1217">演習 12.17</a></h2>
<div class="panel-primary">
<p>$D\times M$行列を$\mathbf{W}$とおき，その列ベクトルが，$D$次元のデータ空間に埋め込まれた$M$次元の線型部分空間を定義するものとする．$\boldsymbol{\mu}$を$D$次元ベクトルとおく．$n=1,\ldots,N$に対しデータ集合$\mathbf{x}_n$が与えられたものとする．このとき，各$\mathbf{x}_n$を，$M$次元ベクトルの集合$\mathbf{z}_n$からの線形写像を用いて$\mathbf{W}\mathbf{z}_n + \boldsymbol{\mu}$で近似することを考える．再構成のコスト関数は，二乗和からなる</p>
<p>$$
J=\sum_{n=1}^{N}\left|\mathbf{x}<em>{n}-\boldsymbol{\mu}-\mathbf{W}{\mathbf{z}</em>{n}}\right|^{2} \tag{12.95}
$$</p>
<p>を使う．まず，$\boldsymbol{\mu}$についての$J$の最小化が，$\mathbf{x}_n$と$\mathbf{z}_n$をそれぞれ平均$0$の変数$\mathbf{x}_n - \overline{\mathbf{x}}$と$\mathbf{z}_n - \overline{\mathbf{z}}$で置き換えたのと同様な表現に導くことを示せ．ただし$\overline{\mathbf{x}}$と$\overline{\mathbf{z}}$はサンプル平均を表す．次に，$\mathbf{W}$を固定したまま$J$を$\mathbf{z}_n$について最小化すると，主成分分析のEステップ(12.58)が出ることを示せ．また{$\mathbf{z}_n$}を固定したまま$J$を$\mathbf{W}$について最小化すると，主成分分析のM ステップ(12.59)が出ることを示せ．</p>
</div>
<p>(i) $\boldsymbol{\mu}$による$J$の最小化
$$
\begin{aligned}
\frac{\partial J}{\partial \boldsymbol{\mu}} &amp;=\sum_{n=1}^{N} 2\left(\boldsymbol{\mu}-\left(\mathbf{x}<em>{n}-\mathbf{W}\mathbf{z}</em>{n}\right)\right} \
&amp;=2 N{\boldsymbol{\mu}-(\overline{\mathbf{x}}-\mathbf{W} \overline{\mathbf{z}})}
\end{aligned}
$$
より$J$を最小化する$\boldsymbol{\mu}$は
$$
\boldsymbol{\mu}=\mathbf{x}-\mathbf{W} \overline{\mathbf{z}}
$$
であり、このとき、
$$
J=\sum_{n=1}^{N}\left|\left(\mathbf{x}<em>{n}-\overline{\mathbf{x}}\right)-\mathbf{W}\left(\mathbf{z}</em>{n}-\overline{\mathbf{z}}\right)\right|^{2}
$$
となる。
ここで
$$
\mathbf{u}<em>{n}=\mathbf{x}</em>{n}-\overline{\mathbf{x}}\
\mathbf{v}<em>{n}=\mathbf{z}</em>{n}-\overline{\mathbf{z}}
$$
とすると
$$
J=\sum_{n=1}^{N}\left|\mathbf{u}<em>{n}-\mathbf{W}\mathbf{v}</em>{n}\right|^{2}
$$
と表せる。</p>
<p>(ii) $\mathbf{z}<em>{n}$による$J$の最小化
$$
\begin{aligned}
\frac{\partial J}{\partial \mathbf{z}</em>{n}}
&amp;=\frac{\partial J}{\partial \mathbf{v}<em>{n}} \&amp;=\frac{\partial}{\partial \mathbf{v}</em>{n}}\left|\mathbf{u}<em>{n}-\mathbf{W}\mathbf{v}</em>{n}\right|^{2} \
&amp;=-2 \mathbf{W}^{T}\left(\mathbf{u}<em>{n}-\mathbf{W}\mathbf{v}</em>{n}\right)
\end{aligned}
$$
より、
$$
\mathbf{u}<em>n=\mathbf{W}\mathbf{v}</em>{n}
$$
すなわち
$$
\mathbf{v}<em>n=\left(\mathbf{W}^{T}\mathbf{W}\right)\mathbf{W}^T\mathbf{u}</em>{n}
$$
が$J$を最小化する$\mathbf{v}<em>{n}$であり、これは
$$
\tilde{\mathbf{X}}^{T}=\left(\mathbf{u}</em>{1} \mathbf{u}<em>{2} \cdots \mathbf{u}</em>{N}\right)\
\mathbf{\Omega}=\left(\mathbf{v}<em>{1} \mathbf{v}</em>{2} \cdots \mathbf{v}_{N}\right)
$$
とすると(12.58)式である
$$
\mathbf{\Omega}=\left(\mathbf{W}^{T}\mathbf{W}\right)\mathbf{W}^T\tilde{\mathbf{X}}^{T}
$$
と等しい。</p>
<p>(iii) $\mathbf{W}$による$J$の最小化
$$
\frac{\partial J}{\partial \mathbf{W}}=-2 \sum_{n=1}^{N}\left(\mathbf{u}<em>{n}-\mathbf{W} \mathbf{v}</em>{n}\right) \mathbf{v}<em>{n}^{T}
$$
より、$J$を最小化する$\mathbf{W}$は
$$
W \sum</em>{n=1}^{N} \mathbf{v}<em>{n}\mathbf{v}</em>{n}^{T}=\sum_{n=1}^{N} \mathbf{u}<em>{n} \mathbf{v}</em>{n}^{T}\
\mathbf{W}  \mathbf{\Omega} \mathbf{\Omega}^{T}=\tilde{\mathbf{X}}^{T} \mathbf{\Omega}^{T}\
\mathbf{W}=\tilde{\mathbf{X}}^{T} \mathbf{\Omega}^{T}\left(\mathbf{\Omega} \mathbf{\Omega}^{T}\right)^{-1}
$$
となりこれは(12.59)式に一致する</p>
<h2 id="演習-1218"><a class="header" href="#演習-1218">演習 12.18</a></h2>
<div class="panel-primary">
<p>12.2.4節で説明された因子分析モデルについて，独立なパラメータの数の表現を与える式を導け．</p>
</div>
<p>※</p>
<p>因子分析における共分散行列 (式12.65)　より、独立なパラメータの数は</p>
<ul>
<li>$D \times M$行列$\mathbf{W}$</li>
</ul>
<p>及び</p>
<ul>
<li>$M \times M$次元の対角行列$\mathbf{\Psi}$</li>
</ul>
<p>に依存する。また、確率的主成分分析における議論 (pp.293-294) と同様に、$M(M - 1)/2$個のパラメータは回転に関して冗長。つまり、因子分析モデルにおける独立なパラメータの数は</p>
<p>$$
\begin{aligned}
&amp; DM + D - M(M - 1)/2 \
=&amp; D(M + 1) - M(M - 1)/2
\end{aligned}
$$</p>
<p>となる。</p>
<h2 id="演習-1219"><a class="header" href="#演習-1219">演習 12.19</a></h2>
<div class="panel-primary">
<p>12.2.4節で説明した因子分析モデルが潜在変数空間の座標の回転に対して不変であることを示せ．</p>
</div>
<p>※P.302の観測変数の周辺分布$p(\mathbf{x}) = \mathcal{N}(\mathbf{x}\mid \boldsymbol{\mu}, \mathbf{WW}^{\mathrm T}+\mathbf{\Psi})$が、潜在変数空間$\mathbf{z}$での回転$\mathbf{z} \to \mathbf{\tilde{z}}$としても不変、すなわち同じ式となることを示す。</p>
<p>P.289のような$M \times M$の直交行列（回転行列）$\mathbf{R}$を定義し（$\mathbf{RR}^{\mathrm T} = \mathbf{I}$）、潜在変数空間上で$\mathbf{z}$を$\mathbf{R}$で回転させた$\mathbf{\tilde{z}}$を考える。すなわち$\mathbf{\tilde{z}} = \mathbf{Rz}$となる。潜在変数上の事前分布は</p>
<p>$$
p(\mathbf{\tilde{z}}) = \mathcal{N}(\mathbf{\tilde{z}} \mid \mathbf{0}, \mathbf{I}) \tag{12.31}
$$</p>
<p>であり、これについて展開していくと</p>
<p>$$
\begin{aligned}
p(\widetilde{\mathbf{z}}) &amp;=\mathcal{N}\left(\mathbf{R}\mathbf{z} \mid \mathbf{0}, \mathbf{I}\right) \
&amp;=\left(\frac{1}{2 \pi}\right)^{M / 2} \exp \left{-\frac{1}{2}\left(\mathbf{R}\mathbf{z}\right)^{\mathrm T}\left(\mathbf{R}\mathbf{z}\right)\right} \
&amp;=\left(\frac{1}{2 \pi}\right)^{M / 2} \exp \left{-\frac{1}{2} \mathbf{z}^{\mathrm T} \mathbf{z}\right} \
&amp;=\mathcal{N}(\mathbf{z} \mid \mathbf{0}, \mathbf{I}) \
&amp;=p(\mathbf{z})
\end{aligned}
$$</p>
<p>であり、事前分布は回転不変である。また、潜在変数$\mathbf{z}$を与えられたときの観測変数$\mathbf{x}$の条件付き分布$p(\mathbf{x}\mid \mathbf{z})$について、P.289で用いた$\mathbf{\widetilde{W}} = \mathbf{WR}$を用いると</p>
<p>$$
\begin{aligned}
p(\mathbf{x} \mid \widetilde{\mathbf{z}}) &amp;=N(\mathbf{x} \mid \mathbf{W} \tilde{\mathbf{z}}+\boldsymbol{\mu}, \mathbf{\Psi}) \
&amp;=\mathcal{N}(\mathbf{x} \mid \mathbf{WRz}+\boldsymbol{\mu}, \mathbf{\Psi}) \
&amp;=\mathcal{N}(\mathbf{x} \mid \widetilde{\mathbf{W}}\mathbf{z}+\boldsymbol{\mu}, \mathbf{\Psi}) \end{aligned}
$$</p>
<p>$(2.115)$を用いて周辺分布$p(\mathbf{x})$を計算すると</p>
<p>$$
\begin{aligned} p(\mathbf{x})
&amp;=\mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Psi}+\widetilde{\mathbf{W}} \mathbf{I} \widetilde{\mathbf{W}}^{\mathrm T}\right) \
&amp;=\mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Psi}+\mathbf{WRR}^{\mathrm T} \mathbf{W}^{\mathrm T}\right) \
&amp;=\mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Psi}+\mathbf{WW}^{\mathrm T}\right) \end{aligned}
$$</p>
<p>これは回転しない場合でも同式である。したがって潜在変数空間における回転不変性が示された。</p>
<h2 id="演習-1220"><a class="header" href="#演習-1220">演習 12.20</a></h2>
<div class="panel-primary">
<p>2次導関数を考えることによって12.2.4節で説明した因子分析モデルの対数尤度関数の，パラメータ$\boldsymbol{\mu}$に対する唯一の停留点が，</p>
<p>$$
\overline{\mathbf{x}}=\frac{1}{N} \sum_{n=1}^{N} \mathbf{x}_{n} \tag{12.1}
$$</p>
<p>で定義されたサンプル平均で与えられることを示せ．さらに，この停留点が最大値を与えることを示せ．</p>
</div>
<p>※対数尤度関数$\ln L$の$\boldsymbol{\mu}$に対する微分を計算すれば良い。</p>
<p>$$
\begin{aligned} \ln L &amp;=\ln p\left(\mathbf{X} \mid \boldsymbol{\mu}, \mathbf{W}, \sigma^{2}\right) \
&amp;=\sum_{n=1}^{N} \ln p\left(\mathbf{x}_{n} \mid \boldsymbol{\mu}, \mathbf{W}, \sigma^{2}\right) \end{aligned}
$$</p>
<p>今、因子分析モデルでは$p(\mathbf{x}) = \mathcal{N}(\mathbf{x}\mid \boldsymbol{\mu}, \mathbf{C})$と置いているので</p>
<p>$$
\begin{aligned} \ln L &amp;=\sum_{n=1}^{N} \ln \mathcal{N}\left(\mathbf{x}<em>{n} \mid \boldsymbol{\mu}, \mathbf{C} \right) \
&amp;=\sum</em>{n=1}^{N} \ln \left(\left(\frac{1}{2 \pi}\right)^{D / 2}\left|\mathbf{C}^{-1}\right|^{1 / 2} \exp \left(-\frac{1}{2}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}\right)^{\mathrm T} \mathbf{C}^{-1}\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}\right)\right)\right) \
&amp;=\sum_{n=1}^{N}\left{-\frac{D}{2} \ln (2 \pi)-\frac{1}{2} \ln |\mathbf{C}|-\frac{1}{2}\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}\right)^{\mathrm T} \mathbf{C}^{-1}\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}\right)\right) \end{aligned}
$$</p>
<p>$\boldsymbol{\mu}$について微分すると</p>
<p>$$
\frac{\partial \ln L}{\partial \boldsymbol{\mu}} = \sum_{n=1}^{N}\mathbf{C}^{-1}(\mathbf{x}<em>{n}-\boldsymbol{\mu}) = 0 \
N\boldsymbol{\mu} = \sum</em>{n=1}^{N}\mathbf{x}_{n}
$$</p>
<p>よって停留点はサンプル平均である$\displaystyle \overline{\mathbf{x}}=\frac{1}{N} \sum_{n=1}^{N} \mathbf{x}_{n}$となる。</p>
<p>また対数尤度関数のヘッセ行列$\mathbf{H}$を計算すると</p>
<p>$$
\mathbf{H} = \nabla \otimes \nabla \ln L = -N\mathbf{C}^{-1}
$$</p>
<p>$\mathbf{C} = \mathbf{WW}^{\mathrm T} + \mathbf{\Psi}$は正定置行列なので、ヘッセ行列は負定置になる。これより停留点は極大値となり、唯一の停留点なので極大かつ最大となる。</p>
<h2 id="演習-1221"><a class="header" href="#演習-1221">演習 12.21</a></h2>
<div class="panel-primary">
<p>因子分析についてのEMアルゴリズムのEステップに対する公式</p>
<p>$$
\mathbb{E}\left[\mathbf{z}<em>{n}\right] =\mathbf{G} \mathbf{W}^{\mathrm{T}} \mathbf{\Psi}^{-1}\left(\mathbf{x}</em>{n}-\overline{\mathbf{x}}\right) \tag{12.66}
$$</p>
<p>$$\mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm{T}}\right] =\mathbf{G}+\mathbb{E}\left[\mathbf{z}<em>{n}\right] \mathbb{E}\left[\mathbf{z}</em>{n}\right]^{\mathrm{T}} \tag{12.67}
$$</p>
<p>を導け．演習問題12.20の結果から，パラメータ$\boldsymbol{\boldsymbol{\mu}}$はサンプル平均$\overline{\mathbf{x}}$で置き換えられることに注意せよ．</p>
</div>
<p>事前分布は、$p(\mathbf{z}) = \mathcal{N}(\mathbf{z}\mid \mathbf{0}, \mathbf{I})$
因子分析モデルでは$p(\mathbf{x} \mid \mathbf{z})=\mathcal{N}(\mathbf{x} \mid \mathbf{W z}+\boldsymbol{\mu}, \Psi)$</p>
<p>またここでは$\boldsymbol{\mu} = \mathbf{\bar{x}}$と置き換えられる。$(2.116)$式を利用して事後分布を求めると</p>
<p>$$
p(\mathbf{z} \mid \mathbf{x})=\mathcal{N}\left(\mathbf{z} \mid \mathbf{G}\left{\mathbf{w}^{\mathrm T} \mathbf{\Psi}^{-1}(\mathbf{x}-\bar{\mathbf{x}})\right}, \mathbf{G}\right)
$$</p>
<p>ただし、ここで</p>
<p>$$
\mathbf{G}=\left(\mathbf{I}+\mathbf{W}^{\mathrm{T}} \mathbf{\Psi}^{-1} \mathbf{W}\right)^{-1} \tag{12.68}
$$</p>
<p>である。</p>
<p>この事後分布の期待値を計算すると</p>
<p>$$
\begin{aligned}
\mathbb{E}<em>{\mathbf{z}</em>{n} \sim p\left(\mathbf{z}<em>{n} \mid \mathbf{x}</em>{n}\right)}\left[\mathbf{z}<em>{n}\right] &amp;=\mathbb{E}\left[\mathcal{N}\left(\mathbf{z}</em>{n} \mid \mathbf{G}\left(\mathbf{W}^{\mathrm T} \mathbf{\Psi}^{-1}(\mathbf{x}<em>{n}-\bar{\mathbf{x}})\right), \mathbf{G}\right)\right] \
&amp;=\mathbf{G}\mathbf{W}^{\mathrm T} \mathbf{\Psi}^{-1}(\mathbf{x}</em>{n}-\bar{\mathbf{x}})
\end{aligned}
$$</p>
<p>また</p>
<p>$$
\begin{aligned}
\mathbb{E}<em>{\mathbf{z}</em>{n} \sim p\left(\mathbf{z}<em>{n} \mid \mathbf{x}</em>{n}\right)}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm T}\right] &amp;=\operatorname{cov}\left[\mathbf{z}<em>{n}\right]+\mathbb{E}\left[\mathbf{z}</em>{n}\right] \mathbb{E}\left[\mathbf{z}<em>{n}\right]^{\mathrm T} \
&amp;=\mathbf{G}+\mathbb{E}\left[\mathbf{z}</em>{n}\right] \mathbb{E}\left[\mathbf{z}_{n}\right]^{\mathrm T}
\end{aligned}
$$</p>
<p>以上で導かれた。</p>
<h2 id="演習-1222"><a class="header" href="#演習-1222">演習 12.22</a></h2>
<div class="panel-primary">
<p>因子分析モデルの完全データ対数尤度関数の期待値の式を書き下せ．また，対応するMステップの式</p>
<p>$$
\mathbf{W}<em>{\text {new }}=\left[\sum</em>{n=1}^{N}\left(\mathbf{x}<em>{n}-\overline{\mathbf{x}}\right) \mathbb{E}\left[\mathbf{z}</em>{n}\right]^{\mathrm{T}}\right]\left[\sum_{n=1}^{N} \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm{T}}\right]\right]^{-1} \tag{12.69}
$$</p>
<p>と</p>
<p>$$
\mathbf{\Psi}<em>{\text {new }}=\operatorname{diag}\left{\mathbf{S}-\mathbf{W}</em>{\text {new }} \frac{1}{N} \sum_{n=1}^{N} \mathbb{E}\left[\mathbf{z}<em>{n}\right]\left(\mathbf{x}</em>{n}-\overline{\mathbf{x}}\right)^{\mathrm{T}}\right} \tag{12.70}
$$</p>
<p>を導け．</p>
</div>
<p>※P.294のような形でまず完全データの対数尤度関数$\ln L$を書くと（途中で演習12.20の結果である$\boldsymbol{\mu} = \mathbf{\overline{x}}$を用いている）</p>
<p>$$
\begin{aligned} \ln L=&amp; \sum_{n=1}^{N}\left{\ln p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right) + \ln p\left(\mathbf{z}<em>{n}\right)\right} \
=&amp; \sum</em>{n=1}^{N}\left{\ln \mathcal{N}\left(\mathbf{x}<em>{n} \mid \mathbf{Wz}</em>{n} + \boldsymbol{\mu}, \mathbf{\Psi}\right) + \ln \mathcal{N}\left( \mathbf{z}<em>{n}\mid \mathbf{0}, \mathbf{I}\right) \right} \
=&amp; \sum</em>{n=1}^{N}\left[-\frac{D}{2} \ln (2 \pi)-\frac{1}{2} \ln |\mathbf{\Psi}|-\frac{1}{2}\left{\left(\mathbf{x}<em>{n}-\mathbf{Wz}</em>{n}-\boldsymbol{\mu}\right)^{\mathrm T} \mathbf{\Psi}^{-1}\left(\mathbf{x}<em>{n}-\mathbf{Wz}</em>{n}-\boldsymbol{\mu}\right)\right}\right. \
&amp;\left.-\frac{M}{2} \ln (2 \pi)-\frac{1}{2} \mathbf{z}<em>{n}^{\mathrm T} \mathbf{z}</em>{n}\right] \
=&amp;\ \frac{1}{2} \sum_{n=1}^{N}\left{-M \ln (2 \pi)-\mathbf{z}<em>{n}^{\mathrm{T}} \mathbf{z}</em>{n}-D \ln (2 \pi)-\ln |\mathbf{\Psi}|\right.\ &amp;\left.-\left(\mathbf{x}<em>{n}-\overline{\mathbf{x}}-\mathbf{W} \mathbf{z}</em>{n}\right)^{\mathrm{T}} \mathbf{\Psi}^{-1}\left(\mathbf{x}<em>{n}-\overline{\mathbf{x}}-\mathbf{W} \mathbf{z}</em>{n}\right)\right} \end{aligned}
$$</p>
<p>この対数尤度関数について期待値をとると</p>
<p>$$
\begin{aligned}
\mathbb{E}[\ln L]&amp;=\frac{1}{2} \sum_{n=1}^{N}\left{-\ln |\mathbf{\Psi}|-\mathbb{E}\left[\left(\mathbf{Wz}<em>{n}\right)^{\mathrm T} \mathbf{\Psi}^{-1}\left(\mathbf{Wz}</em>{n}\right)\right] \right.\
&amp;\left.+\ 2 \mathbb{E}\left[\left(\mathbf{Wz}<em>{n}\right)^{\mathrm T} \mathbf{\Psi}^{-1}\left(\mathbf{x}</em>{n}-\overline{\mathbf{x}}\right)\right]-\mathbb{E}\left[\left(\mathbf{x}<em>{n}-\overline{\mathbf{x}}\right)^{\mathrm T} \mathbf{\Psi}^{-1}\left(\mathbf{x}</em>{n}-\overline{\mathbf{x}}\right)\right]\right}+\text { const } \
&amp;=\frac{1}{2} \sum_{n=1}^{N}\left{-\ln |\mathbf{\Psi}|-\operatorname{Tr}\left[\mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm T}\right] \mathbf{W}^{\mathrm T} \mathbf{\Psi}^{-1} \mathbf{W}\right] \right.\
&amp;\left.+\ 2 \mathbb{E}\left[\mathbf{z}<em>{n}\right]^{\mathrm T} \mathbf{W}^{\mathrm T} \mathbf{\Psi}^{-1}\left(\mathbf{x}</em>{n}-\overline{\mathbf{x}}\right)\right} -\frac{1}{2} N \operatorname{Tr}\left[\mathbf{S} \mathbf{\Psi}^{-1}\right]+\text { const }
\end{aligned}
$$</p>
<p>途中でトレースと期待値演算子の交換、またデータ共分散行列$\mathbf{S}$</p>
<p>$$
\mathbf{S}=\frac{1}{N} \sum_{n=1}^{N}\left(\mathbf{x}<em>{n}-\overline{\mathbf{x}}\right)\left(\mathbf{x}</em>{n}-\overline{\mathbf{x}}\right)^{\mathrm{T}} \tag{12.3}
$$</p>
<p>を利用した。</p>
<p>この$\mathbb{E}[\ln L]$について$\mathbf{W}$の更新を行うために微分を取る。用意として</p>
<p>$$
\begin{aligned}
\frac{\partial}{\partial \mathbf{W}} \operatorname{Tr}\left[\mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm T}\right] \mathbf{W}^{\mathrm T} \mathbf{\Psi}^{-1} \mathbf{W}\right] &amp;=\ \frac{\partial}{\partial \mathbf{W}} \operatorname{Tr}\left[\mathbf{W}^{\mathrm T} \mathbf{\Psi}^{-1} \mathbf{W} \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm T}\right]\right] \
&amp;=\ \mathbf{\Psi}^{-1} \mathbf{W} \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm T}\right]+\left(\mathbf{\Psi}^{-1}\right)^{\mathrm T} \mathbf{W} \mathbb{E}\left[\mathbf{z}<em>{n}\mathbf{z}</em>{n}^{\mathrm T}\right]^{\mathrm T} (\because \textrm{Matrix Cookbook (117)})\
&amp;=\ 2 \mathbf{\Psi}^{-1} \mathbf{W} \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n} ^{\mathrm T}\right] (\because \mathbf{\Psi}と\mathbb{E}[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm T}]は対称行列)
\end{aligned}
$$</p>
<p>$$
\frac{\partial}{\partial \mathbf{W}}\mathbb{E}\left[\mathbf{z}<em>{n}\right]^{\mathrm T} \mathbf{W}^{\mathrm T} \mathbf{\Psi}^{-1}\left(\mathbf{x}</em>{n}-\overline{\mathbf{x}}\right) = 2\mathbf{\Psi}^{-1}\left(\mathbf{x}<em>{n}-\overline{\mathbf{x}}\right)\mathbb{E}\left[\mathbf{z}</em>{n}\right]^{\mathrm T} (\because \textrm{Matrix Cookbook (71)})
$$</p>
<p>これらを用いると</p>
<p>$$
\frac{\partial}{\partial \mathbf{W}}\mathbb{E}[\ln L] = \frac{1}{2}\sum_{n=1}^{N}
\left{
-2 \mathbf{\Psi}^{-1} \mathbf{W} \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n} ^{\mathrm T}\right] + 2\mathbf{\Psi}^{-1}\left(\mathbf{x}<em>{n}-\overline{\mathbf{x}}\right)\mathbb{E}\left[\mathbf{z}</em>{n}\right]^{\mathrm T}
\right}
$$</p>
<p>となり、停留点を求めるため$\displaystyle \frac{\partial}{\partial \mathbf{W}}\mathbb{E}[\ln L] = 0$として$\mathbf{W} \to \mathbf{W}_{\textrm{new}}$とすると</p>
<p>$$
\mathbf{\Psi}^{-1} \mathbf{W}<em>{\textrm{new}}\sum</em>{n=1}^{N} \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm T}\right]=\mathbf{\Psi}^{-1} \sum_{n=1}^{N}\left(\mathbf{x}<em>{n}-\overline{\mathbf{x}}\right) \mathbb{E}\left[\mathbf{z}</em>{n}\right]^{\mathrm T}
$$</p>
<p>$$
\mathbf{W}<em>{\text {new }}=\left[\sum</em>{n=1}^{N}\left(\mathbf{x}<em>{n}-\overline{\mathbf{x}}\right) \mathbb{E}\left[\mathbf{z}</em>{n}\right]^{\mathrm{T}}\right]\left[\sum_{n=1}^{N} \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm{T}}\right]\right]^{-1} \tag{12.69}
$$</p>
<p>を得られる。</p>
<p>次に$\mathbf{\Psi}$の更新について、同様に$\mathbf{\Psi}$の微分を計算すると</p>
<p>$$
\begin{aligned} \frac{\partial}{\partial \mathbf{\Psi}} \mathbb{E}\left[\ln L\right]=\frac{1}{2} \sum_{n=1}^{N}{&amp;-\mathbf{\Psi}^{-\mathrm T}+\left(\mathbf{\Psi}^{-1} \mathbf{W} \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm T}\right] \mathbf{W}^{\mathrm T} \mathbf{\Psi}^{-1}\right)^{\mathrm T} \ &amp;\left.-2 \mathbf{\Psi}^{-\mathrm T} \mathbf{W} \mathbb{E}\left[\mathbf{z}<em>{n}\right]\left(\mathbf{x}</em>{n}-\overline{\mathbf{x}}\right)^{\mathrm T} \mathbf{\Psi}^{-\mathrm T}\right} \ &amp;-\frac{N}{2}\left(-\mathbf{\Psi}^{-\mathrm T} \mathbf{S}^{\mathrm T} \mathbf{\Psi}^{-\mathrm T}\right) \end{aligned}
$$</p>
<p>上の変形にはMatrix Cookbookの(57), (61), (63)の公式を利用した。左と右からそれぞれ$\mathbf{\Psi}^{\mathrm T}$をかけ、$0$に等しいとすると</p>
<p>$$
\frac{1}{2} \sum_{n=1}^{N}\left{-\mathbf{\Psi}^{\mathrm T}+\left(\mathbf{W} \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm T}\right]^{\mathrm T} \mathbf{W}^{\mathrm T}\right)-2 \mathbf{W} \mathbb{E}\left[\mathbf{z}<em>{n}\right]\left(\mathbf{x}</em>{n}-\overline{\mathbf{x}}\right)^{\mathrm T}\right}+\frac{N}{2} \mathbf{S}^{\mathrm T}=0
$$</p>
<p>$$
N \mathbf{\Psi}^{\mathrm T}-\mathbf{W}\left[\sum_{n=1}^{N} \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm T}\right]^{\mathrm T}\right] \mathbf{W}^{\mathrm T}+2 \mathbf{W}\left[\sum_{n=1}^{N} \mathbb{E}\left[\mathbf{z}<em>{n}\right]\left(\mathbf{x}</em>{n}-\overline{\mathbf{x}}\right)^{\mathrm T}\right] -N \mathbf{S}^{\mathrm T}=0
$$</p>
<p>転置をとり、$\mathbf{\Psi}$を分離しつつ、$\mathbf{W} \to \mathbf{W}<em>{\text{new}}, \mathbf{\Psi} \to \mathbf{\Psi}</em>{\text{new}}$とすると(対称行列なので$\mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm T}\right] = \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm T}\right]^{\mathrm T}$である)</p>
<p>$$
\mathbf{\Psi}<em>{\text {new }}=\mathbf{S}-\frac{2}{N} \mathbf{W}</em>{\text{new}}\left[\sum_{n=1}^{N}\left(\mathbf{x}<em>{n}-\overline{\mathbf{x}}\right) \mathbb{E}\left[\mathbf{z}</em>{n}\right]^{\mathrm T}\right]+\frac{1}{N} \mathbf{W}<em>{\text{new}}\left[\sum</em>{n=1}^{N} \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm T}\right]\right] \mathbf{W}_{\text{new}}^{\mathrm T}
$$</p>
<p>$(12.69)$を上式の最後の$\mathbf{W}_{\text{new}}^{\mathrm T}$にのみ適用させると</p>
<p>$$
\begin{aligned}
\mathbf{\Psi}<em>{\text {new }} &amp;=\mathbf{S}-\frac{2}{N} \mathbf{W}</em>{\text{new}}\left[\sum_{n=1}^{N}\left(\mathbf{x}<em>{n}-\overline{\mathbf{x}}\right) \mathbb{E}\left[\mathbf{z}</em>{n}\right]^{\mathrm T}\right] \ &amp;+\frac{1}{N} \mathbf{W}<em>{\text{new}}\left[\sum</em>{n=1}^{N} \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm T}\right]\right]\left[\sum_{n=1}^{N} \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm T}\right]\right]^{-\mathrm T}\left[\sum_{n=1}^{N}\left(\mathbf{x}<em>{n}-\overline{\mathbf{x}}\right) \mathbb{E}\left[\mathbf{z}</em>{n}\right]^{\mathrm T}\right]^{\mathrm T} \ &amp;=\mathbf{S}-\frac{2}{N} \mathbf{W}<em>{\text{new}}\left[\sum</em>{n=1}^{N} \mathbb{E}\left[\mathbf{z}<em>{n}\right]^{\mathrm T}\left(\mathbf{x}</em>{n}-\overline{\mathbf{x}}\right)\right]+\frac{1}{N} \mathbf{W}<em>{\text{new}}\left[\sum</em>{n=1}^{N} \mathbb{E}\left[\mathbf{z}<em>{n}\right]\left(\mathbf{x}</em>{n}-\overline{\mathbf{x}}\right)^{\mathrm T}\right] \ &amp;=\mathbf{S}-\frac{1}{N} \mathbf{W}<em>{\text{new}}\left[\sum</em>{n=1}^{N} \mathbb{E}\left[\mathbf{z}<em>{n}\right]\left(\mathbf{x}</em>{n}-\overline{\mathbf{x}}\right)^{\mathrm T}\right]
\end{aligned}
$$</p>
<p>$\mathbf{\Psi}$はもとより対角行列なので、$\textrm{diag}$演算子をつければ$(12.70)$式を得る。</p>
<h2 id="演習-1223"><a class="header" href="#演習-1223">演習 12.23</a></h2>
<div class="panel-primary">
<p>確率的主成分分析モデルの離散個の混合を考え，その確率的有向グラフィカルモデルを描け．個々の主成分分析モデルは$\mathbf{W},\boldsymbol{\mu},\sigma^2$という独自のパラメータ値を持つ．次に，これらのパラメータ値が混合モデルの各要素に共有される場合のグラフを描け．</p>
</div>
<p>離散個（$K$個）の主成分分析モデルが存在し、混合係数$\pi_{k}$によって1つの確率的主成分分析モデルが構成されていることを考える。</p>
<p>前半は「個々の主成分分析モデルは$\mathbf{W},\boldsymbol{\mu},\sigma^2$という独自のパラメータ値を持つ」場合、後半はこれらが共通している場合を考える。</p>
<p>確率的有向グラフィカルモデルは以下の通り。左が独立な場合。右が共通の場合。
<img src="https://i.imgur.com/QqRtN8I.png" alt="" /></p>
<p>（公式解答によれば$\mathbf{s}$はパラメータ$\boldsymbol{\pi}$によって規定される、各主成分分析モデルの混合比を規定する$K$値の潜在変数……と言っているが、これまで扱ってきた混合係数$\pi_{k}$とは違うんだろうか？）</p>
<h2 id="演習-1224"><a class="header" href="#演習-1224">演習 12.24</a></h2>
<div class="panel-primary">
<p>2.3.7節において，スチューデントのt分布が，連続的潜在変数について周辺化されたガウス分布の無限個の混合とみなされることを学んだ．この表現を利用して，多変数のスチューデントt分布の対数尤度関数を，与えられた観測データ集合に対して最大化し， EステップとMステップの形を導け．</p>
</div>
<p>(2.161)式の離散変数バージョンで$\eta$を潜在変数とみなすと、完全データ対数尤度関数は、</p>
<p>$$
\begin{aligned}
\ln p(\mathbf{X}, \boldsymbol{\eta} | \boldsymbol\mu , \boldsymbol\Lambda , \nu ) = \sum _{n=1}^N \left{ \ln \mathcal{N} \big( \mathbf{x}_n | \boldsymbol\mu , (\eta_n \boldsymbol\Lambda ) ^{-1} \big) + \ln {\rm Gam} (\eta_n |\frac{\nu}{2},\frac{\nu}{2}) \right}
\end{aligned}
$$</p>
<p>となる。（$\boldsymbol{\eta}$は、$N$個の潜在変数$\eta_n$を並べたベクトル）</p>
<p>$\boldsymbol{\eta}$は潜在変数なので$\boldsymbol{\eta}$で期待値を取ると、</p>
<p>$$
\begin{aligned} \mathbb{E}<em>{\eta}[\ln p(\mathbf{X}, \boldsymbol{\eta} \mid \boldsymbol{\mu}, \boldsymbol{\Lambda}, \nu)] &amp;=-\frac{1}{2} \sum</em>{n=1}^{N}\left{D\left(\ln (2 \pi)-\mathbb{E}\left[\ln \eta_{n}\right]\right)-\ln |\Lambda|+\mathbb{E}\left[\eta_{n}\right]\left(\mathbf{x}<em>{n}^{\mathrm{T}} \boldsymbol{\Lambda} \mathbf{x}</em>{n}-2 \mathbf{x}<em>{n}^{\mathrm{T}} \boldsymbol{\Lambda} \boldsymbol{\mu}+\boldsymbol{\mu}^{\mathrm{T}} \boldsymbol{\Lambda} \boldsymbol{\mu}\right)(*)\right.\ &amp;\left.+2 \ln \Gamma\left(\frac{\nu}{2}\right)-\nu(\ln \nu-\ln 2)-(\nu-2) \mathbb{E}\left[\ln \eta</em>{n}\right]+\nu \mathbb{E}\left[\eta_{n}\right]\right} \end{aligned}
$$</p>
<p>と、(12.53)式に対応する式が導かれる。（第１行はガウス分布の展開から、第２行はガンマ分布の展開から出てくる。）</p>
<p>次に、Eステップでの$\mathbb{E} [\eta_n]$、$\mathbb{E} [\ln\eta_n]$の更新式を導く。$\boldsymbol\eta$の確率分布は、</p>
<p>$$
\begin{aligned}
p(\boldsymbol{\eta} | \mathbf{X}, \boldsymbol\mu , \boldsymbol\Lambda , \nu )
&amp;= \prod _{n=1}^N p(\eta_n | \mathbf{x}_n, \boldsymbol\mu , \boldsymbol\Lambda , \nu ) \
&amp;\propto \prod _{n=1}^N p(\mathbf{x}_n | \eta_n, \boldsymbol\mu , \boldsymbol\Lambda , \nu ) p( \eta_n | \boldsymbol\mu , \boldsymbol\Lambda , \nu )\
&amp;= \prod _{n=1}^N \mathcal{N} (\mathbf{x}_n | \boldsymbol\mu , (\eta_n \boldsymbol\Lambda ^{-1})) {\rm Gam} (\eta_n |\frac{\nu}{2},\frac{\nu}{2})
\end{aligned}
$$</p>
<p>となる。ガウス分布の精度$\eta_n$の事前分布がGam$(\eta_n |\nu/2,\nu/2)$であるときの事後分布を表すから、積の中身はガンマ分布Gam$(\eta_n | a_n, b_n)$に従う。パラメータ$a_n, b_n$の値は、(2.150)式と(2.151)式より、
$$
\begin{aligned}
a_n &amp;= \frac{\nu+D}{2}\
b_n &amp;= \frac{\nu + (\mathbf{x}_n - \boldsymbol\mu)^{\rm T}\boldsymbol\Lambda (\mathbf{x}_n - \boldsymbol\mu) }{2}
\end{aligned}
$$</p>
<p>である。このパラメータ$a_n, b_n$を用いると、ガンマ分布の公式(B.27)と(B.30)より、</p>
<p>$$
\begin{aligned}
\mathbb{E} [\eta_n] &amp;= \frac{a_n}{b_n}\
\mathbb{E} [\ln\eta_n] &amp;= \psi (a_n)-\ln b_n
\end{aligned}
$$</p>
<p>と書ける。（$\psi( \cdot )$は、(B.25)で定義されるディガンマ関数）</p>
<p><strong>Mステップの更新式(1/3)</strong>
上記の(*)式を$\boldsymbol{\mu}$で微分した値をゼロとおくと、</p>
<p>$$
\begin{aligned} \frac{\partial}{\partial \boldsymbol{\mu}} \mathbb{E}<em>{\boldsymbol{\eta}}[\ln p(\mathbf{X}, \boldsymbol{\eta} \mid \boldsymbol{\mu}, \boldsymbol{\Lambda}, \nu)] &amp;=-\frac{1}{2} \sum</em>{n=1}^{N}\left{\mathbb{E}\left[\eta_{n}\right]\left(-2 \mathbf{\Lambda} \mathbf{x}<em>{n}+2 \boldsymbol{\Lambda} \boldsymbol{\mu}\right)\right} \ &amp;=\mathbf{\Lambda}\left(\sum</em>{n=1}^{N} \mathbb{E}\left[\eta_{n}\right] \mathbf{x}<em>{n}-\boldsymbol{\mu} \sum</em>{n=1}^{N} \mathbb{E}\left[\eta_{n}\right]\right)=0 \end{aligned}
$$</p>
<p>$\boldsymbol{\Lambda}$は精度行列なので、逆行列（＝分散行列）が存在する。その逆行列を左から乗じて整理すると、
$$
\begin{aligned}
\boldsymbol\mu_{\rm ML}
= \frac{\sum_{n=1}^N \mathbb{E} [\eta_n] \mathbf{x}<em>n }{\sum</em>{n=1}^N \mathbb{E}[\eta_n]}
\end{aligned}
$$</p>
<p><strong>Mステップの更新式(2/3)</strong>
上記の(*)式を$\boldsymbol{\Lambda}$で微分した値をゼロと置くのだが、準備として任意のベクトル$\boldsymbol\xi,\boldsymbol\zeta$に対して、
$$
\begin{aligned}
\frac{\partial}{\partial \boldsymbol\Lambda}
(\boldsymbol\xi^{\rm T}\boldsymbol\Lambda\boldsymbol\zeta )
&amp;= \frac{\partial}{\partial \boldsymbol\Lambda}
{\rm Tr} (\boldsymbol\xi^{\rm T}\boldsymbol\Lambda\boldsymbol\zeta )\
&amp;= \frac{\partial}{\partial \boldsymbol\Lambda}
{\rm Tr} (\boldsymbol\Lambda\boldsymbol\zeta \boldsymbol\xi^{\rm T})\
&amp;= \boldsymbol\zeta \boldsymbol\xi^{\rm T}
\tag{C.25より}
\end{aligned}
$$</p>
<p>であることに注目すると、
$$
\begin{aligned} \frac{\partial}{\partial \boldsymbol{\Lambda}} \mathbb{E}<em>{\boldsymbol{\eta}}[\ln p(\mathbf{X}, \boldsymbol{\eta} \mid \boldsymbol{\mu}, \boldsymbol{\Lambda}, \nu)] &amp;=-\frac{1}{2} \sum</em>{n=1}^{N}\left{-\Lambda^{-1}+\mathbb{E}\left[\eta_{n}\right]\left(\mathbf{x}<em>{n} \mathbf{x}</em>{n}^{\mathrm{T}}-2 \boldsymbol{\eta} \mathbf{x}<em>{n}^{\mathrm{T}}+\boldsymbol{\eta} \boldsymbol{\eta}^{\mathrm{T}}\right)\right}=0 \ &amp; \Leftrightarrow \Lambda</em>{\mathrm{ML}}=\left(\frac{1}{N} \sum_{n=1}^{N} \mathbb{E}\left[\eta_{n}\right]\left(\mathbf{x}<em>{n} \mathbf{x}</em>{n}^{\mathrm{T}}-2 \boldsymbol{\eta} \mathbf{x}_{n}^{\mathrm{T}}+\boldsymbol{\eta} \boldsymbol{\eta}^{\mathrm{T}}\right)\right)^{-1} \end{aligned}
$$</p>
<p><strong>Mステップの更新式(3/3)</strong>
上記の(*)式を$\nu$で微分した値をゼロと置くと、
$$
\begin{aligned} \frac{\partial}{\partial \nu} \mathbb{E}<em>{\eta}[\ln p(\mathbf{X}, \boldsymbol{\eta} \mid \boldsymbol{\mu}, \boldsymbol{\Lambda}, \nu)] &amp;=-\frac{1}{2} \sum</em>{n=1}^{N}\left{\frac{2}{\Gamma\left(\frac{\nu}{2}\right)} \frac{\partial}{\partial \nu} \Gamma(\nu / 2)-(\ln \nu-\ln 2)-1-\mathbb{E}\left[\ln \eta_{n}\right]+\mathbb{E}\left[\eta_{n}\right]\right}=0 \ &amp; \Leftrightarrow \frac{\psi(\nu / 2)}{\Gamma(\nu / 2)}-(\ln \nu-\ln 2)-1-\frac{1}{N} \sum_{n=1}^{N}\left(\mathbb{E}\left[\ln \eta_{n}\right]-\mathbb{E}\left[\eta_{n}\right]\right)=0 \end{aligned}
$$</p>
<p>となる。これは$\nu$について解析的に解くことはできない。</p>
<h2 id="演習-1225"><a class="header" href="#演習-1225">演習 12.25</a></h2>
<div class="panel-primary">
<p>潜在変数の空間分布$p(\mathbf{z}) = \mathcal{N}(\mathbf{x}\mid \mathbf{0}, \mathbf{I})$を持つ線形ガウス潜在変数モデルを考える．観測変数に対する条件付き分布を$p(\mathbf{x} \mid \mathbf{z})=\mathcal{N}(\mathbf{x} \mid \mathbf{W z}+\boldsymbol{\mu}, \mathbf{\Phi})$とおく．ただし$\mathbf{\Phi}$はノイズ項の対称かつ正定値の任意の共分散行列である．$D \times D$行列$\mathbf{A}$を使ってデータ変数に非特異的な線形変換$\mathbf{x} \to \mathbf{Ax}$を行うものとする．もし$\boldsymbol{\mu}<em>{\mathrm{ML}}$と$\mathbf{W}</em>{\mathrm{ML}}$と$\mathbf{\Phi}<em>{\mathrm{ML}}$がもともとの非変換データに対応した最尤解を表すものとすれば，$\mathbf{A} \mu</em>{\mathrm{ML}}$と$\mathbf{AW}<em>{\mathrm{ML}}$と$\mathbf{A} \mathbf{\Phi}</em>{\mathrm{ML}} \mathbf{A}^{\mathrm{T}}$が。変換されたデータ集合の最尤解に対応していることを示せ．最後に，以下の2つの場合にモデルの形が保存されることを示せ．(i) $\mathbf{A}$が対角行列で，$\mathbf{\Phi}$も対角行列．これは因子分析に対応する．変換された$\mathbf{\Phi}$は対角的なままであり，したがって，因子分析モデルは，データ変数の要素ごとの尺度変更に<strong>共変的(covariant)</strong> である．(ii) $\mathbf{A}$が対角的で，$\mathbf{\Phi}$が単位行列に比例する場合，つまり$\mathbf{\Phi} = \sigma^2\mathbf{I}$である場合，これは確率的主成分分析に対応している．変換された$\mathbf{\Phi}$行列は単位行列に比例したままとなり，それゆえ確率的主成分分析は通常の主成分分析がそうであるように，データ空間の軸の回転の下で共変的である．</p>
</div>
<p>※
確率的主成分分析の対数尤度関数は12.2節の議論から以下のようにかける．</p>
<p>$$
\begin{aligned}
L(\boldsymbol{\mu}, \mathbf{W}, \mathbf{\Phi})=-\frac{N D}{2} \ln (2 \pi)-\frac{N}{2} \ln \left|\mathbf{W} \mathbf{W}^{\mathrm{T}}+\mathbf{\Phi}\right|
-\frac{1}{2} \sum_{n=1}^{N}\left{\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}\right)^{\mathrm{T}}\left(\mathbf{W} \mathbf{W}^{\mathrm{T}}+\mathbf{\Phi}\right)^{-1}\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}\right)\right}
\end{aligned}
$$</p>
<p>いま$\mathbf{x} \to \mathbf{Ax}$の変換を考えているので</p>
<p>$$
\begin{aligned}
&amp;L_{\mathbf{A}}(\boldsymbol{\mu}, \mathbf{W}, \mathbf{\Phi})=-\frac{N D}{2} \ln (2 \pi)-\frac{N}{2} \ln \left|\mathbf{W} \mathbf{W}^{\mathrm{T}}+\mathbf{\Phi}\right|-\frac{1}{2} \sum_{n=1}^{N}\left{\left(\mathbf{A} \mathbf{x}<em>{n}-\boldsymbol{\mu}\right)^{\mathrm{T}}\left(\mathbf{W} \mathbf{W}^{\mathrm{T}}+\mathbf{\Phi}\right)^{-1}\left(\mathbf{A} \mathbf{x}</em>{n}-\boldsymbol{\mu}\right)\right}
\end{aligned}
$$</p>
<p>以上のように書き換えることができる．ここで$\boldsymbol{\mu}$の最尤解を求めるので$\boldsymbol{\mu}$についての微分を0とおいて$\boldsymbol{\mu}$について解くと</p>
<p>$$
\boldsymbol{\mu}<em>{\mathrm{A}}=\frac{1}{N} \sum</em>{n=1}^{N} \mathbf{A} \mathbf{x}<em>{n}=\mathbf{A} \overline{\mathbf{x}}=\mathbf{A} \boldsymbol{\mu}</em>{\mathrm{ML}}
$$</p>
<p>が得られる．これを対数尤度関数に代入して$\displaystyle \mathbf{S}=\frac{1}{N} \sum_{n=1}^{N}\left(\mathbf{x}<em>{n}-\overline{\mathbf{x}}\right)\left(\mathbf{x}</em>{n}-\overline{\mathbf{x}}\right)^{\mathrm{T}}$を用いると</p>
<p>$$
\begin{aligned}
L_{\mathbf{A}}(\boldsymbol{\mu}, \mathbf{W}, \mathbf{\Phi})=-\frac{N D}{2} \ln (2 \pi)-\frac{N}{2} \ln \left|\mathbf{W} \mathbf{W}^{\mathrm{T}}+\mathbf{\Phi}\right|-\frac{1}{2} \sum_{n=1}^{N} \operatorname{Tr}\left{\left(\mathbf{W} \mathbf{W}^{\mathrm{T}}+\mathbf{\Phi}\right)^{-1} \mathbf{A} \mathbf{S} \mathbf{A}^{\mathrm{T}}\right}
\end{aligned}
$$</p>
<p>以上のように書ける．ここで以下の定義を用いることで</p>
<p>$$
\mathbf{\Phi}<em>{\mathbf{A}}=\mathbf{A} \mathbf{\Phi}^{-1} \mathbf{A}^{\mathrm{T}}, \quad \mathbf{W}</em>{\mathbf{A}}=\mathbf{A} \mathbf{W}
$$</p>
<p>対数尤度関数の最終項について変換$A$をあらわに用いない形で書き換えることができる</p>
<p>$$
\begin{aligned}
&amp;L_{\mathbf{A}}\left(\boldsymbol{\mu}<em>{\mathbf{A}}, \mathbf{W}</em>{\mathbf{A}}, \mathbf{\Phi}<em>{\mathbf{A}}\right)=-\frac{N D}{2} \ln (2 \pi)-\frac{N}{2} \ln \left|\mathbf{W}</em>{\mathbf{A}} \mathbf{W}<em>{\mathbf{A}}^{\mathrm{T}}+\mathbf{\Phi}</em>{\mathbf{A}}\right|-\frac{1}{2} \sum_{n=1}^{N}\left{\left(\mathbf{x}<em>{n}-\boldsymbol{\mu}</em>{\mathbf{A}}\right)^{\mathrm{T}}\left(\mathbf{W}<em>{\mathbf{A}} \mathbf{W}</em>{\mathbf{A}}^{\mathrm{T}}+\mathbf{\Phi}<em>{\mathbf{A}}\right)^{-1}\left(\mathbf{x}</em>{n}-\boldsymbol{\mu}_{\mathbf{A}}\right)\right}-N \ln |\mathbf{A}|
\end{aligned}
$$</p>
<p>いまこの式は最初の対数尤度関数と比べて$-\ln A$の項だけが異なっている．従って変換$A$をによるデータセットの$\mathbf{W}, \mathbf{\Phi}$の最尤解は変換前の最尤解と同じ形で書け，</p>
<p>$$
\mathbf{W}<em>{\mathbf{A}}=\mathbf{AW}</em>{\mathrm{ML}}
$$</p>
<p>$$
\mathbf{\Phi}<em>{\mathbf{A}}=\mathbf{A} \mathbf{\Phi}</em>{\mathrm{ML}} \mathbf{A}^{\mathrm{T}}
$$</p>
<p>となることがわかる．</p>
<p>．
次にこの変換の前後で$\mathbf{\Phi}$についての制約が保たれるか確認する．</p>
<p>(i) $\mathbf{A}$が対角行列で$\mathbf{\Phi}$ も対角行列のとき(因子分析に対応)</p>
<p>このとき</p>
<p>$$
\mathbf{\Phi}_{A}=\mathbf{A}\mathbf{\Phi}^{-1}\mathbf{A}^{\mathrm{T}}
$$</p>
<p>と書くことができて，これは対角行列同士の積であるため，$\mathbf{\Phi}_A$も対角行列であることがわかる．
従ってこの場合データベクトルの要素ごとの尺度を変更すると対応する$\mathbf{\Phi}_A$の要素の尺度変更にデータの尺度変更が吸収されることがわかる．</p>
<p>(ii) $\mathbf{A}$が対角行列で$\mathbf{\Phi}$ が単位行列に比例する場合(確率的主成分分析に対応)</p>
<p>このとき</p>
<p>$$
\mathbf{\Phi}_{A}=\mathbf{A}\mathbf{\Phi}^{-1}\mathbf{A}^{\mathrm{T}}
$$</p>
<p>であることから変換$\mathbf{A}$の前後ともに$\mathbf{\Phi}=\sigma^2\mathbf{I}$を満たすとき</p>
<p>$$
\mathbf{A}\mathbf{A}^{\mathrm{T}}=\mathbf{I}
$$</p>
<p>となるので$\mathbf{A}$は直交行列である必要がある．このとき変換$\mathbf{A}$は座標系の回転に相当する
<img src="https://i.imgur.com/dNe3lFQ.png" alt="" /></p>
<h2 id="演習-1226"><a class="header" href="#演習-1226">演習 12.26</a></h2>
<div class="panel-primary">
<p>$$
\mathbf{K} \mathbf{a}<em>{i}=\lambda</em>{i} N \mathbf{a}_{i} \tag{12.80}
$$</p>
<p>を満たす任意のベクトル$\mathbf{a}_i$が</p>
<p>$$
\mathbf{K}^{2} \mathbf{a}<em>{i}=\lambda</em>{i} N \mathbf{K} \mathbf{a}_{i} \tag{12.79}
$$</p>
<p>も満たすことを示せ．また固有値$\lambda$を持つ$(12.80)$の任意の解に，$\mathbf{K}$のゼロ固有値に属する固有ベクトルのいかなる定数倍を加えたものもまた固有値$\lambda$を持つ$(12.79)$の解であることを示せ．最後にゼロ固有値に属する固有ベクトルの成分を加える変更が，</p>
<p>$$
y_{i}(\mathbf{x})=\boldsymbol{\phi}(\mathbf{x})^{\mathrm{T}} \mathbf{v}<em>{i}=\sum</em>{n=1}^{N} a_{i n} \boldsymbol{\phi}(\mathbf{x})^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}<em>{n}\right)=\sum</em>{n=1}^{N} a_{i n} k\left(\mathbf{x}, \mathbf{x}_{n}\right) \tag{12.82}
$$</p>
<p>で与えられる主成分による射影に影響しないことを示せ．</p>
</div>
<p>※ （復習）$\mathbf{K}$は6章のP.3で定義されているグラム行列であり、$n$番目の行が$\boldsymbol{\phi}(\mathbf{x}_{n}^{\mathrm T})$で表される計画行列$\mathbf{\Phi}$に対して$\mathbf{K} = \mathbf{\Phi}\mathbf{\Phi}^{\mathrm T}$で定義される。これは$N \times N$の対称行列であり、その要素は</p>
<p>$$
K_{n m}=\boldsymbol{\phi}\left(\mathbf{x}<em>{n}\right)^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}</em>{m}\right)=k\left(\mathbf{x}<em>{n}, \mathbf{x}</em>{m}\right) \tag{6.6}
$$</p>
<p>である。</p>
<p>$(1)$
$(12.80)$が成立する前提で$(12.79)$が成立することを示す。$(12.79)$の左辺に$(12.80)$を代入すると</p>
<p>$$
\mathbf{K}^2\mathbf{a}<em>{i} = \mathbf{K}(\lambda</em>{i}N\mathbf{a}<em>{i}) = \lambda</em>{i}N\mathbf{Ka}_{i}
$$</p>
<p>となるので成立する。</p>
<p>$(2)$
$(12.80)$の解のうち、固有値$\lambda_{i}$に対応する固有ベクトルを$\mathbf{a}<em>{i}$とし、$\mathbf{K}$のゼロ固有値に属する固有ベクトルを$\mathbf{b}</em>{i}$とおく。このとき定数$m$を用いた線形結合のベクトル$\widetilde{\mathbf{a}<em>{i}} = \mathbf{a}</em>{i} + m\mathbf{b}_{i}$を$(12.79)$の左辺に代入したとき、右辺が得られることを示す。</p>
<p>設定から$\mathbf{Kb}<em>{i} = \lambda</em>{i}N\mathbf{b}_{i}( = \mathbf{0})$となるので</p>
<p>$$
\begin{aligned}
\mathbf{K}^{2}\widetilde{\mathbf{a}<em>{i}} &amp;= \mathbf{K}^{2}(\mathbf{a}</em>{i} + m\mathbf{b}<em>{i}) \
&amp;=\mathbf{K}^{2}\mathbf{a}</em>{i} + m\mathbf{K}^{2}\mathbf{b}<em>{i} \
&amp;=\lambda</em>{i}N\mathbf{K}\mathbf{a}<em>{i} + m\mathbf{K}(\lambda</em>{i}N\mathbf{b}<em>{i}) \
&amp;=\lambda</em>{i}N\mathbf{K}(\mathbf{a}<em>{i} + m\mathbf{b}</em>{i}) \
&amp;=\lambda_{i}N\mathbf{K}\widetilde{\mathbf{a}_{i}}
\end{aligned}
$$</p>
<p>よって$\widetilde{\mathbf{a}_{i}}$は$(12.79)$の解であることが示された。</p>
<p>$(3)$ $(12.82)$の$a_{in}$を$\widetilde{\mathbf{a}<em>{i}}$の成分$\tilde{a}</em>{in}$に置き換えても$(12.82)$の結果が変化しないことを示す。$(2)$で用いた$\mathbf{K}\mathbf{b}<em>{i}=0$より、$\sum</em>{n=1}^{N} b_{in}k(\mathbf{x}, \mathbf{x}_{n}) = 0$であるから、</p>
<p>$$
\begin{aligned}
\boldsymbol{\phi}(\mathbf{x})^{\mathrm{T}} \tilde{\mathbf{v}}<em>{i}&amp;=\sum</em>{n=1}^{N} \tilde{a}<em>{i n} \boldsymbol{\phi}(\mathbf{x})^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}</em>{n}\right) \
&amp;=\sum_{n=1}^{N} (a_{in}+mb_{in}) \boldsymbol{\phi}(\mathbf{x})^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}<em>{n}\right) \
&amp;=\sum</em>{n=1}^{N} a_{in}k(\mathbf{x}, \mathbf{x}<em>{n}) + m\sum</em>{n=1}^{N} b_{in}k(\mathbf{x}, \mathbf{x}<em>{n})\
&amp;=\sum</em>{n=1}^{N} a_{in}k(\mathbf{x}, \mathbf{x}_{n})
\end{aligned}
$$</p>
<p>よって変化しないことが示された。</p>
<h2 id="演習-1227"><a class="header" href="#演習-1227">演習 12.27</a></h2>
<div class="panel-primary">
<p>$k(\mathbf{x}, \mathbf{x}^{\prime}) = \mathbf{x}^{\mathrm T}\mathbf{x}^{\prime}$で与えられる線形のカーネル関数を選択すれば，カーネル主成分分析の特別な場合として普通の主成分分析アルゴリズムが再現できることを示せ．</p>
</div>
<p>※カーネル関数$k(\mathbf{x},\mathbf{x}^{\prime}) = \mathbf{x}^{\mathrm T}\mathbf{x}^{\prime}$を用いて、あるデータ点$\mathbf{x}_{m}$について$(12.82)$を用いた固有ベクトル$i$への射影</p>
<p>$$
y_{i}(\mathbf{x}) = \sum_{n=1}^{N} a_{i n} k\left(\mathbf{x}, \mathbf{x}_{n}\right)\tag{12.82}
$$</p>
<p>が、通常の主成分分析での射影と同じ結果、すなわち、あるデータ点$\mathbf{x}<em>{m}$が固有ベクトル$\mathbf{u}</em>{i}$へ射影された$\mathbf{u}<em>{i}^{\mathrm T}\mathbf{x}</em>{m}$に一致することを示せば良い。これは</p>
<p>$$
\begin{aligned} \sum_{n=1}^{N} a_{i n} k\left(\mathbf{x}<em>{m}, \mathbf{x}</em>{n}\right) &amp;=\sum_{n=1}^{N} a_{m} \mathbf{x}<em>{m}^{\mathrm T} \mathbf{x}</em>{n} \
&amp;=\left(\sum_{n=1}^{N} a_{i n} \mathbf{x}<em>{n}^{\mathrm T}\right) \mathbf{x}</em>{m} \
&amp;\equiv {\mathbf{u}<em>{i}^{\star}}^{\mathrm T}\mathbf{x}</em>{m}
\end{aligned}
$$</p>
<p>この$\displaystyle {\mathbf{u}<em>{i}^{\star}}\equiv \sum</em>{n=1}^{N} a_{i n} \mathbf{x}<em>{n}$が、通常の主成分分析の主成分の定義である$\mathbf{Su}</em>{i} = \lambda_{i}\mathbf{u}<em>{i}$によって定義される$\mathbf{u}</em>{i}$と一致することを示す。左辺から展開していって</p>
<p>$$
\begin{aligned}
\mathbf{Su}<em>{i} &amp;=\left(\frac{1}{N} \sum</em>{m=1}^{N} \mathbf{x}<em>{m} \mathbf{x}</em>{m}^{\mathrm T}\right)\left(\sum_{n=1}^{N} a_{i n} \mathbf{x}<em>{n}\right) \
&amp;=\frac{1}{N} \sum</em>{m=1}^{N} \mathbf{x}<em>{m} \sum</em>{n=1}^{N} a_{i n} \mathbf{x}<em>{m}^{\mathrm T} \mathbf{x}</em>{n} \
&amp;=\frac{1}{N} \sum_{m=1}^{N} \mathbf{x}<em>{m} \sum</em>{n=1}^{N} a_{i n} k\left(\mathbf{x}<em>{m}, \mathbf{x}</em>{n}\right) \
&amp;=\frac{1}{N} \sum_{m=1}^{N} \mathbf{x}<em>{m} \mathbf{K} \mathbf{a}</em>{i} \
&amp;=\frac{1}{N} \sum_{m=1}^{N} \mathbf{x}<em>{m} \lambda</em>{i}N \mathbf{a}<em>{i} \quad(\because(12.80)) \
&amp;=\lambda</em>{i} \sum_{m=1}^{N} a_{i m} \mathbf{x}<em>{m} \
&amp;=\lambda</em>{i} \mathbf{u}_{i}^{\star}
\end{aligned}
$$</p>
<p>以上から、題意が示された。</p>
<h2 id="演習-1228"><a class="header" href="#演習-1228">演習 12.28</a></h2>
<div class="panel-primary">
<p>関数$f(x)$による非線形変数変換$y=f(x)$を施すことにより，ある固定された密度$q(x)$から，任意の密度関数$p(y)$が得られることを示せ．また$f(x)$が満足する微分方程式を書き下し，密度の変換を説明する図を描け．ただし$q(x)$はいたるところで非ゼロとし，$f(x)$は単調と仮定する．したがって$0\leqslant f^{\prime}(x) \lt \infty$である．上記を示す際，変数変換による確率密度の変換に関する性質</p>
<p>$$
\begin{aligned} p_{y}(y) &amp;=p_{x}(x)\left|\frac{\mathrm{d} x}{\mathrm{~d} y}\right| \ &amp;=p_{x}(g(y))\left|g^{\prime}(y)\right| \end{aligned} \tag{1.27}
$$</p>
<p>を使え．</p>
</div>
<p>固定された密度（確率密度）は$q(x) \gt 0$（非ゼロ）で、非線形変数変換$f(x)$は$0\leqslant f^{\prime}(x) \lt \infty$より単調増加関数。単調増加なので、逆関数が存在し、$x = f^{-1}(y)$が存在する。このとき1.2.1 確率密度や演習1.4でも見たように、$(1.27)$の変数変換を用いて</p>
<p>$$
p(y) = q(f^{-1}(y))\left|\frac{\mathrm{d} f^{-1}(y)}{\mathrm{~d} y}\right|
$$</p>
<p>と書くことができる。</p>
<p>今、$f$の制約は単調であることのみなので、$x$に対する確率質量を$y$に任意に配分することができる。密度の変換はこんなイメージ（演習問題1.4のときの図を使用）
<img src="https://i.imgur.com/FC9IHEz.png" alt="" /></p>
<p>$(1.27)$について赤いガウス分布$p_{x}(x)$について$x=g(y)$（この場合はシグモイド曲線）で変換すると、緑の曲線$p_{x}(g(y))$が得られる。しかし$(1.27)$でやっているのはそれにさらに$|g^{\prime}(y)|$をかけており、これで微小区間$dx, dy$を用いて$p_y(y)dy = p_{x}(x)dx$の微小面積を保存するように変換している。これにより、紫の曲線が得られる。この問題では$p_{x}(x)$が$q(x)$、$g^{-1}(x) = f(x)$, $p_{y}(y) = p(y)$である。$f(x)$の傾きを$0\leqslant f^{\prime}(x) \lt \infty$の範囲で任意に調節すれば、自在に$p(y)$の形状を設定できる。</p>
<p>$y=f(x)$が満足する微分方程式は、上式を変形して</p>
<p>$$
\left| \frac{dy}{dx}\right| = \left| \frac{dy}{df^{-1}(y)}\right| = \frac{q(f^{-1}(y))}{p(y)} = \frac{q(x)}{p(f(x))}
$$</p>
<p>である。</p>
<h2 id="演習-1229"><a class="header" href="#演習-1229">演習 12.29</a></h2>
<div class="panel-primary">
<p>2つの変数$z_1$と$z_2$が独立で，$p(z_1,z_2) = p(z_1)p(z_2)$となると仮定する．これらの2つの変数に対する共分散行列が対角的であることを示せ．これは変数の独立性が，2つの変数が無相関となることの十分条件であることを示している．次に，2つの変数$y_1$と$y_2$を考え，$y_1$が$0$を中心に対称に分布しており，また，$y_2 = {y_1}^2$を満たすと仮定する．条件付き分布$p(y_2\mid y_1)$を書き下し，これが$y_1$に依存しており，2つの変数が独立とはならないことを示せ．次に，これら2つの変数に対する共分散行列が対角的であることを示せ．これを示すために，関係式$p\left(y_{1}, y_{2}\right)=p\left(y_{1}\right) p\left(y_{2} \mid y_{1}\right)$を用いて，非対角成分が$0$となることを示せ．この反例は，相関が$0$であることが，独立性の十分条件にはならないことを示している．</p>
</div>
<p>※前半は演習1.6とほぼ同じ</p>
<p>$z_1, z_2$の共分散$\operatorname{cov}[z_1,z_2]$を求める。</p>
<p>$$
\begin{aligned} \operatorname{cov}\left[z_{1}, z_{2}\right] &amp;=\mathbb{E}\left[\left(z_{1}-\mathbb{E}\left[z_{1}\right]\right)\left(z_{2}-\mathbb{E}\left[z_{2}\right]\right)\right] \
&amp;=\mathbb{E}\left[z_{1} z_{2}\right]-\mathbb{E}\left[z_{1}\right] \mathbb{E}\left[z_{2}\right] \
&amp;=\iint z_{1} z_{2} p\left(z_{1}, z_{2}\right) d z_{1} d z_{2}-\mathbb{E}\left[z_{1}\right] \mathbb{E}\left[z_{2}\right] \
&amp;=\iint z_{1} z_{2} p\left(z_{1}\right) p\left(z_{2}\right) d z_{1} d z_{2}\quad (\because p(z_1,z_2) = p(z_1)p(z_2))\
&amp;=\int z_{1} p\left(z_{1}\right) d z_{1} \int z_{2} p\left(z_{2}\right) d z_{2}-\mathbb{E}\left[z_{1}\right] \mathbb{E}\left[z_{2}\right] \
&amp;=\mathbb{E}\left[z_{1}\right] \mathbb{E}\left[z_{2}\right]-\mathbb{E}\left[z_{1}\right] \mathbb{E}\left[z_{2}\right] \
&amp;=0 \end{aligned}
$$</p>
<p>$z_1, z_2$の共分散行列は</p>
<p>$$
\begin{aligned} \Sigma &amp;=\left(\begin{array}{cc}\operatorname{var}\left[z_{1}\right] &amp; \operatorname{cov}\left[z_{1}, z_{2}\right] \ \operatorname{cov}\left[z_{1}, z_{2}\right] &amp; \operatorname{var}\left[z_{2}\right]\end{array}\right) \
&amp;=\left(\begin{array}{cc}\operatorname{var}\left[z_{1}\right] &amp; 0 \ 0 &amp; \operatorname{var}\left[z_{2}\right]\end{array}\right) \end{aligned}
$$</p>
<p>となるので、共分散行列が対角的であることになる。</p>
<p>後半は、$\operatorname{cov}[y_1,y_2]=0$であっても$y_1, y_2$が独立でないことがあることを示す問題。</p>
<p>問題設定から、$y_1$が$0$を中心に対称に分布し（$\mathbb{E}[y_1]=0$）、かつ$y_2 = {y_1}^{2}$の関係が成立する2変数について考えると、$y_1$が与えられた下での$y_2$の条件つき確率$p(y_2\mid y_1)$は、性質上$y_2 \neq y_1^2$ならば確率$0$で$y_2 = y_1^2$に常に存在するので、数式上</p>
<p>$$
p(y_2 \mid y_1) = \delta(y_2-y_1^{2})
$$</p>
<p>となる。</p>
<p>これは明らかに$p(y_2\mid y_1)$が$y_1$に依存しているので、$y_1$と$y_2$は独立ではない。この共分散を求めると</p>
<p>$$
\begin{aligned} \operatorname{cov}\left[y_{1}, y_{2}\right] &amp;=\mathbb{E}\left[y_{1}, y_{2}\right]-\mathbb{E}\left[y_{1}\right] \mathbb{E}\left[y_{2}\right] \
&amp;=\iint y_{1} y_{2} p\left(y_{1}, y_{2}\right) d y_{1} d y_{2}-0 \quad (\because \mathbb{E}[y_1]=0)\
&amp;=\iint \delta\left(y_{2}-y_{1}^{2}\right) p\left(y_{1}\right) y_{1} y_{2} d y_{1} d y_{2} \
&amp;=\int y_{1} p\left(y_{1}\right) \underbrace{\int \delta\left(y_{2}-y_{1}^{2}\right) y_{2} d y_{2}}<em>{y_1^2} d y</em>{1} \
&amp;=\int y_{1} p\left(y_{1}\right) y_{1}^{2} d y_{1} \
&amp;=\int y_{1}^{3} p\left(y_{1}\right) d y_{1} \end{aligned}
$$</p>
<p>最後に、$p(y_1)$は偶関数で$y_1^3$は奇関数であるから、この積分は$0$となる。すなわち、$y_1$と$y_2$は独立ではない場合でも共分散は$0$となることがあることが示された。</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="prml第13章演習問題解答1311317"><a class="header" href="#prml第13章演習問題解答1311317">PRML第13章演習問題解答（13.1〜13.17）</a></h1>
<head>
<style>
  div.panel-primary {
	border: 1px solid #000;
    margin: 10px 5px;
    padding: 16px 10px 0px;
  }
</style>
</head>
<h2 id="演習-131-1"><a class="header" href="#演習-131-1">演習 13.1</a></h2>
<div class="panel-primary">
<p><img src="https://i.imgur.com/OVMrByu.png" alt="" />
<img src="https://i.imgur.com/Gq9roJ7.png" alt="" /></p>
<p>8.2節で議論した有向分離のテクニックを使って図13.3に示す全部で$N$個のノードをもつマルコフモデルが、$n=2, \ldots, N$について条件付き独立性</p>
<p>$$
p\left(\mathbf{x}<em>{n} \mid \mathbf{x}</em>{1}, \ldots, \mathbf{x}<em>{n-1}\right)=p\left(\mathbf{x}</em>{n} \mid \mathbf{x}_{n-1}\right) \tag{13.3}
$$</p>
<p>を満たすことを示せ．同様に、図13.4のグラフで記述される全部で$N$個のノードをもつモデルが、$n=3,\ldots,N$について以下の条件付き独立性を満たすことを示せ．</p>
<p>$$
p\left(\mathbf{x}<em>{n} \mid \mathbf{x}</em>{1}, \ldots, \mathbf{x}<em>{n-1}\right)=p\left(\mathbf{x}</em>{n} \mid \mathbf{x}<em>{n-1}, \mathbf{x}</em>{n-2}\right) \tag{13.122}
$$</p>
</div>
<p>.<img src="https://i.imgur.com/G2ZfBBt.png" alt="" /></p>
<p>※8.2.2 有向分離の節で用いられた手法で考える。図のように$A, B, C$の部分集合を指定する。$A$に属する任意のノードから$B$に属するノードへの可能な経路は必ず$C$を通ることになる。
今、$C$は観測されており、head-to-tailとなっているので、P.91の議論から$A$から$B$への経路は$C$で遮断されており、$A \perp !!! \perp B \mid C$が成立する。これより</p>
<p>$$
\begin{aligned}
p(A,B\mid C) &amp;= p(A\mid C)p(B\mid C) \
\end{aligned}
$$</p>
<p>が成立する。両辺に$p(C)$をかけると</p>
<p>$$
\begin{aligned}
p(A,B, C) &amp;= p(A,C)p(B\mid C) \
\end{aligned}
$$</p>
<p>となる。</p>
<p>問題は今$p\left(\mathbf{x}<em>{n} \mid \mathbf{x}</em>{1}, \ldots, \mathbf{x}_{n-1}\right) = p(B\mid A, C)$なので</p>
<p>$$
\begin{aligned}
p(B\mid A, C) &amp;= \frac{p(A, B, C)}{p(A,C)} \
&amp;=p(B\mid C)
\end{aligned}
$$</p>
<p>これを書き直すと</p>
<p>$$
p\left(\mathbf{x}<em>{n} \mid \mathbf{x}</em>{1}, \ldots, \mathbf{x}<em>{n-1}\right)=p\left(\mathbf{x}</em>{n} \mid \mathbf{x}_{n-1}\right) \tag{13.3}
$$</p>
<p>が得られる。</p>
<p><img src="https://i.imgur.com/BPSwm9c.png" alt="" /></p>
<p>二次マルコフ連鎖の場合、$n=3$について$(13.122)$式は自明に成立する。
$n=4,\ldots,N$の二次マルコフ連鎖の場合は上の図のようになる。このとき、同様に$A$から$B$への経路は$C$に含まれるすべてのノードで遮断されている。つまり$A \perp !!! \perp B \mid C$が成立する。これより</p>
<p>$$
\begin{aligned}
p\left(\mathbf{x}<em>{n} \mid \mathbf{x}</em>{1}, \ldots, \mathbf{x}<em>{n-1}\right) &amp;= p(B\mid A, C) \
&amp;= \frac{p(A, B, C)}{p(A,C)} \
&amp;=p(B\mid C) \
&amp;=p(\mathbf{x}</em>{n} \mid \mathbf{x}<em>{n-1}, \mathbf{x}</em>{n-2})
\end{aligned}
$$</p>
<p>よって$(13.122)$式が導かれた。</p>
<h2 id="演習-132-1"><a class="header" href="#演習-132-1">演習 13.2</a></h2>
<div class="panel-primary">
<p>図13.3の有向グラフに対応する同時確率分布</p>
<p>$$
p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{N}\right)=p\left(\mathbf{x}<em>{1}\right) \prod</em>{n=2}^{N} p\left(\mathbf{x}<em>{n} \mid \mathbf{x}</em>{n-1}\right) \tag{13.2}
$$</p>
<p>について考えよう。確率の加法・乗法定理を用い、この同時確率が$n=2, \ldots, N$について条件付き独立性</p>
<p>$$
p\left(\mathbf{x}<em>{n} \mid \mathbf{x}</em>{1}, \ldots, \mathbf{x}<em>{n-1}\right)=p\left(\mathbf{x}</em>{n} \mid \mathbf{x}_{n-1}\right) \tag{13.3}
$$</p>
<p>を満たすことを示せ。同様に、同時分布</p>
<p>$$
p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{N}\right)=p\left(\mathbf{x}<em>{1}\right) p\left(\mathbf{x}</em>{2} \mid \mathbf{x}<em>{1}\right) \prod</em>{n=3}^{N} p\left(\mathbf{x}<em>{n} \mid \mathbf{x}</em>{n-1}, \mathbf{x}_{n-2}\right) \tag{13.4}
$$</p>
<p>によって記述される二次マルコフモデルが、$n=3,\ldots,N$について以下の条件付き独立性を満たすことを示せ。</p>
<p>$$
p\left(\mathbf{x}<em>{n} \mid \mathbf{x}</em>{1}, \ldots, \mathbf{x}<em>{n-1}\right)=p\left(\mathbf{x}</em>{n} \mid \mathbf{x}<em>{n-1}, \mathbf{x}</em>{n-2}\right) \tag{13.123}
$$</p>
</div>
<p>$(13.3)$の左辺を展開し、$\mathbf{x}_{n}$で周辺化しておく。</p>
<p>$$
\begin{aligned}
p\left(\mathbf{x}<em>{n} \mid \mathbf{x}</em>{1}, \ldots, \mathbf{x}<em>{n-1}\right) &amp;= \frac{p\left(\mathbf{x}</em>{1}, \ldots, \mathbf{x}<em>{n}\right)}{p\left(\mathbf{x}</em>{1}, \ldots, \mathbf{x}<em>{n-1}\right)} \
&amp;= \frac{p\left(\mathbf{x}</em>{1}, \ldots, \mathbf{x}<em>{n}\right)}{\sum</em>{\mathbf{x}<em>{n}} p\left(\mathbf{x}</em>{1}, \ldots, \mathbf{x}_{n}\right)}
\end{aligned}
$$</p>
<p>そこで、$(13.2)$式を利用して$p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n}\right)$を求める。</p>
<p>$$
\begin{aligned} p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n}\right) &amp;=\sum_{\mathbf{x}<em>{n+1}} \cdots \sum</em>{\mathbf{x}<em>{N}} p\left(\mathbf{x}</em>{1}, \ldots, \mathbf{x}<em>{N}\right) \ &amp;=\sum</em>{\mathbf{x}<em>{n+1}} \cdots \sum</em>{\mathbf{x}<em>{N}} p\left(\mathbf{x}</em>{1}\right) \prod_{m=2}^{N} p\left(\mathbf{x}<em>{m} \mid \mathbf{x}</em>{m-1}\right) \ &amp;=p\left(\mathbf{x}<em>{1}\right) \prod</em>{m=2}^{n} p\left(\mathbf{x}<em>{m} \mid \mathbf{x}</em>{m-1}\right) \end{aligned}
$$</p>
<p>と書けるので、もう一度$p\left(\mathbf{x}<em>{n} \mid \mathbf{x}</em>{1}, \ldots, \mathbf{x}_{n-1}\right)$を計算すると</p>
<p>$$
\begin{aligned}
p\left(\mathbf{x}<em>{n} \mid \mathbf{x}</em>{1}, \ldots, \mathbf{x}<em>{n-1}\right) &amp;= \frac{p\left(\mathbf{x}</em>{1}, \ldots, \mathbf{x}<em>{n}\right)}{\sum</em>{\mathbf{x}<em>{n}} p\left(\mathbf{x}</em>{1}, \ldots, \mathbf{x}<em>{n}\right)} \
&amp;= \frac{p\left(\mathbf{x}</em>{1}\right) \prod_{m=2}^{n} p\left(\mathbf{x}<em>{m} \mid \mathbf{x}</em>{m-1}\right)}{\sum_{\mathbf{x}<em>{n}} p\left(\mathbf{x}</em>{1}\right) \prod_{m=2}^{n} p\left(\mathbf{x}<em>{m} \mid \mathbf{x}</em>{m-1}\right)} \
&amp;= \frac{p\left(\mathbf{x}<em>{n} \mid \mathbf{x}</em>{n-1}\right)}{\sum_{\mathbf{x}<em>{n}} p\left(\mathbf{x}</em>{n} \mid \mathbf{x}<em>{n-1}\right)} \
&amp;= p\left(\mathbf{x}</em>{n} \mid \mathbf{x}_{n-1}\right)
\end{aligned}
$$</p>
<p>途中で$\mathbf{x}_{n}$に依存する項以外が分母分子でキャンセルされることを用いた。これより、$(13.3)$の右辺が得られた。</p>
<p>二次マルコフモデルについても同様に行うと、$(13.4)$の同時分布からはじめて</p>
<p>$$
\begin{aligned} p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n}\right) &amp;=\sum_{\mathbf{x}<em>{n+1}} \cdots \sum</em>{\mathbf{x}<em>{N}} p\left(\mathbf{x}</em>{1}, \ldots, \mathbf{x}<em>{N}\right) \ &amp;=\sum</em>{\mathbf{x}<em>{n+1}} \cdots \sum</em>{\mathbf{x}<em>{N}} p\left(\mathbf{x}</em>{1}\right) p\left(\mathbf{x}<em>{2} \mid \mathbf{x}</em>{1}\right) \prod_{m=3}^{N} p\left(\mathbf{x}<em>{m} \mid \mathbf{x}</em>{m-1}, \mathbf{x}<em>{m-2}\right) \ &amp;=p\left(\mathbf{x}</em>{1}\right) p\left(\mathbf{x}<em>{2} \mid \mathbf{x}</em>{1}\right) \prod_{m=3}^{n} p\left(\mathbf{x}<em>{m} \mid \mathbf{x}</em>{m-1}, \mathbf{x}_{m-2}\right) \end{aligned}
$$</p>
<p>となるので、</p>
<p>$$
\begin{aligned} p\left(\mathbf{x}<em>{n} \mid \mathbf{x}</em>{1}, \ldots, \mathbf{x}<em>{n-1}\right) &amp;=\frac{p\left(\mathbf{x}</em>{1}, \ldots, \mathbf{x}<em>{n}\right)}{\sum</em>{\mathbf{x}<em>{n}} p\left(\mathbf{x}</em>{1}, \ldots, \mathbf{x}<em>{n}\right)} \ &amp;=\frac{p\left(\mathbf{x}</em>{1}\right) p\left(\mathbf{x}<em>{2} \mid \mathbf{x}</em>{1}\right) \prod_{m=3}^{n} p\left(\mathbf{x}<em>{m} \mid \mathbf{x}</em>{m-1}, \mathbf{x}<em>{m-2}\right)}{\sum</em>{\mathbf{x}<em>{n}} p\left(\mathbf{x}</em>{1}\right) p\left(\mathbf{x}<em>{2} \mid \mathbf{x}</em>{1}\right) \prod_{m=3}^{n} p\left(\mathbf{x}<em>{m} \mid \mathbf{x}</em>{m-1}, \mathbf{x}<em>{m-2}\right)} \
&amp;= \frac{p\left(\mathbf{x}</em>{n} \mid \mathbf{x}<em>{n-1}, \mathbf{x}</em>{n-2}\right)}{\sum_{\mathbf{x}<em>{n}} p\left(\mathbf{x}</em>{n} \mid \mathbf{x}<em>{n-1}, \mathbf{x}</em>{n-2}\right)} \
&amp;= p\left(\mathbf{x}<em>{n} \mid \mathbf{x}</em>{n-1}, \mathbf{x}_{n-2}\right)
\end{aligned}
$$</p>
<p>を得る。</p>
<h2 id="演習-133-1"><a class="header" href="#演習-133-1">演習 13.3</a></h2>
<div class="panel-primary">
<p><img src="https://i.imgur.com/GRbf8ob.png" alt="" />
有向分離を用いて、図13.5の有向グラフで表現される状態空間モデルにおける観測データの分布$p(\mathbf{x}_1,\ldots,\mathbf{x}_N )$が何の条件付き独立性も満足せず、したがって、どのような有限次数のマルコフ性ももたないことを示せ.</p>
</div>
<p>図13.5によると、任意の2つの変数$\mathbf{x}_n$と$\mathbf{x}_m$, $m \neq n$について、それぞれのノード間には直接的な経路は存在せず、$\mathbf{z}$変数に対応する1つ以上のノードを通過する経路が存在している。これらのノードはいずれも観測データの分布$p(\mathbf{x}_1,\ldots,\mathbf{x}_N )$の条件集合に含まれていない。したがって、$\mathbf{x}_n$と$\mathbf{x}_m$の間には遮断されていない経路が存在し、モデルは条件付き独立性または有限次マルコフ性を満たさないことになる。</p>
<h2 id="演習-134-1"><a class="header" href="#演習-134-1">演習 13.4</a></h2>
<div class="panel-primary">
<p>線形回帰モデルやニューラルネットワークのように、出力密度がパラメトリックなモデル$p(\mathbf{x}\mid \mathbf{z},\mathbf{w})$で表される隠れマルコフモデルを考えよう。ここで、$\mathbf{w}$は適応パラメータのベクトルである。データから最尤推定によってパラメータ$\mathbf{w}$を学習する方法を述べよ.</p>
</div>
<p>教科書p.330に記載されている、「隠れマルコフモデルとして離散分布、ガウス分布、混合ガウス分布といった広い範囲の出力分布に加えて、ニューラルネットワークなどの識別的モデルを利用することも可能」という記述への補足のための演習問題。</p>
<p>教科書では、出力分布$p(\mathbf{x}_n|\phi_k)$としてガウス分布（p.335）や離散多項分布（p.336）の例が記載されている。このパラメータ$\phi$を$\mathbf{w}$で置き換えると、識別的モデルにおいても同様の定式化が可能。</p>
<p>実際の更新式の形は、回帰モデルの形や、回帰モデルがどのように使われているかに依って異なる。一例としては、図13.8（p.351）では$\mathbf{x}$の生成密度が、隠れ変数$\mathbf{z}$だけでなく観測変数系列$\mathbf{u}$に依存する。
この場合、回帰モデルは$\mathbf{u}$から$\mathbf{x}$への写像（写像の関数形が$\mathbf{z}$に応じて異なる）とみなすことができる。
ニューラルネットワークのような非線形な関数形では、Mステップにおける$\mathbf{w}$の最適解が解析的に解けないことが多い。</p>
<h2 id="演習-135-1"><a class="header" href="#演習-135-1">演習 13.5</a></h2>
<div class="panel-primary">
<p>隠れマルコフモデルの初期状態確率と遷移確率のパラメータについてのMステップの方程式
$$
\pi_{k}=\frac{\gamma\left(z_{1 k}\right)}{\sum_{j=1}^{K} \gamma\left(z_{1 j}\right)} \tag{13.18}
$$
$$
A_{j k}=\frac{\sum_{n=2}^{N} \xi\left(z_{n-1, j}, z_{n k}\right)}{\sum_{l=1}^{K} \sum_{n=2}^{N} \xi\left(z_{n-1, j}, z_{n l}\right)} \tag{13.19}
$$
を、完全データに対する対数尤度関数の期待値</p>
<p>$$
\begin{aligned} Q\left(\pmb{\theta}, \pmb{\theta}^{\text {old }}\right)=&amp; \sum_{k=1}^{K} \gamma\left(z_{1 k}\right) \ln \pi_{k}+\sum_{n=2}^{N} \sum_{j=1}^{K} \sum_{k=1}^{K} \xi\left(z_{n-1, j}, z_{n k}\right) \ln A_{j k} \ &amp;+\sum_{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right) \ln p\left(\mathbf{x}<em>{n} \mid \phi</em>{k}\right) \end{aligned} \tag{13.17}
$$</p>
<p>を最大化することにより確かめよ。その際、適当なラグランジュ乗数を用いて$\pmb{\pi}$と$\mathbf{A}$の成分に対する和の制約を与えること.</p>
</div>
<p>P.334~335参照。Mステップの更新式に相当する。</p>
<p>$\displaystyle \sum_{l=1}^{K} \pi_{l}=1$の条件下で$Q\left(\pmb{\theta}, \pmb{\theta}^{\text {old }}\right)$を$\pi_{l}$について最大化する。</p>
<p>$$
\begin{aligned}
&amp;\frac{\partial}{\partial \pi_{k}}\left[Q\left(\pmb{\theta}, \pmb{\theta}^{\text {old }}\right)-\lambda\left(\sum_{l=1}^{K} \pi_{l}-1\right)\right] \
=&amp;\ \gamma\left(z_{1 k}\right) \cdot \frac{1}{\pi_{k}}-\lambda(=0)
\end{aligned}
$$</p>
<p>両辺に$\pi_{k}$を乗じて、$k$について和をとって整理すると、</p>
<p>$$
\sum_{k=1}^{K}\gamma(z_{1k}) = \lambda \underbrace{\sum_{k=1}^{K}\pi_{k}}_{=1}
$$</p>
<p>よって、$\displaystyle \lambda = \sum_{k=1}^{K}\gamma(z_{1k})$。これを代入して</p>
<p>$$
\pi_{k}=\frac{1}{\lambda} \gamma\left(z_{1 k}\right)=\frac{\gamma\left(z_{1 k}\right)}{\sum_{j=1}^{K} \gamma\left(z_{1 j}\right)} \tag{13.18}
$$</p>
<p>次に、$\displaystyle \sum_{k=1}^{K}A_{jk} = 1$の条件下で$Q\left(\pmb{\theta}, \pmb{\theta}^{\text {old }}\right)$を$A_{jk}$について最大化する。</p>
<p>$$
\begin{aligned}
&amp;\frac{\partial}{\partial A_{j k}}\left[Q\left(\pmb{\theta}, \pmb{\theta}^{\text {old }}\right)-\sum_{l} \mu_{l}\left(\sum_{m} A_{j m}-1\right)\right] \
=&amp;\ \sum_{n=2}^{N} \sum_{l} \sum_{m} \xi\left(z_{n-1, l}, z_{n m}\right) \cdot \frac{1}{A_{lm}} \cdot \delta_{j l} \delta_{k m} - \mu_{l}\delta_{jl} \
=&amp;\ \sum_{n=2}^{N} \xi\left(\varepsilon_{n-1, j}, z_{n k}\right) \cdot \frac{1}{A_{j k}}-\mu_{j}(=0)
\end{aligned}
$$</p>
<p>両辺に$A_{jk}$を乗じて$k$について和をとって整理すると</p>
<p>$$
\sum_{k=1}^{K} \sum_{n=2}^{N} \xi\left(z_{n-1, j}, z_{n k}\right)=\mu_{j} \underbrace{\sum_{k}A_{jk}}_{=1}
$$</p>
<p>求めた$\mu_{j}$を代入して</p>
<p>$$
A_{j k} =\frac{1}{\mu_{j}} \sum_{n=2}^{N} \xi\left(z_{n-1, j}, z_{n k}\right) = \frac{\sum_{n=2}^{N} \xi\left(z_{n-1, j}, z_{n k}\right)}{\sum_{l=1}^{K} \sum_{n=2}^{N} \xi\left(z_{n-1, j}, z_{n l}\right)} \tag{13.19}
$$</p>
<p>を得る。</p>
<h2 id="演習-136-1"><a class="header" href="#演習-136-1">演習 13.6</a></h2>
<div class="panel-primary">
<p>もし、隠れマルコフモデルのパラメータ$\pmb{\pi}$か$\mathbf{A}$のいずれかの要素が最初にゼロに設定された場合、それらの要素素はEMアルゴリズムのその後に続く更新においてゼロのままであることを示せ。</p>
</div>
<p>まずパラメータ$\mathbf{\pi}$について考える．
$\mathbf{\pi}$の特定の要素$\pi_k$が0に初期化されているとする.最初のEステップでは, $\alpha(z_{1k})$は (13.37)から以下のようにかける.</p>
<p>$$
\mathbf{\alpha}(z_{1k})=\pi_k p(\mathbf{x}_1\mid\phi_k)\tag{13.37}
$$</p>
<p>$\pi_k$が0に初期化されているためこの値は0であることがわかる.また(13.33)から,</p>
<p>$$
\gamma(z_{1k})=\frac{\alpha(z_{1k})\beta(z_{1k})}{p(\mathbf{x})}\tag{13.33}
$$</p>
<p>$\gamma(z_{1k})$$もゼロになる．また次のMステップで</p>
<p>$$
\pi_{k}=\frac{\gamma\left(z_{1 k}\right)}{\sum_{j=1}^{K} \gamma\left(z_{1 j}\right)}\tag
{13.18}
$$</p>
<p>により更新される$\pi_k$の値は再びゼロとなることがわかる。これはその後のEM step に当てはまるので、この$\pi_k$の値はゼロのままである.</p>
<p>次に$\mathbf{A}$の要素$A_{jk}$が0で初期化されているとする。</p>
<p>$$
\begin{aligned}
\xi &amp;\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right)=p\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n} \mid \mathbf{X}\right) \
&amp;=\frac{\left.\alpha\left(\mathbf{z}<em>{n-1}\right) p\left(\mathbf{x}</em>{n} \mid \mathbf{z}<em>{n}\right) p\left(\mathbf{z}</em>{n} \mid \mathbf{z}<em>{n-1}\right) \beta\left(\mathbf{z}</em>{n}\right)\right)}{p(\mathbf{X})} .
\end{aligned}
$$</p>
<p>(13.43)の式中で、遷移行列部分の$p(z_{nk}\mid z_{n-1,j})=A_{jk}$が0であり、$\xi \left(\mathbf{z}<em>{n-1,k}, \mathbf{z}</em>{nj}\right)$が0になることがわかる。続くMステップでは、$A_{jk}$の新しい値は</p>
<p>$$
A_{j k}=\frac{\sum_{n=2}^{N} \xi\left(z_{n-1, j}, z_{n k}\right)}{\sum_{l=1}^{K} \sum_{n=2}^{N} \xi\left(z_{n-1, j}, z_{n l}\right)}
$$</p>
<p>で与えられ、これも0になりことがわかる．以上により示された．</p>
<h2 id="演習-137-1"><a class="header" href="#演習-137-1">演習 13.7</a></h2>
<div class="panel-primary">
<p>ガウス出力密度をもつ隠れマルコフモデルを考える。ガウス分布の平均と共分散のパラメータについての関数$Q(\pmb{\theta},\pmb{\theta}^{\mathrm{old}})$の最大化が、Mステップの方程式
$$
\pmb{\mu}<em>{k}=\frac{\sum</em>{n=1}^{N} \gamma\left(z_{n k}\right) \mathrm{x}<em>{n}}{\sum</em>{n=1}^{N} \gamma\left(z_{n k}\right)} \tag{13.20}
$$
と
$$
\mathbf{\Sigma}<em>{k}=\frac{\sum</em>{n=1}^{N} \gamma\left(z_{n k}\right)\left(\mathbf{x}<em>{n}-\pmb{\mu}</em>{k}\right)\left(\mathbf{x}<em>{n}-\pmb{\mu}</em>{k}\right)^{\mathrm{T}}}{\sum_{n=1}^{N} \gamma\left(z_{n k}\right)} \tag{13.21}
$$
を導くことを示せ。</p>
</div>
<p>P.335の後半を参照。$Q(\pmb{\theta},\pmb{\theta}^{\mathrm{old}})$を$\pmb{\phi}<em>{k}$に関して最大化する場合、$(13.17)$の最後の項だけが$\pmb{\phi}</em>{k}$に依存している。パラメータ$\pmb{\phi}<em>{k}$が成分ごとに独立ならば、この項は$k$の各々の値ごとの項の和に分解され、そのそれぞれの項は独立に最大化することができる。出力分布がガウス密度分布の場合には$p\left(\mathbf{x} \mid \pmb{\phi}</em>{k}\right)=\mathcal{N}\left(\mathbf{x} \mid \pmb{\mu}<em>{k}, \mathbf{\Sigma}</em>{k}\right)$と書けて、「$\pmb{\phi}<em>{k}$に関しての最大化」をガウス分布の平均$\pmb{\mu}</em>{k}$と分散$\mathbf{\Sigma}_{k}$についての最大化に分けて考える。</p>
<p>$$
\begin{aligned}
&amp; \sum_{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right) \ln p\left(\mathbf{x}<em>{n} \mid \phi</em>{k}\right)=\sum_{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right) \ln \mathcal{N}\left(\mathbf{x}<em>{n} \mid \pmb{\mu}</em>{k}, \mathbf{\Sigma}<em>{k}\right) \
=&amp; \sum</em>{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right)\left{-\frac{D}{2} \ln (2 \pi)-\frac{1}{2} \ln \left|\mathbf{\Sigma}<em>{k}\right|-\frac{1}{2}\left(\mathbf{x}</em>{n}-\pmb{\mu}<em>{k}\right)^{\mathrm{T}} \mathbf{\Sigma}</em>{k}^{-1}\left(\mathbf{x}<em>{n}-\pmb{\mu}</em>{k}\right)\right}
\end{aligned}
$$</p>
<p>(1) $\pmb{\mu}_{k}$について最大化する。</p>
<p>$$
\begin{aligned}
\frac{\partial}{\partial \pmb{\mu}<em>{k}} Q\left(\pmb{\theta}, \pmb{\theta}^{\text {old }}\right) &amp;=\frac{\partial}{\partial \pmb{\mu}</em>{k}}\left{\sum_{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right) \cdot\left(-\frac{1}{2}\left(\mathbf{x}<em>{n}-\pmb{\mu}</em>{k}\right)^{\mathrm{T}} \mathbf{\Sigma}<em>{k}^{-1}\left(\mathbf{x}</em>{n}-\pmb{\mu}<em>{k}\right)\right)\right} \
&amp;=\sum</em>{n=1}^{N} \gamma\left(z_{n k}\right) \mathbf{\Sigma}<em>{k}^{-1}\left(\mathbf{x}</em>{n}-\pmb{\mu}_{k}\right)=0
\end{aligned}
$$</p>
<p>$$
\sum_{n=1}^{N} \gamma\left(z_{n k}\right) \mathbf{\Sigma}<em>{k}^{-1} \pmb{\mu}</em>{k} = \sum_{n=1}^{N} \gamma\left(z_{n k}\right) \mathbf{\Sigma}<em>{k}^{-1} \mathbf{x}</em>{n}
$$</p>
<p>両辺に$\mathbf{\Sigma}_{k}$を乗じて整理すると</p>
<p>$$
\pmb{\mu}<em>{k}=\frac{\sum</em>{n=1}^{N} \gamma\left(z_{n k}\right) \mathbf{x}<em>{n}}{\sum</em>{n=1}^{N} \gamma\left(z_{n k}\right)} \tag{13.20}
$$</p>
<p>を得る。</p>
<p>(2) $\mathbf{\Sigma}_{k}$について最大化する。</p>
<p>$$
\begin{aligned}
\frac{\partial}{\partial \mathbf{\Sigma}<em>{k}} Q\left(\pmb{\theta}, \pmb{\theta}^{\text {old }}\right) &amp;=\frac{\partial}{\partial \mathbf{\Sigma}</em>{k}} \sum_{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right)\left{-\frac{1}{2} \ln \left|\mathbf{\Sigma}<em>{k}\right|-\frac{1}{2}\left(\mathbf{x}</em>{n}-\pmb{\mu}<em>{k}\right)^{\mathrm{T}} \mathbf{\Sigma}</em>{k}^{-1}\left(\mathbf{x}<em>{n}-\pmb{\mu}</em>{k}\right)\right} \
&amp;=\sum_{n=1}^{N} \gamma\left(z_{n k}\right)\left{-\frac{1}{2}\left(\mathbf{\Sigma}<em>{k}^{-1}\right)^{\mathrm{T}}-\frac{1}{2} \frac{\partial}{\partial \mathbf{\Sigma}</em>{k}} \operatorname{Tr}\left(\mathbf{\Sigma}<em>{k}^{-1}\left(\mathbf{x}</em>{n}-\pmb{\mu}<em>{k}\right)\left(\mathbf{x}</em>{n}-\pmb{\mu}<em>{k}\right)^{\mathrm{T}}\right)\right} \
&amp;=-\frac{1}{2} \sum</em>{n=1}^{N} \gamma\left(z_{n k}\right)\left{\mathbf{\Sigma}<em>{k}^{-1}-\mathbf{\Sigma}</em>{k}^{-1}\left(\mathbf{x}<em>{n}-\pmb{\mu}</em>{k}\right)\left(\mathbf{x}<em>{n}-\pmb{\mu}</em>{k}\right)^{\mathrm{T}} \mathbf{\Sigma}_{k}^{-1}\right}=0 \end{aligned}
$$</p>
<p>上式について左と右から1回ずつ$\mathbf{\Sigma}_{k}$を乗じて整理すると、</p>
<p>$$
\sum_{n=1}^{N} \gamma\left(z_{n k}\right)\mathbf{\Sigma}<em>{k} = \sum</em>{n=1}^{N} \gamma\left(z_{n k}\right)\mathbf{\Sigma}<em>{k}\left(\mathbf{x}</em>{n}-\pmb{\mu}<em>{k}\right)\left(\mathbf{x}</em>{n}-\pmb{\mu}_{k}\right)^{\mathrm{T}}
$$</p>
<p>$$
\mathbf{\Sigma}<em>{k} = \frac{\sum</em>{n=1}^{N} \gamma\left(z_{n k}\right)\mathbf{\Sigma}<em>{k}\left(\mathbf{x}</em>{n}-\pmb{\mu}<em>{k}\right)\left(\mathbf{x}</em>{n}-\pmb{\mu}<em>{k}\right)^{\mathrm{T}}}{\sum</em>{n=1}^{N} \gamma\left(z_{n k}\right)} \tag{13.21}
$$</p>
<p>を得る。</p>
<h2 id="演習-138-1"><a class="header" href="#演習-138-1">演習 13.8</a></h2>
<div class="panel-primary">
<p>多項分布に従う離散観測値をもつ隠れマルコフモデルにおいて、隠れ変数を与えたときの観測値の条件付き分布が
$$
p(\mathbf{x} \mid \mathbf{z})=\prod_{i=1}^{D} \prod_{k=1}^{K} \mu_{i k}^{x_{i} z_{k}} \tag{13.22}
$$
で与えられ、また対応するMステップの方程式が
$$
\mu_{i k}=\frac{\sum_{n=1}^{N} \gamma\left(z_{n k}\right) x_{n i}}{\sum_{n=1}^{N} \gamma\left(z_{n k}\right)} \tag{13.23}
$$
で与えられることを示せ。同様に、複数の二値の出力変数をもち、そのそれぞれがベルヌーイ条件付き分布に従う隠れマルコフモデルについて、条件付き分布についての式とMステップの方程式を書き下せ。
ヒント：必要に応じて、2.1節と2.2節における、独立同分布に従うデータに対する最尤推定の解法についての、対応する議論を参照せよ。</p>
</div>
<p>(13.17)  の $Q\left(\theta, \theta^{\text {old }}\right)$ のうち、$\mu$ に依存しているのは最終項のみなので、最終項のみを考える.</p>
<p>$$
\sum_{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right) \ln p\left(\mathbf{x}<em>{n} \mid \phi</em>{k}\right)=\sum_{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right) \sum_{i=1}^{D} x_{n i} \ln \mu_{k i}
$$</p>
<p>ラグランジュの未定乗数法を用いると、最大化すべき関数は、
$$
\sum_{n=1}^{N} \sum_{k=1}^{K} \gamma\left(z_{n k}\right) \sum_{i=1}^{D} x_{n i} \ln \mu_{k i}+\sum_{k=1}^{K} \lambda_{k}\left(\sum_{i=1}^{D} \mu_{k i}-1\right)
$$
$\mu$に関して微分して、
$$
0=\sum_{n=1}^{N} \gamma\left(z_{n k}\right) \frac{x_{n i}}{\mu_{k i}}+\lambda_{k}
$$</p>
<p>$\mu_{k i}$を両辺にかけて, $i$で和を取って, $\sum_{i} x_{n i}=1$と$\sum_{i} \mu_{n i}=1$ を利用して
$$
\lambda_{k}=-\sum_{n=1}^{N} \gamma\left(z_{n k}\right)
$$
$\lambda_{k}$を
$$
0=\sum_{n=1}^{N} \gamma\left(z_{n k}\right) \frac{x_{n i}}{\mu_{k i}}+\lambda_{k}
$$
に代入して、整理すると、
$$
\mu_{i k}=\frac{\sum_{n=1}^{N} \gamma\left(z_{n k}\right) x_{n i}}{\sum_{n=1}^{N} \gamma\left(z_{n k}\right)} \tag{13.23}
$$
を得る.</p>
<p>.</p>
<h2 id="演習-139-1"><a class="header" href="#演習-139-1">演習 13.9</a></h2>
<div class="panel-primary">
<p>隠れマルコフモデルの</p>
<p>$$
p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{N}, \mathbf{z}<em>{1}, \ldots, \mathbf{z}</em>{N}\right)=p\left(\mathbf{z}<em>{1}\right)\left[\prod</em>{n=2}^{N} p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}\right)\right] \prod_{n=1}^{N} p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right) \tag{13.6}
$$</p>
<p>で定義される同時分布が条件付き独立性$(13.24)-(13.31)$</p>
<p>$$
\begin{aligned}
p\left(\mathbf{X} \mid \mathbf{z}<em>{n}\right) =\ &amp;p\left(\mathbf{x}</em>{1}, \ldots, \mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right) \times \
&amp;p\left(\mathbf{x}<em>{n+1}, \ldots, \mathbf{x}</em>{N} \mid \mathbf{z}_{n}\right)
\end{aligned}
\tag{13.24}
$$</p>
<p>$$
p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n-1} \mid \mathbf{x}<em>{n}, \mathbf{z}</em>{n}\right)= p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n-1} \mid \mathbf{z}_{n}\right) \tag{13.25}
$$</p>
<p>$$
p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n-1} \mid \mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right)= p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n-1} \mid \mathbf{z}_{n-1}\right) \tag{13.26}
$$</p>
<p>$$
p\left(\mathbf{x}<em>{n+1}, \ldots, \mathbf{x}</em>{N} \mid \mathbf{z}<em>{n}, \mathbf{z}</em>{n+1}\right)= p\left(\mathbf{x}<em>{n+1}, \ldots, \mathbf{x}</em>{N} \mid \mathbf{z}_{n+1}\right) \tag{13.27}
$$</p>
<p>$$
p\left(\mathbf{x}<em>{n+2}, \ldots, \mathbf{x}</em>{N} \mid \mathbf{z}<em>{n+1}, \mathbf{x}</em>{n+1}\right)= p\left(\mathbf{x}<em>{n+2}, \ldots, \mathbf{x}</em>{N} \mid \mathbf{z}_{n+1}\right) \tag{13.28}
$$</p>
<p>$$
\begin{aligned}
p\left(\mathbf{X} \mid \mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right) =\ &amp;p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n-1} \mid \mathbf{z}<em>{n-1}\right) \times \
&amp; p\left(\mathbf{x}</em>{n} \mid \mathbf{z}<em>{n}\right) p\left(\mathbf{x}</em>{n+1}, \ldots, \mathbf{x}<em>{N} \mid \mathbf{z}</em>{n}\right)
\end{aligned}\tag{13.29}
$$</p>
<p>$$
p\left(\mathbf{x}<em>{N+1} \mid \mathbf{X}, \mathbf{z}</em>{N+1}\right)= p\left(\mathbf{x}<em>{N+1} \mid \mathbf{z}</em>{N+1}\right) \tag{13.30}
$$</p>
<p>$$
p\left(\mathbf{z}<em>{N+1} \mid \mathbf{z}</em>{N}, \mathbf{X}\right)= p\left(\mathbf{z}<em>{N+1} \mid \mathbf{z}</em>{N}\right) \tag{13.31}
$$</p>
<p>を満たすことを、有向分離の規準を用いて確かめよ。</p>
</div>
<p>P.84, 85の条件付き独立性とP.90の有向分離の規準を復習すると、任意の重複しないノード集合$A, B, C$があり、$A$の任意のノードから$B$の任意のノードへのすべての経路を考えたときに$C$がP.90の(a), (b)の条件を満たすとき、「$A$は$C$によって$B$から有向分離されている」と呼び、$A \perp !!! \perp B \mid C$で表す。このとき、</p>
<p>$$
p(A\mid B, C) = p(A\mid C) \iff p(B\mid A, C) = p(B\mid C) \iff p(A,B\mid C) = p(A\mid C)p(B\mid C)
$$</p>
<p>が成立する。日本語でまとめると、</p>
<ul>
<li>$A$と$B$が$C$を与えたもとで条件付き独立</li>
<li>$C$が分かっている状況で新たに$B$が分かっても、$A$に関する情報は得られない</li>
<li>$C$が分かっている状況で新たに$A$が分かっても、$B$に関する情報は得られない</li>
</ul>
<p>隠れマルコフモデルのグラフィカルモデルは</p>
<img src="https://i.imgur.com/GRbf8ob.png" width="70%">
<p>これを踏まえて$(13.24)-(13.31)$を1つずつ示していく。</p>
<p>$(13.24)$について、${\mathbf{x}<em>{1},\ldots,\mathbf{x}</em>{n}}$の任意のノードから${\mathbf{x}<em>{n+1},\ldots,\mathbf{x}</em>{N}}$の任意のノードへの経路は$\mathbf{z}<em>{n}$によって遮断されている。なぜなら、$\mathbf{z}</em>{n}$でhead-to-tailまたはtail-to-tail($\mathbf{x}<em>{n}$のみ)となっているからである。すなわち${\mathbf{x}</em>{1},\ldots,\mathbf{x}<em>{n}} \perp !!! \perp {\mathbf{x}</em>{n+1},\ldots,\mathbf{x}<em>{N}} \mid \mathbf{z}</em>{n}$と書けるので、</p>
<p>$$
\begin{aligned}
p\left(\mathbf{X} \mid \mathbf{z}<em>{n}\right) =\ &amp;p\left(\mathbf{x}</em>{1}, \ldots, \mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right) \times \
&amp;p\left(\mathbf{x}<em>{n+1}, \ldots, \mathbf{x}</em>{N} \mid \mathbf{z}_{n}\right)
\end{aligned}
$$</p>
<p>となる。</p>
<p>$(13.25)$について、${\mathbf{x}<em>{1},\ldots,\mathbf{x}</em>{n-1}}$の任意のノードから$\mathbf{x}<em>{n}$のノードへの経路は$\mathbf{z}</em>{n}$でhead-to-tailになっているので遮断されている。すなわち${\mathbf{x}<em>{1},\ldots,\mathbf{x}</em>{n-1}} \perp !!! \perp \mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}$と書けるので、</p>
<p>$$
p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n-1} \mid \mathbf{x}<em>{n}, \mathbf{z}</em>{n}\right)= p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n-1} \mid \mathbf{z}_{n}\right)
$$</p>
<p>となる。</p>
<p>$(13.26)$について、${\mathbf{x}<em>{1},\ldots,\mathbf{x}</em>{n-1}}$の任意のノードから$\mathbf{z}<em>{n}$のノードへの経路は$\mathbf{z}</em>{n-1}$でhead-to-tail（$\mathbf{x}<em>{n-1}$以外）またはtail-to-tail（$\mathbf{x}</em>{n-1}$のみ）になっているので遮断されている。すなわち${\mathbf{x}<em>{1},\ldots,\mathbf{x}</em>{n-1}} \perp !!! \perp \mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}$と書けるので、</p>
<p>$$
p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n-1} \mid \mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right) = p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n-1} \mid \mathbf{z}<em>{n}, \mathbf{z}</em>{n-1}\right)= p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n-1} \mid \mathbf{z}_{n-1}\right)
$$</p>
<p>となる。</p>
<p>$(13.27)$について、ノード$\mathbf{z}<em>{n}$から${\mathbf{x}</em>{n+1},\ldots,\mathbf{x}<em>{N}}$の任意のノードへの経路は$\mathbf{z}</em>{n+1}$でhead-to-tailになっているので遮断されている。すなわち$\mathbf{z}<em>{n} \perp !!! \perp {\mathbf{x}</em>{n+1},\ldots,\mathbf{x}<em>{N}} \mid \mathbf{z}</em>{n+1}$と書けるので、</p>
<p>$$
p\left(\mathbf{x}<em>{n+1}, \ldots, \mathbf{x}</em>{N} \mid \mathbf{z}<em>{n}, \mathbf{z}</em>{n+1}\right)= p\left(\mathbf{x}<em>{n+1}, \ldots, \mathbf{x}</em>{N} \mid \mathbf{z}_{n+1}\right)
$$</p>
<p>$(13.28)$について、ノード$\mathbf{x}<em>{n+1}$から${\mathbf{x}</em>{n+2},\ldots,\mathbf{x}<em>{N}}$の任意のノードへの経路は$\mathbf{z}</em>{n+1}$でtail-to-tailになっているので遮断されている。すなわち$\mathbf{x}<em>{n+1} \perp !!! \perp {\mathbf{x}</em>{n+2},\ldots,\mathbf{x}<em>{N}} \mid \mathbf{z}</em>{n+1}$と書けるので、</p>
<p>$$
p\left(\mathbf{x}<em>{n+2}, \ldots, \mathbf{x}</em>{N} \mid \mathbf{z}<em>{n+1}, \mathbf{x}</em>{n+1}\right)= p\left(\mathbf{x}<em>{n+2}, \ldots, \mathbf{x}</em>{N} \mid \mathbf{z}_{n+1}\right)
$$</p>
<p>$(13.29)$について、まずノード${\mathbf{x}<em>{1},\ldots,\mathbf{x}</em>{n-1}}$から${\mathbf{x}<em>{n},\ldots,\mathbf{x}</em>{N}}$の任意のノードへの経路は$\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}$でhead-to-tailまたはtail-to-tailになっているので遮断されている。すなわち$\mathbf{x}<em>{1}, \ldots ,\mathbf{x}</em>{n-1} \perp !!! \perp \mathbf{x}<em>{n}, \ldots , \mathbf{x}</em>{N} \mid \mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}$と書けるので、</p>
<p>$$
p\left(\mathbf{X} \mid \mathbf{z}<em>{n-1}, \mathbf{z}</em>{n} \right) = p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n-1} \mid \mathbf{z}<em>{n-1},\mathbf{z}</em>{n} \right)p\left(\mathbf{x}<em>{n}, \ldots, \mathbf{x}</em>{N} \mid \mathbf{z}<em>{n-1},\mathbf{z}</em>{n} \right)
$$</p>
<p>右辺第一項は${\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n-1}}$から$\mathbf{z}<em>{n}$までの経路が$\mathbf{z}</em>{n-1}$で遮断されているので${\mathbf{x}<em>{n+1}, \ldots, \mathbf{x}</em>{N}} \perp !!! \perp \mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}$となり</p>
<p>$$
p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n-1} \mid \mathbf{z}<em>{n-1},\mathbf{z}</em>{n} \right) = p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n-1} \mid \mathbf{z}_{n-1} \right)
$$</p>
<p>右辺第二項は$\mathbf{x}<em>{n}$から${\mathbf{x}</em>{n+1}, \ldots, \mathbf{x}<em>{N}}$までの経路が$\mathbf{z}</em>{n}$で遮断されているので</p>
<p>$$
p\left(\mathbf{x}<em>{n}, \ldots, \mathbf{x}</em>{N} \mid \mathbf{z}<em>{n-1},\mathbf{z}</em>{n} \right) = p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n-1}, \mathbf{z}<em>{n} \right)p\left(\mathbf{x}</em>{n+1},\ldots,\mathbf{x}<em>{N} \mid \mathbf{z}</em>{n-1}, \mathbf{z}_{n} \right)
$$</p>
<p>さらにこの右辺について、同様に考えると、$\mathbf{z}<em>{n-1}\perp !!! \perp \mathbf{x}</em>{n} \mid \mathbf{z}<em>{n}$と$\mathbf{z}</em>{n-1}\perp !!! \perp {\mathbf{x}<em>{n+1}, \ldots, \mathbf{x}</em>{N}}\mid \mathbf{z}_n$が成立しているので、</p>
<p>$$
\begin{aligned}
p\left(\mathbf{x}<em>{n}, \ldots, \mathbf{x}</em>{N} \mid \mathbf{z}<em>{n-1},\mathbf{z}</em>{n} \right) &amp;= p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n-1}, \mathbf{z}<em>{n} \right)p\left(\mathbf{x}</em>{n+1},\ldots,\mathbf{x}<em>{N} \mid \mathbf{z}</em>{n-1}, \mathbf{z}<em>{n} \right) \
&amp;= p(\mathbf{x}</em>{n} \mid \mathbf{z}<em>{n})p(\mathbf{x}</em>{n+1},\ldots,\mathbf{x}<em>{N} \mid \mathbf{z}</em>{n})
\end{aligned}
$$</p>
<p>以上を合わせて</p>
<p>$$
\begin{aligned}
p\left(\mathbf{X} \mid \mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right) =\ &amp;p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n-1} \mid \mathbf{z}<em>{n-1}\right) \times \
&amp; p\left(\mathbf{x}</em>{n} \mid \mathbf{z}<em>{n}\right) p\left(\mathbf{x}</em>{n+1}, \ldots, \mathbf{x}<em>{N} \mid \mathbf{z}</em>{n}\right)
\end{aligned}
$$</p>
<p>$(13.30)$について、$\mathbf{x}<em>{N+1}$ノードから任意の$\mathbf{X}$ノードへは$\mathbf{z}</em>{N+1}$で遮断されている。$\mathbf{X} \perp !!! \perp \mathbf{x}<em>{N+1} \mid \mathbf{z}</em>{N+1}$なので、</p>
<p>$$
p\left(\mathbf{x}<em>{N+1} \mid \mathbf{X}, \mathbf{z}</em>{N+1}\right)= p\left(\mathbf{x}<em>{N+1} \mid \mathbf{z}</em>{N+1}\right)
$$</p>
<p>$(13.31)$について、任意の$\mathbf{X}$ノードから$\mathbf{z}<em>{N+1}$ノードへは$\mathbf{z}</em>{N}$で遮断されている。$\mathbf{X} \perp !!! \perp \mathbf{z}<em>{N+1} \mid \mathbf{z}</em>{N}$なので、</p>
<p>$$
p\left(\mathbf{z}<em>{N+1} \mid \mathbf{z}</em>{N}, \mathbf{X}\right)= p\left(\mathbf{z}<em>{N+1} \mid \mathbf{X}, \mathbf{z}</em>{N}\right) = p\left(\mathbf{z}<em>{N+1} \mid \mathbf{z}</em>{N}\right)
$$</p>
<h2 id="演習-1310"><a class="header" href="#演習-1310">演習 13.10</a></h2>
<div class="panel-primary">
<p>隠れマルコフモデルの
$$
p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{N}, \mathbf{z}<em>{1}, \ldots, \mathbf{z}</em>{N}\right)=p\left(\mathbf{z}<em>{1}\right)\left[\prod</em>{n=2}^{N} p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}\right)\right] \prod_{n=1}^{N} p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right) \tag{13.6}
$$
で定義される同時分布が条件付き独立性$(13.24)-(13.31)$を満たすことを、確率の加法乗法定理を用いて確かめよ。</p>
</div>
<p>$(13.24)$を加法・乗法定理から求める</p>
<p>$$
\begin{aligned}
p\left(\mathbf{x}<em>{1}, \ldots \mathbf{x}</em>{n}, \mathbf{z}<em>{n}\right)&amp;=\sum</em>{\mathbf{x}<em>{n+1}} \cdots \sum</em>{\mathbf{x}<em>{N}} p\left(\mathbf{X}, \mathbf{Z}</em>{n}\right)=\sum_{\mathbf{x}<em>{n+1}} \cdots \sum</em>{\mathbf{x}<em>{N}} \sum</em>{\mathbf{Z}<em>{/n}} p(\mathbf{X}, \mathbf{Z}) \
&amp;=\sum</em>{\mathbf{x}<em>{n+1}} \cdots \sum</em>{\mathbf{x}<em>{N}} \sum</em>{\mathbf{Z}<em>{/n}} p\left(\mathbf{z}</em>{1}\right)\left[\prod_{i=2}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=1}^{N} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right) \
&amp;=\sum_{\mathbf{Z}<em>{/n}} \sum</em>{\mathbf{x}<em>{n+1}} \ldots \sum</em>{\mathbf{x}<em>{N}} p\left(\mathbf{z}</em>{1}\right)\left[\prod_{i=2}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=1}^{N} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right) \
&amp;=\sum_{\mathbf{Z}<em>{/n}} p\left(\mathbf{z}</em>{1}\right)\left[\prod_{i=2}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=1}^{n} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right) \sum_{\mathbf{x}<em>{n+1}} \ldots \sum</em>{\mathbf{x}<em>{N}} \prod</em>{i= n+1}^{N} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right) \
&amp;=\sum_{\mathbf{Z}<em>{/n}} p\left(\mathbf{z}</em>{1}\right)\left[\prod_{i=2}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=1}^{n} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right) \prod_{i=n+1}^{N} \underbrace{\sum_{\mathbf{x}<em>{i}} p\left(\mathbf{x}</em>{i} \mid \mathbf{z}<em>{i}\right)}</em>{=1} \
&amp;=\sum_{\mathbf{Z}<em>{/n}} p\left(\mathbf{z}</em>{i}\right)\left[\prod_{i=2}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=1}^{n} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right) \
&amp;=\sum_{\mathbf{z}<em>{1}} \cdots \sum</em>{\mathbf{z}<em>{n-1}} \sum</em>{\mathbf{z}<em>{n+1}} \cdots \sum</em>{\mathbf{z}<em>{N}} p\left(\mathbf{z}</em>{1}\right)\left[\prod_{i=2}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=1}^{n} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right) \
&amp;=\sum_{\mathbf{z}<em>{1}} \ldots \sum</em>{\mathbf{z}<em>{n-1}} p\left(\mathbf{z}</em>{1}\right)\left[\prod_{i=2}^{n} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=1}^{n} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right)\left(\sum_{\mathbf{z}<em>{n+1}} \cdots \sum</em>{\mathbf{z}<em>{N}}\left[\prod</em>{i=n+1}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right]\right) \
&amp;=\sum_{\mathbf{z}<em>{1}} \cdots \sum</em>{\mathbf{z}<em>{n-1}} p\left(\mathbf{z}</em>{i}\right)\left[\prod_{i=2}^{n} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=1}^{n} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right)
\end{aligned}
$$</p>
<p>となる。同様に</p>
<p>$$
\begin{aligned}
p\left(\mathbf{x}<em>{n+1}, \cdots \mathbf{x}</em>{N}, \mathbf{z}<em>{n}\right)&amp;=\sum</em>{\mathbf{x}<em>{1}} \cdots \sum</em>{\mathbf{x}<em>{n}} \sum</em>{\mathbf{Z}<em>{/n}} p(\mathbf{X}, \mathbf{Z}) \
&amp;=\sum</em>{\mathbf{x}<em>{1}} \ldots \sum</em>{\mathbf{x}<em>{n}} \sum</em>{\mathbf{Z}<em>{/n}} p\left(\mathbf{z}</em>{1}\right)\left[\prod_{i=2}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=1}^{N} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right) \
&amp;=\sum_{\mathbf{Z}<em>{/n}} p\left(\mathbf{z}</em>{i}\right)\left[\prod_{i=2}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=n+1}^{N} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right) \underbrace{\sum_{\mathbf{x}<em>{1}} \sum</em>{\mathbf{x}<em>{n}} \prod</em>{i=1}^{n} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right)}<em>{1} \
&amp;=\sum</em>{\mathbf{Z}<em>{/n}} p\left(\mathbf{z}</em>{1}\right)\left[\prod_{i=2}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=n+1}^{N} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right) \
&amp;=\sum_{\mathbf{z}<em>{n+1}} \sum</em>{\mathbf{z}<em>{N}}\left[\prod</em>{i=n+1}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=n+1}^{N} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right) \sum_{\mathbf{z}<em>{1}} \cdots \sum</em>{\mathbf{z}<em>{n-1}} p\left(\mathbf{z}</em>{i}\right)\left[\prod_{1-2}^{n} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \
&amp;=\left(\sum_{\mathbf{z}<em>{n+1}} \cdots \sum</em>{\mathbf{z}<em>{N}}\left[\prod</em>{i=n+1}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=n+1}^{N} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right)\right)\left(\sum_{\mathbf{z}<em>{1}} \cdots \sum</em>{\mathbf{z}<em>{n-1}} p\left(\mathbf{z}</em>{1}\right)\left[\prod_{i=2}^{n} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right]\right)
\end{aligned}
$$</p>
<p>となる。ここで</p>
<p>$$
\begin{aligned}
p\left(\mathbf{z}<em>{n}\right)=\sum</em>{\mathbf{X}} \sum_{\mathbf{Z}<em>{/n}} p\left(\mathbf{X}, \mathbf{z}\right)&amp;=\sum</em>{\mathbf{X}} \sum_{\mathbf{Z}<em>{/n}} p\left(\mathbf{z}\right)\left[\prod</em>{i=2}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=1}^{N} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right) \
&amp;=\sum_{\mathbf{Z}<em>{/n}} p\left(\mathbf{z}\right)\left[\prod</em>{i=2}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \underbrace{\sum_{\mathbf{x}<em>{i=1}}^{N} p\left(\mathbf{z}</em>{i} \mid \mathbf{z}<em>{i}\right)}</em>{=1} \
&amp;=\sum_{\mathbf{Z}<em>{/n}} p\left(\mathbf{z}</em>{1}\right)\left[\prod_{i=2}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \
&amp;=\sum_{\mathbf{z}<em>{1}} \cdots \sum</em>{\mathbf{z}<em>{n-1}} \sum</em>{\mathbf{z}<em>{n+1}} \cdots \sum</em>{\mathbf{z}<em>{N}} p\left(\mathbf{z}</em>{1}\right)\left[\prod_{i=2}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \
&amp;=\sum_{\mathbf{z}} \cdots \sum_{\mathbf{z}<em>{n-1}} p\left(\mathbf{z}\right)\left[\prod</em>{i=2}^{n} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \underbrace{\sum_{\mathbf{z}<em>{n+1}}\cdots\sum</em>{\mathbf{z}<em>{N}}\left[\prod</em>{i=n+1}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right]}<em>{=1}\
&amp;=\sum</em>{\mathbf{z}} \cdots \sum_{\mathbf{z}<em>{n-1}} p\left(\mathbf{z}\right)\left[\prod</em>{i=2}^{n} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right]
\end{aligned}
$$</p>
<p>なので</p>
<p>$$
\begin{aligned}
&amp;p\left(\mathbf{x}<em>{n+1}{ }, \ldots, \mathbf{x}</em>{N}, \mathbf{z}<em>{n}\right)=\left(\sum</em>{\mathbf{z}<em>{n+1}}\cdots \sum</em>{\mathbf{z}<em>{N}}\left[\prod</em>{i=n+1}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=n+1}^{N} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right)\right) p\left(\mathbf{z}<em>{n}\right) \
&amp;\left.\therefore p\left(\mathbf{x}</em>{n+1}{ }, \ldots, \mathbf{x}<em>{N} \mid \mathbf{z}</em>{n}\right)=\ \sum_{\mathbf{z}<em>{n+1}} \ldots \sum</em>{\mathbf{z}<em>{N}}\left[\prod</em>{i=n+1}^{n} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=n+1}^{N} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right)\right)
\end{aligned}
$$</p>
<p>よって</p>
<p>$$
\begin{aligned}
&amp; p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n}, \mathbf{z}<em>{n}\right) p\left(\mathbf{x}</em>{n+1}, \ldots,\mathbf{x}<em>{N} \mid \mathbf{z}</em>{n}\right) \
=&amp;\ \left(\sum_{\mathbf{z}<em>{1}} \ldots \sum</em>{\mathbf{z}<em>{n-1}} p\left(\mathbf{z}\right)\left[\prod</em>{i=2}^{n} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=1}^{n} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right)\right)\left(\sum_{\mathbf{z}<em>{n+1}} \sum</em>{\mathbf{z}<em>{N}}\left[\prod</em>{i=n+1}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=n+1}^{N} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right)\right) \
=&amp;\ \sum_{\mathbf{z}<em>{n+1}}\cdots\sum</em>{\mathbf{z}<em>{N}}\left(\sum</em>{\mathbf{z}<em>{1}}\cdots\sum</em>{\mathbf{z}<em>{n-1}} p\left(\mathbf{z}\right)\left[\prod</em>{i=2}^{n} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=1}^{n} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right)\right) \underbrace{\left.\prod_{i=n+1}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=n+1}^{N} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right)}) \
=&amp;\ \sum_{\mathbf{z}<em>{n+1}} \cdots \sum</em>{\mathbf{z}<em>{N}}\left(\sum</em>{\mathbf{z}<em>{1}} \cdots \sum</em>{\mathbf{z}<em>{n-1}} p\left(\mathbf{z}\right)\left[\prod</em>{i=2}^{n} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=1}^{n} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right)\left[\prod_{i=n+1}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=n+1}^{N} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right)\right) \
=&amp;\ \sum_{\mathbf{z}} \cdots \sum_{\mathbf{z}<em>{n-1}} \sum</em>{\mathbf{z}<em>{n+1}} \cdots \sum</em>{\mathbf{z}<em>{N}} p\left(\mathbf{z}\right)\left[\prod</em>{i=2}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=1}^{N} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right) \
=&amp;\ \sum_{\mathbf{Z}<em>{/n}} p(\mathbf{X}, \mathbf{Z}) = p\left(\mathbf{X}, \mathbf{z}</em>{n}\right)
\end{aligned}
$$</p>
<p>を得る。両辺を$p\left(\mathbf{z}_{n}\right)$で割ると</p>
<p>$$
\frac{p\left(\mathbf{x}<em>{1} \ldots \mathbf{x}</em>{n}, \mathbf{z}<em>{n}\right) p\left(\mathbf{x}</em>{n+1} \ldots \mathbf{x}<em>{N} \mid \mathbf{z}</em>{n}\right)}{p\left(\mathbf{z}<em>{n}\right)}=\frac{p\left(\mathbf{X}, \mathbf{z}</em>{n}\right)}{p\left(\mathbf{z}_{n}\right)}
$$</p>
<p>$$
\therefore p\left(\mathbf{x}<em>{1} \ldots \mathbf{x}</em>{n} \mid \mathbf{z}<em>{n}\right) p\left(\mathbf{x}</em>{n+1} \cdots \mathbf{x}<em>{N} \mid \mathbf{z}</em>{n}\right)=p\left(\mathbf{X} \mid \mathbf{z}_{n}\right) \tag{13.24}
$$</p>
<h2 id="演習-1311"><a class="header" href="#演習-1311">演習 13.11</a></h2>
<div class="panel-primary">
<p>因子グラフにおける一つの因子における変数の周辺分布の表現</p>
<p>$$
p\left(\mathbf{x}<em>{s}\right)=f</em>{s}\left(\mathbf{x}<em>{s}\right) \prod</em>{i \in \operatorname{ne}\left(f_{s}\right)} \mu_{x_{i} \rightarrow f_{s}}\left(x_{i}\right) \tag{8.72}
$$</p>
<p>を出発点として、13.2.3節で得られた積和アルゴリズムにおけるメッセージの結果も用いて、隠れマルコフモデルにおける2つの連続した潜在変数上の同時事後確率分布の結果$(13.43)$
$$
\begin{aligned}
\xi\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right)&amp;=p\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n} \mid \mathbf{X}\right) \
&amp;=\frac{p\left(\mathbf{X} \mid \mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right) p\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right)}{p(\mathbf{X})} \
&amp;=\frac{p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n-1} \mid \mathbf{z}<em>{n-1}\right) p\left(\mathbf{x}</em>{n} \mid \mathbf{z}<em>{n}\right) p\left(\mathbf{x}</em>{n+1}, \ldots, \mathbf{x}<em>{N} \mid \mathbf{z}</em>{n}\right) p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}\right) p\left(\mathbf{z}<em>{n-1}\right)}{p(\mathbf{X})} \
&amp;=\frac{\alpha\left(\mathbf{z}</em>{n-1}\right) p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right) p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}\right) \beta\left(\mathbf{z}_{n}\right)}{p(\mathbf{X})}
\end{aligned}
$$
を導け.</p>
</div>
<p>考え方としては、pp.344-345がノード$\mathbf{z}<em>{n}$ 1つについてHMMの積和アルゴリズムを用いて$\gamma(\mathbf{z}</em>{n})$を求めているのを参考しながら、図13.15の因子$f_{n}$に結合している$\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}$ 2つの変数ノードについて計算すれば良い。注意として、$\mathbf{X} = {\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{N}}$は与えられており、条件付きの形で考える必要がある。</p>
<p><img src="https://i.imgur.com/7JHcpzo.png" alt="" /></p>
<p>まず定義から$\xi\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right)=p\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n} \mid \mathbf{X}\right)$である。pp.344-345の議論のように、$\mathbf{X}$が与えられている状態で単純化された因子グラフを書くと図13.15のようになる。</p>
<p>$(13.50)$と$(13.52)$で見たように、$\alpha(\mathbf{z}<em>{n}), \beta(\mathbf{z}</em>{n})$は</p>
<p>$$
\alpha\left(\mathbf{z}<em>{n}\right)=\mu</em>{f_{n} \rightarrow \mathbf{z}<em>{n}}\left(\mathbf{z}</em>{n}\right) \tag{13.50}
$$</p>
<p>$$
\beta\left(\mathbf{z}<em>{n}\right)=\mu</em>{f_{n+1} \rightarrow \mathbf{z}<em>{n}}\left(\mathbf{z}</em>{n}\right) \tag{13.52}
$$</p>
<p>と定義できるので、これを用いて$(8.72)$を利用してある因子$f_{n}$に関連する変数ノード$\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}$全体上の周辺分布を求めると、すでに$\mathbf{X} = {\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{N}}$で条件付けられていることに注意して</p>
<p>$$
\begin{aligned}
p\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}, \mathbf{X}\right) &amp;=f_{n}\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right) \mu_{\mathbf{z}<em>{n-1} \rightarrow f</em>{n}}\left(\mathbf{z}<em>{n-1}\right) \mu</em>{\mathbf{z}<em>{n} \rightarrow f</em>{n}}\left(\mathbf{z}<em>{n}\right) \
&amp;=p\left(\mathbf{z}</em>{n} \mid \mathbf{z}<em>{n-1}\right) p\left(\mathbf{x}</em>{n} \mid \mathbf{z}<em>{n}\right) \mu</em>{f_{n-1} \rightarrow \mathbf{z}<em>{n-1}}\left(\mathbf{z}</em>{n-1}\right) \mu_{f_{n+1} \rightarrow \mathbf{z}<em>{n}}\left(\mathbf{z}</em>{n}\right) \
&amp;=\alpha\left(\mathbf{z}<em>{n-1}\right) p\left(\mathbf{x}</em>{n} \mid \mathbf{z}<em>{n}\right) p\left(\mathbf{z}</em>{n} \mid \mathbf{z}<em>{n-1}\right) \beta\left(\mathbf{z}</em>{n}\right)
\end{aligned}
$$</p>
<p>となる。この式変形では</p>
<p>$$
\begin{aligned}
\mu_{x_{m} \rightarrow f_{s}}\left(x_{m}\right) &amp;=\prod_{l \in \operatorname{ne}\left(x_{m}\right) \backslash f_{s}}\left[\sum_{X_{l m}} F_{l}\left(x_{m}, X_{l m}\right)\right] \
&amp;=\prod_{l \in \operatorname{ne}\left(x_{m}\right) \backslash f_{s}} \mu_{f_{l} \rightarrow x_{m}}\left(x_{m}\right)
\end{aligned} \tag{8.69}
$$</p>
<p>も利用した。これについて両辺を$p(\mathbf{X})$で割ると</p>
<p>$$
\begin{aligned}
\xi\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right)&amp;=p\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n} \mid \mathbf{X}\right) \
&amp;= \frac{\alpha\left(\mathbf{z}<em>{n-1}\right) p\left(\mathbf{x}</em>{n} \mid \mathbf{z}<em>{n}\right) p\left(\mathbf{z}</em>{n} \mid \mathbf{z}<em>{n-1}\right) \beta\left(\mathbf{z}</em>{n}\right)}{p(\mathbf{X})}
\end{aligned}\tag{13.43}
$$</p>
<p>を得る。</p>
<h2 id="演習-1312"><a class="header" href="#演習-1312">演習 13.12</a></h2>
<div class="panel-primary">
<p>隠れマルコフモデルを$R$個の独立した観測系列から構成されるデータを用いた最尤推定で学習したいとする。ここで、これらの観測行列を$\mathbf{X}^{(r)}, r=1, \ldots, R$で表す。EMアルゴリズムのEステップにおいて潜在変数の事後確率を求めるためには、各々の系列に対して独立に$\alpha$再帰と$\beta$再帰を実行すればよいことを示せ。また、Mステップにおいては、初期確率と遷移確率のパラメータは
$$
\pi_{k}=\frac{\gamma\left(z_{1 k}\right)}{\sum_{j=1}^{K} \gamma\left(z_{1 j}\right)} \tag{13.18}
$$
$$
A_{j k}=\frac{\sum_{n=2}^{N} \xi\left(z_{n-1, j}, z_{n k}\right)}{\sum_{l=1}^{K} \sum_{n=2}^{N} \xi\left(z_{n-1, j}, z_{n l}\right)} \tag{13.19}
$$
を以下のように修正した式で再推定されることを示せ.</p>
<p>$$
\pi_{k}= \frac{\sum_{r=1}^{R} \gamma\left(z_{1 k}^{(r)}\right)}{\sum_{r=1}^{R} \sum_{j=1}^{K} \gamma\left(z_{1 j}^{(r)}\right)} \tag{13.124}
$$</p>
<p>$$
A_{j k}= \frac{\sum_{r=1}^{R} \sum_{n=2}^{N} \xi\left(z_{n-1, j}^{(r)}, z_{n, k}^{(r)}\right)}{\sum_{r=1}^{R} \sum_{l=1}^{K} \sum_{n=2}^{N} \xi\left(z_{n-1, j}^{(r)}, z_{n, l}^{(r)}\right)} \tag{13.125}
$$</p>
<p>ここで、表記を簡単にするために、系列はすべて同じ長さをもつと仮定した(長さの異なる系列への一般化は簡単である) 。同様に、ガウス出力モデルの平均の再推定のためのMステップの方程式が以下の形で与えられることを示せ。</p>
<p>$$
\pmb{\mu}<em>{k}=\frac{\sum</em>{r=1}^{R} \sum_{n=1}^{N} \gamma\left(z_{n k}^{(r)}\right) \mathbf{x}<em>{n}^{(r)}}{\sum</em>{r=1}^{R} \sum_{n=1}^{N} \gamma\left(z_{n k}^{(r)}\right)}
$$</p>
<p>他の出力モデルパラメータや出力分布に対するMステップの方程式も類似の形になることに注意せよ。</p>
</div>
<p>隠れマルコフモデルの定義から、観察値$\mathbf{X}^{(r)}$には対応する潜在変数$\mathbf{Z}^{(r)}$が存在する。よって、その同時分布は以下のように表せる</p>
<p>$$
\begin{aligned}
p(\mathbf{X}, \mathbf{Z}|\mathbf{\theta}) = \prod_r^R p(\mathbf{X}^{(r)}, \mathbf{Z}^{(r)}|\mathbf{\theta})
\end{aligned}
$$</p>
<p>そして、Eステップでは、事後分布を求める。
$$
\begin{aligned}
p(\mathbf{Z}|\mathbf{X}, \mathbf{\theta}) &amp;= \frac{p(\mathbf{X}, \mathbf{Z}|\mathbf{\theta})}{\sum_{\mathbf{Z}} p(\mathbf{X}, \mathbf{Z}|\mathbf{\theta})} \
&amp;= \frac{\prod_r p(\mathbf{X}^{(r)}, \mathbf{Z}^{(r)}|\mathbf{\theta})}{\sum_{\mathbf{Z}^{(1)}} \cdots \sum_{\mathbf{Z}^{(R)}}\prod_r p(\mathbf{X}^{(r)}, \mathbf{Z}^{(r)}|\mathbf{\theta})}\
&amp;= \prod_r \frac{p(\mathbf{X}^{(r)}, \mathbf{Z}^{(r)}|\mathbf{\theta})}{\sum_{\mathbf{Z}^{(r)}} p(\mathbf{X}^{(r)}, \mathbf{Z}^{(r)}|\mathbf{\theta})}\
&amp;= \prod_r  p( \mathbf{Z}^{(r)}|\mathbf{X}^{(r)},\mathbf{\theta})
\end{aligned}
$$</p>
<p>ここで、
$$
\begin{aligned}
p( \mathbf{Z}^{(r)}|\mathbf{X}^{(r)}) &amp;= p( \mathbf{z}<em>{N}^{(r)}| \mathbf{z}</em>{N-1}^{(r)}, \cdots \mathbf{z}<em>{1}^{(r)}, \mathbf{X}^{(r)}) p( \mathbf{z}</em>{N-1}^{(r)}, \cdots \mathbf{z}<em>{1}^{(r)}|\mathbf{X}^{(r)}) \
&amp;= p( \mathbf{z}</em>{N}^{(r)}| \mathbf{z}<em>{N-1}^{(r)}, \mathbf{X}^{(r)}) p( \mathbf{z}</em>{N-1}^{(r)}, \cdots \mathbf{z}<em>{1}^{(r)}|\mathbf{X}^{(r)}) \
&amp;= p( \mathbf{z}</em>{N}^{(r)}| \mathbf{z}<em>{N-1}^{(r)}, \mathbf{X}^{(r)})  \cdots p( \mathbf{z}</em>{2}^{(r)}|\mathbf{z}<em>{1}^{(r)}\mathbf{X}^{(r)}) p( \mathbf{z}</em>{1}^{(r)}|\mathbf{X}^{(r)}) \
&amp;= \frac{p( \mathbf{z}<em>{N}^{(r)}, \mathbf{z}</em>{N-1}^{(r)}|\mathbf{X}^{(r)})}{p( \mathbf{z}<em>{N-1}^{(r)}|\mathbf{X}^{(r)}) } \cdots \frac{p( \mathbf{z}</em>{2}^{(r)}, \mathbf{z}<em>{1}^{(r)}|\mathbf{X}^{(r)})}{p( \mathbf{z}</em>{1}^{(r)}|\mathbf{X}^{(r)}) }p( \mathbf{z}<em>{1}^{(r)}|\mathbf{X}^{(r)}) \
&amp;= \frac{\xi^{(r)}(\mathbf{z}</em>{N}^{(r)}, \mathbf{z}<em>{N-1}^{(r)})}{\gamma^{(r)}(\mathbf{z}</em>{N-1}^{(r)})} \cdots \frac{\xi^{(r)}(\mathbf{z}<em>{2}^{(r)}, \mathbf{z}</em>{1}^{(r)})}{\gamma^{(r)}(\mathbf{z}<em>{1}^{(r)})}\gamma^{(r)}(\mathbf{z}</em>{1}^{(r)})\
&amp;= \frac{\prod_{n=2}^N  \xi^{(r)}(\mathbf{z}<em>{n}^{(r)}, \mathbf{z}</em>{n-1}^{(r)})}{\prod_{n=2}^{N-1} \gamma^{(r)}(\mathbf{z}_{n}^{(r)})}
\end{aligned}
$$</p>
<p>である。</p>
<p>よって、</p>
<p>$$
\begin{aligned}
p( \mathbf{Z}|\mathbf{X}, \mathbf{\theta}) &amp;= \prod_r \frac{\prod_{n=2}  \xi^{(r)}(\mathbf{z}<em>{n}^{(r)}, \mathbf{z}</em>{n-1}^{(r)})}{\prod_{n=2} \gamma^{(r)}(\mathbf{z}_{n}^{(r)})}
\end{aligned}
$$</p>
<p>これより、Eステップで求める事後分布は$\xi$と$\gamma$により計算できる。また、$\xi$と$\gamma$はそれぞれ$\alpha^{(r)}(\mathbf{z}_n)$と$\beta^{(r)}(\mathbf{z}_n)$を再帰で求めることができる。</p>
<p>次にMステップを考える。Eステップで求めた事後分布と$\theta_{old}$を用いて、完全データ対数尤度の期待値を求める。</p>
<p>$$
\begin{aligned}
Q(\mathbf{\theta}, \mathbf{\theta_{old}}) &amp;= E_{\mathbf{Z}} [\ln p(\mathbf{X, Z}|\mathbf{\theta}) ] \
&amp;=  E_{\mathbf{Z}} [\sum_r^R \ln p(\mathbf{X}^{(r)}, \mathbf{Z}^{(r)}|\mathbf{\theta}) ] \
&amp;= \sum_r^R p( \mathbf{Z}^{(r)}|\mathbf{X}^{(r)},\mathbf{\theta}<em>{old})  \ln p(\mathbf{X}, \mathbf{Z}|\mathbf{\theta}) \
&amp;= \sum_r^R \sum_k^K \gamma(z</em>{1k}^{(r)}) \ln \pi_k + \sum_r^R \sum_N^n \sum_j^K \sum_k^K \xi(z_{n-1, j}^{(r)}, z_{n, k}^{(r)}) \ln A_{jk}+ \sum_r^R \sum_N^n \sum_k^K \gamma(z_{nk}^{(r)}) \ln p(\mathbf{x}_n^{(r)}|\phi_k) &amp;(EX13.12.1)
\end{aligned}
$$</p>
<p>最後の形は、いつも通りラグランジュ乗数法を用いて$\pi$と$\mathbf{A}$について最大化することができる。よって(13.124)、(13.125)が導かれる。詳細はhttp://sioramen.sub.jp/prml_wiki/lib/exe/fetch.php/wiki/13.12.pdf</p>
<p>次に、出力分布をガウス分布とする。すなわち、
$$
\begin{aligned}
p(\mathbf{x}_n^{(r)}|\phi_k) = N(\mathbf{x}_n^{(r)}|\mathbf{\mu}_k, \mathbf{\Sigma}_k)
\end{aligned}
$$
とする。</p>
<p>この時、$Q(\mathbf{\theta}, \mathbf{\theta_{old}})$は(EX13.12.1)を用いて、$\mathbf{\mu}<em>k$を含まない部分は$C$にまとめて、
$$
\begin{aligned}
Q(\mathbf{\theta}, \mathbf{\theta</em>{old}}) &amp;= C+\sum_r^R \sum_N^n \sum_k^K \gamma(z_{nk}^{(r)}) \ln p(\mathbf{x}<em>n^{(r)}|\phi_k) \
&amp;= C+\sum_r^R \sum_N^n \sum_k^K \gamma(z</em>{nk}^{(r)}) \ln \frac{1}{(2\pi)^{D/2}}\frac{1}{|\mathbf{\Sigma}_k|^{1/2}} \exp{ -\frac{1}{2}(\mathbf{x}_n^{(r)}-\mathbf{\mu}_k)^T \mathbf{\Sigma}_k^{-1}(\mathbf{x}_n^{(r)}-\mathbf{\mu}<em>k)} \
&amp;= C+\sum_r^R \sum_N^n \sum_k^K \gamma(z</em>{nk}^{(r)}) { -\frac{1}{2}(\mathbf{x}_n^{(r)}-\mathbf{\mu}_k)^T \mathbf{\Sigma}_k^{-1}(\mathbf{x}_n^{(r)}-\mathbf{\mu}_k)}
\end{aligned}
$$</p>
<p>なお、最後の式変形は、$\ln$の後の一部を$C$にまとめた。あとはいつも通り$\mathbf{\mu}_k$について停留条件を求めると、Mステップの方程式が得られる。</p>
<h2 id="演習-1313"><a class="header" href="#演習-1313">演習 13.13</a></h2>
<div class="panel-primary">
<p>因子グラフにおける因子ノードから変数ノードへ渡されるメッセージの定義</p>
<p>$$
\mu_{f_{s} \rightarrow x}(x) \equiv \sum_{X_{s}} F_{s}\left(x, X_{s}\right) \tag{8.64}
$$</p>
<p>と隠れマルコフモデルの同時分布の表現</p>
<p>$$
p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{N}, \mathbf{z}<em>{1}, \ldots, \mathbf{z}</em>{N}\right)=p\left(\mathbf{z}<em>{1}\right)\left[\prod</em>{n=2}^{N} p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}\right)\right] \prod_{n=1}^{N} p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right) \tag{13.6}
$$</p>
<p>を用いて、$\alpha$メッセージの定義</p>
<p>$$
\alpha\left(\mathbf{z}<em>{n}\right)=\mu</em>{f_{n} \rightarrow \mathbf{z}<em>{n}}\left(\mathbf{z}</em>{n}\right) \tag{13.50}
$$</p>
<p>が</p>
<p>$$
\alpha\left(\mathbf{z}<em>{n}\right) \equiv p\left(\mathbf{x}</em>{1}, \ldots, \mathbf{x}<em>{n}, \mathbf{z}</em>{n}\right) \tag{13.34}
$$</p>
<p>の定義と同一であることを示せ。</p>
</div>
<p>$(8.64)$の定義からスタートする。</p>
<p>$$
\begin{aligned}
\alpha\left(\mathbf{z}<em>{n}\right) &amp;=\mu</em>{f_{n} \rightarrow \mathbf{z}<em>{n}}\left(\mathbf{z}</em>{n}\right) \
&amp;=\sum_{\mathbf{z}<em>{n-1}} f</em>{n}\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right) \mu_{\mathbf{z}<em>{n-1}\to f</em>{n}}\left(\mathbf{z}<em>{n-1}\right) \
&amp;=\sum</em>{\mathbf{z}<em>{n-1}} f</em>{n}\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right) \mu_{f_{n-1} \rightarrow \mathbf{z}<em>{n-1}}\left(\mathbf{z}</em>{n-1}\right)\
&amp;=\cdots \
&amp;=\sum_{\mathbf{z}<em>{1},\ldots,\mathbf{z}</em>{n-1}} h\left(\mathbf{z}<em>{1}\right) \prod</em>{i=2}^{n} f_{i}\left(\mathbf{z}<em>{i-1}, \mathbf{z}</em>{i}\right)
\end{aligned}
$$</p>
<p>ここで、図13.15の因子グラフでは</p>
<p>$$
h\left(\mathbf{z}<em>{1}\right) =p\left(\mathbf{z}</em>{1}\right) p\left(\mathbf{x}<em>{1} \mid \mathbf{z}</em>{1}\right) \tag{13.45}
$$
$$
f_{n}\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right) =p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}\right) p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right) \tag{13.46}
$$</p>
<p>が成立しているので、これを利用して更に変形すると、周辺化を利用して</p>
<p>$$
\begin{aligned}
\alpha\left(\mathbf{z}<em>{n}\right) &amp;=\sum</em>{\mathbf{z}<em>{1}, \cdots, \mathbf{z}</em>{n-1}} p\left(\mathbf{z}<em>{1}\right) p\left(\mathbf{x}</em>{1} \mid \mathbf{z}<em>{1}\right) \prod</em>{i=2}^{n} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right) p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right) \
&amp;=\sum_{\mathbf{z}<em>{1}, \cdots, \mathbf{z}</em>{n-1}} p\left(\mathbf{z}<em>{1}\right)\left[\prod</em>{i=2}^{n} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right)\right] \prod_{i=1}^{n} p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right) \
&amp;=\sum_{\mathbf{z}<em>{1}, \cdots, \mathbf{z}</em>{n-1}} p\left(\mathbf{x}<em>{1}, \cdots, \mathbf{x}</em>{n}, \mathbf{z}<em>{1}, \cdots, \mathbf{z}</em>{n}\right)\ (\because (13.6))\
&amp;=p\left(\mathbf{x}<em>{1}, \cdots, \mathbf{x}</em>{n}, \mathbf{z}_{n}\right)
\end{aligned}
$$</p>
<p>となり、$(13.34)$の定義が得られる。</p>
<h2 id="演習-1314"><a class="header" href="#演習-1314">演習 13.14</a></h2>
<div class="panel-primary">
<p>因子グラフにおける因子ノードから変数ノードへ渡されるメッセージの定義</p>
<p>$$
\mu_{f_{s} \rightarrow x}(x) \equiv \sum_{\mathbf{x}<em>{s}} F</em>{s}\left(x, \mathbf{x}_{s}\right) \tag{8.64}
$$</p>
<p>と隠れマルコフモデルの同時分布の表現</p>
<p>$$
p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{N}, \mathbf{z}<em>{1}, \ldots, \mathbf{z}</em>{N}\right)=p\left(\mathbf{z}<em>{1}\right)\left[\prod</em>{n=2}^{N} p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}\right)\right] \prod_{n=1}^{N} p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right) \tag{13.6}
$$</p>
<p>を用いて、$\beta$メッセージの定義</p>
<p>$$
\beta\left(\mathbf{z}<em>{n}\right)=\mu</em>{f_{n+1} \rightarrow \mathbf{z}<em>{n}}\left(\mathbf{z}</em>{n}\right) \tag{13.52}
$$</p>
<p>が</p>
<p>$$
\beta\left(\mathbf{z}<em>{n}\right) \equiv p\left(\mathbf{x}</em>{n+1}, \ldots, \mathbf{x}<em>{N} \mid \mathbf{z}</em>{n}\right) \tag{13.35}
$$</p>
<p>の定義と同一であることを示せ.</p>
</div>
<p>演習問題13.13と似たような問題だが、最後の式変形が少し難しくなる。同様に図13.15の因子グラフを用いて考える。</p>
<p>$(13.52)$の定義からスタートして</p>
<p>$$
\begin{aligned}
\beta\left(\mathbf{z}<em>{n}\right) &amp;=\mu</em>{f_{n+1} \rightarrow \mathbf{z}<em>{n}}\left(\mathbf{z}</em>{n}\right) \
&amp;=\sum_{\mathbf{z}<em>{n+1}} f</em>{n+1}\left(\mathbf{z}<em>{n}, \mathbf{z}</em>{n+1}\right) \mu_{\mathbf{z}<em>{n+1}\to f</em>{n+1}}\left(\mathbf{z}<em>{n+1}\right) \
&amp;=\sum</em>{\mathbf{z}<em>{n+1}} f</em>{n+1}\left(\mathbf{z}<em>{n}, \mathbf{z}</em>{n+1}\right) \mu_{f_{n+2}\to\mathbf{z}<em>{n+1}}\left(\mathbf{z}</em>{n+1}\right) \
&amp;=\cdots \
&amp;=\sum_{\mathbf{z}<em>{n+1}, \cdots, \mathbf{z}</em>{N}} \prod_{i=n+1}^{N}f_{i}\left(\mathbf{z}<em>{i-1}, \mathbf{z}</em>{i}\right) \underbrace{\mu_{\mathbf{z}<em>{N}\to f</em>{N}}\left(\mathbf{z}<em>{N}\right)}</em>{=1} \
&amp;=\sum_{\mathbf{z}<em>{n+1}, \cdots, \mathbf{z}</em>{N}} \prod_{i=n+1}^{N}f_{i}\left(\mathbf{z}<em>{i-1}, \mathbf{z}</em>{i}\right) \
&amp;=\sum_{\mathbf{z}<em>{n+1}, \cdots, \mathbf{z}</em>{N}} \prod_{i=n+1}^{N} p\left(\mathbf{z}<em>{i} \mid \mathbf{z}</em>{i-1}\right) p\left(\mathbf{x}<em>{i} \mid \mathbf{z}</em>{i}\right) \quad (\because (13.46))\
&amp;=\sum_{\mathbf{z}<em>{n+1}, \cdots, \mathbf{z}</em>{N}} p\left(\mathbf{z}<em>{n+1} \mid \mathbf{z}</em>{n}\right) p\left(\mathbf{x}<em>{n+1} \mid \mathbf{z}</em>{n+1}\right) \cdots p\left(\mathbf{z}<em>{N} \mid \mathbf{z}</em>{N-1}\right) p\left(\mathbf{x}<em>{N} \mid \mathbf{z}</em>{N}\right) \
&amp;=\sum_{\mathbf{z}<em>{n+1}, \cdots, \mathbf{z}</em>{N}} p\left(\mathbf{z}<em>{n+1} \mid \mathbf{z}</em>{n}\right) p\left(\mathbf{x}<em>{n+1} \mid \mathbf{z}</em>{n}, \mathbf{z}<em>{n+1}\right) p\left(\mathbf{z}</em>{n+2} \mid \mathbf{z}<em>{n+1}\right) p\left(\mathbf{x}</em>{n+2} \mid \mathbf{z}<em>{n+2}\right)\cdots p\left(\mathbf{z}</em>{N} \mid \mathbf{z}<em>{N-1}\right) p\left(\mathbf{x}</em>{N} \mid \mathbf{z}<em>{N}\right) \quad (\because (13.27)) \
&amp;=\sum</em>{\mathbf{z}<em>{n+1}, \cdots, \mathbf{z}</em>{N}} p\left(\mathbf{x}<em>{n+1}, \mathbf{z}</em>{n+1} \mid \mathbf{z}<em>{n}\right) p\left(\mathbf{z}</em>{n+2} \mid \mathbf{x}<em>{n+1}, \mathbf{z}</em>{n+1}\right) p\left(\mathbf{x}<em>{n+2} \mid \mathbf{x}</em>{n+1}, \mathbf{z}<em>{n+1}, \mathbf{z}</em>{n+2}\right) \cdots p\left(\mathbf{x}<em>{N} \mid \mathbf{x}</em>{N-1}, \mathbf{z}<em>{n+1}, \cdots, \mathbf{z}</em>{N}\right)\quad (\because (13.28)) \
&amp;=\sum_{\mathbf{z}<em>{n+1}, \cdots, \mathbf{z}</em>{N}} p\left(\mathbf{x}<em>{n+1}, \cdots, \mathbf{x}</em>{N}, \mathbf{z}<em>{n+1}, \cdots, \mathbf{z}</em>{N} \mid \mathbf{z}<em>{n}\right) \
&amp;=p\left(\mathbf{x}</em>{n+1}, \cdots, \mathbf{x}<em>{N} \mid \mathbf{z}</em>{n}\right)
\end{aligned}
$$</p>
<p>以上で$(13.35)$式が示された。</p>
<h2 id="演習-1315"><a class="header" href="#演習-1315">演習 13.15</a></h2>
<div class="panel-primary">
<p>隠れマルコフモデルの周辺分布の表現</p>
<p>$$
\gamma\left(\mathbf{z}<em>{n}\right)=\frac{p\left(\mathbf{x}</em>{1}, \ldots, \mathbf{x}<em>{n}, \mathbf{z}</em>{n}\right) p\left(\mathbf{x}<em>{n+1}, \ldots, \mathbf{x}</em>{N} \mid \mathbf{z}<em>{n}\right)}{p(\mathbf{X})}=\frac{\alpha\left(\mathbf{z}</em>{n}\right) \beta\left(\mathbf{z}_{n}\right)}{p(\mathbf{X})} \tag{13.33}
$$</p>
<p>と</p>
<p>$$
\xi(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}) = p(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n} \mid \mathbf{X}) = \frac{\alpha\left(\mathbf{z}<em>{n-1}\right) p\left(\mathbf{x}</em>{n} \mid \mathbf{z}<em>{n}\right) p\left(\mathbf{z}</em>{n} \mid \mathbf{z}<em>{n-1}\right) \beta\left(\mathbf{z}</em>{n}\right)}{p(\mathbf{X})} \tag{13.43}
$$</p>
<p>を用いて、スケーリングされた変数についての対応する結果</p>
<p>$$
\gamma\left(\mathbf{z}<em>{n}\right) =\widehat{\alpha}\left(\mathbf{z}</em>{n}\right) \widehat{\beta}\left(\mathbf{z}_{n}\right) \tag{13.64}
$$</p>
<p>$$
\xi\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right) =\left(c_{n}\right)^{-1} \widehat{\alpha}\left(\mathbf{z}<em>{n-1}\right) p\left(\mathbf{x}</em>{n} \mid \mathbf{z}<em>{n}\right) p\left(\mathbf{z}</em>{n} \mid \mathbf{z}<em>{n-1}\right) \widehat{\beta}\left(\mathbf{z}</em>{n}\right) \tag{13.65}
$$</p>
<p>を導け.</p>
</div>
<p>$$
\begin{aligned}
\gamma\left(\mathbf{z}<em>{n}\right)&amp;=\frac{\alpha\left(\mathbf{z}</em>{n}\right) \beta\left(\mathbf{z}<em>{n}\right)}{p(\mathbf{X})} \
&amp;=\frac{p(\mathbf{x}<em>1,\cdots , \mathbf{x}<em>n)\widehat{\alpha}\left(\mathbf{z}</em>{n}\right) \cdot \left( \prod</em>{m=n+1}^N c_m\right) \widehat{\beta}\left(\mathbf{z}</em>{n}\right)}{\prod_{m=1}^N c_m } \
&amp;= \frac{\left(\prod_{m=1}^n c_m \right) \widehat{\alpha}\left(\mathbf{z}<em>{n}\right) \cdot \left( \prod</em>{m=n+1}^N c_m\right) \widehat{\beta}\left(\mathbf{z}<em>{n}\right)}{\prod</em>{m=1}^N c_m } \
&amp;=\widehat{\alpha}\left(\mathbf{z}<em>{n}\right) \widehat{\beta}\left(\mathbf{z}</em>{n}\right)
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
\xi\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right) &amp;=\frac{\alpha\left(\mathbf{z}<em>{n-1}\right) p\left(\mathbf{x}</em>{n} \mid \mathbf{z}<em>{n}\right) p\left(\mathbf{z}</em>{n} \mid \mathbf{z}<em>{n-1}\right) \beta\left(\mathbf{z}</em>{n}\right)}{p(\mathbf{X})}\
&amp;=\frac{\left(\prod_{m=1}^{n-1} c_m \right) \widehat{\alpha}\left(\mathbf{z}<em>{n-1}\right) \cdot
p\left(\mathbf{x}</em>{n} \mid \mathbf{z}<em>{n}\right) p\left(\mathbf{z}</em>{n} \mid \mathbf{z}<em>{n-1}\right) \cdot \left(\prod</em>{m=n+1}^{N} c_m \right)
\widehat{\beta}\left(\mathbf{z}<em>{n}\right)}{\prod</em>{m=1}^{N} c_m }\
&amp;=\left(c_{n}\right)^{-1} \widehat{\alpha}\left(\mathbf{z}<em>{n-1}\right) p\left(\mathbf{x}</em>{n} \mid \mathbf{z}<em>{n}\right) p\left(\mathbf{z}</em>{n} \mid \mathbf{z}<em>{n-1}\right) \widehat{\beta}\left(\mathbf{z}</em>{n}\right) \end{aligned}
$$</p>
<h2 id="演習-1316"><a class="header" href="#演習-1316">演習 13.16</a></h2>
<div class="panel-primary">
<p>この演習問題では、Viterbiアルゴリズムのフォワードメッセージパッシングの式を同時分布の表現</p>
<p>$$
p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{N}, \mathbf{z}<em>{1}, \ldots, \mathbf{z}</em>{N}\right)=p\left(\mathbf{z}<em>{1}\right)\left[\prod</em>{n=2}^{N} p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}\right)\right] \prod_{n=1}^{N} p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right) \tag{13.6}
$$</p>
<p>から直接導く。これは、すべての隠れ変数$\mathbf{z}<em>1,\ldots,\mathbf{z}</em>{N}$についての最大化を含む。対数を取り、最大化と和の順序を交換することにより、再帰式</p>
<p>$$
\omega\left(\mathbf{z}<em>{n+1}\right)=\ln p\left(\mathbf{x}</em>{n+1} \mid \mathbf{z}<em>{n+1}\right)+\max <em>{\mathbf{z}</em>{n}}\left{\ln p\left(\mathbf{z}</em>{n+1} \mid \mathbf{z}<em>{n}\right)+\omega\left(\mathbf{z}</em>{n}\right)\right} \tag{13.68}
$$</p>
<p>を求めよ。ここで、$\omega(\mathbf{z}_n)$は</p>
<p>$$
\omega\left(\mathbf{z}<em>{n}\right)=\max <em>{\mathbf{z}</em>{1}, \ldots, \mathbf{z}</em>{n-1}} \ln p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{n}, \mathbf{z}<em>{1}, \ldots, \mathbf{z}</em>{n}\right) \tag{13.70}
$$</p>
<p>で定義される。この再帰式の初期条件が</p>
<p>$$
\omega\left(\mathbf{z}<em>{1}\right)=\ln p\left(\mathbf{z}</em>{1}\right)+\ln p\left(\mathbf{x}<em>{1} \mid \mathbf{z}</em>{1}\right) \tag{13.69}
$$</p>
<p>で与えられることを示せ。</p>
</div>
<p>(13.6)を書き換えて</p>
<p>$$
p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{N}, \mathbf{z}<em>{1}, \ldots, \mathbf{z}</em>{N}\right)=p\left(\mathbf{z}<em>{1}\right) p\left(\mathbf{x}</em>{1} \mid \mathbf{z}<em>{1}\right) \prod</em>{n=2}^{N} p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right) p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}\right)
$$</p>
<p>を得る．この式のlogをとると</p>
<p>$$
\begin{aligned}
\ln p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{N},\right.&amp;\left.\mathbf{z}<em>{1}, \ldots, \mathbf{z}</em>{N}\right)
=\ln p\left(\mathbf{z}<em>{1}\right)+\ln p\left(\mathbf{x}</em>{1} \mid \mathbf{z}<em>{1}\right)+\sum</em>{n=2}^{N}\left(\ln p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right)+\ln p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}\right)\right)
\end{aligned}
$$</p>
<p>となる．ここで最初の2項は(13.69)式で書き換えられる．この式を$\mathbf{z}_1,\dots,\mathbf{z}_N$について最大化すると</p>
<p>$$
\begin{aligned}
\max <em>{\mathbf{z}</em>{1}, \ldots, \mathbf{z}<em>{N}} &amp;\left{\omega\left(\mathbf{z}</em>{1}\right)+\sum_{n=2}^{N}\left[\ln p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right)+\ln p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}\right)\right]\right} \
=&amp; \max <em>{\mathbf{z}</em>{2}, \ldots, \mathbf{z}<em>{N}}\left{\ln p\left(\mathbf{x}</em>{2} \mid \mathbf{z}<em>{2}\right)+\max <em>{\mathbf{z}</em>{1}}\left{\ln p\left(\mathbf{z}</em>{2} \mid \mathbf{z}<em>{1}\right)+\omega\left(\mathbf{z}</em>{1}\right)\right}\right.
\left.+\sum_{n=3}^{N}\left[\ln p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right)+\ln p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}\right)\right]\right} \
=&amp; \max <em>{\mathbf{z}</em>{2}, \ldots, \mathbf{z}<em>{N}}\left{\omega\left(\mathbf{z}</em>{2}\right)+\sum_{n=3}^{N}\left[\ln p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right)+\ln p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}\right)\right]\right}
\end{aligned}
$$</p>
<p>ここで最大化と総和の順序を入れ替えて、$n = 2$ の (13.68) を得た。この式の最初の行と最後の行は添字$n$が一つ増えただけで同じ形であり、これによってすべての n &gt; 2 について再帰的に(13.68)が成り立つことがわかる。</p>
<h2 id="演習-1317"><a class="header" href="#演習-1317">演習 13.17</a></h2>
<div class="panel-primary">
<p><img src="https://i.imgur.com/JBSYbFA.png" alt="" /></p>
<p><img src="https://i.imgur.com/IADpE0Q.png" alt="" /></p>
<p>図13.18のinput-output隠れマルコフモデルに対する有向グラフが図13.15に示す形の木構造因子グラフで表現されることを示せ。そして、初期因子$h(\mathbf{z}<em>{1})$と一般的な因子$f</em>{n}(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n})$を書き下せ。ここで、$2\leqslant n \leqslant N$である。</p>
</div>
<p>教科書の図8.42あたりの記述も参考にする。$h$や因子$f_{n}(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n})$は出力確率$\mathbf{x}<em>{n}$や入力確率$\mathbf{u}</em>{n}$を吸収した形になる。図13.14, 13.15と同様のやり方を踏まえれば</p>
<p>$$
h\left(\mathbf{z}<em>{1}\right)=p\left(\mathbf{z}</em>{1} \mid \mathbf{u}<em>{1}\right) p\left(\mathbf{x}</em>{1} \mid \mathbf{z}<em>{1}, \mathbf{u}</em>{1}\right)
$$
$$
f\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right)=p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}, \mathbf{u}<em>{n}\right) p\left(\mathbf{x}</em>{n} \mid \mathbf{z}<em>{n}, \mathbf{u}</em>{n}\right)
$$</p>
<p>とすればよい。</p>
<h2 id="演習-1318"><a class="header" href="#演習-1318">演習 13.18</a></h2>
<div class="panel-primary">
<p>演習問題13.17の結果を用いて、図13.18に示すinput-output隠れマルコフモデルのフォワード-バックワードアルゴリズムの再帰式を初期条件とともに導け.</p>
</div>
<p>フォワード-バックワードアルゴリズムの再帰式は$(13.36)$,$(13.38)$のような式のこと。また、演習13.17で得た結果</p>
<p>$$
f\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right)=p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}, \mathbf{u}<em>{n}\right) p\left(\mathbf{x}</em>{n} \mid \mathbf{z}<em>{n}, \mathbf{u}</em>{n}\right)
$$</p>
<p>を利用すれば簡単である。まず$\alpha(\mathbf{z}_{n})$について考えると、$(13.49), (13.50)$を参考にして</p>
<p>$$
\begin{aligned}
\alpha\left(\mathbf{z}<em>{n}\right) &amp;=\mu</em>{f_{n} \rightarrow \mathbf{z}<em>{n}}\left(\mathbf{z}</em>{n}\right) \
&amp;=\sum_{\mathbf{z}<em>{n-1}} f</em>{n}\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right) \underbrace{\mu_{f_{n-1} \rightarrow \mathbf{z}<em>{n-1}}\left(\mathbf{z}</em>{n-1}\right)}<em>{\alpha\left(\mathbf{z}</em>{n-1}\right)} \
&amp;=\sum_{\mathbf{z}<em>{n-1}} p\left(\mathbf{z}</em>{n} \mid \mathbf{z}<em>{n-1}, \mathbf{u}</em>{n}\right) p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}, \mathbf{u}<em>{n}\right) \alpha\left(\mathbf{z}</em>{n-1}\right)
\end{aligned}
$$</p>
<p>となる。ここで、初期条件$\alpha(\mathbf{z}_{1})$は演習13.17で求めたように</p>
<p>$$
\alpha(\mathbf{z}<em>{1}) = h\left(\mathbf{z}</em>{1}\right)=p\left(\mathbf{z}<em>{1} \mid \mathbf{u}</em>{1}\right) p\left(\mathbf{x}<em>{1} \mid \mathbf{z}</em>{1}, \mathbf{u}_{1}\right)
$$</p>
<p>である。同様に$\beta(\mathbf{z}_{n})$について</p>
<p>$$
\begin{aligned}
\beta\left(\mathbf{z}<em>{n}\right) &amp;=\mu</em>{f_{n+1} \rightarrow \mathbf{z}<em>{n}}\left(\mathbf{z}</em>{n}\right) \
&amp;=\sum_{\mathbf{z}<em>{n+1}} f</em>{n+1}\left(\mathbf{z}<em>{n}, \mathbf{z}</em>{n+1}\right) \underbrace{\mu_{f_{n+2} \rightarrow \mathbf{z}<em>{n+1}}\left(\mathbf{z}</em>{n+1}\right)}<em>{\beta(\mathbf{z}</em>{n+1})} \
&amp;=\sum_{\mathbf{z}<em>{n+1}} p\left(\mathbf{z}</em>{n+1} \mid \mathbf{z}<em>{n}, \mathbf{u}</em>{n+1}\right) p\left(\mathbf{x}<em>{n+1} \mid \mathbf{z}</em>{n+1}, \mathbf{u}<em>{n+1}\right) \beta\left(\mathbf{z}</em>{n+1}\right)
\end{aligned}
$$</p>
<p>となる。初期条件（最初のメッセージ）は$\beta(\mathbf{z}<em>{N})=1$のままで、これは入力確率$\mathbf{u}</em>{n}$が存在する場合でも同じである。</p>
<h2 id="演習-1319"><a class="header" href="#演習-1319">演習 13.19</a></h2>
<div class="panel-primary">
<p>線形動的システムにおいては、すべての観測変数により条件付けられた個々の潜在変数に対する事後分布を、カルマンフィルタとカルマンスムーザの方程式を用いて効率的に求めることができる。これらの事後分布の各々を独立に最大化することにより得られる潜在変数の系列が、潜在変数の値の最も確からしい系列と同ーであることを示せ。これを実行する際に、線形動的システムにおいては、すべての潜在変数と観測変数の同時分布はガウス分布であり、したがって、すべての条件付き分布と周辺分布もガウス分布であることに注意して、</p>
<p>$$
p\left(\mathbf{x}<em>{a}\right)=\mathcal{N}\left(\mathbf{x}</em>{a} \mid \boldsymbol{\mu}<em>{a}, \Sigma</em>{a a}\right) \tag{2.98}
$$</p>
<p>の結果を用いよ.</p>
</div>
<p>線形動的システムにおいては全ての潜在変数と観測変数の同時分布はガウス分布であるため，任意の変数の組について事後分布を最大化することができる．したがってすべての潜在変数の同時分布を最大化することも、各潜在変数についての周辺分布を個別に最大化することも可能である。しかし、(2.98)から、結果の平均はどちらの場合も同じになることがわかり、ガウス分布では平均と潜在変数の最も確からしい値は一致するので、潜在変数の事後分布をそれぞれに最大化した系列でも潜在変数の同時分布を最大化した系列でも、同じ結果になることがわかる。</p>
<h2 id="演習-1320"><a class="header" href="#演習-1320">演習 13.20</a></h2>
<div class="panel-primary">
<p>$$
p(\mathbf{y})=\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \boldsymbol{\mu}+\mathbf{b}, \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}\right) \tag{2.115}
$$</p>
<p>の結果を用いて</p>
<p>$$
\begin{array}{c}\int \mathcal{N}\left(\mathbf{z}<em>{n} \mid \mathbf{A} \mathbf{z}</em>{n-1}, \boldsymbol{\Gamma}\right) \mathcal{N}\left(\mathbf{z}<em>{n-1} \mid \boldsymbol{\mu}</em>{n-1}, \mathbf{V}<em>{n-1}\right) \mathrm{d} \mathbf{z}</em>{n-1} \ =\mathcal{N}\left(\mathbf{z}<em>{n} \mid \mathbf{A} \boldsymbol{\mu}</em>{n-1}, \mathbf{P}_{n-1}\right)\end{array} \tag{13.87}
$$</p>
<p>を証明せよ.</p>
</div>
<p>(2.113)~(2.115)の議論の結果を変数の対応をとりながら利用することができる</p>
<p>$\mathrm{x}$ の周辺ガウス分布と, $\mathrm{x}$ が与えられたときの $\mathrm{y}$ の条件付きガウス分布が次式で 与えられたとする.
$$
\begin{aligned}
p(\mathbf{x}) &amp;=\mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}, \boldsymbol{\Lambda}^{-1}\right) \
p(\mathbf{y} \mid \mathbf{x}) &amp;=\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \mathbf{x}+\mathbf{b}, \mathbf{L}^{-1}\right)
\end{aligned}
$$
$\mathrm{y}$ の周辺分布は
$$
p(\mathbf{y})=\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \boldsymbol{\mu}+\mathbf{b}, \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}\right)
$$</p>
<p>これと今回の変数の対応を考えると，$\mathbf{x}\rightarrow\mathbf{z}<em>{n-1},\mathbf{\mu}\rightarrow\mathbf{\mu}</em>{n-1},\boldsymbol{\Lambda}^{-1}\rightarrow\mathbf{V}_{n-1},\mathbf{y}\rightarrow\mathbf{z}_n,\mathbf{A}\rightarrow\mathbf{A},\mathbf{b}\rightarrow\mathbf{0},\mathbf{L}^{-1}\rightarrow\boldsymbol{\Gamma}$となる．この結果を用いると(13.87)が示される．</p>
<h2 id="演習-1321"><a class="header" href="#演習-1321">演習 13.21</a></h2>
<div class="panel-primary">
<p>$$
p(\mathbf{y})=\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \boldsymbol{\mu}+\mathbf{b}, \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}\right) \tag{2.115}
$$</p>
<p>$$
p(\mathbf{x} \mid \mathbf{y}) =\mathcal{N}\left(\mathbf{x} \mid \Sigma\left{\mathbf{A}^{\mathrm{T}} \mathbf{L}(\mathbf{y}-\mathbf{b})+\mathbf{\Lambda} \mu\right}, \boldsymbol{\Sigma}\right) \tag{2.116}
$$</p>
<p>の結果と、</p>
<p>$$
\left(\mathbf{P}^{-1}+\mathbf{B}^{\mathrm{T}} \mathbf{R}^{-1} \mathbf{B}\right)^{-1} \mathbf{B}^{\mathrm{T}} \mathbf{R}^{-1}=\mathbf{P B}^{\mathrm{T}}\left(\mathbf{B P B}^{\mathrm{T}}+\mathbf{R}\right)^{-1} \tag{C.5}
$$</p>
<p>$$
\left(\mathbf{A}+\mathbf{B D}^{-1} \mathbf{C}\right)^{-1}=\mathbf{A}^{-1}-\mathbf{A}^{-1} \mathbf{B}\left(\mathbf{D}+\mathbf{C A}^{-1} \mathbf{B}\right)^{-1} \mathbf{C A}^{-1} \tag{C.7}
$$</p>
<p>の行列恒等式をともに用いて</p>
<p>$$
\boldsymbol{\mu}<em>{n} =\mathbf{A} \boldsymbol{\mu}</em>{n-1}+\mathbf{K}<em>{n}\left(\mathbf{x}</em>{n}-\mathbf{C A} \boldsymbol{\mu}<em>{n-1}\right) \tag{13.89}
$$
$$
\mathbf{V}</em>{n} =\left(\mathbf{I}-\mathbf{K}<em>{n} \mathbf{C}\right) \mathbf{P}</em>{n-1} \tag{13.90}
$$
$$
c_{n} =\mathcal{N}\left(\mathbf{x}<em>{n} \mid \mathbf{C A} \boldsymbol{\mu}</em>{n-1}, \mathbf{C P}_{n-1} \mathbf{C}^{\mathrm{T}}+\Sigma\right) \tag{13.91}
$$</p>
<p>の結果を導け。ここで、カルマン利得行列$\mathbf{K}_{n}$は</p>
<p>$$
\mathbf{K}<em>{n}=\mathbf{P}</em>{n-1} \mathbf{C}^{\mathrm{T}}\left(\mathbf{C P}_{n-1} \mathbf{C}^{\mathrm{T}}+\mathbf{\Sigma}\right)^{-1} \tag{13.92}
$$</p>
<p>で定義される.</p>
</div>
<p>(2.113)~(2.117)の議論より$\mathrm{x}$ の周辺ガウス分布と, $\mathrm{x}$ が与えられたときの $\mathrm{y}$ の条件付きガウス分布が次式で 与えられたとする.
$$
p(\mathrm{x}) =\mathcal{N}\left(\mathrm{x} \mid \boldsymbol{\mu}, \boldsymbol{\Lambda}^{-1}\right)\tag{2.113}
$$</p>
<p>$$
p(\mathbf{y} \mid \mathrm{x}) =\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \mathbf{x}+\mathbf{b}, \mathbf{L}^{-1}\right)\tag{2.114}
$$
$\mathrm{y}$ の周辺分布と , $\mathrm{y}$ が与えられたときの $\mathrm{x}$ の条件付き分布は
$$
p(\mathbf{y}) =\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \boldsymbol{\mu}+\mathbf{b}, \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}\right) \tag{2.115}
$$</p>
<p>$$
p(\mathbf{x} \mid \mathbf{y}) =\mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\Sigma}\left{\mathbf{A}^{\mathrm{T}} \mathbf{L}(\mathbf{y}-\mathbf{b})+\boldsymbol{\Lambda} \boldsymbol{\mu}\right}, \boldsymbol{\Sigma}\right)\tag{2.116}
$$
で与えられる. ただし,
$$
\Sigma=\left(\Lambda+\mathrm{A}^{\mathrm{T}} \mathbf{L} \mathbf{A}\right)^{-1}\tag{2.117}
$$</p>
<p>である．(13.87)の周辺分布の計算結果を用いて(13.86)を書き直すと以下のようになる．</p>
<p>$$
c_{n} \mathcal{N}\left(\mathbf{z}<em>{n} \mid \boldsymbol{\mu}</em>{n}, \mathbf{V}<em>{n}\right)=\mathcal{N}\left(\mathbf{x}</em>{n} \mid \mathbf{C} \mathbf{z}<em>{n}, \mathbf{\Sigma}\right) \mathcal{N}\left(\mathbf{z}</em>{n} \mid \mathbf{A} \boldsymbol{\mu}<em>{n-1}, \mathbf{P}</em>{n-1}\right)
$$</p>
<p>この式の右辺は$\mathbf{x}_n$と$\mathbf{z}_n$の同時分布で，$\mathbf{z}_n$を与えられたときの$\mathbf{x}_n$の条件付き分布と$\mathbf{z}_n$の分布の積の形で表されている．これらはそれぞれ(2.114)と(2.113)に対応している．
ここで右辺の同時分布の分解を$\mathbf{x}_n$を与えられたときの$\mathbf{z}_n$の条件付き分布と$\mathbf{x}_n$の分布の積の形に書き換える．このときそれぞれ(2.116)と(2.115)に対応することになる</p>
<p>$$
\begin{array}{ll}
\mathbf{x} \Rightarrow \mathbf{z}<em>{n} \quad \boldsymbol{\mu} \Rightarrow \mathbf{A} \boldsymbol{\mu}</em>{n-1} \quad \mathbf{\Lambda}^{-1} \Rightarrow \mathbf{P}<em>{n-1} \
\mathbf{y} \Rightarrow \mathbf{x}</em>{n} \quad \mathbf{A} \Rightarrow \mathbf{C} \quad \mathbf{b} \Rightarrow \mathbf{0} \quad \mathbf{L}^{-1} \Rightarrow \boldsymbol{\Sigma} &amp;
\end{array}
$$</p>
<p>これらを代入すると</p>
<p>$(2.113)$,$(2.114),(2.115)$ により $(13.91)$の右辺を得る</p>
<p>また(2.116)から
$$
p\left(\mathbf{z}<em>{n} \mid \mathbf{x}</em>{n}\right)=\mathcal{N}\left(\mathbf{z}<em>{n} \mid \boldsymbol{\mu}</em>{n}, \mathbf{V}<em>{n}\right)=\mathcal{N}\left(\mathbf{z}</em>{n} \mid \mathbf{M}\left(\mathbf{C}^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} \mathbf{x}<em>{n}+\mathbf{P}</em>{n-1}^{-1} \mathbf{A} \boldsymbol{\mu}<em>{n-1}\right), \mathbf{M}\right)\tag{1}
$$
ただし(2.117)を用いて$\mathbf{M}$を以下のように定めた．
$$
\mathbf{M}=\left(\mathbf{P}</em>{n-1}^{-1}+\mathbf{C}^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} \mathbf{C}\right)^{-1}\tag{2}
$$
(C.7)と(13.92)を使って, (2)を書き換えると
$$
\begin{aligned}
\mathbf{M} &amp;=\left(\mathbf{P}<em>{n-1}^{-1}+\mathbf{C}^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} \mathbf{C}\right)^{-1} \
&amp;=\mathbf{P}</em>{n-1}-\mathbf{P}<em>{n-1} \mathbf{C}^{\mathrm{T}}\left(\boldsymbol{\Sigma}+\mathbf{C P}</em>{n-1} \mathbf{C}^{\mathrm{T}}\right)^{-1} \mathbf{C P}<em>{n-1} \
&amp;=\left(\mathbf{I}-\mathbf{P}</em>{n-1} \mathbf{C}^{\mathrm{T}}\left(\boldsymbol{\Sigma}+\mathbf{C P}<em>{n-1} \mathbf{C}^{\mathrm{T}}\right)^{-1} \mathbf{C}\right) \mathbf{P}</em>{n-1} \
&amp;=\left(\mathbf{I}-\mathbf{K}<em>{n} \mathbf{C}\right) \mathbf{P}</em>{n-1},
\end{aligned}
$$</p>
<p>となり，これは(13.90)の右辺と一致する</p>
<p>(2), (C.5),(13.92)を用いて</p>
<p>$$
\begin{aligned}
\mathbf{M} \mathbf{C}^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} &amp;=\left(\mathbf{P}<em>{n-1}^{-1}+\mathbf{C}^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} \mathbf{C}\right)^{-1} \mathbf{C}^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} \
&amp;=\mathbf{P}</em>{n-1} \mathbf{C}^{\mathrm{T}}\left(\mathbf{C} \mathbf{P}<em>{n-1} \mathbf{C}^{\mathrm{T}}+\mathbf{\Sigma}\right)^{-1}=\mathbf{K}</em>{n}
\end{aligned}
$$
これと(13.90)を用いると, (1)の平均を書き換えることができて
$$
\begin{aligned}
\mathbf{M}\left(\mathbf{C}^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} \mathbf{x}<em>{n}+\mathbf{P}</em>{n-1}^{-1} \mathbf{A} \boldsymbol{\mu}<em>{n-1}\right) &amp;=\mathbf{M} \mathbf{C}^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} \mathbf{x}</em>{n}+\left(\mathbf{I}-\mathbf{K}<em>{n} \mathbf{C}\right) \mathbf{A} \boldsymbol{\mu}</em>{n-1} \
&amp;=\mathbf{K}<em>{n} \mathbf{x}</em>{n}+\mathbf{A} \boldsymbol{\mu}<em>{n-1}-\mathbf{K}</em>{n} \mathbf{C A} \boldsymbol{\mu}<em>{n-1} \
&amp;=\mathbf{A} \boldsymbol{\mu}</em>{n-1}+\mathbf{K}<em>{n}\left(\mathbf{x}</em>{n}-\mathbf{C A} \boldsymbol{\mu}_{n-1}\right)
\end{aligned}
$$
を得る．これは(13.89)である．</p>
<h2 id="演習-1322"><a class="header" href="#演習-1322">演習 13.22</a></h2>
<div class="panel-primary">
<p>$$
c_{1} \widehat{\alpha}\left(\mathbf{z}<em>{1}\right)=p\left(\mathbf{z}</em>{1}\right) p\left(\mathbf{x}<em>{1} \mid \mathbf{z}</em>{1}\right) \tag{13.93}
$$</p>
<p>を、</p>
<p>$$
p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right)=\mathcal{N}\left(\mathbf{x}<em>{n} \mid \mathbf{Cz}</em>{n}, \mathbf{\Sigma}\right) \tag{13.76}
$$</p>
<p>$$
p\left(\mathbf{z}<em>{1}\right)=\mathcal{N}\left(\mathbf{z}</em>{1} \mid \pmb{\mu}<em>{0}, \mathbf{P}</em>{0}\right) \tag{13.77}
$$</p>
<p>の定義と、</p>
<p>$$
p(\mathbf{y})=\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \boldsymbol{\mu}+\mathbf{b}, \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}\right) \tag{2.115}
$$</p>
<p>の結果とともに用いて</p>
<p>$$
c_{1}=\mathcal{N}\left(\mathbf{x}<em>{1} \mid \mathbf{C} \boldsymbol{\mu}</em>{0}, \mathbf{CP}_{0} \mathbf{C}^{\mathrm{T}}+\mathbf{\Sigma}\right) \tag{13.96}
$$</p>
<p>を導け</p>
</div>
<p>$(13.57)$の定義からスケーリング係数$c_{1}$は$c_{1} = p(\mathbf{x}<em>{1})$である。$(13.93)$と比較すれば$\widehat{\alpha}\left(\mathbf{z}</em>{1}\right) = p(\mathbf{z}<em>{1}\mid \mathbf{x}</em>{1})$である。PRMLの上巻P.90の議論を用いれば、$p\left(\mathbf{x}<em>{1} \mid \mathbf{z}</em>{1}\right)$と$p(\mathbf{z}_{1})$が与えられていればこれらの値を求めることができる。</p>
<p>$$
\begin{aligned}
c_1 = p(\mathbf{x}<em>{1}) &amp;= p\left(\mathbf{z}</em>{1}\right) p\left(\mathbf{x}<em>{1} \mid \mathbf{z}</em>{1}\right) \
&amp;= \mathcal{N}\left(\mathbf{z}<em>{1} \mid \pmb{\mu}</em>{0}, \mathbf{P}<em>{0}\right) \mathcal{N}\left(\mathbf{x}</em>{1} \mid \mathbf{Cz}<em>{1}, \mathbf{\Sigma}\right) \
&amp;=\mathcal{N}\left(\mathbf{x}</em>{1} \mid \mathbf{C} \pmb{\mu}<em>{0}, \mathbf{CP}</em>{0} \mathbf{C}^{\mathrm{T}}+\mathbf{\Sigma}\right)
\end{aligned}
$$</p>
<h2 id="演習-1323"><a class="header" href="#演習-1323">演習 13.23</a></h2>
<div class="panel-primary">
<p>$$
c_{1} \widehat{\alpha}\left(\mathbf{z}<em>{1}\right)=p\left(\mathbf{z}</em>{1}\right) p\left(\mathbf{x}<em>{1} \mid \mathbf{z}</em>{1}\right) \tag{13.93}
$$</p>
<p>を、</p>
<p>$$
p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right)=\mathcal{N}\left(\mathbf{x}<em>{n} \mid \mathbf{Cz}</em>{n}, \mathbf{\mathbf{\Sigma}}\right) \tag{13.76}
$$</p>
<p>$$
p\left(\mathbf{z}<em>{1}\right)=\mathcal{N}\left(\mathbf{z}</em>{1} \mid \boldsymbol{\mu}<em>{0}, \mathbf{P}</em>{0}\right) \tag{13.77}
$$</p>
<p>の定義と、</p>
<p>$$
p(\mathbf{x} \mid \mathbf{y})=\mathcal{N}\left(\mathbf{x} \mid \mathbf{\mathbf{\Sigma}}\left{\mathbf{A}^{\mathrm{T}} \mathbf{L}(\mathbf{y}-\mathbf{b})+\mathbf{\Lambda} \boldsymbol{\mu}\right}, \mathbf{\mathbf{\Sigma}}\right) \tag{2.116}
$$</p>
<p>の結果とともに用いて、</p>
<p>$$
\boldsymbol{\mu}<em>{1} =\boldsymbol{\mu}</em>{0}+\mathbf{K}<em>{1}\left(\mathbf{x}</em>{1}-\mathbf{C} \boldsymbol{\mu}_{0}\right) \tag{13.94}
$$</p>
<p>$$
\mathbf{V}<em>{1} =\left(\mathbf{I}-\mathbf{K}</em>{1} \mathbf{C}\right) \mathbf{P}_{0} \tag{13.95}
$$</p>
<p>$$
\mathbf{K}<em>{1} =\mathbf{P}</em>{0} \mathbf{C}^{\mathrm{T}}\left(\mathbf{CP}_{0} \mathbf{C}^{\mathrm{T}}+\mathbf{\mathbf{\Sigma}}\right)^{-1} \tag{13.97}
$$</p>
<p>を導け.</p>
</div>
<p>力技の計算問題。</p>
<p>$\widehat{\alpha}\left(\mathbf{z}<em>{1}\right)=p(\mathbf{z}</em>{1}\mid\mathbf{x}<em>{1})=\mathcal{N}(\mathbf{z}</em>{1}\mid \pmb{\mu}<em>{1},\mathbf{V}</em>{1})$を$(2.116)$の公式と$(C.7)$を用いて求める。</p>
<p>$$
p\left(\mathbf{z}<em>{1} \mid \mathbf{x}</em>{1}\right)=\mathcal{N}\left(\mathbf{z}<em>{1} \mid\left(\mathbf{P}</em>{0}^{-1}+\mathbf{C}^{\mathrm T} \mathbf{\Sigma}^{-1} \mathbf{C}\right)^{-1}\left{\mathbf{C}^{\mathrm T} \mathbf{\Sigma}^{-1} \mathbf{x}<em>{1}+\mathbf{P}</em>{0}^{-1} \pmb{\mu}<em>{0}\right}, \left(\mathbf{P}</em>{0}^{-1}+\mathbf{C}^{\mathrm T} \mathbf{\Sigma}^{-1} \mathbf{C}\right)^{-1}\right)
$$</p>
<p>これより</p>
<p>$$
\begin{aligned}
\mathbf{V}<em>{1} &amp;=\left(\mathbf{P}</em>{0}^{-1}+\mathbf{C}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \mathbf{C}\right)^{-1} \
&amp;=\mathbf{P}<em>{0}-\mathbf{P}</em>{0} \mathbf{C}^{\mathrm{T}}\left(\mathbf{\Sigma}+\mathbf{C P}<em>{0} \mathbf{C}^{\mathrm{T}}\right)^{-1} \mathbf{C P}</em>{0} \
&amp;=\left(\mathbf{I}-\mathbf{K}<em>{1} \mathbf{C}\right) \mathbf{P}</em>{0}
\end{aligned}
$$</p>
<p>また、今求めた$\mathbf{V}_{1}$を用いて、</p>
<p>$$
\begin{aligned}
\pmb{\mu}<em>{1} &amp;=\mathbf{V}</em>{1}\left(\mathbf{C}^{\mathrm T} \mathbf{\Sigma}^{-1} \mathbf{x}<em>{1}+\mathbf{P}</em>{0}^{-1} \pmb{\mu}<em>{0}\right) \
&amp;=\left(\mathbf{I}-\mathbf{K}</em>{1} \mathbf{C}\right) \mathbf{P}<em>{0}\left(\mathbf{C}^{\mathrm T} \mathbf{\Sigma}^{-1} \mathbf{x}</em>{1}+\mathbf{P}<em>{0}^{-1} \pmb{\mu}</em>{0}\right) \
&amp;=\pmb{\mu}<em>{0}-\mathbf{K}</em>{1} \mathbf{C} \pmb{\mu}<em>{0}+\mathbf{V}</em>{1} \mathbf{C}^{\mathrm T} \mathbf{\Sigma}^{-1} \mathbf{x}<em>{1} \
&amp;=\pmb{\mu}</em>{0}+\mathbf{K}<em>{1}\left(\mathbf{x}</em>{1}-\mathbf{C} \pmb{\mu}_{0}\right)
\end{aligned}
$$</p>
<p>この変形での$\mathbf{V}_{1} \mathbf{C}^{\mathrm T} \mathbf{\Sigma}^{-1}$部分は</p>
<p>$$
\begin{aligned} \mathbf{V}<em>{1} \mathbf{C}^{\mathrm{T}} \mathbf{\Sigma}^{-1}=&amp; \mathbf{P}</em>{0} \mathbf{C}^{\mathrm{T}} \mathbf{\Sigma}^{-1}-\mathbf{K}<em>{1} \mathbf{C} \mathbf{P}</em>{0} \mathbf{C}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \=&amp; \mathbf{P}<em>{0} \mathbf{C}^{\mathrm{T}}\left(\mathbf{I}-\left(\mathbf{\Sigma}+\mathbf{C P}</em>{0} \mathbf{C}^{\mathrm{T}}\right)^{-1} \mathbf{C P}<em>{0} \mathbf{C}^{\mathrm{T}}\right) \mathbf{\Sigma}^{-1} \
=&amp; \mathbf{P}</em>{0} \mathbf{C}^{\mathrm T}\left(\left(\mathbf{\Sigma}+\mathbf{CP}<em>{0} \mathbf{C}^{\mathrm T}\right)^{-1}\left(\mathbf{\Sigma}+\mathbf{CP}</em>{0} \mathbf{C}^{\mathrm T}\right)-\left(\mathbf{\Sigma}+\mathbf{CP}<em>{0} \mathbf{C}^{\mathrm T}\right)^{-1} \mathbf{CP}</em>{0} \mathbf{C}^{\mathrm T}\right) \mathbf{\Sigma}^{-1}
\
=&amp; \mathbf{P}<em>{0} \mathbf{C}^{\mathrm T}\left( \left(\mathbf{\Sigma}+\mathbf{CP}</em>{0} \mathbf{C}^{\mathrm T}\right)^{-1} \mathbf{\Sigma} \right) \mathbf{\Sigma}^{-1} \
=&amp; \mathbf{P}<em>{0} \mathbf{C}^{\mathrm{T}}\left(\mathbf{\Sigma}+\mathbf{C P}</em>{0} \mathbf{C}^{\mathrm{T}}\right)^{-1}=\mathbf{K}_{1} \end{aligned}
$$</p>
<p>となることを利用した。</p>
<h2 id="演習-1324"><a class="header" href="#演習-1324">演習 13.24</a></h2>
<div class="panel-primary">
<p>以下の式で表されるようなガウス平均の定数$\mathbf{a}$と$\mathbf{c}$を含んだ、</p>
<p>$$
p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}\right)=\mathcal{N}\left(\mathbf{z}<em>{n} \mid \mathbf{A z}</em>{n-1}, \mathbf{\Gamma}\right) \tag{13.75}
$$</p>
<p>$$
p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right)=\mathcal{N}\left(\mathbf{x}<em>{n} \mid \mathbf{Cz}</em>{n}, \mathbf{\mathbf{\Sigma}}\right) \tag{13.76}
$$</p>
<p>の一般化について考える.</p>
<p>$$
p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}\right) =\mathcal{N}\left(\mathbf{z}<em>{n} \mid \mathbf{A} \mathbf{z}</em>{n-1}+\mathbf{a}, \boldsymbol{\Gamma}\right) \tag{13.127}
$$</p>
<p>$$
p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right) =\mathcal{N}\left(\mathbf{x}<em>{n} \mid \mathbf{C} \mathbf{z}</em>{n}+\mathbf{c}, \mathbf{\Sigma}\right) \tag{13.128}
$$</p>
<p>1に固定された付加的な項をもつ状態ベクトル$\mathbf{z}$を定義して、パラメータ$\mathbf{a}$と$\mathbf{c}$に対応した列を行列$\mathbf{A}$と$\mathbf{C}$に付け加えることにより、この拡張された式がこの章で議論した枠組みの中で説明できることを示せ。</p>
</div>
<p>$$
\boldsymbol{\mu}<em>{0}^{\prime}=\left[\begin{array}{c}\boldsymbol{\mu}</em>{0} \ 1\end{array}\right] \quad \mathbf{V}<em>{0}^{\prime}=\left[\begin{array}{ll}\mathbf{V}</em>{0} &amp; \mathbf{0} \ \mathbf{0} &amp; 0\end{array}\right] \quad \boldsymbol{\Gamma}^{\prime}=\left[\begin{array}{ll}\mathbf{\Gamma} &amp; \mathbf{0} \ \mathbf{0} &amp; 0\end{array}\right]
$$
$$
\mathbf{A}^{\prime}=\left[\begin{array}{ll}\mathbf{A} &amp; \mathbf{a} \ \mathbf{0} &amp; 1\end{array}\right] \quad \mathbf{C}^{\prime}=\left[\begin{array}{ll}\mathbf{C} &amp; \mathbf{c}\end{array}\right]
$$
に、各変数を置き換えると、1に固定された$\mathbf{z_n}$の付加項によって(13.127), (13.128)が実現できる。
このようにしてほとんどの枠組みが実現できるが、$\mathbf{z_n}$の付加項が分散0であるという事実が、分散の逆行列を考慮する場合にのみ効いてきて(式13.92など)、正しくは
$$
\left(\mathbf{P}<em>{n-1}^{\prime}\right)^{-1}=\left[\begin{array}{cc}\mathbf{P}</em>{n-1}^{-1} &amp; \mathbf{0} \ \mathbf{0} &amp; 0\end{array}\right]
$$
のようにする必要がある。</p>
<h2 id="演習-1325"><a class="header" href="#演習-1325">演習 13.25</a></h2>
<div class="panel-primary">
<p>この演習問題では、カルマンフィル夕方程式が独立の観測に対し用いられたときに、それらが2.3節で与えた単一ガウス分布の最尤推定法の結果に帰着することを示そう。独立の観測値の集合$\left{x_{1}, \ldots, x_{N}\right}$が与えられたときに、単ーガウス分布に従う確率変数$x$の平均$\mu$を求める問題を考える。これをモデル化するために、</p>
<p>$$
p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}\right)=\mathcal{N}\left(\mathbf{z}<em>{n} \mid \mathbf{A z}</em>{n-1}, \mathbf{\Gamma}\right) \tag{13.75}
$$</p>
<p>$$
p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right)=\mathcal{N}\left(\mathbf{x}<em>{n} \mid \mathbf{Cz}</em>{n}, \mathbf{\Sigma}\right) \tag{13.76}
$$</p>
<p>で支配される線形動的システムを使うことができる。ここで、潜在変数$\left{z_{1}, \ldots, z_{N}\right}$において、各々の観測が互いに独立なため、<s>Cは単位行列であり、遷移確率A=0</s>$\mathbf{C}=1,\mathbf{A}=1,\mathbf{\Gamma}=0$となる。最初の状態のパラメータ$\pmb{\mu}<em>{0}, \mathbf{P}</em>{0}$を、それぞれ$\mu_0, \sigma_{0}^2$と書くことにし、$\mathbf{\Sigma}$は$\sigma^2$になると考える。対応するカルマンフィル夕方程式を、一般的な結果</p>
<p>$$
\pmb{\mu}<em>{n} =\mathbf{A} \pmb{\mu}</em>{n-1}+\mathbf{K}<em>{n}\left(\mathbf{x}</em>{n}-\mathbf{C A} \pmb{\mu}_{n-1}\right) \tag{13.89}
$$</p>
<p>$$
\mathbf{V}<em>{n} =\left(\mathbf{I}-\mathbf{K}</em>{n} \mathbf{C}\right) \mathbf{P}_{n-1} \tag{13.90}
$$</p>
<p>から出発し、</p>
<p>$$
\pmb{\mu}<em>{1} =\pmb{\mu}</em>{0}+\mathbf{K}<em>{1}\left(\mathbf{x}</em>{1}-\mathbf{C} \pmb{\mu}_{0}\right) \tag{13.94}
$$</p>
<p>$$
\mathbf{V}<em>{1} =\left(\mathbf{I}-\mathbf{K}</em>{1} \mathbf{C}\right) \mathbf{P}_{0} \tag{13.95}
$$</p>
<p>を用いて書き下せ。さらに、これが直接独立なデータを考えたときに得られる結果
$$
\mu_{N} =\frac{\sigma^{2}}{N \sigma_{0}^{2}+\sigma^{2}} \mu_{0}+\frac{N \sigma_{0}^{2}}{N \sigma_{0}^{2}+\sigma^{2}} \mu_{\mathrm{ML}} \tag{2.141}
$$
$$
\frac{1}{\sigma_{N}^{2}} =\frac{1}{\sigma_{0}^{2}}+\frac{N}{\sigma^{2}}  \tag{2.142}
$$
と同一であることを示せ。</p>
</div>
<p>※ 問題文は誤っており、前提条件として$\mathbf{C}=1,\mathbf{A}=1,\mathbf{\Gamma}=0$となる。この上で問題を解く。</p>
<p>まず$\pmb{\mu}<em>{1}$と$\mathbf{V}</em>{1}$を求める。初期パラメータは$\pmb{\mu}<em>{0} = \mu</em>{0}$と$\mathbf{P}<em>{0} = \sigma</em>{0}^{2}$, $\mathbf{\Sigma} = \sigma^2$なので、$(13.97)$式は</p>
<p>$$
K_{1}=\sigma_{0}^{2}\left(\sigma_{0}^{2}+\sigma^{2}\right)^{-1}=\frac{\sigma_{0}^{2}}{\sigma_{0}^{2}+\sigma^{2}}
$$</p>
<p>となり、これを用いて$(13.94)(13.95)$の$\mu_{1}, \mathbf{V}<em>{1}$を求めると、
$$
\begin{aligned}
\pmb{\mu}</em>{1} &amp;=\mu_{0}+\frac{\sigma_{0}^{2}}{\sigma_{0}^{2}+\sigma^{2}}\left(x_{1}-\mu_{0}\right) \
&amp;=\frac{1}{\sigma_{0}^{2}+\sigma^{2}}\left(\sigma_{0}^{2} x_{1}+\sigma^{2} \mu_{0}\right)
\end{aligned}\tag{A}
$$</p>
<p>$$
\begin{aligned}
\mathbf{V}<em>{1} &amp;=\left(1-\frac{\sigma</em>{0}^{2}}{\sigma_{0}^{2}+\sigma^{2}}\right) \sigma_{0}^{2} \
&amp;=\frac{\sigma_{0}^{2} \sigma^{2}}{\sigma_{0}^{2}+\sigma^{2}}
\end{aligned}\tag{B}
$$</p>
<p>となる。これらの結果を、まず$N=1$のときの$(2.141),(2.142)$式と比較する。$(2.143)$式で$\mu_{\textrm{ML}}$はサンプル平均$\displaystyle \mu_{\textrm{ML}}=\frac{1}{N} \sum_{n=1}^{N} x_{n}$と定義されていることに注意して</p>
<p>$$
\begin{aligned}
\mu_{1} &amp;=\frac{\sigma^{2}}{\sigma_{0}^{2}+\sigma^{2}} \mu_{0}+\frac{\sigma_{0}^{2}}{\sigma_{0}^{2}+\sigma^{2}} x_{1} \
&amp;=\frac{1}{\sigma_{0}^{2}+\sigma^{2}}\left(\sigma_{0}^{2} x_{1}+\sigma^{2} \mu_{0}\right) \
\sigma_{1}^{2} &amp;=1 /\left(\frac{1}{\sigma_{0}^{2}}+\frac{1}{\sigma^{2}}\right)=\frac{\sigma_{0}^{2} \sigma^{2}}{\sigma_{0}^{2}+\sigma^{2}}
\end{aligned}
$$</p>
<p>より、$\pmb{\mu}<em>{1} = \mu</em>{1}$、$\mathbf{V}<em>{1} = \sigma</em>{1}^{2}$とすると同じであることがわかる。</p>
<p>そこで、誘導に従って任意の正の整数$N$について$\pmb{\mu}<em>{N} = \mu</em>{N}$、$\mathbf{V}<em>{N} = \sigma</em>{N}^{2}$が成立していると仮定する。ここで$\mu_{N}$と$\sigma_{N}^{2}$は$(2.141)(2.142)$の式である。数学的帰納法の考えに基づいて、$N+1$のときについて調べると、まず$(13.88)$式の定義から</p>
<p>$$
\begin{aligned}
\mathbf{P}<em>{N}&amp;=\mathbf{AV}</em>{N} \mathbf{A}^{\mathrm{T}}+\mathbf{\Gamma} \
&amp;=\mathbf{V}<em>{N} = \sigma</em>{N}^{2}\
\end{aligned}
$$</p>
<p>が成り立つことに注意して、カルマン利得行列の定義$(13.92)$から</p>
<p>$$
\begin{aligned}
\mathbf{K}<em>{N+1}&amp;=\mathbf{P}</em>{N} \mathbf{C}^{\mathrm{T}}\left(\mathbf{CP}<em>{N} \mathbf{C}^{\mathrm{T}}+\mathbf{\Sigma}\right)^{-1} \
&amp;=\mathbf{P}</em>{N}(\mathbf{P}<em>{N} + \sigma^2)^{-1} \
&amp;=\frac{\sigma</em>{N}^{2}}{\sigma_{N}^{2} + \sigma^{2}}
\end{aligned}
$$</p>
<p>となるので、$(13.90)$に代入して$\mathbf{V}_{N+1}$を計算すると</p>
<p>$$
\begin{aligned}
\mathbf{V}<em>{N+1}&amp;=\left(1-\frac{\sigma</em>{N}^{2}}{\sigma_{N}^{2}+\sigma^{2}}\right) \sigma_{N}^{2} \
&amp;=\frac{\sigma^{2} \sigma_{N}^{2}}{\sigma_{N}^{2}+\sigma^{2}} \
&amp;=\left(\frac{1}{\sigma_{N}^{2}}+\frac{1}{\sigma^{2}}\right)^{-1} \
&amp;=\left(\frac{1}{\sigma_{0}{ }^{2}}+\frac{N+1}{\sigma^{2}}\right)^{-1} \
&amp;=\frac{\sigma_{0}{ }^{2} \sigma^{2}}{(N+1) \sigma_{0}{ }^{2}+\sigma^{2}} \
&amp;=\sigma_{N+1}^{2}
\end{aligned}
$$</p>
<p>また$(13.89)$式への代入から</p>
<p>$$
\begin{aligned}
\pmb{\mu}<em>{N+1} &amp;=\mu</em>{N}+\frac{\sigma_{N}^{2}}{\sigma_{N}^{2}+\sigma^{2}}\left(x_{N+1}-\mu_{N}\right) \
&amp;=\frac{1}{\sigma_{N}^{2}+\sigma^{2}}\left(\sigma_{N}^{2} x_{N+1}+\sigma^{2} \mu_{N}\right) \
&amp;=\frac{\sigma_{N}^{2}}{\sigma_{N}^{2}+\sigma^{2}}\left(x_{N+1}+\frac{\sigma^{2}}{\sigma_{N}^{2}} \frac{\sigma^{2} \mu_{0}+\sigma_{0}^{2} \sum_{n=1}^{N} x_{n}}{N \sigma_{0}^{2}+\sigma^{2}}\right) \
&amp;=\frac{\sigma_{0}^{2}}{(N+1) \sigma_{0}^{2}+\sigma^{2}}\left(x_{N+1}+\frac{\sigma^{2}}{\sigma_{0}^{2}} \mu_{0}+\sum_{n=1}^{N} x_{n}\right) \quad (\because \mathbf{V}<em>{N+1} = \sigma</em>{N=1}^{2}の途中の式変形 )\
&amp;=\frac{\sigma^{2} \mu_{0}+(N+1) \sum_{n=1}^{N+1} x_{n}}{(N+1) \sigma_{0}^{2}+\sigma^{2}} \
&amp;=\mu_{N+1}
\end{aligned}
$$</p>
<p>が得られる。したがって、独立同分布の$N$個の1次元観測値$\left{x_{1}, \ldots, x_{N}\right}$が得られている場合のカルマンフィルタ方程式は、$(2.141),(2.142)$式と同型になることが示された。</p>
<h2 id="演習-1326"><a class="header" href="#演習-1326">演習 13.26</a></h2>
<div class="panel-primary">
<p>13.3節の線形動的システムの特別な例について考える。この例は、確率的PCAと等価なものであり、したがって、遷移行列は$\mathbf{A}=\mathbf{0}$、共分散は$\mathbf{\Gamma}=\mathbf{I}$、ノイズの共分散は$\mathbf{\Sigma}=\sigma^2\mathbf{I}$である。行列の恒等式</p>
<p>$$
\left(\mathbf{A}+\mathbf{B D}^{-1} \mathbf{C}\right)^{-1}=\mathbf{A}^{-1}-\mathbf{A}^{-1} \mathbf{B}\left(\mathbf{D}+\mathbf{C A}^{-1} \mathbf{B}\right)^{-1} \mathbf{C A}^{-1} \tag{C.7}
$$</p>
<p>を用いることにより、出力密度行列$\mathbf{C}$を$\mathbf{W}$と書くとき、</p>
<p>$$
\mu_{n} =\mathbf{A} \mu_{n-1}+\mathbf{K}<em>{n}\left(\mathbf{x}</em>{n}-\mathbf{C A} \mu_{n-1}\right) \tag{13.89}
$$</p>
<p>$$
\mathbf{V}<em>{n} =\left(\mathbf{I}-\mathbf{K}</em>{n} \mathbf{C}\right) \mathbf{P}_{n-1} \tag{13.90}
$$</p>
<p>で定義される隠れ状態の事後確率が、</p>
<p>$$
p(\mathbf{z} \mid \mathbf{x})=\mathcal{N}\left(\mathbf{z} \mid \mathbf{M}^{-1} \mathbf{W}^{\mathrm{T}}(\mathbf{x}-\boldsymbol{\mu}), \sigma^{2} \mathbf{M}^{-1}\right) \tag{12.42}
$$</p>
<p>で$\boldsymbol{\mu}=\mathbf{0}$の仮定の下での確率的PCAのときの結果$(12.42)$と同じになることを示せ.</p>
</div>
<p>(13.88)式〜(13.92)式に$\mathbf{A}=\mathbf{O}$、$\mathbf{\Gamma}=\mathbf{I}$、$\mathbf{\Sigma}=\sigma^2\mathbf{I}$、$\mathbf{C}=\mathbf{W}$を代入して、</p>
<p>$$
\begin{align*}
\mathbf{P}<em>{n-1}
&amp;= \mathbf{A}\mathbf{V}</em>{n-1}\mathbf{A}^{\rm T}+\boldsymbol\Gamma\tag{13.88}\
&amp;= \mathbf{I}\
\mathbf{K}<em>n
&amp;= \mathbf{P}</em>{n-1}\mathbf{C}^{\rm T}(\mathbf{CP}<em>{n-1}\mathbf{C}^{\rm T}+\boldsymbol\Sigma)^{-1}\tag{13.92}\
&amp;= \mathbf{W}^{\rm T} (\mathbf{W}\mathbf{W}^{\rm T}+\sigma^2 \mathbf{I})^{-1}\
\mu</em>{n}
&amp;=\mathbf{A} \mu_{n-1}+\mathbf{K}<em>{n}\left(\mathbf{x}</em>{n}-\mathbf{C A} \mu_{n-1}\right)\tag{13.89}\
&amp;= \mathbf{W}^{\rm T} (\mathbf{W}\mathbf{W}^{\rm T}+\sigma^2 \mathbf{I})^{-1}\mathbf{x}<em>{n}\
&amp;= \frac{1}{\sigma^2}\mathbf{W}^{\rm T} (\mathbf{I}+\frac{1}{\sigma^2} \mathbf{W}\mathbf{W}^{\rm T})^{-1}\mathbf{x}</em>{n}\
&amp;= \frac{1}{\sigma^2} (\mathbf{I}+ \mathbf{W}^{\rm T}\frac{1}{\sigma^2}\mathbf{W})^{-1}\mathbf{W}^{\rm T}\mathbf{x}<em>{n}\ \ \ \because {\rm (C.6)の恒等式}(\mathbf{I+AB)^{-1}A=A(I+BA)^{-1}}\
&amp;= (\sigma^2\mathbf{I}+ \mathbf{W}^{\rm T}\mathbf{W})^{-1}\mathbf{W}^{\rm T}\mathbf{x}</em>{n} \
&amp;= \mathbf{M}^{-1}\mathbf{W}^{\rm T}\mathbf{x}<em>{n} \
\mathbf{V}</em>{n}
&amp;=\left(\mathbf{I}-\mathbf{K}<em>{n} \mathbf{C}\right) \mathbf{P}</em>{n-1} \tag{13.90}\
&amp;= \mathbf{I}-{ \mathbf{W}^{\rm T} (\mathbf{W}\mathbf{W}^{\rm T}+\sigma^2 \mathbf{I})^{-1}}\mathbf{W}\
&amp;= \left[ \mathbf{I}- \left( -\mathbf{W}^{\rm T}\right) \left{ \mathbf{W}\mathbf{W}^{\rm T}+\sigma^2\mathbf{I} + \mathbf{W}\left(-\mathbf{W}^{\rm T}\right) \right}^{-1} \mathbf{W} \right] ^{-1}\ \ \ \because {\rm (C.7)の恒等式}\left(\mathbf{A}+\mathbf{B D}^{-1} \mathbf{C}\right)^{-1}=\mathbf{A}^{-1}-\mathbf{A}^{-1} \mathbf{B}\left(\mathbf{D}+\mathbf{C A}^{-1} \mathbf{B}\right)^{-1} \mathbf{C A}^{-1} \
&amp;= \left( \mathbf{I} + \mathbf{W}^{\rm T} (\sigma^2)^{-1} \mathbf{W} \right)^{-1}\
&amp;= \sigma^2 \left( \sigma^2 \mathbf{I} + \mathbf{W}^{\rm T} \mathbf{W} \right)^{-1}\
&amp;= \sigma^2 \mathbf{M}^{-1}
\end{align*}
$$</p>
<p>ここで求めた$\mu_n$と$\mathbf{V}_n$は、(12.42)式にて$\mu=\mathbf{0}$とした場合の結果に一致する。
なお、$\mathbf{V}_n$の式変形でWoodburyの公式を適用するときの対応関係は、$\mathbf{A}=\mathbf{I}, \ \mathbf{B}=-\mathbf{W}^{\rm T},\  \mathbf{C}=\mathbf{W}, \ \mathbf{D}=\mathbf{WW}^{\rm T}+\sigma^2\mathbf{I}$である。</p>
<h2 id="演習-1327"><a class="header" href="#演習-1327">演習 13.27</a></h2>
<div class="panel-primary">
<p>13.3節で議論した形式をもつ、ある線形動的システムについて考察する。このシステムは観測されるノイズの大きさがゼロであり、したがって$\mathbf{\Sigma}=\mathbf{0}$である。$\mathbf{C}=\mathbf{I}$のとき、$\mathbf{z}<em>{n}$の事後分布は、平均が$\mathbf{x}</em>{n}$となり、分散がゼロとなることを示せ。これは、もしノイズがなければ、過去のすべての観測を無視して現在の観測値$\mathbf{x}<em>{n}$を用いて状態変数$\mathbf{z}</em>{n}$を推定すればよいという我々の直感とつじつまが合う.</p>
</div>
<p>$\mathbf{z}<em>n$の事後分布は$\widehat{\alpha}\left(\mathbf{z}</em>{n}\right)=\mathcal{N}\left(\mathbf{z}<em>{n} \mid \mu</em>{n}, \mathbf{V}_{n}\right)$で表されるので，</p>
<p>$$
\mu_{n} =\mathbf{A} \mu_{n-1}+\mathbf{K}<em>{n}\left(\mathbf{x}</em>{n}-\mathbf{C A} \boldsymbol{\mu}<em>{n-1}\right)\tag{13.89}
$$
$$
\mathbf{V}</em>{n} =\left(\mathbf{I}-\mathbf{K}<em>{n} \mathbf{C}\right) \mathbf{P}</em>{n-1}\tag{13.90}
$$
$$
\mathbf{K}<em>{n}=\mathbf{P}</em>{n-1} \mathbf{C}^{\mathrm{T}}\left(\mathbf{C P}_{n-1} \mathbf{C}^{\mathrm{T}}+\mathbf{\Sigma}\right)^{-1}\tag{13.92}
$$</p>
<p>に$\mathbf{\Sigma}=\mathbf{0}, \mathbf{C}=\mathbf{I}$を代入する．その結果$\mathbf{K}_n=\mathbf{I},\mathbf{V}<em>n=\mathbf{0},\mu_n=\mathbf{x}<em>n$が得られ，$\mathbf{z}</em>{n}$の事後分布は、平均が$\mathbf{x}</em>{n}$となることがわかる．</p>
<h2 id="演習-1328"><a class="header" href="#演習-1328">演習 13.28</a></h2>
<div class="panel-primary">
<p>13.3節の線形動的システムの特別な例について考える.この例では、状態変数$\mathbf{z}<em>n$は前の状態の状態変数と等しくなるよう制約されており、したがって、$\mathbf{A}=\mathbf{I}$であり、$\mathbf{\Gamma}=\mathbf{0}$である。簡単のために、さらに$\mathbf{C}=\mathbf{I}$と$\mathbf{P}</em>{0}\to \infty$を仮定する。この仮定により$\mathbf{z}$の初期条件は重要ではなくなり、予測値はデータのみから決定される。帰納法による証明を用いて、状態$\mathbf{z}<em>{n}$の事後平均が、$\mathbf{x}</em>{1},\ldots,\mathbf{x}_{n}$の平均で与えられることを示せ。このことは、もし状態変数が一定なら、最も良い推定値は観測値を平均することにより得られるという直感と一致する。</p>
</div>
<p>数学的帰納法により証明する．まずn=1のとき
(13.75),(13.77)についてガウス分布の条件付き分布に対する一般的な性質(2.113)-(2.117)を適用し$\mathbf{P}_0\rightarrow\infty, \mathbf{C}=\mathbf{I}$を用いると</p>
<p>$$
p\left(\mathbf{z}<em>{1} \mid \mathbf{x}</em>{1}\right)=\mathcal{N}\left(\mathbf{z}<em>{1} \mid \boldsymbol{\mu}</em>{1}, \mathbf{V}_{1}\right)
$$</p>
<p>が得られる．ただし</p>
<p>$$
\begin{aligned}
\boldsymbol{\mu}<em>{1} &amp;=\mathbf{V}</em>{1}\left(\mathbf{C}^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} \mathbf{x}<em>{1}+\mathbf{P}</em>{0}^{-1} \boldsymbol{\mu}<em>{0}\right)=\mathbf{x}</em>{1} \
\mathbf{V}<em>{1} &amp;=\left(\mathbf{P}</em>{0}^{-1}+\mathbf{C}^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} \mathbf{C}\right)^{-1}=\boldsymbol{\Sigma}
\end{aligned}
$$</p>
<p>である．</p>
<p>いま$N$のとき</p>
<p>$$
\begin{aligned}
\boldsymbol{\mu}<em>{N} &amp;=\overline{\mathbf{x}}</em>{N}=\frac{1}{N} \sum_{n=1}^{N} \mathbf{x}<em>{n} \
\mathbf{V}</em>{N} &amp;=\frac{1}{N} \boldsymbol{\Sigma}
\end{aligned}
$$</p>
<p>が成り立つと仮定する．また</p>
<p>$$
\mathbf{P}<em>{n-1}=\mathbf{A} \mathbf{V}</em>{n-1} \mathbf{A}^{\mathrm{T}}+\Gamma\tag{13.88}
$$</p>
<p>から$\mathbf{P}<em>{N}=\mathbf{V}</em>{N}=\frac{1}{N} \Sigma$が成り立つ．</p>
<p>$\mathbf{C}=\mathbf{I}, \mathbf{\Gamma}=\mathbf{0}$と</p>
<p>$$
\mathbf{K}<em>{n}=\mathbf{P}</em>{n-1} \mathbf{C}^{\mathrm{T}}\left(\mathbf{C P}_{n-1} \mathbf{C}^{\mathrm{T}}+\boldsymbol{\Sigma}\right)^{-1}\tag{13.92}
$$</p>
<p>を用いて$N+1$のとき</p>
<p>$$
\begin{aligned}
\mathbf{K}<em>{N+1} &amp;=\mathbf{P}</em>{N} \mathbf{C}^{\mathrm{T}}\left(\mathbf{C P}<em>{N} \mathbf{C}^{\mathrm{T}}+\boldsymbol{\Sigma}\right)^{-1} \
&amp;=\mathbf{P}</em>{N}\left(\mathbf{P}_{N}+\boldsymbol{\Sigma}\right)^{-1} \
&amp;=\frac{1}{N} \boldsymbol{\Sigma}\left(\frac{1}{N} \boldsymbol{\Sigma}+\boldsymbol{\Sigma}\right)^{-1} \
&amp;=\frac{1}{N} \boldsymbol{\Sigma}\left(\frac{N+1}{N} \boldsymbol{\Sigma}\right)^{-1} \
&amp;=\frac{1}{N+1} \mathbf{I}
\end{aligned}
$$</p>
<p>この結果を(13.89)と(13.90)に代入して</p>
<p>$$
\begin{aligned}
\boldsymbol{\mu}<em>{N+1} &amp;=\mathbf{A} \mu</em>{N}+\mathbf{K}<em>{N＋1}\left(\mathbf{x}</em>{N＋1}-\mathbf{C A} \mu_{N}\right) \
&amp;=\boldsymbol{\mu}<em>{N}+\frac{1}{N+1}\left(\mathbf{x}</em>{N+1}-\boldsymbol{\mu}<em>{N}\right) \
&amp;=\overline{\mathbf{x}}</em>{N}+\frac{1}{N+1}\left(\mathbf{x}<em>{N+1}-\overline{\mathbf{x}}</em>{N}\right) \
&amp;=\frac{1}{N+1} \mathbf{x}<em>{N+1}+\left(1-\frac{1}{N+1}\right) \frac{1}{N} \sum</em>{n=1}^{N} \
&amp;=\frac{1}{N+1} \sum_{n=1}^{N+1} \mathbf{x}<em>{n}=\overline{\mathbf{x}}</em>{N+1} \
\mathbf{V}<em>{N+1} &amp;=\left(\mathbf{I}-\mathbf{K}</em>{N+1} \mathbf{C}\right) \mathbf{P}_{N} \
&amp;=\left(\mathbf{I}-\frac{1}{N+1} \mathbf{I}\right) \frac{1}{N} \boldsymbol{\Sigma} \
&amp;=\frac{1}{N+1} \boldsymbol{\Sigma}
\end{aligned}
$$</p>
<p>が成り立つ．
以上から$N+1$の場合でも$N$と同様の形式でかけるため全ての$N\geq 1$に対して成り立つ．</p>
<h2 id="演習-1329"><a class="header" href="#演習-1329">演習 13.29</a></h2>
<div class="panel-primary">
<p>ガウシアン線形動的システムにおいて、バックワード再帰式</p>
<p>$$
c_{n+1} \widehat{\beta}\left(\mathbf{z}<em>{n}\right)=\int \widehat{\beta}\left(\mathbf{z}</em>{n+1}\right) p\left(\mathbf{x}<em>{n+1} \mid \mathbf{z}</em>{n+1}\right) p\left(\mathbf{z}<em>{n+1} \mid \mathbf{z}</em>{n}\right) \mathrm{d} \mathbf{z}_{n+1} \tag{13.99}
$$</p>
<p>から出発して、RTS平滑化方程式</p>
<p>$$
\widehat{\boldsymbol{\mu}}<em>{n}=\boldsymbol{\mu}</em>{n}+\mathbf{J}<em>{n}\left(\widehat{\mu}</em>{n+1}-\mathbf{A} \mu_{n}\right) \tag{13.100}
$$</p>
<p>$$
\widehat{\mathbf{V}}<em>{n}=\mathbf{V}</em>{n}+\mathbf{J}<em>{n}\left(\widehat{\mathbf{V}}</em>{n+1}-\mathbf{P}<em>{n}\right) \mathbf{J}</em>{n}^{\mathrm{T}} \tag{13.101}
$$</p>
<p>を導け.</p>
</div>
<p>(13.99)の両辺に$\widehat{\alpha}\left(\mathbf{z}<em>{n}\right)$をかけて$\gamma\left(\mathbf{z}</em>{n}\right)=\widehat{\alpha}\left(\mathbf{z}<em>{n}\right) \widehat{\beta}\left(\mathbf{z}</em>{n}\right)=\mathcal{N}\left(\mathbf{z}<em>{n} \mid \widehat{\mu}</em>{n}, \widehat{\mathbf{V}}<em>{n}\right)$を用いると
$$
c</em>{n+1} \mathcal{N}\left(\mathbf{z}<em>{n} \mid \widehat{\boldsymbol{\mu}}</em>{n}, \widehat{\mathbf{V}}<em>{n}\right)=\widehat{\alpha}\left(\mathbf{z}</em>{n}\right) \int \widehat{\beta}\left(\mathbf{z}<em>{n+1}\right) p\left(\mathbf{x}</em>{n+1} \mid \mathbf{z}<em>{n+1}\right) p\left(\mathbf{z}</em>{n+1} \mid \mathbf{z}<em>{n}\right) \mathrm{d} \mathbf{z}</em>{n+1}\tag{1}
$$</p>
<p>が得られる．また</p>
<p>$$
p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}\right)=\mathcal{N}\left(\mathbf{z}<em>{n} \mid \mathbf{A z}</em>{n-1}, \mathbf{\Gamma}\right) \tag{13.75}
$$</p>
<p>と</p>
<p>$$
\widehat{\alpha}\left(\mathbf{z}<em>{n}\right)=\mathcal{N}\left(\mathbf{z}</em>{n} \mid \mu_{n}, \mathbf{V}_{n}\right)\tag{13.84}
$$</p>
<p>について(13.75)と(13.84)の辺々かけた式に対して</p>
<p>(2.113)-(2.117)の議論，</p>
<p>『$\mathrm{x}$ の周辺ガウス分布と, $\mathrm{x}$ が与えられたときの $\mathrm{y}$ の条件付きガウス分布が次式で 与えられたとする.
$$
p(\mathrm{x}) =\mathcal{N}\left(\mathrm{x} \mid \boldsymbol{\mu}, \boldsymbol{\Lambda}^{-1}\right)\tag{2.113}
$$</p>
<p>$$
p(\mathbf{y} \mid \mathrm{x}) =\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \mathbf{x}+\mathbf{b}, \mathbf{L}^{-1}\right)\tag{2.114}
$$
$\mathrm{y}$ の周辺分布と , $\mathrm{y}$ が与えられたときの $\mathrm{x}$ の条件付き分布は
$$
p(\mathbf{y}) =\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \boldsymbol{\mu}+\mathbf{b}, \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}\right) \tag{2.115}
$$</p>
<p>$$
p(\mathbf{x} \mid \mathbf{y}) =\mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\Sigma}\left{\mathbf{A}^{\mathrm{T}} \mathbf{L}(\mathbf{y}-\mathbf{b})+\boldsymbol{\Lambda} \boldsymbol{\mu}\right}, \boldsymbol{\Sigma}\right)\tag{2.116}
$$
で与えられる. ただし,
$$
\Sigma=\left(\Lambda+\mathrm{A}^{\mathrm{T}} \mathbf{L} \mathbf{A}\right)^{-1}\tag{2.117}
$$</p>
<p>である．』を適切に変数の対応をとり，$\mathbf{z}<em>n,\mathbf{z}</em>{n+1}$の同時分布について$\mathbf{z}<em>n$が与えられたときの$\mathbf{z}</em>{n+1}$と$\mathbf{z}<em>n$の積で表されていた形式をの同時分布について$\mathbf{z}</em>{n+1}$が与えられたときの$\mathbf{z}<em>{n}$と$\mathbf{z}</em>{n+1}$の積で表されていた形式に書き換えると</p>
<p>$$
\begin{aligned}
\widehat{\alpha}\left(\mathbf{z}<em>{n}\right) p\left(\mathbf{z}</em>{n+1} \mid \mathbf{z}<em>{n}\right) &amp;=\mathcal{N}\left(\mathbf{z}</em>{n} \mid \boldsymbol{\mu}<em>{n}, \mathbf{V}</em>{n}\right) \mathcal{N}\left(\mathbf{z}<em>{n+1} \mid \mathbf{A} \mathbf{z}</em>{n}, \boldsymbol{\Gamma}\right) \
&amp;=\mathcal{N}\left(\mathbf{z}<em>{n+1} \mid \mathbf{A} \boldsymbol{\mu}</em>{n}, \mathbf{A} \mathbf{V}<em>{n} \mathbf{A}+\boldsymbol{\Gamma}\right) \mathcal{N}\left(\mathbf{z}</em>{n} \mid \mathbf{m}<em>{n}, \mathbf{M}</em>{n}\right)
\end{aligned}
$$
が得られる．ここで(2.116)に対応する形で$\mathbf{m}<em>n$は
$$
\mathbf{m}</em>{n}=\mathbf{M}<em>{n}\left(\mathbf{A}^{\mathrm{T}} \boldsymbol{\Gamma}^{-1} \mathbf{z}</em>{n+1}+\mathbf{V}<em>{n}^{-1} \boldsymbol{\mu}</em>{n}\right)\tag{2}
$$</p>
<p>と表される．また(C.7)と(13.102) $\mathbf{J}<em>{n}=\mathbf{V}</em>{n} \mathbf{A}^{\mathrm{T}}\left(\mathbf{P}_{n}\right)^{-1}$の定義を用いて</p>
<p>$$
\begin{aligned}
\mathbf{M}<em>{n} &amp;=\left(\mathbf{A}^{\mathrm{T}} \boldsymbol{\Gamma}^{-1} \mathbf{A}+\mathbf{V}</em>{n}^{-1}\right)^{-1} \
&amp;=\mathbf{V}<em>{n}-\mathbf{V}</em>{n} \mathbf{A}^{\mathrm{T}}\left(\boldsymbol{\Gamma}+\mathbf{A} \mathbf{V}<em>{n} \mathbf{A}^{\mathrm{T}}\right)^{-1} \mathbf{A V}</em>{n} \
&amp;=\mathbf{V}<em>{n}-\mathbf{V}</em>{n} \mathbf{A}^{\mathrm{T}} \mathbf{P}<em>{n}^{-1} \mathbf{A} \mathbf{V}</em>{n} \
&amp;=\left(\mathbf{I}-\mathbf{V}<em>{n} \mathbf{A}^{\mathrm{T}} \mathbf{P}</em>{n}^{-1} \mathbf{A}\right) \mathbf{V}<em>{n} \
&amp;=\left(\mathbf{I}-\mathbf{J}</em>{n} \mathbf{A}\right) \mathbf{V}_{n}
\end{aligned}
$$</p>
<p>とかける．</p>
<p>(1)の右辺において$\widehat{\alpha}\left(\mathbf{z}_{n}\right)$を積分の中に含め，$\mathbf{M}_n$についての結果を代入して(13.85)-(13.88)と(13.98)を用いて</p>
<p>$$
\begin{aligned}
c_{n+1} \mathcal{N}\left(\mathbf{z}<em>{n} \mid \widehat{\boldsymbol{\mu}}</em>{n}, \widehat{\mathbf{V}}<em>{n}\right) &amp;=\int \widehat{\beta}\left(\mathbf{z}</em>{n+1}\right) p\left(\mathbf{x}<em>{n+1} \mid \mathbf{z}</em>{n+1}\right) \mathcal{N}\left(\mathbf{z}<em>{n+1} \mid \mathbf{A} \boldsymbol{\mu}</em>{n}, \mathbf{P}<em>{n}\right)
\mathcal{N}\left(\mathbf{z}</em>{n} \mid \mathbf{m}<em>{n}, \mathbf{M}</em>{n}\right) \mathrm{d} \mathbf{z}<em>{n+1} &amp; \
&amp;=\int \widehat{\beta}\left(\mathbf{z}</em>{n+1}\right) c_{n+1} \widehat{\alpha}\left(\mathbf{z}<em>{n+1}\right) \mathcal{N}\left(\mathbf{z}</em>{n} \mid \mathbf{m}<em>{n}, \mathbf{M}</em>{n}\right) \mathrm{d} \mathbf{z}<em>{n+1} \
&amp;=c</em>{n+1} \int \gamma\left(\mathbf{z}<em>{n+1}\right) \mathcal{N}\left(\mathbf{z}</em>{n} \mid \mathbf{m}<em>{n}, \mathbf{M}</em>{n}\right) \mathrm{d} \mathbf{z}<em>{n+1} \
&amp;=c</em>{n+1} \int \mathcal{N}\left(\mathbf{z}<em>{n+1} \mid \widehat{\boldsymbol{\mu}}</em>{n}, \widehat{\mathbf{V}}<em>{n}\right) \mathcal{N}\left(\mathbf{z}</em>{n} \mid \mathbf{m}<em>{n}, \mathbf{M}</em>{n}\right) \mathrm{d} \mathbf{z}<em>{n+1} .
\end{aligned}
$$
したがって(2)と(2.113)-(2.115)を使って
$$
\widehat{\boldsymbol{\mu}}</em>{n} =\mathbf{M}<em>{n}\left(\mathbf{A}^{\mathrm{T}} \boldsymbol{\Gamma}^{-1} \widehat{\boldsymbol{\mu}}</em>{n+1}+\mathbf{V}<em>{n}^{-1} \boldsymbol{\mu}</em>{n}\right) \tag{3}
$$</p>
<p>$$
\widehat{\mathbf{V}}<em>{n} =\mathbf{M}</em>{n} \mathbf{A}^{\mathrm{T}} \boldsymbol{\Gamma}^{-1} \widehat{\mathbf{V}}<em>{n+1} \boldsymbol{\Gamma}^{-1} \mathbf{A} \mathbf{M}</em>{n}+\mathbf{M}_{n}\tag{4}
$$</p>
<p>と書くことができる．</p>
<p>また$\mathbf{M}_{n}$についての計算の途中で出てきた</p>
<p>$$
\mathbf{M}<em>{n}=\mathbf{V}</em>{n}-\mathbf{V}<em>{n} \mathbf{A}^{\mathrm{T}} \mathbf{P}</em>{n}^{-1} \mathbf{A} \mathbf{V}_{n}
$$</p>
<p>と(13.102) $\mathbf{J}<em>{n}=\mathbf{V}</em>{n} \mathbf{A}^{\mathrm{T}}\left(\mathbf{P}_{n}\right)^{-1}$を使って</p>
<p>$$
\begin{aligned}
\mathbf{M}<em>{n} \mathbf{A}^{\mathrm{T}} \boldsymbol{\Gamma}^{-1} &amp;=\left(\mathbf{V}<em>n-\mathbf{V}</em>{n} \mathbf{A}^{\mathrm{T}} \mathbf{P}</em>{n}^{-1} \mathbf{A} \mathbf{V}<em>{n}\right) \mathbf{A}^{\mathrm{T}} \boldsymbol{\Gamma}^{-1} \
&amp;=\mathbf{V}</em>{n} \mathbf{A}^{\mathrm{T}}\left(\mathbf{I}-\mathbf{P}<em>{n}^{-1} \mathbf{A} \mathbf{V}</em>{n} \mathbf{A}^{\mathrm{T}}\right) \boldsymbol{\Gamma}^{-1} \
&amp;=\mathbf{V}<em>{n} \mathbf{A}^{\mathrm{T}}\left(\mathbf{I}-\mathbf{P}</em>{n}^{-1} \mathbf{A} \mathbf{V}<em>{n} \mathbf{A}^{\mathrm{T}}-\mathbf{P}</em>{n}^{-1} \boldsymbol{\Gamma}+\mathbf{P}<em>{n}^{-1} \boldsymbol{\Gamma}\right) \boldsymbol{\Gamma}^{-1} \
&amp;=\mathbf{V}</em>{n} \mathbf{A}^{\mathrm{T}}\left(\mathbf{I}-\mathbf{P}<em>{n}^{-1} \mathbf{P}</em>{n}+\mathbf{P}<em>{n}^{-1} \boldsymbol{\Gamma}\right) \boldsymbol{\Gamma}^{-1} \
&amp;=\mathbf{V}</em>{n} \mathbf{A}^{\mathrm{T}} \mathbf{P}<em>{n}^{-1}=\mathbf{J}</em>{n}
\end{aligned}
$$</p>
<p>これらの結果から(3)を(13.100)のように書き換えることができる
また(13.102), $\mathbf{M}<em>{n}=\mathbf{V}</em>{n}-\mathbf{V}<em>{n} \mathbf{A}^{\mathrm{T}} \mathbf{P}</em>{n}^{-1} \mathbf{A} \mathbf{V}<em>{n}$ と$\mathbf{M}</em>{n} \mathbf{A}^{\mathrm{T}} \boldsymbol{\Gamma}^{-1}=\mathbf{J}_{n}$を用いて(4)を</p>
<p>$$
\begin{aligned}
\widehat{\mathbf{V}}<em>{n} &amp;=\mathbf{M}</em>{n} \mathbf{A}^{\mathrm{T}} \boldsymbol{\Gamma}^{-1} \widehat{\mathbf{V}}<em>{n+1} \boldsymbol{\Gamma}^{-1} \mathbf{A} \mathbf{M}</em>{n}+\mathbf{M}<em>{n} \
&amp;=\mathbf{J}</em>{n} \widehat{\mathbf{V}}<em>{n+1} \mathbf{J}</em>{n}^{\mathrm{T}}+\mathbf{V}<em>{n}-\mathbf{V}</em>{n} \mathbf{A}^{\mathrm{T}} \mathbf{P}<em>{n}^{-1} \mathbf{A} \mathbf{V}</em>{n} \
&amp;=\mathbf{V}<em>{n}+\mathbf{J}</em>{n}\left(\widehat{\mathbf{V}}<em>{n+1}-\mathbf{P}</em>{n}\right) \mathbf{J}_{n}^{\mathrm{T}}
\end{aligned}
$$</p>
<p>のようにして(13.101)の形に書き直すことができる．</p>
<h2 id="演習-1330"><a class="header" href="#演習-1330">演習 13.30</a></h2>
<div class="panel-primary">
<p>状態空間モデルの2つ組の事後周辺分布の結果</p>
<p>$$
\xi\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right)=\left(c_{n}\right)^{-1} \widehat{\alpha}\left(\mathbf{z}<em>{n-1}\right) p\left(\mathbf{x}</em>{n} \mid \mathbf{z}<em>{n}\right) p\left(\mathbf{z}</em>{n} \mid \mathbf{z}<em>{n-1}\right) \widehat{\beta}\left(\mathbf{z}</em>{n}\right) \tag{13.65}
$$</p>
<p>から出発して、ガウシアン線形動的システムの場合の特別な形式</p>
<p>$$
\begin{aligned}
\xi\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right)&amp;=\left(c_{n}\right)^{-1} \widehat{\alpha}\left(\mathbf{z}<em>{n-1}\right) p\left(\mathbf{x}</em>{n} \mid \mathbf{z}<em>{n}\right) p\left(\mathbf{z}</em>{n} \mid \mathbf{z}<em>{n-1}\right) \widehat{\beta}\left(\mathbf{z}</em>{n}\right) \
&amp;=\frac{\mathcal{N}\left(\mathbf{z}<em>{n-1} \mid \pmb{\mu}</em>{n-1}, \mathbf{V}<em>{n-1}\right) \mathcal{N}\left(\mathbf{z}</em>{n} \mid \mathbf{A} \mathbf{z}<em>{n-1}, \mathbf{\Gamma}\right) \mathcal{N}\left(\mathbf{x}</em>{n} \mid \mathbf{C z}<em>{n}, \mathbf{\Sigma} \right) \mathcal{N}\left(\mathbf{z}</em>{n} \mid \widehat{\pmb{\mu}}<em>{n}, \widehat{\mathbf{V}}</em>{n}\right)}{c_{n} \widehat{\alpha}\left(\mathbf{z}_{n}\right)}
\end{aligned} \tag{13.103}
$$</p>
<p>を導け.</p>
</div>
<p>$(13.65)$式の導出は演習13.15を参照。
これに</p>
<p>$$
\begin{aligned}
p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}\right) &amp;=\mathcal{N}\left(\mathbf{z}<em>{n} \mid \mathbf{A} \mathbf{z}</em>{n-1}, \Gamma\right)&amp; (13.75) \
p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right) &amp;=\mathcal{N}\left(\mathbf{x}<em>{n} \mid \mathbf{C} \mathbf{z}</em>{n}, \mathbf{\Sigma}\right)&amp; (13.76) \
\widehat{\alpha}\left(\mathbf{z}<em>{n-1}\right)&amp;=\mathcal{N}\left(\mathbf{z}</em>{n-1} \mid \pmb{\mu}<em>{n-1}, \mathbf{V}</em>{n-1}\right) &amp; (13.84) \
\gamma\left(\mathbf{z}<em>{n}\right)&amp;=\widehat{\alpha}\left(\mathbf{z}</em>{n}\right) \widehat{\beta}\left(\mathbf{z}<em>{n}\right)=\mathcal{N}\left(\mathbf{z}</em>{n} \mid \widehat{\pmb{\mu}}<em>{n}, \widehat{\mathbf{V}}</em>{n}\right) &amp; (13.98)
\end{aligned}
$$</p>
<p>を組み合わせれば</p>
<p>$$
\xi\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right)=\frac{\mathcal{N}\left(\mathbf{z}<em>{n-1} \mid \pmb{\mu}</em>{n-1}, \mathbf{V}<em>{n-1}\right) \mathcal{N}\left(\mathbf{z}</em>{n} \mid \mathbf{A} \mathbf{z}<em>{n-1}, \mathbf{\Gamma}\right) \mathcal{N}\left(\mathbf{x}</em>{n} \mid \mathbf{C z}<em>{n}, \mathbf{\Sigma} \right) \mathcal{N}\left(\mathbf{z}</em>{n} \mid \widehat{\pmb{\mu}}<em>{n}, \widehat{\mathbf{V}}</em>{n}\right)}{c_{n} \widehat{\alpha}\left(\mathbf{z}_{n}\right)} \tag{13.103}
$$</p>
<p>が直接導出される。</p>
<h2 id="演習-1331"><a class="header" href="#演習-1331">演習 13.31</a></h2>
<div class="panel-primary">
<p>$$
\begin{aligned}
\xi\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right)&amp;=\left(c_{n}\right)^{-1} \widehat{\alpha}\left(\mathbf{z}<em>{n-1}\right) p\left(\mathbf{x}</em>{n} \mid \mathbf{z}<em>{n}\right) p\left(\mathbf{z}</em>{n} \mid \mathbf{z}<em>{n-1}\right) \widehat{\beta}\left(\mathbf{z}</em>{n}\right) \
&amp;=\frac{\mathcal{N}\left(\mathbf{z}<em>{n-1} \mid \pmb{\mu}</em>{n-1}, \mathbf{V}<em>{n-1}\right) \mathcal{N}\left(\mathbf{z}</em>{n} \mid \mathbf{A} \mathbf{z}<em>{n-1}, \mathbf{\Gamma}\right) \mathcal{N}\left(\mathbf{x}</em>{n} \mid \mathbf{C z}<em>{n}, \mathbf{\Sigma}\right) \mathcal{N}\left(\mathbf{z}</em>{n} \mid \widehat{\mu}<em>{n}, \widehat{\mathbf{V}}</em>{n}\right)}{c_{n} \widehat{\alpha}\left(\mathbf{z}_{n}\right)}
\end{aligned} \tag{13.103}
$$</p>
<p>の結果から出発し、</p>
<p>$$
\widehat{\alpha}\left(\mathbf{z}<em>{n}\right)=\mathcal{N}\left(\mathbf{z}</em>{n} \mid \boldsymbol{\mu}<em>{n}, \mathbf{V}</em>{n}\right) \tag{13.84}
$$</p>
<p>を用いて$\widehat{\alpha}\left(\mathbf{z}<em>{n}\right)$を置き換えることにより、$\mathbf{z}</em>{n}$と$\mathbf{z}_{n-1}$の間の共分散の結果</p>
<p>$$
\operatorname{cov}\left[\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right]=\mathbf{J}<em>{n-1} \widehat{\mathbf{V}}</em>{n} \tag{13.104}
$$</p>
<p>を確かめよ。</p>
</div>
<p>$(2.115)-(2.117)$を使用すると</p>
<p>$$
\begin{aligned}
&amp; \mathcal{N}\left(\mathbf{z}<em>{n-1} \mid \pmb{\mu}</em>{n-1}, \mathbf{V}<em>{n-1}\right) \mathcal{N}\left(\mathbf{z}</em>{n} \mid \mathbf{A} \mathbf{z}<em>{n-1}, \mathbf{\Gamma}\right) \
=&amp; \mathcal{N}\left(\mathbf{z}</em>{n} \mid \mathbf{A} \pmb{\mu}<em>{n-1}, \mathbf{\Gamma}+\mathbf{A} \mathbf{V}</em>{n-1} \mathbf{A}^{\mathrm T}\right) \times \mathcal{N}\left(\mathbf{z}<em>{n-1} \mid \mathbf{Y}\left(\mathbf{A}^{\mathrm T} \mathbf{\Gamma}^{-1} \mathbf{z}</em>{n}+\mathbf{V}<em>{n-1}^{-1} \pmb{\mu}</em>{n-1}\right), \mathbf{Y}\right)
\end{aligned}
$$</p>
<p>となる。ただし、</p>
<p>$$
\begin{aligned}
\mathbf{Y}&amp;=\left(\mathbf{V}<em>{n-1}^{-1}+\mathbf{A}^{\mathrm{T}} \mathbf{\Gamma}^{-1} \mathbf{A}\right)^{-1} \
&amp;=\mathcal{N}\left(\mathbf{z}</em>{n} \mid \mathbf{A} \mu_{n-1}, \mathbf{P}<em>{n-1}\right) \mathcal{N}\left(\mathbf{z}</em>{n-1} \mid \mathbf{J}<em>{n-1} \mathbf{z}</em>{n}+\left(\mathbf{I}-\mathbf{J}<em>{n-1} \mathbf{A}\right) \mu</em>{n-1},\left(\mathbf{I}-\mathbf{J}<em>{n-1} A\right) \mathbf{V}</em>{n-1}\right)
\end{aligned}
$$</p>
<p>である。上記式展開は(13.88）と</p>
<p>$$
\begin{aligned} \mathbf{Y}&amp;=\left(\mathbf{V}<em>{n-1}^{-1}+A^{\mathrm{T}
} \Gamma^{-1} A\right)^{-1} \ &amp;=\mathbf{V}</em>{n-1}-\mathbf{V}<em>{n-1} A^{\mathrm{T}
}\left(\Gamma+A \mathbf{V}</em>{n-1} A^{\mathrm{T}
}\right)^{-1} A \mathbf{V}<em>{n-1}\quad(C.7) \
&amp;=\mathbf{V}</em>{n-1}-\mathbf{V}<em>{n-1} A^{\mathrm{T}
} \mathbf{P}</em>{n-1}^{-1} A \mathbf{V}<em>{n-1}\quad(13.88) \
&amp;=\mathbf{V}</em>{n-1}-J_{n-1} A \mathbf{V}<em>{n-1}\quad(13.102) \
&amp;=\left(\mathbf{I}-J</em>{n-1} A\right) \mathbf{V}_{n-1}
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
\mathbf{Y}A^{\mathrm{T}} \Gamma^{-1} &amp;=\left(\mathbf{V}<em>{n-1}^{-1}+A^{\mathrm{T}} \Gamma^{-1} A\right)^{-1} A^{\mathrm{T}} \Gamma^{-1} \ &amp;=\mathbf{V}</em>{n-1} A^{\mathrm{T}}\left(A \mathbf{V}<em>{n-1} A^{\mathrm{T}}+\Gamma\right)^{-1}\quad(C.5) \
&amp;=\mathbf{V}</em>{n-1} A^{\mathrm{T}} \mathbf{P}<em>{n-1}^{-1}\quad(13.88) \
&amp;=J</em>{n-1}\quad(13.102)
\end{aligned}
$$</p>
<p>を使用した。</p>
<p>さらにここで$(2.115)-(2.117)$を使用すると</p>
<p>$$
\begin{aligned}
&amp;\ \mathcal{N}\left(\mathbf{z}<em>{n} \mid \mathbf{A} \pmb{\mu}</em>{n-1}, \mathbf{P}<em>{n-1}\right) \mathcal{N}\left(\mathbf{x}</em>{n} \mid \mathbf{C} \mathbf{z}<em>{n}, \mathbf{\Sigma}\right) \
=&amp;\ \mathcal{N}\left(\mathbf{x}</em>{n} \mid \mathbf{C A \mu _ { n - 1 }}, \mathbf{\Sigma}+\mathbf{C P _ { n - 1 }} \mathbf{C}^{\mathrm T}\right) \times \mathcal{N}\left(\mathbf{z}<em>{n} \mid \mathbf{M}\left(\mathbf{C}^{\mathrm T} \mathbf{\Sigma}^{-1} \mathbf{x}</em>{n}+\mathbf{P}<em>{n-1}^{-1} \mathbf{A} \pmb{\mu}</em>{n-1}\right), \mathbf{M}\right)
\end{aligned}
$$</p>
<p>ただし、
$$
\begin{aligned}
\mathbf{M}&amp;=\left(\mathbf{P}<em>{n-1}^{-1}+\mathbf{C}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \mathbf{C}\right)^{-1} \
&amp;=c</em>{n} \mathcal{N}\left(\mathbf{z}<em>{n} \mid \mathbf{K}</em>{n} \mathbf{x}<em>{n}+\left(\mathbf{I}-\mathbf{K}</em>{n} \mathbf{C}\right) \mathbf{P}<em>{n-1} \mathbf{P}</em>{n-1}^{-1} \mathbf{A} \mu_{n-1},\left(\mathbf{I}-\mathbf{K}<em>{n} \mathbf{C}\right) \mathbf{P}</em>{n-1}\right)\quad(13.91) \
&amp;=c_{n} \mathcal{N}\left(\mathbf{z}<em>{n} \mid \mathbf{A} \mu</em>{n-1}+\mathbf{K}<em>{n}\left(\mathbf{x}</em>{n}-\mathbf{C A} \mu_{n-1}\right),\left(\mathbf{I}-\mathbf{K}<em>{n} \mathbf{C}\right) \mathbf{P}</em>{n-1}\right) \
&amp;=c_{n} \mathcal{N}\left(\mathbf{z}<em>{n} \mid \boldsymbol{\mu}</em>{n}, \mathbf{V}<em>{n}\right)\quad(13.89)、（13.90） \
&amp;=c</em>{n} \hat{\alpha}\left(\mathbf{z}_{n}\right)
\end{aligned}
$$</p>
<p>上記1行目の式は</p>
<p>$$
\begin{aligned}
\mathbf{M} &amp;=\left(\mathbf{P}<em>{n-1}^{-1}+\mathbf{C}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \mathbf{C}\right)^{-1} \ &amp;=\mathbf{P}</em>{n-1}-\mathbf{P}<em>{n-1} \mathbf{C}^{\mathrm{T}}\left(\mathbf{\Sigma}+\mathbf{C} \mathbf{P}</em>{n-1} \mathbf{C}^{\mathrm{T}}\right)^{-1} \mathbf{C} \mathbf{P}<em>{n-1} \quad(C.7) \
&amp;=\mathbf{P}</em>{n-1}-\mathbf{K}<em>{n} \mathbf{C} \mathbf{P}</em>{n-1}\quad(13.92) \
&amp;=\left(\mathbf{I}-\mathbf{K}<em>{n} \mathbf{C}\right) \mathbf{P}</em>{n-1} \
\mathbf{M} \mathbf{C}^{\mathrm{T}} \mathbf{\Sigma}^{-1}&amp;=\left(\mathbf{P}<em>{n-1}^{-1}+\mathbf{C}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \mathbf{C}\right)^{-1} \mathbf{C}^{T} \mathbf{\Sigma}^{-1} \
&amp;=\mathbf{P}</em>{n-1} \mathbf{C}^{\mathrm{T}}\left(\mathbf{C} \mathbf{P}<em>{n-1} \mathbf{C}^{\mathrm{T}}+\mathbf{\Sigma}\right)^{-1}\quad(C.5) \
&amp;=\mathbf{K}</em>{n}\quad(13.92)
\end{aligned}
$$</p>
<p>これらを$(13.103)$に入れると</p>
<p>$$
\begin{aligned}
\xi\left(\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right)&amp;=\frac{\mathcal{N}\left(\mathbf{z}<em>{n-1} \mid \mathbf{J}</em>{n-1} \mathbf{z}<em>{n}+\left(\mathbf{\mathbf{I}}-\mathbf{J}</em>{n-1} \mathbf{A}\right) \pmb{\mu}<em>{n-1},\left(\mathbf{I}-\mathbf{J}</em>{n-1} \mathbf{A}\right) \mathbf{V}<em>{n-1}\right) c</em>{n} \hat{\alpha}\left(\mathbf{z}<em>{n}\right) \mathcal{N}\left(\mathbf{z}</em>{n} \mid \widehat{\pmb{\mu}}<em>{n}, \widehat{\mathbf{V}}</em>{n}\right)}{c_{n} \hat{\alpha}\left(\mathbf{z}<em>{n}\right)} \
&amp;=\mathcal{N}\left(\mathbf{z}</em>{n-1} \mid \mathbf{J}<em>{n-1} \mathbf{z}</em>{n}+\left(\mathbf{\mathbf{I}}-\mathbf{J}<em>{n-1} \mathbf{A}\right) \pmb{\mu}</em>{n-1},\left(\mathbf{\mathbf{I}}-\mathbf{J}<em>{n-1} \mathbf{A}\right) \mathbf{V}</em>{n-1}\right) \mathcal{N}\left(\mathbf{z}<em>{n} \mid \widehat{\pmb{\mu}}</em>{n}, \widehat{\mathbf{V}}_{n}\right)\quad(13.103)^{\prime}
\end{aligned}
$$</p>
<p>を得る。</p>
<p>$(13.103)^{\prime}$の右辺は線形ガウスモデルの周辺分布と条件分布の積になっているので2.3.3節より$\mathbf{z}<em>{n-1}$と$\mathbf{z}</em>{n}$の同時分布はガウス分布で$(2.108)$より平均は</p>
<p>$$
\begin{aligned}
\mathbf{E}\left[\left(\begin{array}{c}
\mathbf{z}<em>{n} \
\mathbf{z}</em>{n-1}
\end{array}\right)\right] &amp;=\left(\begin{array}{c}
\widehat{\pmb{\mu}}<em>{n} \
\mathbf{J}</em>{n-1} \widehat{\pmb{\mu}}<em>{n}+\left(\mathbf{I}-\mathbf{J}</em>{n-1} \mathbf{A}\right) \pmb{\mu}<em>{n-1}
\end{array}\right) \
&amp;=\left(\begin{array}{c}
\widehat{\pmb{\mu}}</em>{n} \
\pmb{\mu}<em>{n-1}+\mathbf{J}</em>{n-1}\left(\widehat{\pmb{\mu}}<em>{n}-\mathbf{A} \pmb{\mu}</em>{n-1}\right)
\end{array}\right) \
&amp;=\left(\begin{array}{c}
\widehat{\pmb{\mu}}<em>{n} \
\widehat{\pmb{\mu}}</em>{n-1}
\end{array}\right)\quad(13.100)
\end{aligned}
$$</p>
<p>となる。$(2.105)$より$\mathbf{z}<em>{n-1}$と$\mathbf{z}</em>{n}$の同時分布の共分散分布は</p>
<p>$$
\operatorname{cov}\left[\left(\begin{array}{l}
\mathbf{z}<em>{n} \
\mathbf{z}</em>{n-1}
\end{array}\right)\right]=\left(\begin{array}{cc}
\widehat{\mathbf{V}}<em>{n} &amp; \widehat{\mathbf{V}}</em>{n} \mathbf{J}<em>{n-1}^{\mathrm T} \
\mathbf{J}</em>{n-1} \widehat{\mathbf{V}}<em>{n} &amp; \left(\mathbf{I}-\mathbf{J}</em>{n-1} \mathbf{A}\right) \mathbf{V}<em>{n-1}+\mathbf{J}</em>{n-1} \widehat{\mathbf{V}}<em>{n} \mathbf{J}</em>{n-1}^{\mathrm T}
\end{array}\right)
$$</p>
<p>となる。$(2.78)$より、$\mathbf{z}<em>{n-1}$と$\mathbf{z}</em>{n}$の共分散は2行1列の要素になるので</p>
<p>$$
\operatorname{cov}\left[\mathbf{z}<em>{n-1}, \mathbf{z}</em>{n}\right]=\mathbf{J}<em>{n-1} \widehat{\mathbf{V}}</em>{n}\tag{13.104}
$$</p>
<p>を得る。</p>
<h2 id="演習-1332"><a class="header" href="#演習-1332">演習 13.32</a></h2>
<div class="panel-primary">
<p>線形動的システムにおける$\boldsymbol{\mu}<em>{0}$と$\mathbf{P}</em>{0}$に対するMステップの方程式の結果</p>
<p>$$
\boldsymbol{\mu}<em>{0}^{\text{new}}=\mathbb{E}\left[\mathbf{z}</em>{1}\right] \tag{13.110}
$$</p>
<p>$$
\mathbf{P}<em>{0}^{\text{new}}=\mathbb{E}\left[\mathbf{z}</em>{1} \mathbf{z}<em>{1}^{\mathrm{T}}\right]-\mathbb{E}\left[\mathbf{z}</em>{1}\right] \mathbb{E}\left[\mathbf{z}_{1}^{\mathrm{T}}\right] \tag{13.111}
$$
を確かめよ.</p>
</div>
<p>状態空間モデルの同時分布は</p>
<p>$$
p\left(\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{N}, \mathbf{z}<em>{1}, \ldots, \mathbf{z}</em>{N}\right)=p\left(\mathbf{z}<em>{1}\right)\left[\prod</em>{n=2}^{N} p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}\right)\right] \prod_{n=1}^{N} p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right) \tag{13.6}
$$</p>
<p>線形動的システム（LDS）を導入すると、$(13.75)(13.76)(13.77)$より</p>
<p>$$
\begin{aligned}
p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}\right) &amp;=\mathcal{N}\left(\mathbf{z}<em>{n} \mid \mathbf{A} \mathbf{z}</em>{n-1}, \mathbf{\Gamma}\right) \
p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}\right) &amp;=\mathcal{N}\left(\mathbf{x}<em>{n} \mid \mathbf{C} \mathbf{z}</em>{n}, \mathbf{\Sigma}\right) \
p\left(\mathbf{z}<em>{1}\right)&amp;=\mathcal{N}\left(\mathbf{z}</em>{1} \mid \boldsymbol{\mu}<em>{0}, \mathbf{P}</em>{0}\right)
\end{aligned}
$$</p>
<p>となる。P.361より完全データの対数尤度関数は、$(13.6)$の対数をとって</p>
<p>$$
\begin{aligned}
\ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})=&amp; \ln p\left(\mathbf{z}<em>{1} \mid \boldsymbol{\mu}</em>{0}, \mathbf{P}<em>{0}\right)+\sum</em>{n=2}^{N} \ln p\left(\mathbf{z}<em>{n} \mid \mathbf{z}</em>{n-1}, \mathbf{A}, \mathbf{\Gamma}\right) \
&amp;+\sum_{n=1}^{N} \ln p\left(\mathbf{x}<em>{n} \mid \mathbf{z}</em>{n}, \mathbf{C}, \mathbf{\Sigma}\right)
\end{aligned} \tag{13.108}
$$</p>
<p>次に事後分布について</p>
<p>$$
Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text {old}}\right)=\mathbb{E}_{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}[\ln p(\mathbf{X}, \mathbf{Z} \mid \boldsymbol{\theta})] \tag{13.109}
$$</p>
<p>これに代入すると</p>
<p>$$
\begin{aligned}
Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text {old}}\right)&amp;=\mathbb{E}<em>{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}\left[\ln \mathcal{N}\left(\mathbf{z}</em>{1} \mid \boldsymbol{\mu}<em>{0}, \mathbf{P}</em>{0}\right)+\sum_{n=2}^{N} \ln \mathcal{N}\left(\mathbf{z}<em>{n} \mid \mathbf{A}{\mathbf{z}</em>{n-1}}, \mathbf{\Gamma}\right)+\sum_{n=1}^{N} \ln \mathcal{N}\left(\mathbf{x}<em>{n} \mid \mathbf{Cz}</em>{n}, \mathbf{\Sigma}\right)\right] \
&amp;=\mathbb{E}<em>{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}\left[-\frac{1}{2} \ln \left|\mathbf{P}</em>{0}\right|-\frac{1}{2}\left(\mathbf{z}<em>{1}-\boldsymbol{\mu}</em>{0}\right)^{\mathrm T} \mathbf{P}<em>{0}^{-1}\left(\mathbf{z}</em>{1}-\boldsymbol{\mu}_{0}\right)\right]+\text { const. }
\end{aligned}
$$</p>
<p>ここで$\boldsymbol{\mu}<em>{0}$あるいは$\mathbf{P}</em>{0}$に依存しないすべての項はconst.項に吸収させている。
これを$\boldsymbol{\mu}_{0}$について最大化すると</p>
<p>$$
\begin{aligned}
\frac{\partial Q}{\partial \boldsymbol{\mu}<em>{0}}&amp;=\mathbb{E}</em>{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}\left[-\frac{1}{2} \cdot-2 \mathbf{P}<em>{0}^{-1}\left(\mathbf{z}</em>{1}-\boldsymbol{\mu}<em>{0}\right)\right]\
&amp;=\mathbf{P}</em>{0}^{-1}\left(\mathbb{E}<em>{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}\left[ \mathbf{z}</em>{1} \right]-\boldsymbol{\mu}<em>{0}\right)=0\
\boldsymbol{\mu}</em>{0}^{\text {new}}&amp;=\mathbb{E}<em>{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}\left[ \mathbf{z}</em>{1} \right]
\end{aligned}
$$</p>
<p>$\mathbf{P}_{0}$について最大化すると</p>
<p>$$
\frac{\partial Q}{\partial \mathbf{P}<em>{0}}=-\frac{1}{2}\left[\mathbf{P}</em>{0}^{-\mathrm{T}}+\mathbb{E}<em>{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}\left[-\mathbf{P}</em>{0}^{-\mathrm{T}}\left(\mathbf{z}<em>{1}-\boldsymbol{\mu}</em>{0}\right)\left(\mathbf{z}<em>{1}-\boldsymbol{\mu}</em>{0}\right)^{\mathrm{T}} \mathbf{P}_{0}^{-\mathrm{T}}\right]\right] =0
$$</p>
<p>$$
\begin{aligned}
\left(\mathbf{P}^{\textrm{new}}<em>{0}\right)^{-\mathrm{T}} &amp;=\mathbb{E}</em>{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}\left[\left(\mathbf{P}^{\textrm{new}}<em>{0}\right)^{-\mathrm{T}}\left(\mathbf{z}</em>{1}-\boldsymbol{\mu}<em>{0}\right)\left(\mathbf{z}</em>{1}-\boldsymbol{\mu}<em>{0}\right)^{\mathrm{T}} \left(\mathbf{P}^{\textrm{new}}</em>{0}\right)^{-\mathrm{T}}\right] \
\mathbf{P}^{\textrm{new}}<em>{0} &amp;=\mathbb{E}</em>{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}\left[\left(\mathbf{z}<em>{1}-\boldsymbol{\mu}</em>{0}\right)\left(\mathbf{z}<em>{1}-\boldsymbol{\mu}</em>{0}\right)^{\mathrm{T}}\right] \
&amp;=\mathbb{E}<em>{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}\left[\mathbf{z}</em>{1} \mathbf{z}<em>{1}^{\mathrm{T}}\right]-2 \mathbb{E}</em>{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}\left[\mathbf{z}<em>{1}\right] \boldsymbol{\mu}</em>{0}^{\mathrm{T}}+\boldsymbol{\mu}<em>{0} \boldsymbol{\mu}</em>{0}^{\mathrm{T}} \
&amp;=\mathbb{E}<em>{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}\left[\mathbf{z}</em>{1} \mathbf{z}<em>{1}^{\mathrm{T}}\right]-2 \mathbb{E}</em>{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}\left[\mathbf{z}<em>{1}\right] \mathbb{E}</em>{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}\left[\mathbf{z}<em>{1}\right]^{\mathrm{T}}+\mathbb{E}</em>{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}\left[\mathbf{z}<em>{1}\right] \mathbb{E}</em>{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}\left[\mathbf{z}<em>{1}\right]^{\mathrm{T}}\
&amp;=\mathbb{E}</em>{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}\left[\mathbf{z}<em>{1} \mathbf{z}</em>{1}^{\mathrm{T}}\right]-\mathbb{E}<em>{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}\left[\mathbf{z}</em>{1}\right] \mathbb{E}<em>{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}\left[\mathbf{z}</em>{1}\right]^{\mathrm{T}}
\end{aligned}
$$</p>
<h2 id="演習-1333"><a class="header" href="#演習-1333">演習 13.33</a></h2>
<div class="panel-primary">
<p>線形動的システムにおける$\mathbf{A}$と$\mathbf{\Gamma}$に対するMステップの方程式の結果</p>
<p>$$
\mathbf{A}^{\text{new}}=\left(\sum_{n=2}^{N} \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n-1}^{\mathrm{T}}\right]\right)\left(\sum_{n=2}^{N} \mathbb{E}\left[\mathbf{z}<em>{n-1} \mathbf{z}</em>{n-1}^{\mathrm{T}}\right]\right)^{-1} \tag{13.113}
$$</p>
<p>$$
\begin{aligned}
\mathbf{\Gamma}^{\text {new}}=&amp;\frac{1}{N-1} \sum_{n=2}^{N}\left{\mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm{T}}\right]-\mathbf{A}^{\text{new}} \mathbb{E}\left[\mathbf{z}<em>{n-1} \mathbf{z}</em>{n}^{\mathrm{T}}\right]\right. \
&amp; \left. -\ \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n-1}^{\mathrm{T}}\right]\left(\mathbf{A}^{\text{new}}\right)^{\mathrm{T}}+\mathbf{A}^{\text{new}} \mathbb{E}\left[\mathbf{z}<em>{n-1} \mathbf{z}</em>{n-1}^{\mathrm{T}}\right]\left(\mathbf{A}^{\text{new}}\right)^{\mathrm{T}}\right}
\end{aligned} \tag{13.114}
$$</p>
<p>を確かめよ.</p>
</div>
<p>演習13.32の$Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text{old}}\right)$のうち、$\mathcal{N}\left(\mathbf{z}<em>{n} \mid  \mathbf{Az}</em>{n-1}, \mathbf{\Gamma}\right)$に関係する項のみを抜き出して</p>
<p>$$
\begin{aligned}
Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text{old}}\right) &amp;=\mathbb{E}<em>{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}\left[\sum</em>{n=2}^{N} \ln \mathcal{N}\left(\mathbf{z}<em>{n} \mid  \mathbf{Az}</em>{n-1}, \mathbf{\Gamma}\right)\right]+\text{ const.} \
&amp;=\mathbb{E}<em>{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}\left[\sum</em>{n=2}^{N}\left{-\frac{1}{2} \ln |\mathbf{\Gamma}|-\frac{1}{2}\left(\mathbf{z}<em>{n}-\mathbf{Az}</em>{n-1}\right)^{\mathrm T} \mathbf{\Gamma}^{-1}\left(\mathbf{z}<em>{n}-\mathbf{Az}</em>{n-1}\right)\right}\right]+\text{ const.}
\end{aligned}
$$</p>
<p>$\mathbf{A}$について</p>
<p>$$
\begin{aligned}
\frac{\partial Q}{\partial \mathbf{A}} &amp;=-\frac{1}{2} \mathbb{E}<em>{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}\left[\sum</em>{n=2}^{N} \frac{\partial}{\partial \mathbf{A}}\left[\left(\mathbf{z}<em>{n}-\mathbf{Az}</em>{n-1}\right)^{\mathrm T} \mathbf{\Gamma}^{-1}\left(\mathbf{z}<em>{n}-\mathbf{Az}</em>{n-1}\right)\right]\right] \
&amp;=-\frac{1}{2} \mathbb{E}<em>{\mathbf{Z} \mid \boldsymbol{\theta}^{\text{old}}}\left[\sum</em>{n=2}^{N}\left(-2 \mathbf{\Gamma}^{-1}\left(\mathbf{z}<em>{n}-\mathbf{Az}</em>{n-1}\right) \mathbf{z}<em>{n-1}^{\mathrm T}\right)\right] \
&amp;=\sum</em>{n=2}^{N} \mathbf{\Gamma}^{-1} \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n-1}^{\mathrm T}\right]-\sum_{n=2}^{N} \mathbf{\Gamma}^{-1} \mathbf{A} \mathbb{E}\left[\mathbf{z}<em>{n-1} \mathbf{z}</em>{n-1}^{\mathrm T}\right]
\end{aligned}
$$</p>
<p>最大化するために$\frac{\partial Q}{\partial \mathbf{A}} =0$として</p>
<p>$$
\sum_{n=2}^{N} \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n-1}^{\mathrm T}\right] = \sum_{n=2}^{N} \mathbf{A}^{\text{new}} \mathbb{E}\left[\mathbf{z}<em>{n-1} \mathbf{z}</em>{n-1}^{\mathrm T}\right]
$$</p>
<p>$$
\mathbf{A}^{\text{new}}=\left(\sum_{n=2}^{N} \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n-1}^{\mathrm{T}}\right]\right)\left(\sum_{n=2}^{N} \mathbb{E}\left[\mathbf{z}<em>{n-1} \mathbf{z}</em>{n-1}^{\mathrm{T}}\right]\right)^{-1} \tag{13.113}
$$</p>
<p>同様に$\mathbf{\Gamma}$について</p>
<p>$$
\frac{\partial Q}{\partial \mathbf{\Gamma}}=-\frac{1}{2} \sum_{n=2}^{N}\left[\mathbf{\Gamma}^{-\mathrm T}+\mathbb{E}\left[-\mathbf{\Gamma}^{-\mathrm T}\left(\mathbf{z}<em>{n}-\mathbf{A} \mathbf{z}</em>{n-1}\right)\left(\mathbf{z}<em>{n}-\mathbf{A} \mathbf{z}</em>{n-1}\right)^{\mathrm T} \mathbf{\Gamma}^{-\mathrm T}\right]\right]=0
$$</p>
<p>最大化するために$\frac{\partial Q}{\partial \mathbf{\Gamma}}=0$として</p>
<p>$$
\begin{aligned} \sum_{n=2}^{N}\left(\mathbf{\Gamma}^{\text{new}}\right)^{-\mathrm{T}} &amp;=\sum_{n=2}^{N} \mathbb{E}\left[\left(\mathbf{\Gamma}^{\text{new}}\right)^{-\mathrm{T}}\left(\mathbf{z}<em>{n}-\mathbf{A}^{\text{new}} \mathbf{z}</em>{n-1}\right)\left(\mathbf{z}<em>{n}-\mathbf{A}^{\text{new}} \mathbf{z}</em>{n-1}\right)^{\mathrm T}\left(\mathbf{\Gamma}^{\text{new}}\right)^{-\mathrm{T}}\right] \ \sum_{n=2}^{N} \mathbf{\Gamma}^{\text{new}} &amp;=\sum_{n=2}^{N} \mathbb{E}\left[\left(\mathbf{z}<em>{n}-\mathbf{A}^{\text{new}} \mathbf{z}</em>{n-1}\right)\left(\mathbf{z}<em>{n}-\mathbf{A}^{\text{new}} \mathbf{z}</em>{n-1}\right)^{\mathrm T}\right] \
(N-1) \mathbf{\Gamma}^{\text{new}}&amp;=\sum_{n=2}^{N}\left{\mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm T}\right]-\mathbf{A}^{\text{new}} \mathbb{E}\left[\mathbf{z}<em>{n-1} \mathbf{z}</em>{n}^{\mathrm T}\right]-\mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n-1}^{\mathrm T}\right]\left(\mathbf{A}^{\text{new}}\right)^{\mathrm T}+\mathbf{A}^{\text{new}} \mathbb{E}\left[\mathbf{z}<em>{n-1} \mathbf{z}</em>{n-1}^{\mathrm T}\right] \mathbf{A}^{\text{new}}\right}
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
\mathbf{\Gamma}^{\text {new}}=&amp;\frac{1}{N-1} \sum_{n=2}^{N}\left{\mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm{T}}\right]-\mathbf{A}^{\text{new}} \mathbb{E}\left[\mathbf{z}<em>{n-1} \mathbf{z}</em>{n}^{\mathrm{T}}\right]\right. \
&amp; \left. -\ \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n-1}^{\mathrm{T}}\right]\left(\mathbf{A}^{\text{new}}\right)^{\mathrm{T}}+\mathbf{A}^{\text{new}} \mathbb{E}\left[\mathbf{z}<em>{n-1} \mathbf{z}</em>{n-1}^{\mathrm{T}}\right]\left(\mathbf{A}^{\text{new}}\right)^{\mathrm{T}}\right}
\end{aligned} \tag{13.114}
$$</p>
<h2 id="演習-1334"><a class="header" href="#演習-1334">演習 13.34</a></h2>
<div class="panel-primary">
<p>線形動的システムにおける$\mathbf{C}$と$\mathbf{\Sigma}$に対するMステップの方程式の結果</p>
<p>$$
\mathbf{C}^{\text{new}} = \left(\sum_{n=1}^{N} \mathbf{x}<em>{n} \mathbb{E}\left[\mathbf{z}</em>{n}^{\mathrm{T}}\right]\right)\left(\sum_{n=1}^{N} \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm{T}}\right]\right)^{-1} \tag{13.115}
$$</p>
<p>$$
\begin{aligned}
\mathbf{\Sigma}^{\text{new}} = &amp; \frac{1}{N} \sum_{n=1}^{N}\left{\mathbf{x}<em>{n} \mathbf{x}</em>{n}^{\mathrm{T}}-\mathbf{C}^{\text{new}} \mathbb{E}\left[\mathbf{z}<em>{n}\right] \mathbf{x}</em>{n}^{\mathrm{T}}\right.\ &amp;\left.-\mathbf{x}<em>{n} \mathbb{E}\left[\mathbf{z}</em>{n}^{\mathrm{T}}\right]\left(\mathbf{C}^{\text{new}}\right)^{\mathrm{T}}+\mathbf{C}^{\text{new}} \mathbb{E}\left[\mathbf{z}<em>{n} \mathbf{z}</em>{n}^{\mathrm{T}}\right]\left(\mathbf{C}^{\text{new}}\right)^{\mathrm{T}}\right}
\end{aligned} \tag{13.116}
$$</p>
<p>を確かめよ.</p>
</div>
<p>演習13.32の$Q\left(\pmb{\theta}, \pmb{\theta}^{\text{old}}\right)$と同様、$\mathbf{C}$と$\mathbf{\Sigma}$についての項を抜き出す。</p>
<p>$$
\begin{aligned}
Q\left(\pmb{\theta}, \pmb{\theta}^{\text{old}}\right) &amp;=\mathbb{E}<em>{\mathbf{Z} \mid \pmb{\theta}^{\text{old}}}\left[\sum</em>{n=1}^{N} \ln N\left(\mathbf{x}<em>{n} \mid \mathbf{Cz}</em>{n}, \mathbf{\Sigma}\right)\right]+\text { const.} \
&amp;=\mathbb{E}<em>{\mathbf{Z} \mid \pmb{\theta}^{\text{old}}}\left[\sum</em>{n=1}^{N}\left(-\frac{1}{2} \ln |\mathbf{\Sigma}|-\frac{1}{2}\left(\mathbf{x}<em>{n}-\mathbf{Cz}</em>{n}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\mathbf{x}<em>{n}-\mathbf{Cz}</em>{n}\right)\right)\right]+\text { const.}
\end{aligned}
$$</p>
<p>$\mathbf{C}$について、これは演習13.33の$\mathbf{A}^{\textrm{new}}$についての変形とほぼ同様に</p>
<p>$$
\frac{\partial Q}{\partial \mathbf{C}}=\sum_{n=1}^{N} \mathbf{\Sigma}^{-1} \mathbb{E}<em>{\mathbf{Z} \mid \pmb{\theta}^{\text{old}}}\left[\mathbf{x}</em>{n} \mathbf{z}<em>{n}^{\mathrm{T}}\right]-\sum</em>{n=1}^{N} \mathbf{\Sigma}^{-1} \mathbf{C} \mathbb{E}<em>{\mathbf{Z} \mid \pmb{\theta}^{\text{old}}}\left[\mathbf{z}</em>{n} \mathbf{z}_{n}^{\mathrm{T}}\right]
$$</p>
<p>$\frac{\partial Q}{\partial \mathbf{C}}=0$として</p>
<p>$$
\mathbf{C}^{\textrm{new}} \sum_{n=1}^{N} \mathbb{E}<em>{\mathbf{Z} \mid \pmb{\theta}^{\text{old}}}\left[\mathbf{x}</em>{n} \mathbf{z}<em>{n}^{\mathrm{T}}\right]=\sum</em>{n=1}^{N} \mathbf{x}<em>{n} \mathbb{E}</em>{\mathbf{Z} \mid \pmb{\theta}^{\text{old}}}\left[\mathbf{z}_{n}^{\mathrm{T}}\right]
$$</p>
<p>$$
\mathbf{C}^{\text{new}} = \left(\sum_{n=1}^{N} \mathbf{x}<em>{n} \mathbb{E}</em>{\mathbf{Z} \mid \pmb{\theta}^{\text{old}}}\left[\mathbf{z}<em>{n}^{\mathrm{T}}\right]\right)\left(\sum</em>{n=1}^{N} \mathbb{E}<em>{\mathbf{Z} \mid \pmb{\theta}^{\text{old}}}\left[\mathbf{z}</em>{n} \mathbf{z}_{n}^{\mathrm{T}}\right]\right)^{-1} \tag{13.115}
$$</p>
<p>同様に$\mathbf{\Sigma}$について</p>
<p>$$
\frac{\partial Q}{\partial \mathbf{\Sigma}}=-\frac{1}{2} \sum_{n=1}^{N}\left[\mathbf{\Sigma}^{-\mathrm{T}}+\mathbb{E}<em>{\mathbf{Z} \mid \pmb{\theta}^{\text{old}}}\left[-\mathbf{\Sigma}^{-\mathrm{T}}\left(\mathbf{x}</em>{n}-\mathbf{Cz}<em>{n}\right)\left(\mathbf{x}</em>{n}-\mathbf{Cz}_{n}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-\mathrm{T}}\right]\right]
$$</p>
<p>$\frac{\partial Q}{\partial \mathbf{\Sigma}}=0$として</p>
<p>$$
\sum_{n=1}^{N}\left( \mathbf{\Sigma}^{\textrm{new}} \right)^{-\mathrm{T}}=\sum_{n=1}^{N} \mathbb{E}<em>{\mathbf{Z} \mid \pmb{\theta}^{\text{old}}}\left[\left(\mathbf{\Sigma}^{\textrm{new}}\right)^{-\mathrm{T}}\left(\mathbf{x}</em>{n}-\mathbf{C}^{\textrm{new}}\mathbf{z}<em>{n}\right)\left(\mathbf{x}</em>{n}-\mathbf{C}^{\textrm{new}}\mathbf{z}_{n}\right)^{\mathrm{T}}\left(\mathbf{\Sigma}^{\textrm{new}}\right)^{-\mathrm{T}}\right]
$$</p>
<p>$$
\begin{aligned}
\mathbf{\Sigma}^{\text{new}} = &amp; \frac{1}{N} \sum_{n=1}^{N}\left{\mathbf{x}<em>{n} \mathbf{x}</em>{n}^{\mathrm{T}}-\mathbf{C}^{\text{new}} \mathbb{E}<em>{\mathbf{Z} \mid \pmb{\theta}^{\text{old}}}\left[\mathbf{z}</em>{n}\right] \mathbf{x}<em>{n}^{\mathrm{T}}\right.\ &amp;\left.-\mathbf{x}</em>{n} \mathbb{E}<em>{\mathbf{Z} \mid \pmb{\theta}^{\text{old}}}\left[\mathbf{z}</em>{n}^{\mathrm{T}}\right]\left(\mathbf{C}^{\text{new}}\right)^{\mathrm{T}}+\mathbf{C}^{\text{new}} \mathbb{E}<em>{\mathbf{Z} \mid \pmb{\theta}^{\text{old}}}\left[\mathbf{z}</em>{n} \mathbf{z}_{n}^{\mathrm{T}}\right]\left(\mathbf{C}^{\text{new}}\right)^{\mathrm{T}}\right}
\end{aligned} \tag{13.116}
$$</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="prml第14章演習問題解答"><a class="header" href="#prml第14章演習問題解答">PRML第14章演習問題解答</a></h1>
<head>
<style>
  div.panel-primary {
	border: 1px solid #000;
    margin: 10px 5px;
    padding: 16px 10px 0px;
  }
</style>
</head>
<h2 id="演習-141-1"><a class="header" href="#演習-141-1">演習 14.1</a></h2>
<div class="panel-primary">
<p>モデルの集合$p\left(\mathbf{t} \mid \mathbf{x}, \mathbf{z}<em>{h}, \boldsymbol{\theta}</em>{h}, h\right)$を考える。ただし、$\mathbf{x}$は入力ベクトル、$\mathbf{t}$は目標ベクトル、$h$はモデルのインデックス、$\mathbf{z}<em>h$はモデル$h$の潜在変数、そして$\boldsymbol{\theta}</em>{h}$はモデル$h$のパラメータ集合とする。そして、モデルの事前確率を$p(h)$とし、訓練集合として$\mathbf{X}=\left{\mathbf{x}<em>{1}, \ldots, \mathbf{x}</em>{N}\right}$と$\mathbf{T}=\left{\mathbf{t}<em>{1}, \ldots, \mathbf{t}</em>{N}\right}$を与える。潜在変数とモデルインデックスを周辺化した予測分布$p(\mathbf{t} \mid \mathbf{x},\mathbf{X},\mathbf{T})$を求める式を書き下せ。この式を利用して、異なるモデルのベイズ平均化と、一つのモデル内での潜在変数を利用することの違いを強調せよ。</p>
</div>
<p>※かなり難解なのであっているかわかりません。公式解答例ではベイズモデル平均化とモデル結合をまとめて書いた形で書き下していますが、ここでは分けて考えることにします。</p>
<p>P.372の議論参照。目的はモデルの結合とベイズモデル平均化の違いを理解することである。</p>
<p>問題文の設定から、モデルの集合を$p\left(\mathbf{t} \mid \mathbf{x}, \mathbf{z}<em>{h}, \boldsymbol{\theta}</em>{h}, h\right)$としている。モデル$h=1,2,\ldots,H$について、それぞれ$\mathbf{z}<em>{h}$という潜在変数と$\boldsymbol{\theta}</em>{h}$というパラメータ集合（例えば混合ガウスモデルでは平均$\boldsymbol{\mu}$と分散$\mathbf{\Sigma}$など）が存在している。このモデルが訓練集合$(\mathbf{\mathbf{x}<em>{n}},\mathbf{t}</em>{n})$と新しい入力ベクトル$\mathbf{x}$とその目標ベクトル$\mathbf{t}$のペア$(\mathbf{x}, \mathbf{t})$を生成している。</p>
<p>(1) ベイズモデル平均化の場合</p>
<p>これは$h$という変数がモデルの不確実性を表し、真の1つのモデルからデータ集合が生成されるので、有向グラフを書くと</p>
<p><img src="https://i.imgur.com/LKL41Na.png" alt="" /></p>
<p>となる。これについて有向分離を用いると</p>
<p>$$
\begin{aligned}
p(\mathbf{t} \mid \mathbf{x}, \mathbf{X}, \mathbf{T}) &amp;=\sum_{h} \sum_{\mathbf{z}<em>{h}} \int p\left(\mathbf{t}, h, \mathbf{z}</em>{h}, \boldsymbol{\theta}<em>{h} \mid \mathbf{x}, \mathbf{X}, \mathbf{T}\right) d \boldsymbol{\theta}</em>{h} \quad (\because 加法定理)\
&amp;=\sum_{h} \sum_{\mathbf{z}<em>{h}} \int p\left(\mathbf{t} \mid h, \mathbf{z}</em>{h}, \boldsymbol{\theta}<em>{h}, \mathbf{x}, \mathbf{X}, \mathbf{T}\right) p\left(h, \mathbf{z}</em>{h}, \boldsymbol{\theta}<em>{h} \mid  \mathbf{x}, \mathbf{X}, \mathbf{T}\right) d \boldsymbol{\theta}</em>{h} \quad (\because 乗法定理)\
&amp;=\sum_{h} \sum_{\mathbf{z}<em>{h}} \int p\left(\mathbf{t} \mid h, \mathbf{z}</em>{h}, \boldsymbol{\theta}<em>{h}, \mathbf{x}\right) p\left(h, \mathbf{z}</em>{h}, \boldsymbol{\theta}<em>{h} \mid \mathbf{X}, \mathbf{T}\right) d \boldsymbol{\theta}</em>{h}\quad (\because 有向分離)\
\end{aligned}
$$</p>
<p>この$p\left(h, \mathbf{z}<em>{h}, \boldsymbol{\theta}</em>{h} \mid \mathbf{X}, \mathbf{T}\right)$について</p>
<p>$$
\begin{aligned}
p\left(h, \mathbf{z}<em>{h}, \boldsymbol{\theta}</em>{h} \mid \mathbf{X}, \mathbf{T}\right) &amp;=\frac{p\left(h, \mathbf{z}<em>{h}, \boldsymbol{\theta}</em>{h}, \mathbf{X}, \mathbf{T}\right)}{p(\mathbf{X}, \mathbf{T})} \
&amp; \propto p\left(\mathbf{X}, \mathbf{T} \mid h, \mathbf{z}<em>{h}, \boldsymbol{\theta}</em>{h}\right) p\left(h, \mathbf{z}<em>{h}, \boldsymbol{\theta}</em>{h}\right) \quad (\because 乗法定理)\
&amp;=p\left(\mathbf{X}, \mathbf{T} \mid h, \mathbf{z}<em>{h}, \boldsymbol{\theta}</em>{h}\right) p(h) p\left(\mathbf{z}<em>{h}\right) p\left(\boldsymbol{\theta}</em>{h}\right)
\end{aligned}
$$</p>
<p>まとめると、</p>
<p>$$
p(\mathbf{t} \mid \mathbf{x}, \mathbf{X}, \mathbf{T})=\frac{1}{p(\mathbf{X},\mathbf{T})}\sum_{h=1}^{H} p(h) \sum_{\mathbf{z}<em>{h}} p\left(\mathbf{z}</em>{h}\right) \int p\left(\mathbf{t} \mid \mathbf{x}, \boldsymbol{\theta}<em>{h}, \mathbf{z}</em>{h}, h\right) p\left(\mathbf{X}, \mathbf{T} \mid h, \mathbf{z}<em>{h}, \boldsymbol{\theta}</em>{h}\right)p(\boldsymbol{\theta}<em>{h})d \boldsymbol{\theta}</em>{h}
$$</p>
<p>(2) モデル結合の場合</p>
<p>1つのモデル$h$につき、観測された点$\mathbf{t}<em>{n}$ごとに対応する$\mathbf{z}</em>{h}$が存在することが大きな違い。よって、$h$個のモデルでは$n\times h$個の潜在変数$\mathbf{z}_{hn}$が存在することになる。有向グラフはこんな感じになると思われる。</p>
<p><img src="https://i.imgur.com/Kq0POzk.png" alt="" /></p>
<p>$$
\begin{aligned}
p(\mathbf{t} \mid \mathbf{x}, \mathbf{X}, \mathbf{T}) &amp;= \sum_{h_{n}} \sum_{h} \sum_{\mathbf{z}<em>{hn}} \sum</em>{\mathbf{z}<em>{h}} \int p\left(\mathbf{t}, \boldsymbol{\theta}</em>{h}, h_{n}, h,\mathbf{z}<em>{hn}, \mathbf{z}</em>{h} \mid \mathbf{x}, \mathbf{X}, \mathbf{T}\right) d \boldsymbol{\theta}<em>{h} \
&amp;=\sum</em>{h_{n}} \sum_{h} \sum_{\mathbf{z}<em>{hn}} \sum</em>{\mathbf{z}<em>{h}} \int p\left(\mathbf{t} \mid \boldsymbol{\theta}</em>{h}, h_{n}, h,\mathbf{z}<em>{hn}, \mathbf{z}</em>{h}, \mathbf{x}, \mathbf{X}, \mathbf{T}\right) \
&amp;\phantom{=\sum_{h_{n}} \sum_{h} \sum_{\mathbf{z}<em>{hn}} \sum</em>{\mathbf{z}<em>{h}} \int}p\left(\boldsymbol{\theta}</em>{h}, h_{n}, h,\mathbf{z}<em>{hn}, \mathbf{z}</em>{h} \mid \mathbf{x}, \mathbf{X}, \mathbf{T}\right) d \boldsymbol{\theta}<em>{h} \
&amp;=\sum</em>{h_{n}} \sum_{h} \sum_{\mathbf{z}<em>{hn}} \sum</em>{\mathbf{z}<em>{h}} \int p\left(\mathbf{t} \mid \mathbf{x}, \boldsymbol{\theta}</em>{h}, \mathbf{z}<em>{h}, h \right) \
&amp;\phantom{=\sum</em>{h_{n}} \sum_{h} \sum_{\mathbf{z}<em>{hn}} \sum</em>{\mathbf{z}<em>{h}} \int}p\left(\boldsymbol{\theta}</em>{h}, h_{n}, h,\mathbf{z}<em>{hn}, \mathbf{z}</em>{h} \mid \mathbf{X}, \mathbf{T}\right) d \boldsymbol{\theta}_{h} \quad (\because 有向分離)\
\end{aligned}
$$</p>
<p>この$p\left(\boldsymbol{\theta}<em>{h}, h</em>{n}, h,\mathbf{z}<em>{hn}, \mathbf{z}</em>{h} \mid \mathbf{X}, \mathbf{T}\right) d \boldsymbol{\theta}_{h}$について</p>
<p>$$
\begin{aligned}
p\left(\boldsymbol{\theta}<em>{h}, h</em>{n}, h,\mathbf{z}<em>{hn}, \mathbf{z}</em>{h} \mid \mathbf{X}, \mathbf{T}\right)  &amp;= \frac{p\left(\boldsymbol{\theta}<em>{h}, h</em>{n}, h,\mathbf{z}<em>{hn}, \mathbf{z}</em>{h}, \mathbf{X}, \mathbf{T}\right)}{p(\mathbf{X},\mathbf{T})} \
&amp;\propto p\left(\boldsymbol{\theta}<em>{h}, h</em>{n}, h,\mathbf{z}<em>{hn}, \mathbf{z}</em>{h}, \mathbf{X}, \mathbf{T}\right) \
&amp;=p\left(\mathbf{X}, \mathbf{T} \mid h_{n}, h, \boldsymbol{\theta}<em>{h},\mathbf{z}</em>{hn},\mathbf{z}<em>{h}\right) p\left( h</em>{n}, h, \boldsymbol{\theta}<em>{h},\mathbf{z}</em>{hn},\mathbf{z}<em>{h} \right) \
&amp;=p\left(\mathbf{X}, \mathbf{T} \mid h</em>{n},\mathbf{z}<em>{hn}, \boldsymbol{\theta}</em>{h}\right) p\left(h_{n}\right) p(h) p\left(\boldsymbol{\theta}<em>{h}\right) p(\mathbf{z}</em>{hn}) p\left(\mathbf{z}_{h}\right) \
\end{aligned}
$$</p>
<p>以上をまとめると</p>
<p>$$
\begin{aligned}
p(\mathbf{t} \mid \mathbf{x}, \mathbf{X}, \mathbf{T}) &amp;= \frac{1}{p(\mathbf{X},\mathbf{T})}\sum_{h=1}^{H} p(h) \sum_{\mathbf{z}<em>{h}}p\left(\mathbf{z}</em>{h}\right)\int p\left(\mathbf{t} \mid \mathbf{x}, \boldsymbol{\theta}<em>{h}, \mathbf{z}</em>{h}, h\right) \
&amp;\phantom{\frac{1}{p(\mathbf{X},\mathbf{T})}\sum_{h=1}^{H} p(h) \sum_{\mathbf{z}<em>{h}}p\left(\mathbf{z}</em>{h}\right)\int}\left{\sum_{h_{n}=1}^{H} \sum_{z_{hn}} p\left(\mathbf{X}, \mathbf{T} \mid h_{n}, \mathbf{z}<em>{hn}, \boldsymbol{\theta}</em>{h}\right) p\left(\mathbf{z}<em>{hn}\right) p\left(h</em>{n}\right) p\left(\boldsymbol{\theta}<em>{h}\right)\right} d \boldsymbol{\theta}</em>{h}
\end{aligned}
$$</p>
<h2 id="演習-142"><a class="header" href="#演習-142">演習 14.2</a></h2>
<div class="panel-primary">
<p>単純なコミッティモデルの二乗和誤差$E_{\textrm{AV}}$の期待値は</p>
<p>$$
E_{\mathrm{AV}}=\frac{1}{M} \sum_{m=1}^{M} \mathbb{E}<em>{\mathbf{x}}\left[\epsilon</em>{m}(\mathbf{x})^{2}\right] \tag{14.10}
$$</p>
<p>で定義され、コミッティそれ自体の誤差の期待値は</p>
<p>$$
\begin{aligned} E_{\mathrm{COM}} &amp;=\mathbb{E}<em>{\mathbf{x}}\left[\left{\frac{1}{M} \sum</em>{m=1}^{M} y_{m}(\mathbf{x})-h(\mathbf{x})\right}^{2}\right] \ &amp;=\mathbb{E}<em>{\mathbf{x}}\left[\left{\frac{1}{M} \sum</em>{m=1}^{M} \epsilon_{m}(\mathbf{x})\right}^{2}\right] \end{aligned} \tag{14.11}
$$</p>
<p>で与えられる。個別の誤差が</p>
<p>$$
\mathbb{E}<em>{\mathbf{x}}\left[\epsilon</em>{m}(\mathbf{x})\right]=0 \tag{14.12}
$$</p>
<p>と</p>
<p>$$
\mathbb{E}<em>{\mathbf{x}}\left[\epsilon</em>{m}(\mathbf{x}) \epsilon_{l}(\mathbf{x})\right]=0, \quad m \neq l \tag{14.13}
$$</p>
<p>を満足することを仮定して、</p>
<p>$$
E_{\mathrm{COM}}=\frac{1}{M} E_{\mathrm{AV}} \tag{14.14}
$$</p>
<p>の結果を導け。</p>
</div>
<p>$(14.11)$より</p>
<p>$$
\begin{aligned}
E_{\text {COM }} &amp;=\mathbb{E}<em>{\mathbf{x}}\left[\left{\frac{1}{M} \sum</em>{m=1}^{M} \epsilon_{m}(\mathbf{x})\right}^{2}\right] \
&amp;=\frac{1}{M^{2}} \mathbb{E}<em>{\mathbf{x}}\left[\left(\epsilon</em>{1}(\mathbf{x})+\cdots \cdots \epsilon_{M}(\mathbf{x})\right)^{2}\right] \
&amp;=\frac{1}{M^{2}} \mathbb{E}<em>{\mathbf{x}}\left[\epsilon</em>{1}^{2}(\mathbf{x})+\cdots \epsilon_{M}^{2}(\mathbf{x})+\epsilon_{1}(\mathbf{x}) \epsilon_{2}(\mathbf{x})+\cdots+\epsilon_{M}(\mathbf{x}) \epsilon_{M-1}(\mathbf{x})\right] \
&amp;=\frac{1}{M^{2}}\left{\mathbb{E}<em>{\mathbf{x}}\left[\epsilon</em>{1}^{2}(\mathbf{x})\right]+\cdots \mathbb{E}<em>{\mathbf{x}}\left[\epsilon</em>{M}^{2}(\mathbf{x})\right]+\mathbb{E}<em>{\mathbf{x}}\left[\epsilon</em>{1}(\mathbf{x}) \epsilon_{2}(\mathbf{x})\right]+\cdots \mathbb{E}<em>{\mathbf{x}}\left[\epsilon</em>{M}(\mathbf{x}) \epsilon_{M-1}(\mathbf{x})\right]\right} \
&amp;=\frac{1}{M^{2}} \sum_{m=1}^{M} \mathbb{E}<em>{\mathbf{x}}\left[\epsilon</em>{m}^{2}(\mathbf{x})\right] \qquad(\because (14.13)) \
&amp;=\frac{1}{M} E_{\mathrm{AV}}\qquad(14.10)
\end{aligned}
$$</p>
<p>よって$(14.14)$が導かれた。</p>
<h2 id="演習-143"><a class="header" href="#演習-143">演習 14.3</a></h2>
<div class="panel-primary">
<p>イェンセンの不等式</p>
<p>$$
f\left(\sum_{i=1}^{M} \lambda_{i} x_{i}\right) \leqslant \sum_{i=1}^{M} \lambda_{i} f\left(x_{i}\right) \tag{1.115}
$$</p>
<p>を利用することで、$f(x) = x^2$という凸関数について、</p>
<p>$$
E_{\mathrm{AV}}=\frac{1}{M} \sum_{m=1}^{M} \mathbb{E}<em>{\mathbf{x}}\left[\epsilon</em>{m}(\mathbf{x})^{2}\right] \tag{14.10}
$$</p>
<p>で与えられる単純なコミッティモデルのメンバーの二乗和誤差の平均期待値$E_{\textrm{AV}}$、そして</p>
<p>$$
\begin{aligned} E_{\mathrm{COM}} &amp;=\mathbb{E}<em>{\mathbf{x}}\left[\left{\frac{1}{M} \sum</em>{m=1}^{M} y_{m}(\mathbf{x})-h(\mathbf{x})\right}^{2}\right] \ &amp;=\mathbb{E}<em>{\mathbf{x}}\left[\left{\frac{1}{M} \sum</em>{m=1}^{M} \epsilon_{m}(\mathbf{x})\right}^{2}\right] \end{aligned} \tag{14.11}
$$</p>
<p>で与えられるコミッティそれ自体の誤差$E_{\textrm{COM}}$の期待値について以下が成り立つことを示せ.</p>
<p>$$
E_{\textrm{COM}} \leq E_{\textrm{AV}} \tag{14.54}
$$</p>
</div>
<p>イェンセンの不等式</p>
<p>$$
f\left(\sum_{i=1}^{M} \lambda_{i} x_{i}\right) \leqslant \sum_{i=1}^{M} \lambda_{i} f\left(x_{i}\right)
$$</p>
<p>ただし、$f$は凸関数において</p>
<p>$$
\begin{gathered}
\lambda_{i}=\frac{1}{M},x_{i}=\epsilon_{i}(x), f(x)=x^{2} \
\left(\sum_{i=1}^{M} \frac{1}{M} \epsilon_{i}(x)\right)^{2} \leqq \sum_{i=1}^{M} \frac{1}{M} \epsilon_{i}^{2}(x)
\end{gathered}
$$</p>
<p>を得る。</p>
<p>両辺に$p(x)$をかけて$x$で積分すると
$$
\int p(x)\left(\sum_{i=1}^{M} \frac{1}{M} \epsilon_{i}(x)d x\right) ^{2}\leqq \int p(x) \sum_{i=1}^{M} \frac{1}{M} \epsilon_{i}(x)^{2} d x
$$</p>
<p>となる。これより</p>
<p>$$
\begin{aligned}
&amp;\mathbb{E}<em>{\mathbf{x}}\left[\left(\sum</em>{i=1}^{M} \frac{1}{M} \epsilon_{i}(x)\right)^{2}\right] \leqq \mathbb{E}<em>{\mathbf{x}}\left[\sum</em>{i=1}^{M} \frac{1}{M} \epsilon_{i}(x)^{2}\right] \
&amp;\therefore \quad \mathbb{E}<em>{\mathbf{x}}\left[\left(\frac{1}{M} \sum</em>{i=1}^{M} \epsilon_{i}(x)\right)^{2}\right] \leqq \frac{1}{M} \sum_{i=1}^{M} \mathbb{E}<em>{\mathbf{x}}\left[\epsilon</em>{i}(x)^{2}\right]
\end{aligned}
$$</p>
<p>(14.10),(14.11)を入れて</p>
<p>$$
E_{\operatorname{COM}} \leqq E_{A V}
$$</p>
<h2 id="演習-144"><a class="header" href="#演習-144">演習 14.4</a></h2>
<div class="panel-primary">
<p>イェンセンの不等式</p>
<p>$$
f\left(\sum_{i=1}^{M} \lambda_{i} x_{i}\right) \leqslant \sum_{i=1}^{M} \lambda_{i} f\left(x_{i}\right) \tag{1.115}
$$</p>
<p>を利用することで、前の演習問題で得られた</p>
<p>$$
E_{\textrm{COM}} \leq E_{\textrm{AV}} \tag{14.54}
$$</p>
<p>の結果が、二乗和誤差以外の任意の$y$の凸関数となる誤差関数$E(y)$においても成り立つことを示せ。</p>
</div>
<p>イェンセンの不等式により、</p>
<p>$$
\begin{aligned}
E_{\mathrm{AV}} &amp;:= \frac{1}{M} \sum_{m=1}^{M} \mathbb{E}<em>{\mathbf{x}}\left[E(y(\mathbf{x}))\right] \
&amp;= \mathbb{E}</em>{\mathbf{x}}\left[ \sum_{m=1}^{M} \frac{1}{M} E(y(\mathbf{x}))\right] \
&amp;\geq \mathbb{E}<em>{\mathbf{x}}\left[ E\left(\sum</em>{m=1}^{M} \frac{1}{M} y(\mathbf{x})\right)\right] \
&amp;=: E_{\mathrm{COM}}
\end{aligned}
$$</p>
<p>となる。なお、一般の誤差関数に対する$E_{\mathrm AV}$と$E_{\mathrm COM}$は、$(14.10)$式と$(14.11)$式に則して定義した。</p>
<h2 id="演習-145"><a class="header" href="#演習-145">演習 14.5</a></h2>
<div class="panel-primary">
<p>構成要素のモデルに一様でない重み付けをしたコミッティとして</p>
<p>$$
y_{\operatorname{COM}}(\mathbf{x})=\sum_{m=1}^{M} \alpha_{m} y_{m}(\mathbf{x}) \tag{14.55}
$$</p>
<p>を考える。予測$y_{\textrm{COM}}(\mathbf{x})$を理にかなった限界内に収まることを保証するため、次式のように個々の$\mathbf{x}$の値がコミッティのメンバーの最小値と最大値の間に制約されることを要求したい</p>
<p>$$
y_{\min }(\mathbf{x}) \leqslant y_{\operatorname{COM}}(\mathbf{x}) \leqslant y_{\max }(\mathbf{x}) \tag{14.56}
$$</p>
<p>この制約の必要十分条件が、係数$\alpha_{m}$が次式を満たすことであることを示せ。</p>
<p>$$
\alpha_{m} \geqslant 0, \quad \sum_{m=1}^{M} \alpha_{m}=1 \tag{14.57}
$$</p>
</div>
<p>まず(14.57)が(14.56)の十分条件であることを示す。
ある$y_m(\mathbf{x})$の集合を考え(14.57)を満たす範囲で$a_m$を変化させて得られる$y_{\mathrm{COM}}(\mathbf{x})$を考える。</p>
<p>$y_{\mathrm{COM}}(\mathbf{x})$が最大となるのは$y_k(\mathbf{x})\geq y_m(\mathbf{x})$となる$k$で$\alpha_{k}=1$、$k$以外で $\alpha_{m}=0$ となるときで，このとき $y_{\mathrm{COM}}(\mathbf{x})=y_{\max }(\mathbf{x})$である。
最小値についても同様に考えることで示すことができる。</p>
<p>その他の場合の$\boldsymbol{\alpha}$について</p>
<p>$$
y_{\min }(\mathbf{x})&lt;y_{\mathrm{COM}}(\mathbf{x})&lt;y_{\max }(\mathbf{x})
$$</p>
<p>を示す。$y_{\mathrm{COM}}(\mathbf{x})$は以下の式を満たすような$y_m(\mathbf{x})$の凸結合である。</p>
<p>$$
\forall m: y_{\min }(\mathbf{x}) \leqslant y_{m}(\mathbf{x}) \leqslant y_{\max }(\mathbf{x})
$$</p>
<p>したがって(14.57)は(14.56)の十条件である。</p>
<p>次に(14.57)が(14.56)の必要条件であることを示す。これは(14.56)が(14.57)の十分条件であることを示せば良い。</p>
<p>(14.56)を満たす任意の$\left{y_{m}(\mathbf{x})\right}$ を選んだとき(14.57)が成り立つことを示す。</p>
<p>$\alpha_k$が$\mathbf{\alpha}$で最小の値を取るとき，すなわち$k \neq m$のkについて$\alpha_{k} \leqslant \alpha_{m}$であるとき</p>
<p>このとき $y_{k}(\mathbf{x})=1$ かつ全ての$m \neq k$に対して $y_{m}(\mathbf{x})=0$である場合を考える. このときコミッティの中で最小の要素は $y_{\min }(\mathbf{x})=0$ である一方で $y_{\mathrm{COM}}(\mathbf{x})=\alpha_{k}$である。
したがって(14.56)から$\alpha_{k} \geqslant 0$であることがわかる。 また $\alpha_{k}$ は $\alpha$ の中で最小の値を選んでいるので任意の$\alpha$について $\alpha \geqslant 0$が成り立つことがわかる。 同様に, 全ての$m$について$y_{m}(\mathbf{x})=1$である場合を考える。このとき $y_{\min }(\mathbf{x})=y_{\max }(\mathbf{x})=1$であり,  $y_{\mathrm{COM}}(\mathbf{x})=\sum_{m} \alpha_{m}$となる. このとき(14.56)から $\sum_{m} \alpha_{m}=1$となることがわかる.</p>
<p>以上から(14.56)が(14.57)の十分条件であることがわかる。</p>
<h2 id="演習-146"><a class="header" href="#演習-146">演習 14.6</a></h2>
<div class="panel-primary">
<p>誤差関数</p>
<p>$$
\begin{aligned} E &amp;=e^{-\alpha_{m} / 2} \sum_{n \in \mathcal{T}<em>{m}} w</em>{n}^{(m)}+e^{\alpha_{m} / 2} \sum_{n \in \mathcal{M}<em>{m}} w</em>{n}^{(m)} \ &amp;=\left(e^{\alpha_{m} / 2}-e^{-\alpha_{m} / 2}\right) \sum_{n=1}^{N} w_{n}^{(m)} I\left(y_{m}\left(\mathbf{x}<em>{n}\right) \neq t</em>{n}\right)+e^{-\alpha_{m} / 2} \sum_{n=1}^{N} w_{n}^{(m)} \end{aligned} \tag{14.23}
$$</p>
<p>を$\alpha_m$について微分することにより、AdaBoostアルゴリズムにおけるパラメータ$\alpha_m$が</p>
<p>$$
\alpha_{m}=\ln \left{\frac{1-\epsilon_{m}}{\epsilon_{m}}\right} \tag{14.17}
$$</p>
<p>を利用して更新されることを示せ。ここで$\epsilon_{m}$は</p>
<p>$$
\epsilon_{m}=\frac{\sum_{n=1}^{N} w_{n}^{(m)} I\left(y_{m}\left(\mathbf{x}<em>{n}\right) \neq t</em>{n}\right)}{\sum_{n=1}^{N} w_{n}^{(m)}} \tag{14.16}
$$</p>
<p>で定義される。</p>
</div>
<p>$$
E=\left(e^{\frac{α_{m}}{2}}-e^{-\frac{α_{m}}{2}}\right) \sum_{n=1}^{N} \omega_{n}^{(m)} I\left(y_{m}\left(\mathbf{x}<em>{n}\right) \neq t</em>{n}\right)+e^{-\frac{αm}{2}} \sum_{n=1}^{N} \omega_{n}^{(m)}
$$</p>
<p>$E$を最小化する$\alpha_{m}$は</p>
<p>$$
0=\frac{\partial E}{\partial \alpha_{m}}=\left(\frac{1}{2} e^{\frac{\alpha_{m}}{2}}+\frac{1}{2} e^{-\frac{\alpha_{m}}{2}}\right) \sum_{n=1}^{N} w_{n}^{(m)} I\left(y_{m}\left(\mathbf{x}<em>{n}\right) \neq t</em>{n}\right) - \frac{1}{2} e^{-\frac{α_{m}}{2}} \sum_{n=1}^{N} w_{n}^{(m)}
$$</p>
<p>これより</p>
<p>$$
0=\left(e^{\alpha_{m}}+1\right) \sum_{n=1}^{N} w_{n}^{(m)} I\left(y_{n}\left(\mathbf{x}<em>{n}\right) \neq t</em>{n}\right)-\sum_{n=1}^{N} w_{n}^{(m)}
$$</p>
<p>$$
e^{\alpha_{m}} \sum_{n=1}^{N} w_{n}^{(m)} I\left(y_{m}\left(\mathbf{x}<em>{n}\right) \neq t</em>{n}\right)=\sum_{n=1}^{N} w_{n}^{(m)}-\sum_{n=1}^{N} w_{n}^{(m)} I\left(y_{m}\left(\mathbf{x}<em>{n}\right) \neq t</em>{n}\right)
$$</p>
<p>$$
\begin{aligned}
e^{\alpha_{m}}&amp;=\frac{\sum_{n=1}^{N} w_{n}^{(m)}}{\sum_{n=1}^{N} w_{n}^{(m)} I\left(y_{m}\left(\mathbf{x}<em>{n}\right) \neq t</em>{n}\right)}-1 \
&amp;=\frac{1}{\epsilon_{m}}-1 \
&amp;=\frac{1-\epsilon_{m}}{\epsilon_{m}}
\end{aligned}
$$</p>
<p>$$
\alpha_{m}=\ln \left{\frac{1-\epsilon_{m}}{\epsilon_{m}}\right}
$$</p>
<h2 id="演習-147"><a class="header" href="#演習-147">演習 14.7</a></h2>
<div class="panel-primary">
<p>$$
\mathbb{E}<em>{\mathbf{x}, t}[\exp {-t y(\mathbf{x})}]=\sum</em>{t} \int \exp {-t y(\mathbf{x})} p(t \mid \mathbf{x}) p(\mathbf{x}) \mathrm{d} \mathbf{x} \tag{14.27}
$$</p>
<p>で与えられる指数誤差関数の期待値を、すべての可能な関数$y(\mathbf{x})$について変分最小化すれば、最小化関数は</p>
<p>$$
y(\mathbf{x})=\frac{1}{2} \ln \left{\frac{p(t=1 \mid \mathbf{x})}{p(t=-1 \mid \mathbf{x})}\right} \tag{14.28}
$$</p>
<p>で与えられることを示せ。</p>
</div>
<p>$$
\begin{aligned}
\mathbb{E}<em>{\mathbf{x}, t}[\exp {-t y(\mathbf{x})}]&amp;=\sum</em>{t} \int \exp {-t y(\mathbf{x})} p(t \mid \mathbf{x}) p(\mathbf{x}) d \mathbf{x} \
&amp;=\int \sum_{t} \exp {-t y(\mathbf{x})} p(t \mid \mathbf{x}) p(\mathbf{x}) d \mathbf{x}
\end{aligned}
$$</p>
<p>ここで、$\mathbb{E}_{x, t}[\exp {-t y(\mathbf{x})}]$の$y$についての停留条件は(D.8)で与えられる。</p>
<p>$$
\begin{aligned}
0&amp;=\frac{\partial G}{\partial y} \
&amp;=\sum_{t}(-t) \exp {-t y(\mathbf{x})} p(t \mid \mathbf{x}) p(\mathbf{x}) \
&amp;={-\exp {-y(\mathbf{x})} p(t=1 \mid \mathbf{x})+\exp {y(\mathbf{x})} p(t=-1 \mid \mathbf{x}] p(\mathbf{x})
\end{aligned}
$$</p>
<p>$$
\therefore \quad \exp {-y(\mathbf{x})} p(t=1 \mid \mathbf{x})=\exp {y(\mathbf{x})} p(t=-1 \mid \mathbf{x})
$$</p>
<p>これより</p>
<p>$$
\exp {2 y(\mathbf{x})}=\frac{p(t=1 \mid \mathbf{x})}{p(t=-1 \mid \mathbf{x})}
$$</p>
<p>よって</p>
<p>$$
y(\mathbf{x})=\frac{1}{2} \ln \frac{p(t=1 \mid \mathbf{x})}{p(t=-1 \mid \mathbf{x})}
$$</p>
<h2 id="演習-148"><a class="header" href="#演習-148">演習 14.8</a></h2>
<div class="panel-primary">
<p>AdaBoostアルゴリズムで最小化される指数誤差関数</p>
<p>$$
E=\sum_{n=1}^{N} \exp \left{-t_{n} f_{m}\left(\mathbf{x}_{n}\right)\right} \tag{14.20}
$$</p>
<p>が、いかなる望ましい性質を持つ(well-behaved)確率モデルの対数尤度にも対応しないことを示せ。このためには、対応する条件付き分布$p(t\mid \mathbf{x})$を正しく正規化できないことを示せばよい。</p>
</div>
<p>一例として、誤差関数と負の対数尤度関数を対応付けると、次のような比例関係で定式化できる</p>
<p>$$
\begin{aligned}
\exp (-E) &amp;=\prod_{n=1}^{N} \exp \left(-\exp \left{-t_{n} f_{m}\left(\mathbf{x}<em>{n}\right)\right}\right) \
p\left(t</em>{n} \mid \mathbf{x}<em>{n}\right) &amp; \propto \exp \left(-\exp \left{-t</em>{n} f_{m}\left(\mathbf{x}_{n}\right)\right}\right)
\end{aligned}
$$</p>
<p>二値変数$t_{n}={-1, 1}$について和を取って分配関数は</p>
<p>$$
Z=\exp \left(-\exp \left{f_{m}\left(\mathbf{x}<em>{n}\right)\right}\right)+\exp \left(-\exp \left{-f</em>{m}\left(\mathbf{x}_{n}\right)\right}\right)
$$</p>
<p>で表せる。これは$f_{m}(\mathbf{x})$に依存しており、正規化された$p\left(t_{n} \mid \mathbf{x}<em>{n}\right)$は最早$\exp \left(-\exp \left{-t</em>{n} f_{m}\left(\mathbf{x}_{n}\right)\right}\right)$ に比例する関数系では表現できない。</p>
<h2 id="演習-149"><a class="header" href="#演習-149">演習 14.9</a></h2>
<div class="panel-primary">
<p>ブースティングのスタイルによる</p>
<p>$$
f_{m}(\mathbf{x})=\frac{1}{2} \sum_{l=1}^{m} \alpha_{l} y_{l}(\mathbf{x}) \tag{14.21}
$$</p>
<p>の形の加算モデルの二乗和誤差関数の逐次最小化では、新しい個別のベース分類器は、直前のモデルから得られた残留誤差$t_{n}-f_{m-1}(\mathbf{x}_{n})$によるフィッティングとなることを示せ。</p>
</div>
<p>2乗和誤差関数は（1.2）より
$$
E=\frac{1}{2} \sum_{n=1}^{N}\left{f_{m}\left(x_{n}, \theta\right)-t_{n}\right}^{2}
$$</p>
<p>ここで、(14.21)より
$$
f_{m}(x, \theta)=\frac{1}{2} \sum_{l=1}^{m} \alpha_{l} y_{l}\left(x, \theta_{l}\right)
$$</p>
<p>とする。</p>
<p>$y_{1} \sim y_{m-1}, \alpha_{1} \sim \alpha_{m-1}$を固定して、$y_{m}, \alpha_{m}$ついてEを逐次最適化する。</p>
<p>Eの式で$y_{m}, \alpha_{m}$を抜き出すと
$$
E=\frac{1}{2} \sum_{n=1}^{N}\left(f_{m-1}+\frac{1}{2} \alpha_{m} y_{m}-t_{n}\right)^{2}
$$</p>
<p>これを最小化する$y_{m}$は
$$
f_{m-1}+\frac{1}{2} \alpha_{m} y_{m}-t_{n}=0
$$</p>
<p>$$
\therefore \quad y_{m}=\frac{2}{\alpha_{m}}\left(t_{n}-f_{m-1}\right)
$$</p>
<p>で与えられる。</p>
<h2 id="演習-1410"><a class="header" href="#演習-1410">演習 14.10</a></h2>
<div class="panel-primary">
<p>訓練集合の値${t_{n}}$との間の二乗和誤差が最小となる単一の予測値$t$を得ようとするなら、$t$の最適解は${t_{n}}$の平均値で与えられることを検証せよ。</p>
</div>
<p>訓練集合の値との二乗和誤差は</p>
<p>$$
E=\sum_{{t_{n}}}\left(t-t_{n}\right)^{2}
$$</p>
<p>$E$を最小にする$t$は</p>
<p>$$
0=\frac{\partial E}{\partial t}=\sum_{{t_{n}}} 2\left(t-t_{n}\right)
$$</p>
<p>で与えられる。これを解けば</p>
<p>$$
\therefore t=\frac{1}{N^{\prime}} \sum_{{t_{n}}} t_{n}
$$</p>
<p>を得る。ここで、$N^{\prime}$は訓練集合の値${t_{n}}$の個数を表している。すなわち、最適な$t$の値は${t_{n}}$の平均値となることが示された。</p>
<h2 id="演習-1411"><a class="header" href="#演習-1411">演習 14.11</a></h2>
<div class="panel-primary">
<p>クラス$\mathcal{C}<em>{1}$からの$400$個のデータ点と、クラス$\mathcal{C}</em>{2}$からの$400$個のデータ点からなるデータ集合を考える。木モデルAは、最初の葉ノード($\mathcal{C}<em>{1}$を予測)に$(300,100)$を割り当て、2番目の葉ノード($\mathcal{C}</em>{2}$を予測)には$(100,300)$を割り当てるように分割すると仮定する。ここで、$(n,m)$は$n$個の点が$\mathcal{C}<em>{1}$に割り当てられ、$m$個の点が$\mathcal{C}</em>{2}$に割り当てられることを示す。同様にして、二つ目の木モデルBでは、それぞれ$(200,400)$と$(200,0)$に分割するものとする。二つの木の誤判別率を評価し、それらが等しいことを示せ。同様に、 二つの木に対する枝刈り基準</p>
<p>$$
C(T)=\sum_{\tau=1}^{|T|} Q_{\tau}(T)+\lambda|T| \tag{14.31}
$$</p>
<p>が、交差エントロピーの場合</p>
<p>$$
Q_{\tau}(T)=-\sum_{k=1}^{K} p_{\tau k} \ln p_{\tau k} \tag{14.32}
$$</p>
<p>とジニ係数の場合</p>
<p>$$
Q_{\tau}(T)=\sum_{k=1}^{K} p_{\tau k}\left(1-p_{\tau k}\right) \tag{14.33}
$$</p>
<p>の両者において、木Aよりも木Bにおいて値が小さくなることを示せ。</p>
</div>
<p>性能を示す誤分類の関数によって結果が異なることを示す、簡単な計算問題。まず、誤判別率は、</p>
<p>$$
\begin{aligned}
R_A = \frac{100+100}{400+400} = \frac{1}{4} \
R_B= \frac{0+200}{400+400} = \frac{1}{4}
\end{aligned}
$$</p>
<p>次に交差エントロピーを用いた枝刈基準は、(14.31)と(14.32)を用いて</p>
<p>$$
\begin{aligned}
&amp;C_{\textrm{Xent}}(T_A) = -2 \left(\frac{100}{400}\ln \frac{100}{400} +\frac{300}{400}\ln \frac{300}{400}\right)+ 2\lambda \simeq 1.12 + 2\lambda \
&amp;C_{\textrm{Xent}}(T_B) = -\frac{400}{400}\ln \frac{400}{400} -\frac{200}{400}\ln \frac{200}{400}-\frac{0}{400}\ln \frac{0}{400}-\frac{200}{400}\ln \frac{200}{400}+ 2\lambda \simeq 0.69 + 2\lambda
\end{aligned}
$$
よって、Bの方が小さい。</p>
<p>同様に、ジニ係数の場合について,
$$
\begin{aligned}
&amp;C_{\textrm{Gini}}(T_A) = 2 \left(\frac{300}{400}\left(1 - \frac{300}{400} \right)+\frac{100}{400}\ln \frac{100}{400}\right)+ 2\lambda =\frac{3}{4} + 2\lambda \
&amp;C_{\textrm{Gini}}(T_B) = \frac{400}{400}\left(1- \frac{400}{400}\right) +\frac{200}{400}\left(1- \frac{200}{400}\right) +\frac{0}{400}\left(1- \frac{0}{400}\right) +\frac{200}{400}\left(1- \frac{200}{400}\right) = \frac{1}{2}+2\lambda
\end{aligned}
$$
よって、Bの方が小さい。</p>
<h2 id="演習-1412"><a class="header" href="#演習-1412">演習 14.12</a></h2>
<div class="panel-primary">
<p>14.5.1節の線形回帰の混合モデルについての結果を、ベクトル$\mathbf{t}$で記述される複数の目標値の場合に拡張せよ。これを行うためは3.1.5節の結果を利用せよ。</p>
</div>
<p>(3.32)式のようにtを多次元にした場合、(14.34)にあたる式は
$$
p(\mathbf{t} \mid \boldsymbol{\theta})=\sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\mathbf{t} \mid \mathbf{W}^{\mathrm{T}} \boldsymbol{\phi}, \beta^{-1} \mathbf{I}\right)
$$
(14.35)-(14.37), $Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text {old }}\right)$を同様に変えていき、(14.39)は
$$
Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text {old }}\right)=\sum_{n=1}^{N} \gamma_{n k}\left{-\frac{\beta}{2}\left|\mathbf{t}<em>{n}-\mathbf{W}^{\mathrm{T}} \boldsymbol{\phi}</em>{n}\right|^{2}\right}+\text { const. }
$$
となる。
同様に(14.42)は
$$
\mathbf{W}<em>{k}=\left(\boldsymbol{\Phi}^{\mathrm{T}} \mathbf{R}</em>{k} \boldsymbol{\Phi}\right)^{-1} \boldsymbol{\Phi}^{\mathrm{T}} \mathbf{R}<em>{k} \mathbf{T}
$$
(14.43)は
$$
Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text {old }}\right)=\sum</em>{n=1}^{N} \sum_{k=1}^{K} \gamma_{n k}\left{\frac{D}{2} \ln \beta-\frac{\beta}{2}\left|\mathbf{t}<em>{n}-\mathbf{W}^{\mathrm{T}} \boldsymbol{\phi}</em>{n}\right|^{2}\right}
$$
となり、(14.44)が
$$
\frac{1}{\beta}=\frac{1}{N D} \sum_{n=1}^{N} \sum_{k=1}^{K} \gamma_{n k}\left|\mathbf{t}<em>{n}-\mathbf{W}^{\mathrm{T}} \boldsymbol{\phi}</em>{n}\right|^{2}
$$
となる。</p>
<h2 id="演習-1413"><a class="header" href="#演習-1413">演習 14.13</a></h2>
<div class="panel-primary">
<p>線形回帰モデルの混合における完全データに対する対数尤度関数が</p>
<p>$$
\ln p(\mathbf{t}, \mathbf{Z} \mid \boldsymbol{\theta})=\sum_{n=1}^{N} \sum_{k=1}^{K} z_{n k} \ln \left{\pi_{k} \mathcal{N}\left(t_{n} \mid \mathbf{w}<em>{k}^{\mathrm{T}} \boldsymbol{\phi}</em>{n}, \beta^{-1}\right)\right} \tag{14.36}
$$</p>
<p>で与えられることを検証せよ。</p>
</div>
<p>$(14.34)$の混合分布から出発して、9.2節で示したガウス分布の混合分布と同じステップを踏むことになる。</p>
<p>$(9.10)$,$(9.11)$と同様に
$$
\begin{aligned}
&amp;p(\mathbf{z} \mid \boldsymbol{\theta})=\prod_{k=1}^{K} \pi_{k}^{z_{k}} \
&amp;p(t \mid \mathbf{z},\boldsymbol{\theta})=\prod_{k=1}^{K} \mathcal{N}\left(t \mid \mathbf{w}<em>{k}^{\mathrm{T}} \boldsymbol{\phi}</em>{n}, \beta^{-1}\right)^{z_{k}}
\end{aligned}
$$
である。これより
$$
\begin{aligned}
p(t, \mathbf{z} \mid \boldsymbol{\theta}) &amp;=p(t \mid \mathbf{z}, \boldsymbol{\theta}) p(\mathbf{z} \mid \boldsymbol{\theta}) \
&amp;=\prod_{k=1}^{K} \mathcal{N}\left(t \mid \mathbf{w}<em>{k}^{\mathrm{T}} \boldsymbol{\phi}</em>{n}, \beta^{-1}\right)^{z_{k}} \prod_{k=1}^{K} \pi_{k}^{z_{k}} \
&amp;=\prod_{k=1}^{K}\left{\pi_{k} \mathcal{N}\left(t \mid \mathbf{w}<em>{k}^{\mathrm{T}} \boldsymbol{\phi}</em>{n}, \beta^{-1}\right)\right}^{z_{k}}
\end{aligned}
$$</p>
<p>二値潜在変数の集合$\mathbf{Z} = {\mathbf{z}<em>{n}}$を導入し、各データ点$n$に対して$\mathbf{z}</em>{n}$は$z_{n k} \in{0,1}$であるとする。$\left(t_{1}, \mathbf{z}<em>{1}\right) \cdots\left(t</em>{N}, \mathbf{z}_{N}\right)$はi.i.dなので</p>
<p>$$
\begin{aligned}
p(\mathbf{t}, \mathbf{Z} \mid \boldsymbol{\theta}) &amp;=\prod_{n=1}^{N} p\left(t_{n}, \mathbf{z}<em>{n} \mid \boldsymbol{\theta}\right) \
&amp;=\prod</em>{n=1}^{N} \prod_{k=1}^{K}\left{\pi_{k} \mathcal{N}\left(t_{n} \mid \mathbf{w}<em>{k}^{\mathrm{T}} \boldsymbol{\phi}</em>{n}, \beta^{-1}\right)\right}^{z_{n k}} \end{aligned}
$$
両辺の対数をとって
$$
\ln p(\mathbf{t}, \mathbf{Z} \mid \boldsymbol{\theta})=\sum_{n=1}^{N} \sum_{k=1}^{K} z_{n k} \ln \left{\pi_{k} \mathcal{N}\left(t_{n} \mid \mathbf{w}<em>{k}^{\mathrm{T}} \boldsymbol{\phi}</em>{n}, \beta^{-1}\right)\right} \quad \tag{14.36}
$$</p>
<h2 id="演習-1414"><a class="header" href="#演習-1414">演習 14.14</a></h2>
<div class="panel-primary">
<p>ラグランジュ未定乗数法(付録E)の技術を利用し、EMアルゴリズムによる最尤推定で訓練される線形回帰モデルの混合では、Mステップで行う混合係数を再推定する式が</p>
<p>$$
\pi_{k}=\frac{1}{N} \sum_{n=1}^{N} \gamma_{n k} \tag{14.38}
$$</p>
<p>で与えられることを示せ.</p>
</div>
<p>ラグランジアンは、
$$
\begin{aligned}
L &amp;=Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\rm{old}}\right)+\lambda\left(\sum_{k=1}^{K} \pi_{k}-1\right) \
&amp;=\sum_{n=1}^{N} \sum_{k=1}^{K} \gamma_{n k}\left{\ln \pi_{k}+\ln \mathcal{N}\left(t_{n} \mid \mathbf{w}<em>{k}^{\mathrm{T}} \boldsymbol{\phi}</em>{n}, \beta^{-1}\right)\right}+\lambda\left(\sum_{k=1}^{K} \pi_{k}-1\right)
\end{aligned}
$$
$\pi_{k}$について停留条件
$$
\begin{aligned}
&amp;0=\frac{\partial L}{\partial \pi_{k}}=\sum_{n=1}^{N} \gamma_{n k} \frac{1}{\pi_{k}}+\lambda \
&amp;\therefore \pi_{k}=-\frac{1}{\lambda} \sum_{n=1}^{N} \gamma_{n k}
\end{aligned}
$$
$\lambda$について停留条件
$$
0=\frac{\partial L}{\partial \lambda}=\sum_{k=1}^{K} \pi_{k}-1
$$
より、
$$
1=\sum_{k=1}^{K} \pi_{k}=\sum_{k=1}^{K} \left( -\frac{1}{\lambda} \sum_{n=1}^{N} \gamma_{n k} \right)
$$</p>
<p>$$
\begin{aligned}
\therefore\quad \lambda&amp;=-\sum_{n=1}^{N} \sum_{k=1}^{K} \gamma_{n k} \
&amp;=-\sum_{n=1}^{N} \underbrace{\sum_{k=1}^{K} \frac{\pi_{k} \mathcal{N}\left(t_{n} \mid \mathbf{w}<em>{k}^{\mathrm{T}} \boldsymbol{\phi}</em>{n}, \beta^{-1}\right)}{\sum_{j} \pi_{j} \mathcal{N}\left(t_{n} \mid \mathbf{w}<em>{j}^{\mathrm{T}} \boldsymbol{\phi}</em>{n}, \beta^{-1}\right)}}_{=1} \quad (\because(14.37))\
&amp;=-N
\end{aligned}
$$</p>
<p>よって、
$$
\pi_{k}=\frac{1}{N} \sum_{n=1}^{N} \gamma_{n k} \tag{14.38}
$$
を得る。</p>
<h2 id="演習-1415"><a class="header" href="#演習-1415">演習 14.15</a></h2>
<div class="panel-primary">
<p>すでに述べたように、回帰問題において二乗損失関数を用いれば、新しい入力ベクトルに対する目標変数の最適な予測値は予測分布の条件付き平均で与えられる。14.5.1節で議論した線形回帰モデルの混合に対しては、条件付き平均が構成要素分布ごとの平均を線形結合した形で与えられることを示せ。ただし、もしも目標データの条件付き分布が多峰性なら、条件付き平均の予測性能は低くなることに注意せよ。</p>
</div>
<p>※「すでに述べたように、回帰問題において二乗損失関数を用いれば、新しい入力ベクトルに対する目標変数の最適な予測値は予測分布の条件付き平均で与えられる。」これは1.5.5節や第3章の冒頭で述べている。</p>
<p>線形回帰モデルの混合の場合について考える。条件付き平均は$\mathbb{E}[t\mid \boldsymbol{\theta}]$と表される。ここで、$\boldsymbol{\theta}$は重みパラメータ$\mathbf{w}<em>{k}$で支配される$K$個の線形回帰モデル内のすべての適応パラメータの集合，すなわち$\mathbf{w}={\mathbf{w}</em>{k}}$，$\boldsymbol{\pi} = {\pi_{k}}$および共通の精度パラメータ$\beta$をまとめて表している。$p(t\mid \boldsymbol{\theta})$は</p>
<p>$$
p(t \mid \boldsymbol{\theta})=\sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(t \mid \mathbf{w}_{k}^{\mathrm{T}} \boldsymbol{\phi}, \beta^{-1}\right) \tag{14.34}
$$</p>
<p>で与えられる。</p>
<p>新しい入力ベクトル$\boldsymbol{\widehat{\phi}}$が与えられたときの予測分布$p(t \mid \widehat{\boldsymbol{\phi}}, \boldsymbol{\boldsymbol{\theta}})$を求める。有向グラフは下図の通り。
<img src="https://i.imgur.com/yFvquyR.png" width=50%></p>
<p>$$
\begin{aligned}
p(t \mid \widehat{\boldsymbol{\phi}}, \boldsymbol{\boldsymbol{\theta}})&amp;=\sum_{\mathbf{\widehat{z}}} p(t, \mathbf{\widehat{z}} \mid \widehat{\boldsymbol{\phi}}, \boldsymbol{\pi}, \beta, \mathbf{W}) \
&amp;=\sum_{\mathbf{\widehat{z}}} p(t \mid \mathbf{\widehat{z}}, \widehat{\boldsymbol{\phi}}, \boldsymbol{\pi}, \beta, \mathbf{W}) p(\mathbf{\widehat{z}} \mid \widehat{\boldsymbol{\phi}}, \boldsymbol{\pi}, \beta, \mathbf{W}) \
&amp;=\sum_{\mathbf{\widehat{z}}} p(t \mid \mathbf{\widehat{z}}, \widehat{\boldsymbol{\phi}}, \beta, \mathbf{W}) p(\mathbf{\widehat{z}} \mid \boldsymbol{\pi}) (\because \text { 有向分離) }\
&amp;=\sum_{\mathbf{\widehat{z}}} p(t \mid \mathbf{\widehat{z}}, \widehat{\boldsymbol{\phi}}, \beta, \mathbf{W}) \left(\prod_{k=1}^{K} \pi_{k}^{\hat{z_{k}}}\right) (\because \text { 演習14.13) }\
&amp;=\sum_{\mathbf{\widehat{z}}} \prod_{k=1}^{K} \pi_{k}^{\hat{z_{k}}} \mathcal{N}\left(t \mid \mathbf{w}<em>{k}^{\mathrm{T}} \widehat{\boldsymbol{\phi}}, \beta^{-1}\right)^{\hat{z</em>{k}}}  (\because \text { 演習14.13) }\
&amp;=\sum_{\mathbf{\widehat{z}}} \prod_{k=1}^{K}\left(\pi_{k} \mathcal{N}\left(t \mid \mathbf{w}<em>{k}^{\mathrm{T}} \widehat{\boldsymbol{\phi}}, \beta^{-1}\right)\right)^{\hat{z</em>{k}}} \
&amp;=\sum_{j=1}^{K} \prod_{k=1}^{K}\left(\pi_{k} \mathcal{N}\left(t \mid \mathbf{w}<em>{k}^{\mathrm{T}} \widehat{\boldsymbol{\phi}}, \beta^{-1}\right)\right)^{\delta</em>{k j}}(\because \text { 演習9.3; 1-of-K 符号化法) } \
&amp;=\sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(t \mid \mathbf{w}_{k}^{\mathrm{T}} \widehat{\boldsymbol{\phi}}, \beta^{-1}\right)
\end{aligned}
$$</p>
<p>よって
$$
\begin{aligned}
\mathbb{E}[t \mid \widehat{\boldsymbol{\phi}}, \boldsymbol{\theta}] &amp;=\int t p(t \mid \widehat{\boldsymbol{\phi}}, \boldsymbol{\theta}) d t \
&amp;=\sum_{k=1}^{K} \pi_{k} \int t \mathcal{N}\left(t \mid \mathbf{w}<em>{k}^{\mathrm{T}} \widehat{\boldsymbol{\phi}}, \beta^{-1}\right) d t \
&amp;=\sum</em>{k=1}^{K} \pi_{k} \mathbb{E}\left[t \mid \widehat{\boldsymbol{\phi}}, \mathbf{w}_{k}, \beta \right]
\end{aligned}
$$</p>
<p>の形となる。これは、「新しい入力ベクトルに対する目標変数の最適な予測値（→条件付き平均）が構成要素分布ごとの平均を線形結合した形で与えられる」ことを表している。</p>
<h2 id="演習-1416"><a class="header" href="#演習-1416">演習 14.16</a></h2>
<div class="panel-primary">
<p>14.5.2節で議論したロジスティック回帰の混合モデルを、$C \geq 2$のクラスを表現できるソフトマックス分類器の混合に拡張せよ。最尤推定を通じて、このモデルのパラメータを決定するためのEMアルゴリズムを書き下せ。</p>
</div>
<p>14.5.2節の議論（2クラス回帰）を、多クラス回帰に拡張する。4.3.2節の議論を4.3.4節の議論に拡張する流れと同様に解けばよい。</p>
<p>$$
\begin{aligned}
p(\mathbf{t} | \mathbf{w}) = \prod_{n=1}^N y_{n}^{t_n}(1-y_n)^{1-t_n}
\end{aligned} \tag{4.89}
$$</p>
<p>の多クラス版として</p>
<p>$$
\begin{aligned}
p(\mathbf{T} | \mathbf{w}<em>1,\cdots ,\mathbf{w}<em>K) = \prod</em>{n=1}^N \prod</em>{k=1}^K y_{nk}^{t_{nk}}
\end{aligned} \tag{4.107}
$$</p>
<p>に拡張したのと同様に、</p>
<p>$$
\begin{aligned}
p(\mathbf{t}|\boldsymbol\theta) = \prod_{n=1}^N \left( \sum_{k=1}^K \pi_k y_k^t (1-y_k)^{1-t} \right)
\end{aligned}\tag{14.46}
$$</p>
<p>の多クラス版は、</p>
<p>$$
\begin{aligned}
p(\mathbf{T}|\boldsymbol\Theta)
= \prod_{n=1}^N \left( \sum_{k=1}^K \pi_k \prod <em>{c=1}^C y</em>{kc}^{t_c} \right)
\end{aligned}
$$</p>
<p>と書ける。ここで、$c$はクラスラベルを表す。（4章ではクラスラベルを$k$で表していたが、14章では$k$を既に混合要素のラベルとして使っているので、問題文で別の文字ラベルが指定されている。）
教科書(14.46)-(14.49)式と同じ議論を行うことで、</p>
<p>$$
\begin{aligned}
Q(\boldsymbol\theta, \boldsymbol\theta ^{\rm old})
= \sum_{n=1}^N \sum_{k=1}^K \gamma_{nk} \ \left[ \ln \pi_k + t_n \ln y_{nk} + (1-t_n) \ln (1-y_{nk}) \right]
\end{aligned} \tag{14.49}
$$</p>
<p>に対応する完全データ対数尤度関数の多クラス版は、$\mathbf{t}$が1-of-K符号なので</p>
<p>$$
\begin{aligned}
Q(\boldsymbol\Theta, \boldsymbol\Theta ^{\rm old}) = \sum_{n=1}^N \sum_{k=1}^K \gamma_{nk} \ \left[ \ln \pi_k + \sum_{c=1}^C t_{nc} \ln y_{nkc} \right]
\end{aligned}
$$</p>
<p>と書ける。（教科書(4.108)式の交差エントロピーに相当する。）</p>
<p>Mステップのうち、$\boldsymbol\pi$の最大化は混合ロジスティックモデルと同様で、(14.50)式そのものである。
次に、Mステップの$\mathbf{W}_1, \cdots ,\mathbf{W}<em>K$での最大化を考える。ここで、$\mathbf{W}<em>k = (\mathbf{w}</em>{k1}, \cdots ,\mathbf{w}</em>{kC} )$を指す。</p>
<p>(14.51)、(14.52)式に相当する勾配$Q=0$の式とヘッシアンの式は、</p>
<p>$$
\begin{aligned}
\nabla <em>{\mathbf{w}</em>{kc}} Q &amp;=\sum_{n=1}^N \gamma_{nk} (t_{nc} - y_{nkc} ) \phi_n = 0\
\mathbf{H}<em>k &amp;= -\nabla <em>{w</em>{kc}} \nabla</em>{w_{kc'}} Q = \sum_{n=1}^N \gamma_{nk} y_{nkc} (I_{cc'} - y_{nkc'}) \phi_n \phi_n^{\rm T}
\end{aligned}
$$</p>
<p>となる。（(4.109)式や(14.51)式の導出と同様。ちなみに、14章の$E$は、4章とは$E$と逆符号で定義されていることに注意。この解答では14章の符号の定義に合わせた。）これは解析的に解けないので、(14.51)式と同様に、反復的な手法（IRLSアルゴリズムなど）を用いて解くことになる。</p>
<h2 id="演習-1417"><a class="header" href="#演習-1417">演習 14.17</a></h2>
<div class="panel-primary">
<p>条件付き分布の混合モデル$p(t \mid \mathbf{x})$として次式を考える。</p>
<p>$$
p(t \mid \mathbf{x})=\sum_{k=1}^{K} \pi_{k} \psi_{k}(t \mid \mathbf{x}) \tag{14.58}
$$</p>
<p>ここでは、各々の混合構成要素$\psi_{k}(t\mid \mathbf{x})$自体が混合モデルである。この2レベルの階層的混合が、標準的な1レベルの混合モデルと等価であることを示せ。ここでさらに、階層的モデルにおける両方のレベルの混合係数は$\mathbf{x}$についての任意の関数であると考える。この階層的モデルも再び、$\mathbf{x}$に依存した混合係数を持つ1レベルモデルと等価であることを示せ。最後に、階層的混合の両方のレベルの混合係数が線形分類（ロジスティックもしくはソフトマックス）モデルに制限される場合を考える。一般的に、階層的混合が混合係数に線形分類モデルを用いた1レベルの混合では表現できないことを示せ。
ヒント：このためには、一つの反例を示せば十分である。そこで、混合係数が線形ロジスティックモデルである二つの構成要素の混合で、それらのうちの一つの構成要素それ自体が二つの構成要素の混合であると考える。そして、これが線形ソフトマックスモデルで決定される混合係数をもつ3つの構成要素による単一レベルの混合では表現できないことを示せばよい。</p>
</div>
<p>ゲート関数が定数の場合</p>
<p>$(14.58)$における$\psi_{k}(t \mid \mathbf{x})$が混合モデルで表せるので
$$
\psi_{k}(t \mid \mathbf{x})=\sum_{m=1}^{M} \lambda_{m k} \phi_{m k}(t \mid \mathbf{x})
$$
と書くことができる。$(14.58)$に代入して
$$
\begin{aligned}
p(t \mid \mathbf{x}) &amp;=\sum_{k=1}^{K} \pi_{k} \sum_{m=1}^{M} \lambda_{m k} \phi_{m k}(t \mid \mathbf{x}) \
&amp;=\sum_{k=1}^{K} \sum_{m=1}^{M} \pi_{k} \lambda_{m k} \phi_{m k}(t \mid \mathbf{x})
\end{aligned}
$$
$\pi_{k} \lambda_{m k}= \eta_{l}$とおいて添字を振り直すことで
$$
p(t \mid \mathbf{x})=\sum_{l=1}^{L} \eta_{l} \phi_{l}(t \mid \mathbf{x})
$$
と書くことができ1レベルモデルと等価であることがわかる。</p>
<p>ゲート関数が$\mathbf{x}$についての任意の関数の場合</p>
<p>$\pi_{k}$と$\lambda_{m k}$が$x$に依存する場合でも非負であり、$x$のあらゆる値に対して合計が1になるという制約のもとで1レベルの混合モデルで書くことができる。</p>
<p>ゲート関数が線形分類モデルの場合</p>
<p>問題文のヒントに従った混合モデルを構成することで示す。</p>
<p><img src="https://i.imgur.com/p2PMfoU.png" alt="" /></p>
<p>図12の左図に示すような木構造の階層的な混合モデルを考える。ルートレベルでは、これは2つの要素を持つ混合モデルである。混合係数は線形ロジスティック回帰モデルによって与えられ、入力に依存する。左のサブツリーは局所条件付き密度モデル$\psi_{1}(t \mid \mathbf{x})$に対応する。右のサブツリーでは、ルートからの構造が複製され、両方のサブツリーが局所条件付き密度モデル、$\psi_{2}(t \mid \mathbf{x})$および$\psi_{3}(t \mid \mathbf{x})$を含むという違いがある。
結果として得られる混合モデルは、以下の混合係数で(14.58)の形式で書くことができる。</p>
<p>$$
\begin{aligned}
&amp;\pi_{1}(\mathbf{x})=\sigma\left(\mathbf{v}<em>{1}^{\mathrm{T}} \mathbf{x}\right) \
&amp;\pi</em>{2}(\mathbf{x})=\left(1-\sigma\left(\mathbf{v}<em>{1}^{\mathrm{T}} \mathbf{x}\right)\right) \sigma\left(\mathbf{v}</em>{2}^{\mathrm{T}} \mathbf{x}\right) \
&amp;\pi_{3}(\mathbf{x})=\left(1-\sigma\left(\mathbf{v}<em>{1}^{\mathrm{T}} \mathbf{x}\right)\right)\left(1-\sigma\left(\mathbf{v}</em>{2}^{\mathrm{T}} \mathbf{x}\right)\right)
\end{aligned}
$$</p>
<p>ここで、$\sigma(-)$は(4.59)で定義され、$\mathbf{v}_1$, $\mathbf{v}<em>2$はロジスティック回帰モデルのパラメータ・ベクトルである。式より$\pi</em>{1}(\mathbf{x})$は$\mathbf{v}_2$に依存しない。これは混合係数が1レベルのソフトマックスモデルを使用してモデル化された場合，すなわち以下の式でゲート関数が書ける場合には当てはまらない。</p>
<p>$$
\pi_{k}(\mathbf{x})=\frac{e^{\mathbf{u}<em>{k}^{\mathrm{T}} \mathbf{x}}}{\sum</em>{j}^{3} e^{\mathbf{u}_{j}^{\mathrm{T}} \mathbf{x}}}
$$</p>
<p>ここで、$\pi_{k}(\mathbf{x})$に対応するパラメータ$\mathbf{u}<em>k$は、分母を通して他の混合係数$\pi</em>{j\neq k}(\mathbf{x})$にも影響する。これにより、線形ソフトマックスモデルと比較して、入力空間上の混合係数のモデル化において、階層的モデルは異なる特性を持つことになる。図12の右図に例を示すが、赤線は入力空間における混合係数が等しい境界を表している。この境界線は2本の直線で形成されており、図12左図の2つのロジスティックユニットに相当する。ソフトマックスモデルで入力空間を分割すると、3本の直線が1点で結ばれ、例えばPRMLの図4.3の赤線のような形になるが、線形3クラスソフトマックスモデルでは図12右パネルのような境界を実装できないことに注意されたい。</p>
<p><img src="https://i.imgur.com/7pFB89n.png" alt="" /></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        
        <!-- Livereload script (if served using the cli tool) -->
        <script type="text/javascript">
            var socket = new WebSocket("ws://localhost:3000/__livereload");
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>
        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
        
        

    </body>
</html>
