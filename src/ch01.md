# PRML第1章演習問題解答

<head>
<style>
  div.panel-primary {
	border: 1px solid #000;
    margin: 10px 5px;
    padding: 16px 10px 0px;
  }
</style>
</head>

## 演習 1.1
<div class="panel-primary">

関数$y(x, \mathbf{w})$が多項式$(1.1)$

$$y(x, \mathbf{w})=w_{0}+w_{1} x+w_{2} x^{2}+\cdots+w_{M} x^{M}=\sum_{j=0}^{M} w_{j} x^{j}$$

で与えられたときの$(1.2)$

$$
E(\mathrm{w})=\frac{1}{2} \sum_{n=1}^{N}\left\{y\left(x_{n}, \mathbf{w}\right)-t_{n}\right\}^{2}
$$

の二乗和誤差関数を考える。この誤差関数を最小にする係数$\mathbf{w}=\left\{w_{i}\right\}$は以下の線形方程式の解として与えられることを示せ．

$$\sum_{j=0}^{M} A_{i j} w_{j}=T_{i} \tag{1.122}$$

ただし、
$$
A_{i j}=\sum_{n=1}^{N}\left(x_{n}\right)^{i+j},\hspace{1em}T_{i}=\sum_{n=1}^{N}\left(x_{n}\right)^{i} t_{n} \tag{1.123}
$$
ここで，下付き添え字の$i$や$j$は成分を表し，$(x)^j$は$x$の$j$乗を表す．

</div>

$(1.2)$に$(1.1)$式を代入すると
$$
E(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N}\left\{\sum_{j=0}^{M} w_{j} {x_{n}}^{j}-t_{n}\right\}^{2}
$$
$E(\mathbf{w})$を最小にする$w_i$を求めるための微分を行うと
$$
\begin{aligned} \frac{\partial E}{\partial w_{i}} &=\frac{1}{2} \sum_{n=1}^{N}\left\{2\left(\sum_{j=0}^{N} w_{j} x_{n}^{j}-t_{n}\right) x_{n}^{i}\right\} \\ &=\sum_{n=1}^{N}\left(x_{n}^{i} \sum_{j=0}^{M} w_{j} x_{n}^{j}\right)-\sum_{n=1}^{N} {t_n} {x_{n}}^{i} \end{aligned}
$$
$\frac{\partial E}{\partial w_{i}}=0$となるために,
$$
\sum_{n=1}^{N}\left({x_{n}}^{i} \sum_{j=0}^{M} w_{j} x_{n}^j \right)=\underbrace{\sum_{n=1}^{N} t_{n} {x_{n}}^{i}}_{T_{i}}
$$

$$
\begin{aligned}(左辺) &=\sum_{n=1}^{N} {x_{n}}^{i}\left(w_{0} {x_{n}}^{0}+w_{1} {x_{n}}^{1}+\cdots+w_{M} {x_{n}}^{M}\right) \\ &=\sum_{n=1}^{N} (w_{0} {x_{n}}^{i}+w_{1} {x_{n}}^{i+1}+\cdots+w_{M} {x_n}^{i+M}) \\ &=\sum_{n=1}^{N} \sum_{j=0}^{M} {x_{n}}^{i+j} w_{j} \\ &=\sum_{j=0}^{M} (\sum_{n=1}^{N} {x_{n}}^{i+j} w_{j}) \\ &=\sum_{j=0}^{M} A_{i j} w_{j} \end{aligned}
$$

よって示された。
## 演習 1.2
<div class="panel-primary">

正則化された二乗和誤差関数$(1.4)$

$$\widetilde{E}(\mathrm{w})=\frac{1}{2} \sum_{n=1}^{N}\left\{y\left(x_{n}, \mathrm{w}\right)-t_{n}\right\}^{2}+\frac{\lambda}{2}\|\mathrm{w}\|^{2}$$

を最小にする係数$w_i$が満たす,$(1.122)$
$$\sum_{j=0}^{M} A_{i j} w_{j}=T_{i}$$
に類似した線形方程式系を書き下せ．
</div>

1.1 と同様に行う。

$$
\tilde{E}(w)=\frac{1}{2} \sum_{n=1}^{N}\left(\sum_{j=0}^{M} w_{i} x_{n}^{j}-t_{n}\right)^{2}+\frac{\lambda}{2}\|\mathrm{w}\|^{2}
$$

$$
0=\frac{\partial \tilde{E}}{\partial w_{i}}=\sum_{n=1}^{N} x_{n}^{i} \sum_{j=0}^{M} w_{j} x_{n}^{j}+\lambda w_{i}-\sum_{n=1}^{N} x_{n}^{i} t_{n}
$$
整理すると

$$
\sum_{n=1}^{N}\left(x_{n}^{i} \sum_{j=0}^{n} w_{j} x_{n}^{j}\right)=T_{i}-\lambda w_{i} \\
$$
$$
\sum_{j=0}^{M} \sum_{n=1}^{N} x_{n}^{i+j} w_{j}=\underbrace{T_{i}-\lambda w_{j}}_{T_{i}^\prime} \\
$$
$$
\sum_{j=0}^{M} A_{i j} w_{j}=T_{i}^\prime
$$



※別例（演習模範回答ーパターン認識と機械学習完全版を参考）


$$
{\tilde{A}_{i j}}=A_{i j}+\lambda I_{i j}
$$
と置くことにする。

$\frac{\partial \tilde{E}}{\partial w_{i}}=0$となるためには、演習1.1同様に考えると、

$$
\sum_{j=0}^{M} A_{i j} w_{j}+\lambda w_{i}＝\underbrace{\sum_{n=1}^{N} t_{n} {x_{n}}^{i}}_{T_{i}}
$$
であれば良い。

ここで上式を代入すると、
$$
\sum_{j=0}^{M} \tilde{A}_{i j} w_{j}＝T_{i}
$$
と書ける。


## 演習 1.3
<div class="panel-primary">

3個の色分けされた箱$r$(赤)， $b$(青)， $g$(緑)を考える. 箱$r$には3個のりんご，4個のオレンジ，3個のライムが入っており，箱$b$には1個のりんご， 1個のオレンジ, 0個のライムが入っており， 箱$g$には3個のりんご， 3個のオレンジ, 4個のライムが入っている箱を$p(r)=0.2, p(b)=0.2, p(g)=0.6$という確率でランダムに選び，果物を箱から1個取り出す（箱の中のものは等確率で選ばれるとする）とき，りんごを選び出す確率を求めよ．また，選んだ果物がオレンジであったとき，それが緑の箱から取り出されたものである確率はいくらか？
</div>

りんご, オレンジ, ライムを取り出すという事象をそれぞれ$A$, $O$, $L$とすると,

$$
p(A)=p(r) p(A|r)+p(b) p(A|b)+p(g) p(A|g)=0.34
$$

また, 同様にオレンジを選び出す確率$p(O)$は,

$$
\begin{aligned}
p(O) &=p(O | r) p(r)+p(O | b) p(b)+p(O|g) p(g) \\
     &=\frac{4}{10} \times 0.2+\frac{1}{2} \times 0.2+\frac{3}{10} \times 0.6=0.36
\end{aligned}
$$

求める値「選んだ果物がオレンジであったとき，それが緑の箱から取り出されたものである確率」はベイズの定理を用いると

$$
p(g|O)=\frac{p(O|g) p(g)}{p(O)}=\frac{3}{10}\times\frac{0.6}{0.36}=0.5
$$

となる。

## 演習 1.4
<div class="panel-primary">

連続変数$x$上で定義された確率密度$p_x(x)$を考える. $x=g(y)$により非線形変換を施すと密度は

$$
\begin{aligned}
p_{y}(y) &=p_{x}(x)\left|\frac{\mathrm{d} x}{\mathrm{d} y}\right| \\
&=p_{x}(g(y))\left|g^{\prime}(y)\right|
\end{aligned} \tag{1.27}
$$

の変換を受ける．$(1.27)$を微分して$y$に関する密度を最大にする位置$\widehat{y}$と$x$に関する密度を最大にする位置$\widehat{x}$とが，ヤコビ因子の影響により一般には単純な$\widehat{x}=g(\widehat{y})$という関係にないことを示せ．これは確率密度の最大値が，（通常の関数と異なり）変数の選択に依存することを示している．線形変換の場合には最大値の位置が変数自身と同じ変換を受けることを確かめよ．
</div>
定義より

$$p'_x(\hat{x})、p'_y(\hat{y})=0$$

である。

$p_y(y)=p_x(g(y))|g'(y)|$を$y$で微分すると$p'_y(y)=sp'_x(g(y))(g'(y))^2+sp_x(g(y))g''(y)$である。（ただし、$s=-1$ or $+1$である)

$x=g(y)$より、$\hat{x}$のときの$y$を$\grave{y}$とすると$\hat{x}=g(\grave{y})$である。

$$
p'_y(\grave{y})=sp'_x(g(\grave{y}))(g'(\grave{y}))^2+sp_x(g(\grave{y}))g''(\grave{y})
$$

$p'_x(\hat{x})=p'_x(g(\grave{y}))=0$であるから

$$
p'_y(\grave{y})=0+sp_x(g(\grave{y}))g''(\grave{y})
$$

一般に$sp_x(g(\grave{y}))g''(\grave{y}) \ne 0$であるので$p'_y(\grave{y})\ne 0$よって$\grave{y} \ne \hat{y}$となり$\hat{x}=g(\hat{y})$はかならずしも成り立たない。

ただし、$x=g(y)$が線形変換の場合、$g''(y)=0$となるので$p'_y(\grave{y})=0$となり、$\grave{y}=\hat{y}$であるから$\hat{x}=g(\hat{y})$になる。

## 演習 1.5
<div class="panel-primary">

$(1.38)$の定義

$$
\operatorname{var}[f]=\mathbb{E}\left[(f(x)-\mathbb{E}[f(x)])^{2}\right] \tag{1.38}
$$

を使って$\mathrm{var}[f(x)]$が

$$
\operatorname{var}[f]=\mathbb{E}\left[f(x)^{2}\right]-\mathbb{E}[f(x)]^{2} \tag{1.39}
$$

を満たすことを示せ．
</div>

$$
\begin{aligned}
\operatorname{var}[f(x)]&=\mathbb{E}\left[(f(x)-\mathbb{E}[f(x)])^{2}\right] \\
&= \mathbb{E}\left[ f(x)^2 + \left( \mathbb{E}[f(x)] \right)^2-2f(x)\mathbb{E}[f(x)]\right] \\
&= \mathbb{E}[f(x)^2] + \left( \mathbb{E}[f(x)]\right)^2 - 2\mathbb{E}[f(x)]\cdot \mathbb{E}[f(x)] \\
&= \mathbb{E}[f(x)^2] - \mathbb{E}[f(x)]^2
\end{aligned}
$$

よって$(1.39)$式が示された．

## 演習 1.6
<div class="panel-primary">

2つの変数$x, y$が独立なら，それらの共分散は$0$になることを示せ．
</div>

$f(x), f(y), f(x, y)$を変数$x,y$の確率密度関数と同時確率密度関数としよう,
変数$x,y$は独立なので

$$
f(x, y)=f(x)f(y)
$$

この式を$x,y$の共分散に代入すると

$$
\begin{aligned}
\operatorname{cov}\left(x, y\right) &= \mathbb{E}_{x,y}[\left\{x-\mathbb{E}\left[x\right]\right\}\left\{y-\mathbb{E}\left[y\right]\right\}]\\
&= \mathbb{E}_{x,y}\left[x,y\right]-\mathbb{E}\left[x\right]\mathbb{E}\left[y\right]\\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} xyf\left(x,y\right)dxdy -\mathbb{E}\left[x\right]\mathbb{E}\left[y\right]\\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} xyf\left(x\right)f\left(y\right)dxdy -\mathbb{E}\left[x\right]\mathbb{E}\left[y\right]\\
&= \int_{-\infty}^{\infty}xf\left(x\right)dx \cdot \int_{-\infty}^{\infty}yf\left(y\right)dy - \mathbb{E}\left[x\right]\mathbb{E}\left[y\right]\\
&= \mathbb{E}\left[x\right]\mathbb{E}\left[y\right] - \mathbb{E}\left[x\right]\mathbb{E}\left[y\right]\\
&= 0
\end{aligned}
$$
となる.


## 演習 1.7
<div class="panel-primary">

この演習問題では,1変数ガウス分布に関する規格化条件

$$
\int_{-\infty}^{\infty} \mathcal{N}\left(x \mid \mu, \sigma^{2}\right) \mathrm{d} x=1 \tag{1.48}
$$

を証明する. このために, 積分
$$I=\int_{-\infty}^{\infty} \exp \left(-\frac{1}{2 \sigma^{2}} x^{2}\right) \mathrm{d} x　\tag{1.124}$$
を考え，その2乗を
$$I^{2}=\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \exp \left(-\frac{1}{2 \sigma^{2}} x^{2}-\frac{1}{2 \sigma^{2}} y^{2}\right) d x d y　\tag{1.125}$$
の形で書いて評価する. 直交座標系$(x,y)$から極座標$(r,\theta)$に変換し, $u=r^2$を代入する. $\theta$と$u$に関する積分を実行し, 両辺の平方根を取ることにより,
$$I=\left(2 \pi \sigma^{2}\right)^{1 / 2}　\tag{1.126}$$
が得られることを示せ.最後にこの結果からガウス分布$\mathcal{N}\left(x | \mu, \sigma^{2}\right)$が規格化されていることを示せ.
</div>

極座標系$x=r\cos\theta, y=r\sin\theta$とおいて、(1.125)式に代入すると、$f(r,\theta)=f(x,y)\left| \frac{\partial(x,y)}{\partial(r,\theta)} \right|$なので、このヤコビアン$\left| \frac{\partial(x,y)}{\partial(r,\theta)} \right|$は

$$
\frac{\partial(x, y)}{\partial(r, \theta)}=\left|\begin{array}{ll}\frac{\partial x}{\partial r} & \frac{\partial x}{\partial \theta} \\
\frac{\partial y}{\partial r} & \frac{\partial y}{\partial \theta}\end{array}\right|=\left|\begin{array}{cc}\cos \theta & -r \sin \theta \\
\sin \theta & r \cos \theta\end{array}\right|=r\left(\cos ^{2} \theta+\sin ^{2} \theta\right)=r
$$

なので、

$$
I^2 = \int_{0}^{2\pi}\int_{0}^{\infty}\exp\left(-\frac{1}{2\sigma^2}r^2 \right) r dr d\theta
$$

$u=r^2$と変換すると、$du=2rdr$なので

$$
\begin{aligned}
I^2 &= \int_{0}^{2\pi}\int_{0}^{\infty}\exp\left(-\frac{1}{2\sigma^2}u \right) \cdot \frac{1}{2}du d\theta \\
&= \pi \int_{0}^{\infty}\exp{\left( -\frac{1}{2\sigma^2}u\right)}dud\theta \\
&= \pi \left[ \exp{\left(-\frac{1}{2\sigma^2}u\right)}\cdot (-2\sigma^2)\right]_{0}^{\infty} \\
&= -2\pi\sigma^2(e^{-\infty} - e^0) = 2\pi\sigma^2
\end{aligned}
$$

両辺の平方根をとって式$(1.126)$の$I=\sqrt{2\pi\sigma^2}$を得る。

また、ガウス積分の式は積分範囲が$\mu$だけずれても$-\infty \to \infty$は変わらないので、

$$
\int_{-\infty}^{\infty} \mathcal{N}\left(x | \mu, \sigma^{2}\right) dx=\frac{1}{\left(2 \pi \sigma^{2}\right)^{1 / 2}} \int_{-\infty}^{\infty} \exp \left\{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right\}  dx= \frac{1}{\sqrt{2\pi\sigma^2}} \cdot \sqrt{2\pi\sigma^2}=1
$$

となり、正規化されていることが示された。

## 演習 1.8
<div class="panel-primary">

変数変換を使って1変数ガウス分布

$$
\mathcal{N}\left(x \mid \mu, \sigma^{2}\right)=\frac{1}{\left(2 \pi \sigma^{2}\right)^{1 / 2}} \exp \left\{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right\} \tag{1.46}
$$

が

$$
\mathbb{E}[x]=\int^{\infty}_{-\infty} \mathcal{N}\left(x \mid \mu, \sigma^{2}\right) x \mathrm{~d} x=\mu \tag{1.49}
$$

を満たすことを確かめよ. 次に, 規格化条件

$$\int_{-\infty}^{\infty} \mathcal{N}\left(x | \mu, \sigma^{2}\right) \mathrm{d} x=1 \tag{1.48}$$

の両辺を$\sigma^2$に関して微分し，ガウス分布が

$$
\mathbb{E}\left[x^{2}\right]=\int_{-\infty}^{\infty} \mathcal{N}\left(x \mid \mu, \sigma^{2}\right) x^{2} \mathrm{~d} x=\mu^{2}+\sigma^{2} \tag{1.50}
$$

を満たすことを確かめよ. 最後に

$$
\operatorname{var}[x]=\mathbb{E}\left[x^{2}\right]-\mathbb{E}[x]^{2}=\sigma^{2} \tag{1.51}
$$

が成り立つことを示せ.

</div>

$(1.46)$より

$$\mathbb{E}[x] = \int_{-\infty}^{\infty}\frac{x}{\sqrt{2\pi\sigma^2}}\exp{(-\frac{1}{2\sigma^2}(x-\mu)^2)}dx$$

ここで、$y=x-\mu$とおくと

$$
\begin{aligned}
\mathbb{E}[x]&=\int_{-\infty}^{\infty}\frac{y+\mu}{\sqrt{2\pi\sigma^2}}\exp{\left( -\frac{y^2}{2\sigma^2} \right)}dy \\
             &=\int_{-\infty}^{\infty}\frac{y}{\sqrt{2\pi\sigma^2}}\exp{\left( -\frac{y^2}{2\sigma^2} \right)}dy+\mu\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left( -\frac{y^2}{2\sigma^2} \right)}dy \\
             &=\int_{-\infty}^{\infty}\frac{y}{\sqrt{2\pi\sigma^2}}\exp{\left( -\frac{y^2}{2\sigma^2} \right)}dy+\mu\int_{-\infty}^{\infty}\mathcal{N}(y|0, \sigma^2)dy
\end{aligned}
$$

第1項は奇関数なので$0$であり、$(1.48)$より

$$\begin{aligned}\mathbb{E}[x]&=0+\mu・1\\&=\mu\end{aligned}$$

ゆえに$(1.46)$は$(1.49)$を満たす。

次に規格化条件に関して、$(1.46)$と$(1.48)$より

$$\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left(-\frac{1}{2\sigma^2}(x-\mu)^2 \right)}dx=1$$

$$\int_{-\infty}^{\infty}\exp{\left( -\frac{1}{2\sigma^2}(x-\mu)^2 \right)}dx=\sqrt{2\pi\sigma^2}$$

両辺を$\sigma^2$で微分すると

$$\int_{-\infty}^{\infty}\frac{(x-\mu)^2}{2(\sigma^2)^2}\exp{(-\frac{1}{2\sigma^2}(x-\mu)^2)}dx=(2\pi)^\frac{1}{2}\frac{(\sigma^2)^-\frac{1}{2}}{2}$$

整理すると

$$\int_{-\infty}^{\infty}\frac{(x-\mu)^2}{\sqrt{2\pi\sigma^2}}\exp{(-\frac{1}{2\sigma^2}(x-\mu)^2)}dx=\sigma^2$$

ここで、$(左辺)=\mathbb{E}[(x-\mu)^2]$なので

$$\mathbb{E}[(x-\mu)^2]=\sigma^2$$
$$\mathbb{E}[x^2-2\mu x+\mu^2]=\sigma^2$$
$$\mathbb{E}[x^2]-2\mu\mathbb{E}[x]+\mu^2=\sigma^2$$

$(1.49)$より$\mathbb{E}[x]=\mu$なので

$$\mathbb{E}[x^2]-2\mu^2+\mu^2=\sigma^2$$

$$\mathbb{E}[x^2]=\mu^2+\sigma^2$$

ゆえにガウス分布は(1.50)を満たす。

また(1.51)について、(1.49)と(1.50)より

$$
\begin{aligned}
\operatorname{var}[x]&=\mathbb{E}[x^2]-\mathbb{E}[x]^2\\&=(\mu^2+\sigma^2)-\mu^2\\&=\sigma^2
\end{aligned}
$$

## 演習 1.9
<div class="panel-primary">

ガウス分布

$$
\mathcal{N}\left(x \mid \mu, \sigma^{2}\right)=\frac{1}{\left(2 \pi \sigma^{2}\right)^{1 / 2}} \exp \left\{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right\} \tag{1.46}
$$

のモード（つまり分布が最大となる$x$の値）が, $\mu$で与えられることを示せ. 同様に, 多変量ガウス分布

$$
\mathcal{N}\left(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}\right) = \frac{1}{(2\pi)^\frac{D}{2}}\frac{1}{|\mathbf{\Sigma}|^\frac{1}{2}}\exp\left\{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\mathrm{T}\mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right\} \tag{1.52}
$$

のモードは$\boldsymbol{\mu}$で与えられることを示せ.

</div>

$$\mathcal{N}\left(x | \mu, \sigma^{2}\right) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{1}{2\sigma^2}(x-\mu)^2\right\}$$

の概形から$x$のモードは$\exp$の中身が最大の時、つまり$x=\mu$となるような$x$であるとわかる。厳密には$x$について微分をとって0になるときを計算すればよい。

また、多変量ガウス分布

$$\mathcal{N}\left(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}\right) = \frac{1}{(2\pi)^\frac{D}{2}}\frac{1}{|\mathbf{\Sigma}|^\frac{1}{2}}\exp\left\{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\mathrm{T}\mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right\}$$

について、これを$\mathbf{x}$で偏微分すると

$\frac{\partial}{\partial \mathbf{x}} \mathbf{x}^\mathrm{T}\mathbf{a}\mathbf{x} = (\mathbf{a}+\mathbf{a}^\mathrm{T})\mathbf{x}$ (1)と$\mathbf{\Sigma}$が対称行列であることを用いて

$$
\begin{aligned}
\frac{\partial}{\partial \mathbf{x}}\mathcal{N}\left(\mathbf{x} | \mu, \mathbf{\Sigma}\right) &= -\frac{1}{2}\mathcal{N}\left(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}\right)\nabla_{\mathbf{x}}\{(\mathbf{x}-\boldsymbol{\mu})^\mathrm{T}\mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\}\\
&=-\mathcal{N}\left(\mathbf{x} | \boldsymbol{\mu}, \mathbf{\Sigma}\right)\mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})
\end{aligned}
$$
となり最大値は$\mathbf{x}=\boldsymbol{\mu}$のときであり、このときモードをとる。

---
1,統計のための行列代数上巻 P.355(3.7)式参照


## 演習 1.10
<div class="panel-primary">

2変数$x,z$が統計的に独立であるとする. それらの和の平均と分散が

$$
\begin{aligned} \mathbb{E}[x+z] &=\mathbb{E}[x]+\mathbb{E}[z] \\ \operatorname{var}[x+z] &=\operatorname{var}[x]+\operatorname{var}[z] \end{aligned}
$$

を満たすことを示せ。

</div>

$x$と$z$は独立より $p(x, z) = p(x)p(z)$ であることに注意すると

$\mathbb{E}[x+z]$ は

$$
\begin{aligned}
\mathbb{E}[x+z] &= \int\int(x+z)p(x, z)\mathrm{d}x\mathrm{d}z \\
&= \int\int(x+z)p(x)p(z)\mathrm{d}x\mathrm{d}z \\
&= \int\int xp(x)p(z)\mathrm{d}x\mathrm{d}z + \int\int zp(x)p(z)\mathrm{d}x\mathrm{d}z \\
&= \int xp(x)\mathrm{d}x \int p(z)\mathrm{d}z + \int zp(z)\mathrm{d}z \int p(x)\mathrm{d}x \\
&= \int xp(x)\mathrm{d}x + \int zp(z)\mathrm{d}z \\
&= \mathbb{E}[x] + \mathbb{E}[z]
\end{aligned}
$$

となる。

また、上式を利用すると$\mathrm{var}[x+z]$は


$$
\begin{aligned}
\mathrm{var}[x+z] &= \mathbb{E}[\{(x+z) - \mathbb{E}[x+z]\}^2] \\
&= \int\int(x+z - \mathbb{E}[x+z])^2p(x, z)\mathrm{d}x\mathrm{d}z \\
&= \int\int(x-\mathbb{E}[x] + z-\mathbb{E}[z])^2p(x)p(z)\mathrm{d}x\mathrm{d}z \\
&= \int\int\{(x-\mathbb{E}[x])^2 + (z-\mathbb{E}[z])^2 + 2(x-\mathbb{E}[x])(z-\mathbb{E}[z])\}p(x)p(z)\mathrm{d}x\mathrm{d}z \\
&= \int(x-\mathbb{E}[x])^2p(x)\mathrm{d}x\int p(z)\mathrm{d}z + \int(z-\mathbb{E}[z])^2p(z)\mathrm{d}z\int p(x)\mathrm{d}x \\&+ 2\int\int(x-\mathbb{E}[x])(z-\mathbb{E}[z])p(x)p(z)\mathrm{d}x\mathrm{d}z \\
\end{aligned}
$$


ここで第3項について

$$
\begin{aligned}
(第3項)
&=2\int\int(x-\mathbb{E}[x])(z-\mathbb{E}[z])p(x)p(z)\mathrm{d}x\mathrm{d}z \\
&= 2\int(x-\mathbb{E}[x])p(x)\mathrm{d}x\int(z-\mathbb{E}[z])p(z)\mathrm{d}z \\
&= 2(\mathbb{E}[x]-\mathbb{E}[x])(\mathbb{E}[z]-\mathbb{E}[z]) = 0
\end{aligned}
$$

であるため

$$
\begin{aligned}
\mathrm{var}[x+z] &= \int(x-\mathbb{E}[x])^2p(x)\mathrm{d}x\int p(z)\mathrm{d}z + \int(z-\mathbb{E}[z])^2p(z)\mathrm{d}z\int p(x)\mathrm{d}x \\
&= \mathrm{var}[x] + \mathrm{var}[z]
\end{aligned}
$$

となる。
## 演習 1.11
<div class="panel-primary">

対数尤度関数

$$
\begin{aligned}
\ln p(\bf{x}|\mu,\sigma^2)
&= - \frac{1}{2\sigma^2} \sum_{n=1}^{N} (x_n-\mu)^2 - \frac{N}{2} \ln \sigma^2 - \frac{N}{2} \ln (2\pi)
\end{aligned} \tag{1.54}
$$

の$\mu$と$\sigma^2$に関する微分を$0$とおいて，

$$
\begin{aligned}
\mu_{\mathrm{ML}} &= \frac{1}{N} \sum_{n=1}^{N} x_n
\end{aligned} \tag{1.55}
$$

と

$$
\begin{aligned}
\sigma^2_{\mathrm{ML}} &= \frac{1}{N} \sum_{n=1}^{N} (x_n-\mu)^2
\end{aligned} \tag{1.56}
$$

を確かめよ．

</div>

対数尤度関数$(1.54)$を$f$と置く。
$\mu$で微分すると、第2・第3項は消えて

$$
\begin{aligned}
\frac{\partial f}{\partial \mu}
&=
\frac{1}{\sigma^2} \sum_{n=1}^{N} (x_n-\mu) =
\frac{1}{\sigma^2} \sum_{n=1}^{N} x_n - \frac{1}{\sigma^2} N\mu
\end{aligned}
$$

これを0と置くことで、$\mu$の最尤解$(1.55)$

$$
\begin{aligned}
\mu_{\mathrm{ML}} &= \frac{1}{N} \sum_{n=1}^{N} x_n
\end{aligned}
$$

を得る。

$\sigma^2$で微分すると、第3項は消えて

$$
\begin{aligned}
\frac{\partial f}{\partial \sigma^2}
&=
\frac{1}{2(\sigma^2)^2} \sum_{n=1}^{N} (x_n-\mu)^2 - \frac{N}{2\sigma^2}
\end{aligned}
$$

これを0と置くことで、$\sigma^2$の最尤解$(1.56)$

$$
\begin{aligned}
\sigma^2_{\mathrm{ML}} &= \frac{1}{N} \sum_{n=1}^{N} (x_n-\mu)^2
\end{aligned}
$$

を得る。

$\mu$と$\sigma^2$について同時に最尤推定を行うことで、$\mu_{\mathrm{ML}}$を$\sigma^2_{\mathrm{ML}}$に代入して

$$
\begin{aligned}
\sigma^2_{\mathrm{ML}} &= \frac{1}{N} \sum_{n=1}^{N} (x_n-\mu_{\mathrm{ML}})^2
\end{aligned}
$$
を得る。

## 演習 1.12
<div class="panel-primary">

$$
\mathbb{E}[x]=\int_{-\infty}^{\infty} \mathcal{N}\left(x \mid \mu, \sigma^{2}\right) x \mathrm{~d} x=\mu \tag{1.49}
$$

と

$$
\mathbb{E}\left[x^{2}\right]=\int_{-\infty}^{\infty} \mathcal{N}\left(x \mid \mu, \sigma^{2}\right) x^{2} \mathrm{~d} x=\mu^{2}+\sigma^{2} \tag{1.50}
$$

を使って

$$
\mathbb{E}\left[x_{n} x_{m}\right]=\mu^{2}+I_{n m} \sigma^{2} \tag{1.130}
$$
を示せ．ただし，$x_n$と$x_m$は平均$\mu$，分散$\sigma^2$のガウス分布から生成されたデータ点を表し，$I_{nm}$は$n=m$のとき$I_{nm}=1$でそれ以外は$I_{nm}=0$であるとする．これから

$$
\mathbb{E}\left[\mu_{\mathrm{ML}}\right]=\mu \tag{1.57}
$$

と

$$
\mathbb{E}\left[\sigma_{\mathrm{ML}}^{2}\right]=\left(\frac{N-1}{N}\right) \sigma^{2} \tag{1.58}
$$

を証明せよ．

</div>

まず，$\mathbb E[x_nx_m]$について

$n=m$のとき

$$\mathbb E[x_nx_m] = \mathbb E[x_n^2] = \mu^2 + \sigma^2$$

$n \ne m$のとき，$x_n$と$x_m$は独立であるから

$$\mathbb E[x_nx_m] = \mathbb E[x_n]\mathbb E[x_m] = \mu^2$$

まとめると

$$
\mathbb E[x_nx_m] = \mu^2 + I_{nm}\sigma^2
$$

となる．次に，$\mathbb E[\mu_{\mathrm{ML}}]$について

$$
\begin{aligned}
\mathbb E[\mu_{\mathrm{ML}}] &= \mathbb E\left[\frac{1}{N}\sum_{n=1}^N x_n\right] \\
&= \frac{1}{N}\sum_{n=1}^N \mathbb E\left[x_n\right] \\
&= \frac{1}{N}N \mu \\
&= \mu \\
\end{aligned}
$$

となる．また，$\mathbb E[\sigma_{\mathrm{ML}}^2]$について

$$
\begin{aligned}
\mathbb E[\sigma_{\mathrm{ML}}^2] &= \mathbb E\left[\frac{1}{N}\sum_{n=1}^N (x_n - \mu_{\mathrm{ML}})^2\right] \\
&= \mathbb E \left[\frac{1}{N}\sum_{n=1}^N \left(x_n - \frac{1}{N}\sum_{m=1}^N x_m\right)^2\right] \\
&= \frac{1}{N} \sum_{n=1}^N \mathbb E \left[x_n^2 - \frac{2}{N}x_n\sum_{m=1}^N x_m + \frac{1}{N^2}\left(\sum_{m=1}^N x_m\right)^2\right] \\
&= \frac{1}{N} \sum_{n=1}^N \left\{\mathbb E [x_n^2] - \frac{2}{N} \mathbb E \left[x_n\sum_{m=1}^N x_m \right] + \frac{1}{N^2} \mathbb E \left[\left(\sum_{m=1}^N x_m\right)^2\right]\right\} \\
&= \frac{1}{N} \sum_{n=1}^N \left\{\mathbb E [x_n^2] - \frac{2}{N} \sum_{m=1}^N \mathbb E \left[x_n x_m \right] + \frac{1}{N^2} \mathbb E \left[\left(\sum_{m=1}^N x_m\right)^2\right]\right\} \\
\end{aligned}
$$

ここで

$$
\begin{aligned}
\left(\sum_{m=1}^N x_m\right)^2 &= (x_1+x_2+\cdots+x_N)(x_1+x_2+\cdots+x_N) \\
&= (x_1^2+x_1x_2+\cdots+x_1x_N) + (x_1x_2+x_2^2+\cdots+x_2x_N) + \cdots + (x_1x_N+x_2x_N+\cdots+x_N^2)\\
&= \sum_{k=1}^N \sum_{l=1}^N x_k x_l
\end{aligned}
$$

より

$$
\begin{aligned}
\mathbb E[\sigma_{\mathrm{ML}}^2] &= \frac{1}{N} \sum_{n=1}^N \left\{\mathbb E [x_n^2] - \frac{2}{N} \sum_{m=1}^N \mathbb E \left[x_n x_m \right] + \frac{1}{N^2} \mathbb E \left[\sum_{k=1}^N \sum_{l=1}^N x_k x_l\right]\right\} \\
&= \frac{1}{N} \sum_{n=1}^N \left\{\mathbb E [x_n^2] - \frac{2}{N} \sum_{m=1}^N \mathbb E \left[x_n x_m \right] + \frac{1}{N^2} \sum_{k=1}^N \sum_{l=1}^N \mathbb E \left[x_k x_l\right]\right\} \\
&= \frac{1}{N} \sum_{n=1}^N \left\{(\mu^2 + \sigma^2) - \frac{2}{N} \left(\mu^2 + \sigma^2 + (N-1)\mu^2\right) + \frac{1}{N^2} \left(N(\mu^2 + \sigma^2) + N(N-1)\mu^2 \right)\right\} \\
&= \frac{1}{N} \sum_{n=1}^N \left(\mu^2 + \sigma^2 - 2\mu^2 - \frac{2}{N} \sigma^2 + \mu^2 + \frac{1}{N} \sigma^2\right) \\
&= \frac{1}{N} \sum_{n=1}^N \left(\frac{N-1}{N} \sigma^2 \right) \\
&= \frac{1}{N} N \left(\frac{N-1}{N} \sigma^2 \right) \\
&= \frac{N-1}{N} \sigma^2 \\
\end{aligned}
$$

となる．

## 演習 1.13
<div class="panel-primary">

ガウス分布の分散の推定値

$$
\sigma_{\mathrm{ML}}^{2}=\frac{1}{N} \sum_{n=1}^{N}\left(x_{n}-\mu_{\mathrm{ML}}\right)^{2} \tag{1.56}
$$

において，最尤推定値$\mu_{\mathrm{ML}}$を真の平均の値$\mu$で置き換えよう．この推定量は期待値が真の分散$\sigma^2$となる性質を持つことを示せ．

</div>

$\mu_{\mathrm {ML}}$を真の平均$\mu$で置き換えたものを$\hat\sigma_{\mathrm{ML}}^2$とし，その期待値をとると

$$
\begin{aligned}
\mathbb E[\hat\sigma_{\mathrm{ML}}^2] &= \mathbb E\left[\frac{1}{N}\sum_{n=1}^N (x_n - \mu)^2\right] \\
&= \frac{1}{N} \sum_{n=1}^N \mathbb E \left[x_n^2 - 2x_n\mu  + \mu^2 \right] \\
&= \frac{1}{N} \sum_{n=1}^N \left\{\mathbb E [x_n^2] - 2\mu \mathbb E \left[x_n \right] + \mu^2 \right\} \\
&= \frac{1}{N} \sum_{n=1}^N \left(\mu^2+\sigma^2 - 2\mu \mu + \mu^2 \right) \\
&= \sigma^2
\end{aligned}
$$

となる．

## 演習 1.14
<div class="panel-primary">

$w_{ij}$を成分とする任意の正方行列は$w_{ij}=w_{ij}^{\mathrm S}$+$w_{ij}^{\mathrm A}$という形に書けることを示せ．ただし,$w_{ij}^{\mathrm S}$と$w_{ij}^{\mathrm A}$はそれぞれ対称行列と反対称行列の成分であり$w_{ij}^{\mathrm S}=w_{ji}^{\mathrm S}$および$w_{ij}^{\mathrm A}=-w_{ji}^{\mathrm A}$がすべての$i, j$について成り立つ．さてここで，$D$次元における高次の多項式の2次の項

$$
\sum_{i=1}^{D} \sum_{j=1}^{D} w_{i j} x_{i} x_{j} \tag{1.131}
$$

を考えると，

$$
\sum_{i=1}^{D} \sum_{j=1}^{D} w_{i j} x_{i} x_{j}=\sum_{i=1}^{D} \sum_{j=1}^{D} w_{i j}^{\mathrm S} x_{i} x_{j} \tag{1.132}
$$

となり，反対称行列の寄与が消えることを示せ．このことから，一般性を失うことなく，係数$w_{ij}$は対称に選んでよく，すべての$D^2$の成分の選び方が独立ではないことがわかる．これを使って，行列$w_{ij}^{\mathrm S}$の独立パラメータの数が$D(D+1)/2$で与えられることを示せ．
</div>

$w_{ij}=w_{ij}^{\mathrm S}+w_{ij}^{\mathrm A}$であることを示す。
これは任意の正方行列の$i$行$j$列めの成分を$w_{ij}$としたとき、これを使って$w_{ij}^{\mathrm S}=\frac{1}{2}(w_{ij}+w_{ji})$とおくと、$w_{ij}^{\mathrm S}$は常に対称行列となる。同様に、$w_{ij}^{\mathrm A}=\frac{1}{2}(w_{ij}-w_{ji})$とおくと、この$w_{ij}^{\mathrm A}$は常に反対称行列となる。これらを用いると

$$
w_{ij}=w_{ij}^{\mathrm S}+w_{ij}^{\mathrm A}
$$

となるので、**任意の正方行列は、$w_{ij}^{\mathrm S}$を成分とする対称行列と、$w_{ij}^{\mathrm A}$を成分とする反対称行列の和で必ず表現できる**ことが示された。

ここで$w_{ij}^{\mathrm A}$について、

$$
\sum_{i=1}^{D} \sum_{j=1}^{D} w_{ij}^{\mathrm A} x_{i} x_{j} = \sum_{i=1}^{D} \sum_{j=1}^{D} \frac{1}{2}(w_{ij}-w_{ji}) x_{i} x_{j} = 0
$$

となるため、

$$
\sum_{i=1}^{D} \sum_{j=1}^{D} w_{i j} x_{i} x_{j}=\sum_{i=1}^{D} \sum_{j=1}^{D} w_{i j}^{\mathrm S} x_{i} x_{j}
$$

が常に成り立つことが示される。また、この結果から**二次形式の係数行列は対称行列とおいても一般性が失われない**という重要な帰結を得ることができる。

$w_{i j}^{\mathrm S}$については、対角成分を挟んで成分が対称になっていなければならないので、この行列の独立なパラメータは$\displaystyle \sum_{n=1}^{D}n=\frac{D(D+1)}{2}$個である。

## 演習 1.15（難）

<div class="panel-primary">

この演習問題と次の演習問題では，多項式の独立パラメータの数が多項式の次数$M$や入力空間の次元$D$に対してどのように増えるかを考える．まず，$D$次元の多項式の$M$次の項を書き下すと，

$$
\sum_{i_1=1}^{D} \sum_{i_2=1}^{D} \cdots \sum_{i_{M=1}}^{D} w_{i_{1} i_{2} \cdots i_{M}} x_{i_{1}} x_{i_{2}} \cdots x_{i_{M}} \tag{1.133}
$$

となる．係数$w_{i_{1}i_{2} \cdots i_{M}}$は$D^M$個あるが,そのうち独立なパラメータの数は$x_{i_1}x_{i_2} \cdots x_{i_M}$の多くの置換対称性からそれよりずっと少なくなる．始めに，$M$次の項を

$$
\sum_{i_{1}=1}^{D} \sum_{i_{2}=1}^{i_{1}} \cdots \sum_{i_{M}=1}^{i_{M-1}} \tilde{w}_{i_{1} i_{2} \cdots i_{M}} x_{i_{1}} x_{i_{2}} \cdots x_{i_{M}} \tag{1.134}
$$

と書き直すことによって係数の冗長性を取り除けることを示せ．ただし，$\tilde{w}$と$w$の厳密な関係は陽に表す必要はないことに注意せよ．この結果を使って，$M$次における独立なパラメータの数、$n(D, M)$が

$$
n(D, M)=\sum_{i=1}^{D} n(i, M-1) \tag{1.135}
$$

という再帰的な関係を満たすことを示せ．さらに，数学的帰納法により以下の結果が成り立つことを示せ．

$$
\sum_{i=1}^{D} \frac{(i+M-2) !}{(i-1) !(M-1) !}=\frac{(D+M-1) !}{(D-1) ! M !} \tag{1.136}
$$

これを示すには，まず$D=1$と任意の$M$の場合を$0!=1$を使って証明し，次に
$D$次元で成り立っていると仮定して，$D+1$次元でも成り立つことを確かめればよい．最後に，上の２つの結果から，数学的帰納法により

$$
n(D, M)=\frac{(D+M-1) !}{(D-1) ! M !} \tag{1.137}
$$

を示せ．これを示すには，まず$M=2$と任意の$D\ge 1$について正しいことを，演習問題1.14の結果との比較によって示し，次に，$(1.135)$と$(1.136)$を使って，$M-1$次で成り立てば$M$次でも成り立つことを示せばよい．

</div>

※ 問題文の意味が分かりにくいが、やろうとしていることは実はただの二項展開である。

問題が分かりにくいので、$D=4, M=2$として$x_1, x_2, x_3, x_4$の4次元の多項式について2次の項を書き下すことを考える。$(1.133)$の記法に従うと

$$
\sum_{i_1=1}^{4} \sum_{i_2=1}^{4} w_{i_{1} i_{2}} x_{i_{1}} x_{i_{2}}  \tag{1.133}
$$

と書けることになるが、この記法では例えば$w_{12}x_1 x_2$と$w_{21}x_2 x_1$が別々に現れて和をとる形になる。しかしかけ算の順序が交換可能であることを用いれば$(w_{12}+w_{21})x_2 x_1$とまとめて書くことができる。

そこで、$x$の添字が常に$i_1 \ge i_2 \ge \cdots i_M$となるようにし、その係数$w$の和をまとめて$\tilde{w}$と書き直す（上の例では$\tilde{w}_{21} = w_{12} + w_{21}$）ことでも一般性は失われない。

以上から、$(1.133)$式は

$$
\sum_{i_{1}=1}^{D} \sum_{i_{2}=1}^{i_{1}} \cdots \sum_{i_{M}=1}^{i_{M-1}} \tilde{w}_{i_{1} i_{2} \cdots i_{M}} x_{i_{1}} x_{i_{2}} \cdots x_{i_{M}} \tag{1.134}
$$

のように書き直せることがわかる（$\tilde{w}$と$w$の関係性を陽に表す必要はない）。

次に、この結果を用いると入力空間の次元$D$に対して$M$次における独立なパラメータの数というのは、すでに冗長性がなくなっているために

$$
\sum_{i_{1}=1}^{D} \sum_{i_{2}=1}^{i_{1}} \cdots \sum_{i_{M}=1}^{i_{M-1}} \tilde{w}_{i_{1} i_{2} \cdots i_{M}}
$$

の$\tilde{w}$の項の数と等しいことがわかる（※このへんでパスカルの三角形や二項展開のことが頭に思い浮かぶかもしれない）。すなわち

$$
n(D,M) = \sum_{i_{1}=1}^{D} \sum_{i_{2}=1}^{i_{1}} \cdots \sum_{i_{M}=1}^{i_{M-1}} 1
$$

となるので、あとはこれを求めれば良い。

この式について$D \to i$, $M \to M-1$とすると

$$
n(i, M-1)=\sum_{i_{1}=1}^{i} \sum_{i_{2}=1}^{i_{1}} \ldots \sum_{i_{(M-1)}=1}^{i_{(M-1)-1}} 1
$$

となるので、$(1.135)$式の右辺から計算すると

$$
\begin{aligned}
& \sum_{i=1}^{D} n(i, M-1) \\
=& \sum_{i=1}^{D} \sum_{i_{1}=1}^{i} \sum_{i_{2}=1}^{i_{1}} \ldots \sum_{i_{M-1}=1}^{i_{(M-1)-1}} 1 \\
=& \sum_{i_{1}=1}^{D} \sum_{i_{2}=1}^{i_{1}} \sum_{i_{3}=1}^{i_{2}} \cdots \sum_{i_{M}=1}^{i_{M-1}} 1 \quad (\because i \rightarrow i_{1}, i_{n} \rightarrow i_{n+1}) \\
=&\ n(D, M)
\end{aligned}
$$

となり、$(1.135)$式が成立することが示された。

後の数学的帰納法については……（書きかけ）

別解としては、結局のところ$n(D,M)$の計算部分は多項式の二項展開と同じなので、**異なる$D$個のパラメータから重複を許して$M$個とってくる場合の数と等しい**ことがわかる。すなわち

$$
n(D, M) = { }_{D-1} \mathrm{H}_{M-1} = { }_{D+M-1} \mathrm{C}_{M} = \frac{(D+M-1)!}{(D-1)!M!}
$$

であり、$(1.137)$式が得られる。

## 演習 1.16（難）

<div class="panel-primary">

演習問題1.15で，$M$次の$D$次元多項式の独立なパラメータの数が(1.135)となることを証明した．次に，$M$次までのすべての項における独立パラメータの総数$N(D,M)$を求めよう．まず$N(D,M)$が

$$
N(D, M)=\sum_{m=0}^{M} n(D, m) \tag{1.138}
$$
を満たすことを示せ．ただし，$n(D,M)$は$m$次の項における独立パラメータの数である．(1.137)の結果と数学的帰納法により，

$$
N(D, M)=\frac{(D+M) !}{D ! M !} \tag{1.139}
$$
を示せ．これを示すには，まず，$M=0$と任意の$D \ge 1$について成り立つことを証明してから，$M$次で成り立つなら$M+1$次で成り立つことを示せばよい．最後にスターリングの近似式，つまり$n$が大きいとき

$$
n ! \simeq n^{n} e^{-n} \tag{1.140}
$$

が成り立つことを使って,$D \gg M$のとき$N(D,M)$が$D^M$で大きくなり，$M \gg D$のときは$M^D$で大きくなることを示せ．$D$次元の3次多項式($M=3$)を考え，独立パラメータの総数を(i)$D=10$ (ii)$D=100$のそれぞれの場合について数値的に評価せよ．これらは典型的な小スケールおよび中スケールの機械学習の応用に対応する．

</div>

演習1.15の結果から$D$次元多項式のある$M$次の独立なパラメータが$n(D,M)$個であることが求まったので、$0 \le m \le M$までの合計のパラメータ数は

$$
N(D,M) = \sum_{m=0}^M n(D,m)
$$

と書ける（証明になっていないけれど自明だと思う）。

次に数学的帰納法を用いて

$$
N(D,M) =\frac{(D+M) !}{D ! M !}
$$

すなわち演習1.15の結果を用いて

$$
\sum_{m=0}^{M} \frac{(D+m-1) !}{(D-1) ! m !}=\frac{(D+M) !}{D ! M !} \tag{*}
$$

であることを示す（$D, M$はそれぞれ$1, 0$以上の整数値）。

(i) $M=0$のとき
$(左辺) = 1$, $(右辺) = 1$となるので成立する。

(ii) $M=k$のとき$(*)$が成立すると仮定する。このとき$M=k+1$において

$$
\begin{aligned}
\sum_{m=0}^{k+1} \frac{(D+m-1) !}{(D-1) ! m !} &=\frac{(D+k) !}{(D-1) !(k+1) !}+\sum_{m=0}^{k} \frac{(D+m-1) !}{(D-1) ! m !} \\
&=\frac{(D+k) !}{(D-1) !(k+1) !}+\frac{(D+k) !}{D ! k !} \\
&=\frac{(D+k) !}{D !(k+1) !}\{D+(k+1)\} \\
&=\frac{(D+(k+1)) !}{D !(k+1) !}
\end{aligned}
$$

となるので、$M=k+1$のときも成立することが示された。

最後に、まず$D \gg M$の条件下において、$\frac{M}{D} \ll 1$であることとスターリングの公式$n! \simeq n^n e^{-n}$を用いると

$$
\begin{aligned}
N(D, M) &=\frac{(D+M) !}{D ! M !} \\
& \simeq \frac{(D+M)^{D+M} e^{-D-M}}{D^{D} e^{-D} \cdot M !} \\
&=\frac{e^{-M}}{M ! D^{D}}(D+M)^{D+M} \\
&=\frac{D^{M} e^{-M}}{M !}\left(1+\frac{M}{D}\right)^{D+M} \\
& \simeq \frac{D^{M} e^{-M}}{M !}\left(1+\frac{M}{D}(D+M)\right) \\
&= \frac{D^M e^{-M}}{M!}\left\{ 1 + M\left( 1+ \frac{M}{D}\right)\right\} \\
& \simeq \frac{e^{-M}}{M!}(1+M)D^M
\end{aligned}
$$

となるのでこれは$D^M$が支配的になる。反対に$M \gg D$の条件下では文字を入れ替えた$\displaystyle \frac{e^{-D}}{D!}(1+D)M^D$となり、$M^D$が支配的になる。

数値的に評価すると$D=10$のとき$N(10, 3) = 286$, $D=100$のとき、$\displaystyle N(100,3) = \frac{103!}{100!3!} = 176851$となる（ちなみにスターリングの公式を用いると$\displaystyle N(100,3) \simeq \frac{e^{-3}}{3!}(1+3)100^3 = 33201.7$くらいなのでまだこの近似式は使えない。もっと大きな数になってくるとオーダーは揃うっぽい。）

## 演習 1.17
<div class="panel-primary">

ガンマ関数は

$$
\Gamma(x) \equiv \int_{0}^{\infty} u^{x-1} e^{-u} \mathrm{d} u \tag{1.141}
$$
で定義される．部分積分を使って関係式$\Gamma(x+1) = x\Gamma(x)$を証明せよ．また$\Gamma(1)=1$を示し，$x$が整数なら$\Gamma(x+1)=x!$となることを示せ．

</div>

$\Gamma(x+1)$について部分積分を行うと、

$$
\begin{aligned}
  \Gamma(x+1) &= \int_{0}^{\infty} u^x(-e^{-u})^{\prime} du \\
              &= \left [ -u^xe^{-u} \right]_{0}^{\infty} + x \int_{0}^{\infty}u^{x-1}e^{-u} du \\
              &= (0 - 0) + x \Gamma(x) \\
              &= x \Gamma(x)
\end{aligned}
$$

となる。（公式 $ \displaystyle \lim_{u \to \infty} u e^{-u} = \displaystyle \lim_{u \to \infty} \frac{u}{e^u} = 0 $ に注意）

また，$x=1$を代入すると

$$
\Gamma(1) = \int_{0}^{\infty}1\cdot e^{-u}du=\left[ -e^{-u} \right]_{0}^{\infty}=1
$$

これを用いると，$x \ge 1$の整数$x$に対して

$$
\begin{aligned}
  \Gamma(x+1) &= x \Gamma(x)\\
              &= x (x-1) (x-2) \cdots 2 \cdot 1 \cdot \Gamma(1) \\
              &= x!
\end{aligned}
$$
となる．

※ ガンマ関数は**階乗の概念を複素数全体に拡張した（複素階乗ともいう）特殊関数である**。 （https://ja.wikipedia.org/wiki/%E3%82%AC%E3%83%B3%E3%83%9E%E9%96%A2%E6%95%B0）

## 演習 1.18
<div class="panel-primary">

$D$次元の単位球の表面積$S_D$，体積$V_D$を導くのに
$$
I=\left(2 \pi \sigma^{2}\right)^{1 / 2} \tag{1.126}
$$
を使うことができる．これにはまず，直交座標から極座標への変換から導かれる

$$
\prod_{i=1}^{D} \int_{-\infty}^{\infty} e^{-x_{i}^{2}} \mathrm{d} x_{i}=S_{D} \int_{0}^{\infty} e^{-r^{2}} r^{D-1} \mathrm{d} r \tag{1.142}
$$

という事実を考える．ガンマ関数の定義$(1.141)$と$(1.126)$から，この式の両辺を評価し，

$$
S_{D}=\frac{2 \pi^{D / 2}}{\Gamma(D / 2)} \tag{1.143}
$$
を示せ．次に半径$0$から$1$まで積分し，$D$次元単位球の体積が

$$
V_D = \frac{S_D}{D} \tag{1.144}
$$
で与えられることを示せ．最後に$\Gamma(1)=1$および$\Gamma(3/2)=\sqrt{\pi} / 2$から，$(1.143)$と$(1.144)$が$D=2$および$D=3$の通常の表現に帰着されることを示せ．

</div>

Wikipediaの超平面や http://www.oit.ac.jp/ge/~nakano/Ex-0002.pdf なども参照。

$(1.126)$式の$\displaystyle \int_{-\infty}^{\infty}\exp\left( -\frac{1}{2\sigma^2}x^2\right)dx = \sqrt{2\pi\sigma^2}$をうまく使う．

$(1.142)$式の左辺の形にするために$\sigma^2 = 1/2$を代入すると，

$$
\prod_{i=1}^{D} \int_{-\infty}^{\infty} e^{-x_{i}^{2}} \mathrm{d} x_{i} = \left( \sqrt{2\pi\cdot (1/2)} \right)^D = \pi^{D/2}
$$

となる。一方で$(1.142)$式の右辺はガンマ関数の形なので$(1.141)$式について$u=r^2$を代入すると、$du=2rdr$なので

$$
\begin{aligned}
\Gamma(D/2) &= \int_{0}^{\infty}r^{D-2}e^{-r^2}\cdot 2rdr \\
            &= 2\int_{0}^{\infty}r^{D-1}e^{-r^2}dr
\end{aligned}
$$

よって、$\displaystyle \int_{0}^{\infty}r^{D-1}e^{-r^2}dr = \frac{\Gamma (D/2)}{2}$である。

$(1.142)$式を成立させるための$S_D$は

$$
S_D = \frac{2}{\Gamma(D/2)}\cdot \pi^{D/2} = \frac{2\pi^{D/2}}{\Gamma(D/2)}
$$

となり，$(1.143)$式が示された．

ここで，次元の考察から$D$次元球の表面積と体積は

$$
S_D(r) = S_D(1)r^{D-1},\hspace{1em}V_D(r) = V_D(1)r^D
$$

と書けることに注意する．$r$が$0 \to 1$を動いて全表面積を積分すれば$V_D(1)$になることを利用して，

$$
V_D(1) = \int_{0}^{1}S_D(1)r^{D-1}dr = S_D(1)\left[ \frac{r^D}{D} \right]_{0}^{1} = \frac{S_D}{D}
$$

もしくは、$V_D(r) = V_D(1)r^D$ の両辺をrで微分すると、

$$
S_D(r) = V_D(1)Dr^{D-1}
$$

$S_D(r) = S_D(1)r^{D-1}$ より、

$$
S_D(1)r^{D-1} = V_D(1)Dr^{D-1}
$$

$$
V_D(1) = S_D(1)/D
$$

これより，$V_D = S_D/D$を得る．$\Gamma(1)=1$, $\Gamma(3/2) = \pi / 2$を用いると、$S_2 = 2\pi, V_2 = \pi$, $S_3 = 4\pi，V_3 = \frac{4\pi}{3}$となり、$D=2$, $D=3$で見慣れた式になることがわかる．

（こちらの演習では、$S_D = S_D(1)$, $V_D = V_D(1)$と定義していることに注意する。）

## 演習 1.19
<div class="panel-primary">

$D$次元の半径$a$の球と，同じ中心を持つ一辺2$a$の超立方体を考える．球面は超立方体の各面の中心で接している．演習問題1.18の結果を使って，球の体積と立方体の体積の比が

$$
\frac{球の体積}{立方体の体積}=\frac{\pi^{D / 2}}{D 2^{D-1} \Gamma(D / 2)} \tag{1.145}
$$

で与えられることを示せ．スターリングの公式

$$
\Gamma(x+1) \simeq(2 \pi)^{1 / 2} e^{-x} x^{x+1 / 2} \tag{1.146}
$$

が$x \gg 1$で成り立つことを使って$D \to \infty$の極限で比の値$(1.145)$が$0$に収束することを示せ．また，超立方体の中心から1つの頂点までの距離を中心から側面までの距離で割った比が$\sqrt{D}$となることを示し，$D \to \infty$のとき$\infty$に発散することを示せ．これらの結果から，高次元空間では立方体の体積のほとんどはたくさんの頂点に集中し，非常に長い「スパイク」になっていることがわかる！

</div>

演習問題1.18の結果から、$D$次元の半径$a$の球の体積は$\displaystyle \frac{2 \pi^{D / 2}}{\Gamma(D / 2)}a^{D}$である。また、$D$次元の一辺$2a$の超立方体の体積は$(2a)^{D}$である。よって、その比率は

$$
\frac{\pi^{D / 2}}{D 2^{D-1} \Gamma(D / 2)}
$$

となる。これを$f(D)$とする。

スターリングの公式を用いると、

$$
\begin{aligned}
f(D) &= \frac{\pi^{D/2}}{2^{D}\Gamma{\left(\frac{D}{2}+1 \right)}} \\
     &\simeq \frac{\pi^{D/2}}{2^{D}\cdot\sqrt{2\pi} \cdot e^{-D/2}\cdot \left( \frac{D}{2} \right)^{\frac{D+1}{2}}} \\
     &= \frac{1}{\sqrt{\pi D}}\cdot \left( \frac{\pi e}{2D} \right)^{\frac{D}{2}}
\end{aligned}
$$

となるので、$D \to \infty$で$f(D) \to 0$となる。

超立方体の中心から側面までの距離は$a$で、中心から1つの頂点までの距離は$\sqrt{D}a$である（$D=2$で$\sqrt{2}a$, $D=3$で$\sqrt{3}a$となることと合致する）。よって比は$\sqrt{D}$となる。このとき$D \to \infty$でこの比$\sqrt{D}$は$\infty$に発散することがわかる。

このことは**高次元になればなるほど超立方体の体積が角に集中することになり、その角は非常に長いスパイク状である**という帰結を表している。

参考：https://windfall.hatenablog.com/entry/2015/07/02/084623 球面集中現象とも。

## 演習 1.20

<div class="panel-primary">

この演習問題では高次元ガウス分布の振る舞いを扱う．$D$次元ガウス分布

$$
p(\mathbf{x})=\frac{1}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \exp \left(-\frac{\|\mathbf{x}\|^{2}}{2 \sigma^{2}}\right) \tag{1.147}
$$

を考えよう．極座標系で，角度方向については積分し，半径に関する密度を求めたい．このために，半径$r$にある厚さ$\epsilon$の薄皮に関して$\mathbf{x}$の確率密度の積分をとると，$\epsilon \ll 1$のとき$p(r)\epsilon$となることを示せ（注：確率密度関数の添え字が（簡略化のため）省略されていて少々紛らわしいが，$p(r)$は半径$r$を確率変数と見たときの確率密度，$p(\mathbf{x})$は$\mathbf{x}$を確率変数と見たときの確率密度を表すことに注意されたい）．ただし，

$$
p(r)=\frac{S_{D} r^{D-1}}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \exp \left(-\frac{r^{2}}{2 \sigma^{2}}\right) \tag{1.148}
$$

であり，$S_D$は$D$次元単位球の表面積である関数$p(r)$が１つの停留点を持ち，$D$が大きいとき，$\widehat{r} \simeq \sqrt{D} \sigma$にあることを示せ．$\epsilon \ll \widehat{r}$について$p(\widehat{r}+\epsilon)$を考え，$D$が大きいとき

$$
p(\widehat{r}+\epsilon)=p(\widehat{r}) \exp \left(-\frac{\epsilon^{2}}{\sigma^{2}}\right) \tag{1.149}
$$

となることを示せ．これから$\widehat{r}$が確率密度の最大値を与える半径となり，$p(r)$が$\hat{r}$での最大値から$\sigma$の長さスケールで指数的に減衰していることがわかる．我々はすでに，$D$が大きいとき$\sigma \ll \widehat{r}$であることを見てきたので，ほとんどの確率質量が大きな半径の薄皮に集中していることがわかる．最後に，$\mathbf{x}$の確率密度$p(\mathbf{x})$は，半径$\hat{r}$にある地点よりも，原点での方が$\exp (D/2)$倍大きいことを示せ．このことから，ほとんどの高次元ガウス分布の確率質量は$\mathbf{x}$の確率密度の高いところとは異なる半径のところにあることがわかる．後の章でモデルパラメータのベイズ推論を考える際に，高次元空間の分布のこの性質を使って重要な結論を導くことになる．

</div>

混乱を避けるため，以下では$p(\mathbf{x})$を$p_{\mathbf{x}}(\mathbf{x})$，$p(r)$を$p_r(r)$と表す．


まず，半径$r$，厚さ$\epsilon$の薄皮の体積を求める．

$S_D$は$D$次元単位超球の表面積なので，半径$r$の位置での表面積は$S_D r^{D-1}$となる．$\epsilon \ll 1$の条件下では近似的に表面積が一定であると考えて良いので，薄皮の体積は$S_D r^{D-1}\epsilon$となる．

次に，$p_{\mathbf{x}}(\mathbf{x})$を薄皮$(shell)$に関して積分する．薄皮なので確率密度も一定と見なせるので

$$
\begin{aligned}
\int_{shell} p_{\mathbf{x}}(\mathbf{x}) \mathrm{d} \mathbf{x} \simeq & ~ p_{\mathbf{x}}(\mathbf{x}=r) S_D r^{D-1}\epsilon \\
=& \frac{1}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \exp \left(-\frac{r^{2}}{2 \sigma^{2}}\right) S_D r^{D-1}\epsilon \\
=& p_r(r)\epsilon \\
\end{aligned}
$$

となる．

また，$(1.148)$を$r$で微分すると

$$
\begin{aligned}
\frac{\mathrm{d}}{\mathrm{d} r} p_r(r) =& \frac{\mathrm{d}}{\mathrm{d} r} \left\{ \frac{S_{D} r^{D-1}}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \exp \left(-\frac{r^{2}}{2 \sigma^{2}}\right) \right\}\\
=& \frac{S_{D}}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \left\{ (D-1)r^{D-2} \exp \left(-\frac{r^{2}}{2 \sigma^{2}}\right) + r^{D-1} \left(-\frac{r}{\sigma^{2}}\right) \exp \left(-\frac{r^{2}}{2 \sigma^{2}}\right) \right\} \\
=& \frac{S_{D}}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \exp \left(-\frac{r^{2}}{2 \sigma^{2}}\right) \left\{ (D-1)r^{D-2} - \frac{r^D}{\sigma^{2}} \right\} \\
\end{aligned}
$$

となり，これが$0$となるときの$r$を$\hat{r}$とすると

$$
\begin{aligned}
& (D-1)\hat{r}^{D-2} - \frac{\hat{r}^D}{\sigma^{2}} = 0 \\
\Leftrightarrow ~ & \hat{r} = \sigma \sqrt{D-1}
\end{aligned}
$$

ここで$D \gg 1$とすると

$$
\hat{r} \simeq \sigma \sqrt{D}
$$

となる．

次に，$p_r(\hat{r}+\epsilon)$を考える．

$$
\begin{aligned}
p_r(\hat{r}+\epsilon) =& \frac{S_{D} (\hat{r}+\epsilon)^{D-1}}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \exp \left(-\frac{(\hat{r}+\epsilon)^{2}}{2 \sigma^{2}}\right) \\
=& \frac{S_{D} \hat{r}^{D-1}}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \exp \left(-\frac{\hat{r}^{2}}{2 \sigma^{2}}\right) \left( 1 + \frac{\epsilon}{\hat{r}} \right)^{D-1} \exp \left(-\frac{1}{2 \sigma^{2}} (2\hat{r}\epsilon + \epsilon^2 )\right) \\
=& p_r(\hat{r}) \left( 1 + \frac{\epsilon}{\hat{r}} \right)^{D-1} \exp \left(-\frac{1}{2 \sigma^{2}} (2\hat{r}\epsilon + \epsilon^2 )\right) \\
=& p_r(\hat{r}) \exp \left\{ \ln \left( 1 + \frac{\epsilon}{\hat{r}} \right)^{D-1} - \frac{1}{2 \sigma^{2}} (2\hat{r}\epsilon + \epsilon^2 )\right\} \\
=& p_r(\hat{r}) \exp \left\{ (D-1) \ln \left( 1 + \frac{\epsilon}{\hat{r}} \right) - \frac{1}{2 \sigma^{2}} (2\hat{r}\epsilon + \epsilon^2 )\right\} \\
\end{aligned}
$$

ここで$\ln (1+x)$のマクローリン展開より

$$
\begin{aligned}
\ln (1+x) =& x - \frac{1}{2}x^2 + \frac{1}{3}x^3 - \cdots \\
\simeq& ~ x - \frac{1}{2}x^2
\end{aligned}
$$

の近似を利用すると

$$
\begin{aligned}
p_r(\hat{r}+\epsilon)
\simeq& ~  p_r(\hat{r}) \exp \left\{ (D-1) \left( \frac{\epsilon}{\hat{r}} - \frac{\epsilon^2}{2 \hat{r}^2} \right) - \frac{1}{2 \sigma^{2}} (2\hat{r}\epsilon + \epsilon^2 )\right\} \\
=& p_r(\hat{r}) \exp \left\{ \frac{\hat{r}^2}{\sigma^2} \left( \frac{\epsilon}{\hat{r}} - \frac{\epsilon^2}{2 \hat{r}^2} \right) - \frac{1}{2 \sigma^{2}} (2\hat{r}\epsilon + \epsilon^2 )\right\} ~(\because \hat{r} = \sigma \sqrt{D-1}) \\
=& p_r(\hat{r}) \exp \left(-\frac{\epsilon^{2}}{\sigma^{2}}\right) \\
\end{aligned}
$$

となり，$(1.149)$が得られる．

また

$$
\begin{aligned}
p_{\mathbf{x}}(\mathbf{x} = \mathbf{0}) =& \frac{1}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \\
p_{\mathbf{x}}(\|\mathbf{x}\| = \hat{r}) =& \frac{1}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \exp \left(-\frac{\hat{r}^{2}}{2 \sigma^{2}}\right) \\
\simeq & \frac{1}{\left(2 \pi \sigma^{2}\right)^{D / 2}} \exp \left(-\frac{D}{2}\right) ~ (D \gg 1) \\
\end{aligned}
$$

であることより，確率密度$p_{\mathbf{x}}(\mathbf{x})$は半径$\hat{r}$にある地点での値$p_{\mathbf{x}}(\|\mathbf{x}\| = \hat{r})$に比べ，原点での方が$\exp (D/2)$倍大きいことが示された。

## 演習 1.21

<div class="panel-primary">

2つの非負の数$a$と$b$があったとき，$a \le b$なら$a \le (ab)^{\frac{1}{2}}$であることを示せ．この結果を使って，2クラスのクラス分類問題の決定領域を誤識別率が最小になるように選ぶと，この確率が

$$
p(\text { 誤り } ) \le \int\left\{p\left(\mathbf{x}, \mathcal{C}_{1}\right) p\left(\mathbf{x}, \mathcal{C}_{2}\right)\right\}^{1 / 2} \mathrm{d} \mathbf{x} \tag{1.150}
$$

を満たすことを示せ．

</div>

$a \le b$の両辺の平方根をとると、

$$
a^{\frac{1}{2}} \le b^{\frac{1}{2}}
$$

さらに両辺に$a^{\frac{1}{2}}$を掛けると、

$$
a \le (ab)^{\frac{1}{2}}
$$

が得られる。

また、$p(\text { 誤り } )$を最小にするには、決定領域$\mathcal{R}_{1}$において$p\left(\mathbf{x}, \mathcal{C}_{2}\right) \le p\left(\mathbf{x}, \mathcal{C}_{1}\right)$、$\mathcal{R}_{2}$において$p\left(\mathbf{x}, \mathcal{C}_{1}\right) \le p\left(\mathbf{x}, \mathcal{C}_{2}\right)$でなければならない。
これと$a \le (ab)^{\frac{1}{2}}$より、

$$
\begin{aligned} p(\text { 誤り } )
&=\int_{R_1} p\left(\mathbf{x}, \mathcal{C}_{2}\right) \ \mathrm{d} \mathbf{x}  + \int_{R_2}\ p\left(\mathbf{x}, \mathcal{C}_{1}\right) \mathrm{d} \mathbf{x} \\
&\le \int_{R_1}\left\{p\left(\mathbf{x}, \mathcal{C}_{1}\right) p\left(\mathbf{x}, \mathcal{C}_{2}\right)\right\}^{1 / 2} \mathrm{d} \mathbf{x} + \int_{R_2}\left\{p\left(\mathbf{x}, \mathcal{C}_{1}\right) p\left(\mathbf{x}, \mathcal{C}_{2}\right)\right\}^{1 / 2} \mathrm{d} \mathbf{x} \\
&=\int\left\{p\left(\mathbf{x}, \mathcal{C}_{1}\right) p\left(\mathbf{x}, \mathcal{C}_{2}\right)\right\}^{1 / 2} \mathrm{d} \mathbf{x}
\end{aligned}
$$

となり、式$(1.150)$が得られる。

## 演習 1.22

<div class="panel-primary">

$L_{kj}$を要素とする損失行列が与えられたとき，期待リスクが最小になるのは，各$\mathbf{x}$に対し，

$$
\sum_{k} L_{k j} p\left(\mathcal{C}_{k} \mid \mathbf{x}\right) \tag{1.81}
$$

を最小にするクラスを選んだときである．損失行列が$L_{kj}=1-I_{kj}$で与えられたとき，これが最大事後確率のクラスを選ぶ規準に帰着されることを確かめよ．ただし，$I_{kj}$は単位行列の成分を表す．また，この損失行列はどのように解釈できるか？

</div>

$(1.81)$式に$L_{kj}=1-I_{kj}$を代入すると、

$$
\sum_k {(1-I_{kj})p(\mathcal{C}_k | x})
$$

となり、$\sum_k {p(\mathcal{C}_k | x)} = 1$であることを用いると、$1-\sum_k{I_{kj}p(\mathcal{C}_k|x)}$と表せる。この式の最小化は$p(\mathcal{C}_k|x)$の最大化と同一である。

損失行列$1-I_{kj}$の解釈としては、損失行列の対角成分が$0$、他の成分が$1$の行列であるから、この行列の期待値の最小化は誤分類の最小化と解釈できる。

## 演習 1.23

<div class="panel-primary">

一般の場合に，損失行列とクラスに対する事前確率が与えられたときに，期待損失を最小にする規準を導け．

</div>

期待損失は損失行列$L$と事後確率$p\left(\mathcal{C}_{k}|x\right)$で下記のように簡単に表すことができる,

$$
\begin{aligned}
\sum_{k}^{}L_{kj}P\left(\mathcal{C}_{k}|x\right)
\end{aligned}
$$

損失行列$L$とクラスの事前確率$p\left(\mathcal{C}_{k}\right)$を代入すると
$$
\begin{aligned}
\sum_{k}^{}L_{kj}P\left(\mathcal{C}_{k}|x\right)
&= \frac{1}{p\left(x\right)}\sum_{k}^{}L_{kj}p\left(\mathcal{C}_{k}\right)p\left(x|\mathcal{C}_{k}\right)
\end{aligned}
$$
となる.

これによって、期待損失を最小化するには$x$を適切な$\mathcal{C}_{k}$に当てはめて、$L_{kj}$と$p\left(\mathcal{C}_{k}\right)$のトレードオフ関係を適切に扱うことが期待損失を最小にする基準となっている。

## 演習 1.24

<div class="panel-primary">

クラス分類問題を考え，クラス$\mathcal{C}_k$からの入力ベクトルをクラス$\mathcal{C}_j$と分類したときの損失行列を$L_{kj}$とし，棄却オプションを選んだときの損失を$\lambda$とする．このとき期待損失を最小とする決定規準を見つけよ．損失行列が$L_{kj}=1-I_{kj}$のときは，決定規準は1.5.3節で議論した棄却規準に帰着されることを確かめよ．また，$\lambda$と棄却しきい値$\theta$にはどんな関係があるか？

</div>

$(1.81)$より、クラス$\mathcal{C}_k$からの入力ベクトルをクラス$\mathcal{C}_j$と分類したとき、棄却を考えない場合は

$$
j = \arg \min_j \sum_{k}^{}L_{kj}p(C_k|\mathbf{x})
$$

この$j$を選んだ場合の損失関数は$\min_j \sum_{k}^{}L_{kj}p(\mathcal{C}_k|\mathbf{x})$となるので、棄却オプションを選んだ時に損失を最小にする決定規準は

$$
\text{choose}\left\{\begin{array}{ll}
\text{class} \quad j & (\min_j \sum_{k}^{}L_{kj}p(\mathcal{C}_k|\mathbf{x}) < \lambda) \\
\text{棄却} & (\text{上記以外})\end{array}\right.
$$


$L_{kj}=1-I_{kj}$のとき、$\sum_{k}^{}L_{kj}p(\mathcal{C}_k|\mathbf{x})=1-p(\mathcal{C}_j|\mathbf{x})$となる(演習1.22参照)。

上記規準に照らし合わせると$1-p(\mathcal{C}_j|\mathbf{x}) \ge \lambda$、つまり$p(\mathcal{C}_j|\mathbf{x}) \le 1-\lambda$のときに棄却となる。

1.5.3節の議論より、$p(\mathcal{C}_j|\mathbf{x}) \le \theta$の時に棄却となるので、$\theta = 1-\lambda$としたとき決定規準は1.5.3節で議論した棄却規準に帰着する。

## 演習 1.25

<div class="panel-primary">

単一の目標変数$t$の

$$
\mathbb{E}[L]=\iint\{y(\mathbf{x})-t\}^{2} p(\mathbf{x}, t) \mathbf{d} \mathbf{x} \mathrm{d} t \tag{1.87}
$$

の二乗損失関数のベクトル値$\mathbf{t}$で表される多変数の場合への以下の一般化について考える．

$$
\mathbb{E}[L(\mathbf{t}, \mathbf{y}(\mathbf{x}))]=\iint\|\mathbf{y}(\mathbf{x})-\mathbf{t}\|^{2} p(\mathbf{x}, \mathbf{t}) \mathrm{d} \mathbf{x} \mathrm{d} \mathbf{t} \tag{1.151}
$$

変分法によって，この期待損失を最小化する関数$\mathbf{y}(\mathbf{x})$が$\mathbf{y}(\mathbf{x})=\mathbb{E}_{\mathbf{t}}[\mathbf{t} | \mathbf{x}]$で与えられることを示せ．単一の目標変数$\mathbf{t}$の場合はこの結果が

$$
y(\mathbf{x})=\frac{\int t p(\mathbf{x}, t) \mathrm{d} t}{p(\mathbf{x})}=\int t p(t \mid \mathbf{x}) \mathrm{d} t=\mathbb{E}_{t}[t \mid \mathbf{x}] \tag{1.89}
$$

に帰着されることを確かめよ．

</div>

$(1.151)$は$(1.87)$で$t$をベクトル$\mathbf{t}$で置き換えることで得られる．

$$
\mathbb{E} \left[ L \right]  = \iint\|\mathbf{y}(\mathbf{x})-\mathbf{t}\|^{2} p(\mathbf{x}, \mathbf{t}) \mathrm{d} \mathbf{x} \mathrm{d} \mathbf{t} = \int G(\mathbf{y}, \mathbf{y}', \mathbf{x}) \mathrm{d} \mathbf{x}
$$

とおき，変分法を用いて期待損失を最小にする$\mathbf{y}$を求める．オイラー方程式より，

$$
\begin{aligned}
\frac{\delta \mathbb{E}[L]}{\delta \mathbf{y}(\mathbf{x})} =& \frac{\partial{G}}{\partial{\mathbf{y}}} - \frac{\mathrm{d}}{\mathrm{d}\mathbf{x}} \left(\frac{\partial{G}}{\partial{\mathbf{y}'}}\right) = \mathbf{0} \\
\end{aligned}
$$

ここで$\displaystyle \frac{\partial{G}}{\partial{\mathbf{y}'}} = \mathbf{0}$であることに注意すると

$$
\begin{aligned}
& \frac{\delta \mathbb{E}[L]}{\delta \mathbf{y}(\mathbf{x})} =\frac{\partial{G}}{\partial{\mathbf{y}}} = \mathbf{0}  \\
\Leftrightarrow ~ & \frac{\partial}{\partial{\mathbf{y}}} \int\|\mathbf{y}(\mathbf{x})-\mathbf{t}\|^{2} p(\mathbf{x}, \mathbf{t}) \mathrm{d} \mathbf{t} = \mathbf{0}  \\
\Leftrightarrow ~ & \int 2\{\mathbf{y}(\mathbf{x})-\mathbf{t}\} p(\mathbf{x}, \mathbf{t}) \mathrm{d} \mathbf{t} = \mathbf{0}  \\
\Leftrightarrow ~ & \mathbf{y}(\mathbf{x}) = \frac{\int \mathbf{t} p(\mathbf{x}, \mathbf{t}) \mathrm{d} \mathbf{t}}{\int p(\mathbf{x}, \mathbf{t}) \mathrm{d} \mathbf{t}} = \frac{\int \mathbf{t} p(\mathbf{x}, \mathbf{t}) \mathrm{d} \mathbf{t}}{p(\mathbf{x})} = \int \mathbf{t} p(\mathbf{t} | \mathbf{x}) \mathrm{d} \mathbf{t} = \mathbb{E}_t \left[ \mathbf{t} | \mathbf{x} \right]\\
\end{aligned}
$$

となる．ここでベクトル$\mathbf{t}$をスカラー$t$で置き換えると$(1.89)$が得られる．


## 演習 1.26

<div class="panel-primary">

$$
\mathbb{E}[L(\mathbf{t}, \mathbf{y}(\mathbf{x}))]=\iint\|\mathbf{y}(\mathbf{x})-\mathbf{t}\|^{2} p(\mathbf{x}, \mathbf{t}) \mathrm{d} \mathbf{x} \mathrm{d} \mathbf{t} \tag{1.151}
$$

の2乗を展開し，

$$
\mathbb{E}[L]=\int\{y(\mathbf{x})-\mathbb{E}[t \mid \mathbf{x}]\}^{2} p(\mathbf{x}) \mathrm{d} \mathbf{x}+\int \operatorname{var}[t \mid \mathbf{x}] p(\mathbf{x}) \mathrm{d} \mathbf{x} \tag{1.90}
$$

に類似の結果を導き，目標変数ベクトル$\mathbf{t}$の場合に期待二乗損失を最小にする関数$\mathbf{y}(\mathbf{x})$がやはり$\mathbf{t}$の条件付き期待値で与えられることを示せ．

</div>

まず$(1.151)$の$2$乗部分を展開すると

$$
\begin{aligned}
& ~ \|\mathbf{y}(\mathbf{x})-\mathbf{t}\|^2 \\
=& ~ \|\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}]+\mathbb{E}[\mathbf{t}|\mathbf{x}]-\mathbf{t}\|^2\\
=& ~ \|\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}]\|^2 + (\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}])^\mathrm{T}(\mathbb{E}[\mathbf{t}|\mathbf{x}]-\mathbf{t}) + (\mathbb{E}[\mathbf{t}|\mathbf{x}]-\mathbf{t})^\mathrm{T}(\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}]) + \|\mathbb{E}[\mathbf{t}|\mathbf{x}]-\mathbf{t}\|^2 \\
=& ~ \|\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}]\|^2 + 2(\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}])^\mathrm{T}(\mathbb{E}[\mathbf{t}|\mathbf{x}]-\mathbf{t}) + \|\mathbb{E}[\mathbf{t}|\mathbf{x}]-\mathbf{t}\|^2 \\
\end{aligned}
$$

となる．ここで第$2$項について，

$$
\begin{aligned}
& \iint(\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}])^\mathrm{T}(\mathbb{E}[\mathbf{t}|\mathbf{x}]-\mathbf{t})p(\mathbf{x},\mathbf{t})\mathrm{d}\mathbf{x}\mathrm{d}\mathbf{t} \\
=& \int(\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}])^\mathrm{T}\int(\mathbb{E}[\mathbf{t}|\mathbf{x}]-\mathbf{t})p(\mathbf{x},\mathbf{t})\mathrm{d}\mathbf{t}\mathrm{d}\mathbf{x} \\
=& \int(\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}])^\mathrm{T}(\mathbb{E}[\mathbf{t}|\mathbf{x}]-\mathbb{E}[\mathbf{t}|\mathbf{x}])\mathrm{d}\mathbf{x} \\
=& \mathbf{0}
\end{aligned}
$$

となる．$2$行目から$3$行目への変形には $\mathbb{E}[\mathbf{t}|\mathbf{x}]$ が定数であることと $\displaystyle \int\mathbf{t}p(\mathbf{x},\mathbf{t})\mathrm{d}\mathbf{t}=\mathbb{E}[\mathbf{t}|\mathbf{x}]$ を用いた．
これらと $\displaystyle \int p(\mathbf{x},\mathbf{t})\mathrm{d}\mathbf{t}=p(\mathbf{x}), \int\|\mathbf{t}-\mathbb{E}[\mathbf{t}|\mathbf{x}]\|^2p(\mathbf{t}|\mathbf{x})\mathrm{d}\mathbf{t}=\mathrm{var}[\mathbf{t}|\mathbf{x}]$ に注意すると， $\mathbb{E}[L]$ は

$$
\begin{aligned}
\mathbb{E}[L]
&= \iint\|\mathbf{y}(\mathbf{x})-\mathbf{t}\|^2p(\mathbf{x},\mathbf{t})\mathrm{d}\mathbf{x}\mathrm{d}\mathbf{t} \\
&= \iint\|\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}]\|^2p(\mathbf{x},\mathbf{t})\mathrm{d}\mathbf{x}\mathrm{d}\mathbf{t} + \iint\|\mathbb{E}[\mathbf{t}|\mathbf{x}]-\mathbf{t}\|^2p(\mathbf{x},\mathbf{t})\mathrm{d}\mathbf{x}\mathrm{d}\mathbf{t}\\
&= \int\|\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}]\|^2\int p(\mathbf{x},\mathbf{t})\mathrm{d}\mathbf{t}\mathrm{d}\mathbf{x} + \iint\|\mathbf{t}-\mathbb{E}[\mathbf{t}|\mathbf{x}]\|^2p(\mathbf{t}|\mathbf{x})\mathrm{d}\mathbf{t}p(\mathbf{x})\mathrm{d}\mathbf{x}\\
&= \int\|\mathbf{y}(\mathbf{x})-\mathbb{E}[\mathbf{t}|\mathbf{x}]\|^2p(\mathbf{x})\mathrm{d}\mathbf{x} + \int\mathrm{var}[\mathbf{t}|\mathbf{x}]p(\mathbf{x})\mathrm{d}\mathbf{x}\\
\end{aligned}
$$

と変形でき，期待二乗損失を最小にする $\mathbf{y}(\mathbf{x})$ が $\mathbf{t}$ の条件付き期待値で与えられることがわかる．

## 演習 1.27

<div class="panel-primary">

回帰の問題で，損失関数$L_q$が

$$
\mathbb{E}\left[L_{q}\right]=\iint|y(\mathbf{x})-t|^{q} p(\mathbf{x}, t) \mathrm{d} \mathbf{x} \mathrm{d} t \tag{1.91}
$$

で与えられるときの期待損失を考える．$y(\mathbf{x})$が$\mathbb{E}[L_q]$を最小化するために満たすべき条件を書き下せ.$q=1$に対しては解が条件付きメディアンになる，つまり，$t \lt y(\mathbf{x})$となる確率質量と$t \ge y(\mathbf{x})$となる確率質量は等しいことを示せ．また，$q \to 0$に対する$L_q$の期待損失を最小にするのは条件付きモード，つまり関数$y(\mathbf{x})$が，各$\mathbf{x}$に対して，$p(t|\mathbf{x})$を最大にする$t$の値に等しくなることを示せ．

</div>

$y(\mathbf{x})$は$\mathbf{x}$と独立なので$\mathbf{x}$に関する積分の中身$\int |y(\mathbf{x})-t|^q p(\mathbf{x},t) \mathrm{d} t$を最小化すれば良い。これを$y(\mathbf{x})$で微分すると、

$$
\begin{aligned}
q \int_{-\infty}^{y(\mathbf{x})} |y(\mathbf{x})-t|^{q-1} p(t|\mathbf{x}) \mathrm{d} t -
q \int_{y(\mathbf{x})}^{\infty} |y(\mathbf{x})-t|^{q-1} p(t|\mathbf{x}) \mathrm{d} t
\end{aligned}
$$

$y(\mathbf{x})$が$\mathbb{E}[L_q]$を最小化するために満たすべき条件は、これが0になることなので

$$
\begin{aligned}
q \int_{-\infty}^{y(\mathbf{x})} |y(\mathbf{x})-t|^{q-1} p(t|\mathbf{x}) \mathrm{d} t
&=
q \int_{y(\mathbf{x})}^{\infty} |y(\mathbf{x})-t|^{q-1} p(t|\mathbf{x}) \mathrm{d} t
\end{aligned}
$$

$q=1$に対しては

$$
\begin{aligned}
q \int_{-\infty}^{y(\mathbf{x})} p(t|\mathbf{x}) \mathrm{d} t
&=
q \int_{y(\mathbf{x})}^{\infty}  p(t|\mathbf{x}) \mathrm{d} t
\end{aligned}
$$

となる。つまり、任意の入力$\mathbf{x}$に対して$t$が$y(\mathbf{x})$の左右にいる確率は等しいのでこれは条件付きmedianである。

$q→0$のとき、再び微分前の式に戻って考える。$|y(\mathbf{x})-t|^q$のグラフを頭の中で頑張って想像して描いてみると、$t$のほとんどの値については$|y(\mathbf{x})-t|^q=1$だが、$y(\mathbf{x})=t$の近傍でだけ（絶対値の中が0に近いので）グラフが急激に落ち込み$|y(\mathbf{x})-t|^q\sim0$となる。

よって$p(\mathbf{x},t)$が最大となる$t$の場所に$y(\mathbf{x})$を合わせると、$\int |y(\mathbf{x})-t|^q p(\mathbf{x},t) \mathrm{d} t$は最小となる。これは条件付きmodeである。

## 演習 1.28

<div class="panel-primary">

1.6節でエントロピー$h(x)$のアイディアを確率分布$p(x)$を持つ確率変数$x$の値を観測することによって増える情報量として導入した．また，変数$x,y$が$p(x,y)=p(x)p(y)$となって独立なときは，エントロピーは加法的で$h(x,y)=h(x)+h(y)$となることを見た．この演習問題では，$h$と$p$の間の関数関係$h(p)$を導く．まず$h(p^2)=2h(p)$となることを示し，数学的帰納法により，正の整数$n$に対し$h(p^n)=nh(p)$となることをを示せ．さらに，正の整数$m$に対し，$h(p^{n/m})=(n/m)h(p)$が成り立つことを示せ．このことから$x$が正の有理数のとき，$h(p^x)=xh(p)$が成り立つが，これは連続性により正の実数値の場合も成り立つ．最後にこのことから$h(p)$が$h(p) \propto \ln p$の形を取らなければならないことを示せ．

</div>

まず1.6節の議論から、確率分布$p(x)$に依存し情報量を表す尺度$h(x)$を導入するとあるが、わかりやすくするため、$h(p(x))$と書くことにする。$p(x,y)=p(x)p(y)$と独立なときはエントロピーは加法的$h(x,y)=h(x)+h(y)$となるべきなので、
$$
\begin{aligned}
&h(x,y) = h(p(x,y)) = h(p(x)p(y)) \\
&h(x)+h(y) = h(p(x)) + h(p(y))
\end{aligned}
$$
となる。すなわち、$h(p(x))+h(p(y)) = h(p(x)p(y))$である。
ここで$y=x$とすると、
$$
\begin{aligned}
&h(p(x))+h(p(x)) = 2h(p(x)) \\
&h(p(x)p(x)) = h(p^2(x))
\end{aligned}
$$

よって$h(p^2)=2h(p)$であることが示された。

$h(p^k)=kh(p)$が成立することを示す。$n=1$のときは自明。$n=2$は上で示した。

$k\ge 3$となる$n=k$のとき$h(p^k)=kh(p)$が成立すると仮定すると、$n=k+1$のとき、
$$
h(p^{k+1})=h(p^kp)=h(p^k)+h(p)=kh(p)+h(p)=(k+1)h(p)
$$
となるので、数学的帰納法から$h(p^k)=kh(p)$であることが示された。

正の整数$m$について、$h(p^m) = mh(p)$となるので
$$
h(p^{n/m}) = nh(p^{1/m}) = \frac{n}{m}mh(p^{1/m}) = \frac{n}{m}h(p)
$$
となる。よって$x$が正の有理数ならば$h(p^x) = xh(p)$となる。またこれは連続性からすべての実数について成立する。

最後に$p=q^k$となるような正の実数$q,k$が存在したとき

$$
\frac{h(p)}{\ln p}=\frac{h(q^k)}{\ln q^k} = \frac{kh(q)}{k\ln q} = \frac{h(q)}{\ln q}
$$

となるので、$h(p)\propto \ln p$である。

## 演習 1.29

<div class="panel-primary">

$M$状態の離散確率変数$x$を考え，イェンセンの不等式

$$
f\left(\sum_{i=1}^{M} \lambda_{i} x_{i}\right) \leq \sum_{i=1}^{M} \lambda_{i} f\left(x_{i}\right) \tag{1.115}
$$

を使って，確率分布$p(x)$のエントロピーが$\mathrm{H}[x] \le \ln M$を満たすことを示せ．

</div>

イェンセンの不等式$(1.115)$は以下の通りである。

$f(x)$を実数上の凸関数（いわゆる下に凸）とする。$p_{1},p_{2},\ldots$ を、$p_{1}+p_{2}+\cdots =1$を満たす正の実数の列とする。また、$x_1, \, x_2, \, \ldots$を実数の列とする。そのとき次式が成り立つ。

$$
\sum_{i=1}^{\infty} p_i f(x_i) \ge f\left( \sum_{i=1}^{\infty} p_i x_i \right)
$$

また関数$f(x)$が凹関数（いわゆる上に凸）ならば上式の不等号は逆になる。

$M$状態存在するとき、確率分布$p(x)$のエントロピーは$(1.98)$式より
$$
\mathrm{H}[x] = -\sum_{i=1}^{M}p(x_i)\ln p(x_i)=\sum_{i=1}^{M}p(x_i)\ln \frac{1}{p(x_i)}
$$
となる。ここで、$p(x_i)\ge 0,\ \sum_{i=1}^{M}p(x_i)=1$である。

$\ln x$は凹関数であることに注意して、イェンセンの不等式の不等号を逆にすると、

$$
\mathrm{H}[x] = \sum_{i=1}^{M}p(x_i)\ln \frac{1}{p(x_i)} \le \ln\left( \sum_{i=1}^{M}p(x_i)\frac{1}{p(x_i)} \right) = \ln \left( \sum_{i=1}^M 1\right) = \ln M
$$

となり、題意は示された。

## 演習 1.30

<div class="panel-primary">

2つのガウス分布$p(x)=\mathcal{N}\left(x | \mu, \sigma^{2}\right)$と$q(x)=\mathcal{N}\left(x | m, s^{2}\right)$の間のカルバック–ライブラーダイバージェンス

$$
\begin{aligned}
\mathrm{KL}(p \| q) &=-\int p(\mathrm{x}) \ln q(\mathrm{x}) \mathrm{d} \mathrm{x}-\left(-\int p(\mathrm{x}) \ln p(\mathrm{x}) \mathrm{d} \mathrm{x}\right) \\
&=-\int p(\mathrm{x}) \ln \left\{\frac{q(\mathrm{x})}{p(\mathrm{x})}\right\} \mathrm{d} \mathrm{x}
\end{aligned} \tag{1.113}
$$

を計算せよ．

</div>
KLの定義より

$$
KL(p\|q)=-\int{p(x)\ln{q(x)}}dx+\int{p(x)\ln{p(x)}}dx
$$

右辺第一項に

$$
q(x)=\frac{1}{(2\pi s^2)^\frac{1}{2}}\exp\left(-\frac{(x-m)^2}{2s^2}\right)
$$

を代入すると

$$
-\int{p(x)\ln{q(x)}}dx=\frac{1}{2}\left(\ln({2\pi s^2})\int{p(x)}dx+\frac{1}{s^2}\left(\int{p(x)x^2}dx-2m\int{p(x)x}dx+m^2\int{p(x)}dx\right)\right)
$$

ここで
$$
\int{p(x)}dx=1
$$

$$
\int{p(x)x}dx=\mu
$$

$$
\int{p(x)x^2}dx=\mu^2+\sigma^2
$$

を代入して
$$
-\int{p(x)\ln{q(x)}}dx=\frac{1}{2}\left(\ln({2\pi s^2}) +\frac{1}{s^2}\left(\mu^2+\sigma^2+m^2-2m\mu \right)\right)
$$
また、KLの式の右辺第二項は微分エントロピーにマイナスをかけたものであるので、
$$
\int{p(x)\ln{p(x)}}dx=-\frac{1}{2}\left(1+\ln{2\pi\sigma^2}\right)
$$
である。

よって
$$
KL(p\|q)=\ln\left({\frac{s}{\sigma}}\right)+\frac{1}{2}\left(\frac{\mu^2+\sigma^2+m^2-2m\mu}{s^2}-1\right)
$$



## 演習 1.31

<div class="panel-primary">

2つの変数$\mathbf{x}, \mathbf{y}$を考え，同時分布を$p(\mathbf{x},\mathbf{y})$とする．この変数の組の微分エントロピーが

$$
\mathrm{H}[\mathbf{x}, \mathbf{y}] \le \mathrm{H}[\mathbf{x}] + \mathrm{H}[\mathbf{y}] \tag{1.152}
$$

を満たし，等号は$\mathbf{x}$と$\mathbf{y}$が統計的に独立なとき，またそのときに限ることを示せ．

</div>

(i)
変数$\mathbf{x}, \mathbf{y}$及び同時分布$p(\mathbf{x},\mathbf{y})$の微分エントロピーは以下の形式に変換することができる
$$
\begin{aligned}
H[x] &= -\int{p\left(x\right)\ln{p(x)}}dx\\
&= -\iint{p\left(x\right)p\left(y\right)\ln{p(x)}}dxdy\\
H[y] &= -\int{p\left(y\right)\ln{p(y)}}dy\\
&=-\iint{p\left(x\right)p\left(y\right)\ln{p(y)}}dxdy\\
H[x,y] &= -\iint{p\left(x, y\right)\ln{p(x,y)}}dxdy
\end{aligned}
$$
すると$\mathrm{H}[\mathbf{x}] + \mathrm{H}[\mathbf{y}]$は

$$
\begin{aligned}
H[x] + H[y] &=-\iint{p\left(x\right)p\left(y\right)\left(\ln{p(x)} + \ln{p(y)}\right)}dxdy\\
&=-\iint{p\left(x\right)p\left(y\right)\ln{p(x)p(y)}}dxdy
\end{aligned}
$$
となる。

$p(x,y) \ge p(x)p(y)$によって,$\mathrm{H}[\mathbf{x}, \mathbf{y}] \le \mathrm{H}[\mathbf{x}] + \mathrm{H}[\mathbf{y}]$が満たされ，等号は$p(x,y) = p(x)p(y)$の時、つまり$\mathbf{x}$と$\mathbf{y}$が統計的に独立なとき，またそのときに限ることが示された．
<br><br>

(ii)
(1.112)より
$$
H[x,y] =H[x|y]+H[x]
$$
であるので、
$$
H[x|y]+H[x]\le H[x]+H[y]
$$
すなわち
$$
H[y]-H[x|y] \ge 0
$$
を示せば良い。
<br>

ここで、(1.121)より(左辺)は
$$
H[y]-H[x|y]=I[x,y]
$$
と書き表せ、これは変数x,yの間の相互情報量を表す。
<br>

よって、カルバックーライブラーダイバージェンスの性質から
$$
I[x,y] \ge 0
$$
$$
H[y]-H[x|y] \ge 0
$$
$$
H[x|y]+H[x]\le H[x]+H[y]
$$
$$
H[x,y]\le H[x]+H[y]
$$

<br>
となり、等号成立は $I[x,y]=0$ すなわち$x$と$y$が独立なときに限ることが示された。


## 演習 1.32

<div class="panel-primary">

連続変数のベクトル$\mathbf{x}$を考え，それが分布$p(\mathbf{x})$とそれに対応するエントロピー$\mathrm{H}[\mathbf{x}]$を持つとする．$\mathbf{x}$に非特異な線形変換を行い，新たな変数$\mathbf{y} = \mathbf{A}\mathbf{x}$を得たとする．対応するエントロピーが$\mathrm{H}[\mathbf{y}] = \mathrm{H}[\mathbf{x}] + \ln |\det (\mathbf{A})|$で与えられることを示せ．ただし$|\det (\mathbf{A})|$は$\mathbf{A}$の行列式の絶対値である．

</div>

(1.27)より

$$p(x) = p(y)\left| \frac{dy}{dx} \right|$$

ここで$\mathbf{y}=\mathbf{A}\mathbf{x}$より、ヤコビアンは

$$\frac{d\mathbf{y}}{d\mathbf{x}}=\mathbf{A}$$

ゆえに

$$p(\mathbf{x})=p(\mathbf{y})|det(\mathbf{A})|$$

であるから、

$$p(\mathbf{y})=|det(\mathbf{A})|^{-1} p(\mathbf{x})$$

$$d\mathbf{y}=|det(\mathbf{A})| d\mathbf{x} $$

と書き換えることができる。

以上より

$$\begin{aligned}H[\mathbf{y}]&=-\int p(\mathbf{y}) \ln p(\mathbf{y}) d\mathbf{y} \\&= -\int \{|det(\mathbf{A})|^{-1} p(\mathbf{x})\} \{\ln (|det(\mathbf{A})|^{-1} p(\mathbf{x})) \}|det(\mathbf{A})| d\mathbf{x} \\&=-\int \{|det(\mathbf{A})|^{-1} p(\mathbf{x})\} \{\ln (|det(\mathbf{A})|^{-1}\}|det(\mathbf{A})| d\mathbf{x}-\{|det(\mathbb{A})|^{-1} p(\mathbf{x})\} \{\ln p(\mathbf{x})\}|det(\mathbf{A})| d\mathbf{x}
\\&=  |det(\mathbf{A})|\int p(\mathbf{x}) \ln d\mathbf{x} - \int p(\mathbf{x}) \ln p(\mathbf{x}) d\mathbf{x} \\&=  \ln |det(\mathbf{A})|+H[\mathbf{x}] \end{aligned} $$

<br>

となり、$\mathrm{H}[\mathbf{y}] = \mathrm{H}[\mathbf{x}] + \ln |\det (\mathbf{A})|$が示された。

## 演習 1.33

<div class="panel-primary">

2つの離散確率変数$x,y$の間の条件付きエントロピー$\mathrm{H}[y|x]$が$0$であるとする．すると$p(x) > 0$なるすべての$x$の値に対し，変数$y$は$x$の関数でなければならない，すなわち，各$x$に対して$p(y|x)\ne 0$である$y$が唯一つ存在することを示せ．

</div>

$$ \tag{1.98}
\mathrm{H}[p] = -\sum_{i}p(x_i)\ln p(x_i)
$$

$$ \tag{1.111}
\mathrm{H}[\mathbf{y} \mid \mathbf{x}] = - \iint p(\mathbf{y}, \mathbf{x}) \ln p(\mathbf{y}  \mid  \mathbf{x}) \mathrm{d}\mathbf{y} \mathrm{d}\mathbf{x}
$$

を参考にすると，離散確率変数の条件付きエントロピーは

$$
\begin{aligned}
\mathrm{H}[y \mid x] =& - \sum_{i}\sum_{j} p(y_i, x_j) \ln p(y_i \mid x_j) \\
=& - \sum_{i}\sum_{j} p(y_i \mid x_j) p(x_j) \ln p(y_i \mid x_j) \\
\end{aligned}
$$

となる．ここで，問題文の$\mathrm{H}[y \mid x]=0$と$p(x) > 0$という条件より，

$$
\begin{aligned}
& \mathrm{H}[y \mid x] = 0 \\
\Leftrightarrow & -\sum_{i}\sum_{j} p(y_i \mid x_j) p(x_j) \ln p(y_i \mid x_j) = 0 \\
\Leftrightarrow & - \sum_{i}\sum_{j} p(y_i \mid x_j) \ln p(y_i \mid x_j) = 0 \\
\end{aligned}
$$

となる必要がある．ここでさらに，$0 \leqslant p(y_i \mid x_j) \leqslant 1$より，$0 < - p(y_i \mid x_j) \ln p(y_i \mid x_j)$であるため結局全ての$i, j$において

$$
p(y_i \mid x_j) \ln p(y_i \mid x_j) = 0
$$

となる必要がある．これが成立するのは，$p(y_i \mid x_j) = 0$または$p(y_i \mid x_j) = 1$のときであるが，$p(y_i \mid x_j)$は確率なので$\displaystyle \sum_i p(y_i \mid x_j) = 1$となる必要がある．つまり，ある$x_j$に対して$p(y_i \mid x_j)$は$1$つだけ$1$で残りが全て$0$でなければならない．これはまさに，各$x$に対して$p(y \mid x)\ne 0$である$y$が唯一つ存在することを示している．

## 演習 1.34

<div class="panel-primary">

変分法を使って，

$$
p(x)=\exp \left\{-1+\lambda_{1}+\lambda_{2} x+\lambda_{3}(x-\mu)^{2}\right\} \tag{1.108}
$$

式の上にある汎関数の停留点が$(1.108)$で与えられることを示せ．また，制約

\begin{align}
\int_{-\infty}^{\infty} p(x) \mathrm{d} x &=1 \tag{1.105} \\
\int_{-\infty}^{\infty} x p(x) \mathrm{d} x &=\mu \tag{1.106} \\
\int_{-\infty}^{\infty}(x-\mu)^{2} p(x) \mathrm{d} x &=\sigma^{2} \tag{1.107}
\end{align}

を使ってラグランジュ乗数を消去し，最大エントロピー解がガウス分布

$$
p(x)=\frac{1}{\left(2 \pi \sigma^{2}\right)^{1 / 2}} \exp \left\{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right\} \tag{1.109}
$$

で与えられることを示せ．

</div>

(1.108)式の上にある汎関数を$\mathrm{F}(p(x))$とおくと

$$
\begin{aligned}
\mathrm{F}(p(x)) &= \int^{\infty}_{-\infty} \{ -\ln p(x) + \lambda_1 + \lambda_2x + \lambda_3(x-\mu)^2 \}p(x)\mathrm{d}x + (-\lambda_1-\lambda_2\mu-\lambda_3\sigma^2) \\
&= \int^{\infty}_{-\infty}\mathrm{G}(p(x))\mathrm{d}x + \mathrm{C}
\end{aligned}
$$

と変形できる。
変分法により

$$
\begin{aligned}
\frac{\delta\mathrm{F}(p(x))}{\delta p(x)} &= \frac{\partial\mathrm{G}(p(x))}{\partial p(x)} \\
&= -1 -\ln p(x) + \lambda_1 + \lambda_2x + \lambda_3(x-\mu)^2
\end{aligned}
$$

これを$0$とおくと

$$
\begin{aligned}
\ln p(x) &= -1 + \lambda_1 + \lambda_2x + \lambda_3(x-\mu)^2 \\
p(x) &= \exp \{-1 + \lambda_1 + \lambda_2x + \lambda_3(x-\mu)^2\}
\end{aligned}
$$

となり、(1.108)が導出できる。
また、最大エントロピー解がガウス分布であるとすると

$$
\begin{aligned}
p(x) &= \exp \{-1 + \lambda_1 + \lambda_2x + \lambda_3(x-\mu)^2\} \\
&= \exp(-1 + \lambda_1 + \lambda_2x) \exp \{\lambda_3(x-\mu)^2\} \\
&= \frac{1}{(2\pi\sigma)^{1/2}}\exp\{-\frac{(x-\mu)^2}{2\sigma^2}\}
\end{aligned}
$$

とおける。
上式より

$$
\begin{aligned}
\lambda_1 &= 1-\frac{1}{2}\ln(2\pi\sigma^2) \\
\lambda_2 &= 0 \\
\lambda_3 &= -\frac{1}{2\sigma^2}
\end{aligned}
$$

とするとこれは(1.105), (1.106), (1.107)を満たし、$p(x)$がガウス分布であることを示す。

## 演習 1.35

<div class="panel-primary">

$$
\int_{-\infty}^{\infty} x p(x) \mathrm{d} x =\mu \tag{1.106}
$$

と

$$
\int_{-\infty}^{\infty}(x-\mu)^{2} p(x) \mathrm{d} x =\sigma^{2} \tag{1.107}
$$

を使って，1変数ガウス分布

$$
p(x)=\frac{1}{\left(2 \pi \sigma^{2}\right)^{1 / 2}} \exp \left\{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right\} \tag{1.109}
$$

のエントロピーが

$$
\mathrm{H}[x]=\frac{1}{2}\left\{1+\ln \left(2 \pi \sigma^{2}\right)\right\} \tag{1.110}
$$

で与えられることを示せ．

</div>

$(1.104)$の微分エントロピーの定義式の$\ln p(x)$の$p(x)$に$(1.105)$を代入すると

$$
\begin{aligned} H[x]
&=-\int p(x) \ln \left(\frac{1}{\left(2 \pi \sigma^{2}\right)^{\frac{1}{2}}} \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right)\right) dx \\
&=-\int p(x)\left(-\frac{1}{2} \ln \left(2 \pi \sigma^{2}\right)-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right)dx \\
&=\frac{1}{2} \ln \left(2 \pi \sigma^{2}\right) \int p(x) d x+\frac{1}{2 \sigma^{2}} \int p(x)(x-\mu)^{2}dx \\
&=\frac{1}{2}\left\{1+\ln \left(2 \pi \sigma^{2}\right)\right\}
\end{aligned}
$$

式(1.106)の$\int_{-\infty}^{\infty} x p(x) \mathrm{d} x=\mu$と式(1.107)の$\int_{-\infty}^{\infty}(x-\mu)^{2} p(x) \mathrm{d} x=\sigma^{2}$を利用した。
## 演習 1.36

<div class="panel-primary">

真に凸な関数はすべての弦が関数の上にあるものとして定義される．これが関数の2階微分が正であることと等価であることを示せ．

</div>

テイラー展開の2次まで使うと、$x=x_0$の周りで展開したとき

$$
f(x)=f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)+\frac{1}{2} f^{\prime\prime}\left(x^{*}\right)\left(x-x_{0}\right)^{2}
$$
となるような$x^{*}$が存在する。今、$f^{\prime \prime}\left(x^{*}\right)>0$なので,

$$
f(x)>f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)
$$

が成り立つ。今、$x_0$を$a, b, \lambda$を使って$x_{0}=\lambda a+(1-\lambda) b$とすると、$x=a$の点で成り立つ式は

$$
\begin{aligned}
f(a)&>f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(a-x_{0}\right) \\
&= f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)(1-\lambda)(a-b)
\end{aligned}\tag{1}
$$
同様に、$x=b$の点で成り立つ式は

$$
\begin{aligned} f(b) &>f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(b-x_{0}\right) \\ &=f\left(x_{0}\right)-f^{\prime}\left(x_{0}\right) \lambda(a-b) \end{aligned}\tag{2}
$$
(1)を$\lambda$倍、(2)を$(1-\lambda)$倍して足し合わせると、

$$
\lambda f(a)+(1-\lambda) f(b)>f\left(x_{0}\right)=f(\lambda a+(1-\lambda) b)
$$
となり、凸性の条件式(1.114)を得る。

## 演習 1.37

<div class="panel-primary">

$$
\mathrm{H}[\mathbf{y} \mid \mathbf{x}]=-\iint p(\mathbf{y}, \mathbf{x}) \ln p(\mathbf{y} \mid \mathbf{x}) \mathrm{d} \mathbf{y} \mathrm{d} \mathbf{x} \tag{1.111}
$$

の定義と確率の乗法定理から，

$$
\mathrm{H}[\mathbf{x}, \mathbf{y}]=\mathrm{H}[\mathbf{y} \mid \mathbf{x}]+\mathrm{H}[\mathbf{x}] \tag{1.112}
$$

を証明せよ．

</div>


式$(1.104)$の$H[\mathbf{x}]= - \int p(\mathbf{x}) \ln p(\mathbf{x}) \mathrm{d}\mathbf{x}$ より、$p(\mathbf{x},\mathbf{y})$の微分エントロピーは、

$$
\begin{aligned}
H[\mathbf{x},\mathbf{y}]
&= - \int \int p(\mathbf{x},\mathbf{y}) \ln p(\mathbf{x},\mathbf{y}) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y}
\end{aligned}
$$

同時分布$p(\mathbf{x},\mathbf{y}) = p(\mathbf{y}|\mathbf{x})p(\mathbf{x})$ より、


$$
\begin{aligned}
H[\mathbf{x},\mathbf{y}]
&= - \int \int p(\mathbf{x},\mathbf{y}) \ln (p(\mathbf{y}|\mathbf{x})p(\mathbf{x})) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y} \\
&= - \int \int p(\mathbf{x},\mathbf{y}) (\ln p(\mathbf{y}|\mathbf{x}) + \ln p(\mathbf{x})) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y} \\
&= - \int \int p(\mathbf{x},\mathbf{y}) \ln p(\mathbf{y}|\mathbf{x}) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y} - \int \int p(\mathbf{x},\mathbf{y}) \ln p(\mathbf{x}) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y}
\end{aligned}
$$

$ \int \int p(\mathbf{x},\mathbf{y}) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y}  = \int p(\mathbf{x}) \mathrm{d}\mathbf{x}$ より、

$$
\begin{aligned}
H[\mathbf{x},\mathbf{y}]
&= - \int \int p(\mathbf{x},\mathbf{y}) \ln p(\mathbf{y}|\mathbf{x}) \mathrm{d}\mathbf{x} \mathrm{d}\mathbf{y} - \int p(\mathbf{x}) \ln p(\mathbf{x}) \mathrm{d}\mathbf{x} \\
&= H[\mathbf{y}| \mathbf{x}] + H[\mathbf{x}]
\end{aligned}
$$


## 演習 1.38

<div class="panel-primary">

数学的帰納法により，凸関数に関する不等式

$$
f(\lambda a+(1-\lambda) b) \leq \lambda f(a)+(1-\lambda) f(b) \tag{1.114}
$$

から
$$
f\left(\sum_{i=1}^{M} \lambda_{i} x_{i}\right) \leq \sum_{i=1}^{M} \lambda_{i} f\left(x_{i}\right) \tag{1.115}
$$

が導かれることを示せ．


</div>

$(1.114)$式は、高校数学でやったJensenの不等式


![Jensenの不等式.png](/attachment/60af4328d83a250c8eef7952)

<br>

$(1.114)$式の凸性を表す式$f(\lambda a+(1-\lambda) b) \leq \lambda f(a)+(1-\lambda) f(b)$から

$$
f\left(\sum_{i=1}^{M} \lambda_{i} x_{i}\right) \leq \sum_{i=1}^{M} \lambda_{i} f\left(x_{i}\right) \cdots(*)
$$

が成立することを数学的帰納法で示す。ここで、$\lambda_i\geq0$および$\sum_{i}\lambda_i = 1$である。

<br>

$(ⅰ) M=1$のとき、

$\lambda=1$なので、$(*)$式は$f(x_1) \le f(x_1)$となるので成立する。

$(ⅱ) M=2$のとき、

$\lambda_2=1-\lambda_1$であることに注意すると

$$
f\left(\lambda_{1} x_{1}+\left(1-\lambda_{1}\right) x_{2}\right) \leq \lambda_{1} f\left(x_{1}\right)+\left(1-\lambda_{1}\right) f\left(x_{2}\right)
$$

これは$(1.114)$式と同じなので、成立する。

$(ⅲ) M=k\ (k\ge 2)$のとき、

$$
f\left(\sum_{i=1}^{k} \lambda_{i} x_{i}\right) \leq \sum_{i=1}^{k} \lambda_{i} f\left(x_{i}\right) \
$$

成立していると仮定すると、$M=k+1$のとき

$$
\begin{aligned}
f\left(\sum_{i=1}^{k+1} \lambda_{i} x_{i}\right) &=f\left(\sum_{i=1}^{k} \lambda_{i} x_{i}+\lambda_{k+1} x_{k+1}\right) \\
&=f\left(\left(1-\lambda_{k+1}\right) \frac{\sum_{i=1}^{k} \lambda_{i} x_{i}}{1-\lambda_{k+1}}+\lambda_{k+1} x_{k+1}\right) \\
& \leq\left(1-\lambda_{k+1}\right) f\left(\frac{\sum_{i=1}^{k} \lambda_{i} x_{i}}{1-\lambda_{k+1}}\right)+\lambda_{k+1} f\left(x_{k+1}\right)\hspace{1em}((1.114)式より)
\end{aligned}
$$

ここで、$\displaystyle \frac{\sum_{i=1}^{k} \lambda_{i}}{1-\lambda_{k+1}}=\frac{1-\lambda_{k+1}}{1-\lambda_{k+1}}=1$となることに注意して、

また、$f\left(\sum_{i=1}^{k} \lambda_{i} x_{i}\right) \leq \sum_{i=1}^{k} \lambda_{i} f\left(x_{i}\right) \$の仮定が適用できるので、

$$
\begin{aligned}
&\left(1-\lambda_{k+1}\right) f\left(\frac{\sum_{i=1}^{k} \lambda_{i} x_{i}}{1-\lambda_{k+1}}\right)+\lambda_{k+1} f\left(x_{k+1}\right) \\
\leq&\left(1-\lambda_{k+1}\right) \sum_{i=1}^{k} \frac{\lambda_{i}}{1-\lambda_{k+1}} f\left(x_{i}\right)+\lambda_{k+1} f\left(x_{k+1}\right)  \\
=&\sum_{i=1}^{k} \lambda_{i} f\left(x_{i}\right)+\lambda_{k+1} f\left(x_{k+1}\right) \\
=&\sum_{i=1}^{k+1} \lambda_{i} f\left(x_{i}\right)
\end{aligned}
$$

となるので、$M=k+1$でも成立することが示された。

したがって数学的帰納法より、$(1.115)$式が成立することが示された。


## 演習 1.39

<div class="panel-primary">

2つの2値変数$x,y$が表1.3の同時分布を持つとする．以下の量を計算せよ．


(a) $\mathrm{H}[x]$, (b) $\mathrm{H}[y]$, (c) $\mathrm{H}[y|x]$, (d) $\mathrm{H}[x|y]$, (e) $\mathrm{H}[x,y]$, (f) $\mathrm{I}[x,y]$

これらのさまざまな量の間の関係を示す図を描け．
</div>

まず表1.3の同時分布$p(x,y)$を元に$p(x),p(y),p(y|x),p(x|y)$の同時分布の表を作成する。$\displaystyle p(y|x) = \frac{p(x,y)}{p(x)}$を用いる。

$$
\begin{array} {rr|rr}
&&&y\\
& & 0 & 1 \\
\hline x & 0 & 1/3 & 1/3 \\
& 1 & 0 & 1/3 \\
\end{array} \\
p(x,y)
$$
$$
\begin{array} {rr|r}
& & \\
\hline x & 0 & 2/3 \\
 & 1 & 1/3 \\
\end{array} \\
p(x)
$$
$$
\begin{array} {rr}
& y \\
0 & 1 \\
\hline 1/3 & 2/3 \\
\end{array} \\
p(y)
$$
$$
\begin{array} {rr|rr}
&&&y\\
& & 0 & 1 \\
\hline x & 0 & 1/2 & 1/2 \\
& 1 & 0 & 1 \\
\end{array} \\
p(y|x)
$$
$$
\begin{array} {rr|rr}
&&&y\\
& & 0 & 1 \\
\hline x & 0 & 1 & 1/2 \\
& 1 & 0 & 1/2 \\
\end{array} \\
p(x|y)
$$

**(a)**
$$
\begin{aligned}
\mathrm{H}[x]&= -\sum_{i}p(x)\ln p(x) \\
             &= -\left\{ \frac{2}{3}\ln\frac{2}{3}+\frac{1}{3}\ln\frac{1}{3} \right\} \\
             &= \ln 3 - \frac{2}{3}\ln 2
\end{aligned}
$$

**(b)**
$$
\begin{aligned}
\mathrm{H}[y]&= -\left\{ \frac{1}{3}\ln\frac{1}{3}+\frac{2}{3}\ln\frac{2}{3} \right\} \\
             &= \ln 3 - \frac{2}{3}\ln 2
\end{aligned}
$$

**(c)**
$$
\begin{aligned}
\mathrm{H}[y|x]&= -\sum_{i}\sum_{j}p(y,x)\ln p(y|x)dydx \\
               &= -\left\{ \frac{1}{3}\ln\frac{1}{2}+\frac{1}{3}\ln\frac{1}{2} + \frac{1}{3}\ln 1 \right\} \\
               &= \frac{2}{3}\ln 2
\end{aligned}
$$

**(d)**
$$
\begin{aligned}
\mathrm{H}[x|y]&= -\left\{ \frac{1}{3}\ln 1 + \frac{1}{3}\ln\frac{1}{2}+\frac{1}{3}\ln\frac{1}{2} \right\} \\
               &= \frac{2}{3}\ln 2
\end{aligned}
$$

**(e)**
$$
\begin{aligned}
\mathrm{H}[x,y]&= -\left\{ \frac{1}{3}\ln \frac{1}{3} + \frac{1}{3}\ln \frac{1}{3}+\frac{1}{3}\ln\frac{1}{3} \right\} \\
               &= \ln 3
\end{aligned}
$$
または$\mathrm{H}[x,y]=\mathrm{H}[y|x]+\mathrm{H}[x]=\mathrm{H}[x|y]+\mathrm{H}[y]=\ln 3$からも求まる。

**(f)**
$$
\begin{aligned}
\mathrm{I}[x,y]&= \mathrm{H}[x] - \mathrm{H}[x|y] = \mathrm{H}[y] - \mathrm{H}[y|x]  \\
               &= \ln 3 - \frac{4}{3}\ln 2
\end{aligned}
$$

<img src="/attachment/5f9a86d38fcbd9600d3963ed" width="600px">

## 演習 1.40

<div class="panel-primary">

イェンセンの不等式

$$
f\left(\sum_{i=1}^{M} \lambda_{i} x_{i}\right) \leq \sum_{i=1}^{M} \lambda_{i} f\left(x_{i}\right) \tag{1.115}
$$

を$f(x)=\ln x$に適用し，実数集合の算術平均が，幾何平均より決して小さくならないことを示せ．

</div>

算術平均$\displaystyle x_a=\frac{1}{N}\sum_{i=1}^N x_i$, 幾何平均$\displaystyle x_g = \left( \prod_{i=1}^N x_i\right)^{\frac{1}{N}}$である。
これについて$x_a \ge x_g$が成立することをイェンセンの不等式を用いて示す。

問題設定で$f(x) = \ln x$に適用し、とあるので$x>0$である。よって$\ln x_a \ge \ln x_g$であることを示すことにする。すなわち

$$
\ln x_a = \ln \left( \sum_{i=1}^N \frac{1}{N}x_i \right)
$$

$$
\ln x_g = \ln \left( \prod_{i=1}^N x_i \right)^{\frac{1}{N}} = \sum_{i=1}^N \frac{1}{N} \ln x_i
$$

凹関数である$f(x)=\ln x$を用いてイェンセンの不等式を適用すると

$$
\ln x_a = \ln \left( \sum_{i=1}^N \frac{1}{N}x_i \right) \ge \sum_{i=1}^N \frac{1}{N} \ln x_i = \ln x_g
$$

となるので、$x_a \ge x_g$となることが示された。


## 演習 1.41

<div class="panel-primary">

確率の加法･乗法定理を使って，相互情報量$I(\mathbf{x},\mathbf{y})$が

$$
\mathrm{I}[\mathbf{x}, \mathbf{y}]=\mathrm{H}[\mathbf{x}]-\mathrm{H}[\mathbf{x} \mid \mathbf{y}]=\mathrm{H}[\mathbf{y}]-\mathrm{H}[\mathbf{y} \mid \mathbf{x}] \tag{1.121}
$$

の関係を満たすことを示せ．

</div>

$$
\begin{aligned}
\mathrm{I}[\mathbf{x}, \mathbf{y}] &=\mathrm{KL}(p(\mathbf{x}, \mathbf{y}) \| p(\mathbf{x}) p(\mathbf{y})) \\
&=-\iint p(\mathbf{x}, \mathbf{y}) \ln \left(\frac{p(\mathbf{x}) p(\mathbf{y})}{p(\mathbf{x}, \mathbf{y})}\right) d\mathbf{x} d\mathbf{y}
\end{aligned}
$$

である。ここで確率の乗法定理$p(\mathbf{x},\mathbf{y})=p(\mathbf{y}|\mathbf{x})p(\mathbf{x})$から

$$
\begin{aligned}
\mathrm{I}[\mathbf{x}, \mathbf{y}]&=-\iint p(\mathbf{x}, \mathbf{y}) \ln \left(\frac{p(\mathbf{x}) p(\mathbf{y})}{p(\mathbf{y} \mid \mathbf{x})p(\mathbf{x})}\right) d \mathbf{x} d \mathbf{y} \\
&=-\iint p(\mathbf{x}, \mathbf{y}) \ln \left(\frac{p(\mathbf{y})}{p(\mathbf{y} \mid \mathbf{x})}\right) d \mathbf{x} d \mathbf{y} \\
&=-\iint p(\mathbf{x}, \mathbf{y}) \ln p(\mathbf{y}) d \mathbf{x} d \mathbf{y}+\iint p(\mathbf{x}, \mathbf{y}) \ln p(\mathbf{y} \mid \mathbf{x}) d \mathbf{x} d \mathbf{y}\\
&=-\int p(\mathbf{y}) \ln p(\mathbf{y}) d \mathbf{y}+\iint p(\mathbf{x}, \mathbf{y}) \ln p(\mathbf{y} \mid \mathbf{x}) d \mathbf{x} d \mathbf{y} \\
&=\mathrm{H}[\mathbf{y}]-\mathrm{H}[\mathbf{y} | \mathbf{x}]
\end{aligned}
$$

最後は

$$
\mathrm{H}[\mathbf{y} \mid \mathbf{x}]=-\iint p(\mathbf{y}, \mathbf{x}) \ln p(\mathbf{y} \mid \mathbf{x}) \mathrm{d} \mathbf{y} \mathrm{d} \mathbf{x}  \tag{1.111}
$$

式を用いた。また、$p(\mathbf{x}, \mathbf{y})=p(\mathbf{x}|\mathbf{y})p(\mathbf{y})$を用いれば同様にして

$$
\mathrm{I}[\mathbf{x}, \mathbf{y}] =\mathrm{H}[\mathbf{x}]-\mathrm{H}[\mathbf{x|y}]
$$

が求まる。