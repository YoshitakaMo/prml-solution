# PRML第6章演習問題解答

<head>
<style>
  div.panel-primary {
	border: 1px solid #000;
    margin: 10px 5px;
    padding: 16px 10px 0px;
  }
</style>
</head>

## 演習 6.1
<div class="panel-primary">

6.1節で紹介した最小二乗法線形回帰問題の双対表現を示せ．また解のべクトル$\mathbf{a}$の要素$a_n$がベクトル$\boldsymbol{\phi}(\mathbf{x}_n)$の要素の線形結合で表されることを示せ．それらの係数をベクトル$\mathbf{w}$として双対表現の双対表現がもともとの表現に戻ることを，$\mathbf{w}$をパラメータベクトルとして示せ．

</div>

$(6.2)$から始めて双対表現を得るところまでを復習する。

$$
J(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N}\left\{\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)-t_{n}\right\}^{2}+\frac{\lambda}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w} \tag{6.2}
$$

行列形式では

$$
J(\mathbf{w})=\frac{1}{2} \left( \mathbf{\Phi}\mathbf{w} - \mathbf{t} \right)^{\mathrm T}\left( \mathbf{\Phi}\mathbf{w} - \mathbf{t} \right) + \frac{\lambda}{2}\mathbf{w}^{\mathrm T}\mathbf{w}
$$

P.3にならって$\displaystyle \frac{\partial J}{\partial \mathbf{w}} = 0$をとると

$$
\begin{aligned}
\frac{\partial J}{\partial \mathbf{w}} &=\mathbf{\Phi}^{\mathrm T}(\mathbf{\Phi} \mathbf{w}-\mathbf{t})+\lambda \mathbf{w} \hspace{1em} \left(\because \frac{\partial}{\partial s}(\mathbf{A} \mathbf{s}-\mathbf{b})^{\mathrm T}(\mathbf{A} \mathbf{s}-\mathbf{b})=2 \mathbf{A}^{\mathrm T}(\mathbf{A} \mathbf{s}-\mathbf{b})\right) \\
&=\mathbf{\Phi}^{\mathrm T} \mathbf{\Phi} \mathbf{w}-\mathbf{\Phi}^{\mathrm T} \mathbf{t}+\lambda \mathbf{w}=0
\end{aligned}
$$

これを解いて

$$
\begin{aligned}
\mathbf{w} &= -\frac{1}{\lambda} \mathbf{\Phi}^{\mathrm T}\left( \mathbf{\Phi} \mathbf{w} - \mathbf{t} \right) \\
&= \mathbf{\Phi}^{\mathrm T}\mathbf{a}
\end{aligned}
$$

これは$(6.3)$と同じで、$\displaystyle \mathbf{a} = -\frac{1}{\lambda} \left( \mathbf{\Phi} \mathbf{w} - \mathbf{t} \right)$と置いた。

これを$(6.2)$に代入し直すと

$$
\begin{aligned}
J(\mathbf{a}) &=\frac{1}{2}\left(\mathbf{\Phi} \mathbf{\Phi}^{\mathrm T} \mathbf{a}-\mathbf{t}\right)^{\mathrm T}\left(\mathbf{\Phi} \mathbf{\Phi}^{\mathrm T} \mathbf{a}-\mathbf{t}\right)+\frac{\lambda}{2} \mathbf{a}^{\mathrm T} \mathbf{\Phi} \mathbf{\Phi}^{\mathrm T} \mathbf{a} \\
&=\frac{1}{2}\left(\mathbf{a}^{\mathrm T} \mathbf{\Phi} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi} \mathbf{\Phi}^{\mathrm T} \mathbf{a}-2 \mathbf{a}^{\mathrm T} \mathbf{\Phi} \mathbf{\Phi}^{\mathrm T} \mathbf{t}+\mathbf{t}^{\mathrm T} \mathbf{t}\right)+\frac{\lambda}{2} \mathbf{a}^{\mathrm T} \mathbf{\Phi} \mathbf{\Phi}^{\mathrm T} \mathbf{a} \\
&=\frac{1}{2} \mathbf{a}^{\mathrm T} \mathbf{\Phi} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi} \mathbf{\Phi}^{\mathrm T} \mathbf{a}-\mathbf{a}^{\mathrm T} \mathbf{\Phi} \mathbf{\Phi}^{\mathrm T} \mathbf{t}+\frac{1}{2} \mathbf{t}^{\mathrm T} \mathbf{t}+\frac{1}{2} \mathbf{a}^{\mathrm T} \mathbf{\Phi} \mathbf{\Phi}^{\mathrm T} \mathbf{a}
\end{aligned} \tag{6.5}
$$

この式を$\mathbf{K} = \mathbf{\Phi}\mathbf{\Phi}^{\mathrm T}$で定義されるグラム行列を用いて書くと

$$
J(\mathbf{a})=\frac{1}{2} \mathbf{a}^{\mathrm{T}} \mathbf{K} \mathbf{K} \mathbf{a}-\mathbf{a}^{\mathrm{T}} \mathbf{K} \mathbf{t}+\frac{1}{2} \mathbf{t}^{\mathrm{T}} \mathbf{t}+\frac{\lambda}{2} \mathbf{a}^{\mathrm{T}} \mathbf{K} \mathbf{a} \tag{6.7}
$$

となる。さらにこれは$\mathbf{K}^{\mathrm T} = \mathbf{K}$であるから

$$
J(\mathbf{a})=\frac{1}{2}(\mathbf{K} \mathbf{a}-\mathbf{t})^{\mathrm T}(\mathbf{K} \mathbf{a}-\mathbf{t})+\frac{\lambda}{2} \mathbf{a}^{\mathrm T} \mathbf{Ka}
$$

となる。これが最小二乗法の線形回帰問題の双対表現である。

以上が教科書3ページの復習。次に、(6.7)式から出発して(6.2)式を再現する。
$\mathbf{K}=\mathbf{\Phi\Phi^T}$であり、$\mathbf{\Phi}$は$N\times M$行列なので、$N\times N$行列である$\mathbf{K}$は$M$次までランク落ちしている。（{$\phi_1(\mathbf{x}), \phi_2(\mathbf{x}),\dots ,\phi_M(\mathbf{x})$}が線型独立なので、$M$次元未満にはならない。）
そこで、(6.7)式の$\mathbf{a}$を$\mathbf{K}$の像空間(image space)と$\mathbf{K}$の核空間(kernel space)に分解して、不定性の残る$\mathbf{K}$の核空間の成分は$\mathbf{0}$（または十分小さい）とする。（$\mathbf{a}$は、(6.7)式において$\mathbf{Ka}$の形でしか登場しないので、$\mathbf{a}$の核空間の成分を$\mathbf{0}$としても$\mathbf{J(a)}$の一般性を失わない。）
$M$個のベクトル{$\phi_1(\mathbf{x}), \phi_2(\mathbf{x}),\dots ,\phi_M(\mathbf{x})$}が（互いに線型独立なので）像空間の基底を成すことから、係数ベクトル$\mathbf{u}$を用いて$\mathbf{a=\Phi u}$と表せる。これを(6.7)式に代入して、
$$
J(\mathbf{u})=\frac{1}{2}\mathbf{(\Phi\Phi^T\Phi u-t)^T(\Phi\Phi^T\Phi u-t)}+\frac{\lambda}{2}\mathbf{u^T\Phi^T\Phi\Phi^T\Phi u}
$$
ここで、$\mathbf{\Phi^T\Phi}$は（$\mathbf{\Phi\Phi^T}$とは異なり）フルランクなので、$\mathbf{u}$の代わりに$\mathbf{\Phi^T\Phi u}$を係数ベクトルとしても等価である。この$\mathbf{\Phi^T\Phi u}$を改めて$\mathbf{w}$と置くと、(6.2)式が再現される。

## 演習 6.2
<div class="panel-primary">

この演習問題では，パーセプトロンの学習アルゴリズムの双対表現を導く．パーセプトロンでの更新則
$$
\mathbf{w}^{(\tau+1)}=\mathbf{w}^{(\tau)}-\eta \nabla E_{\mathrm{P}}(\mathbf{w})=\mathbf{w}^{(\tau)}+\eta \boldsymbol{\phi}_{n} t_{n} \tag{4.55}
$$
を用いて，訓練後の重みベクトル$\mathbf{w}$が，ベクトル$t_n\boldsymbol{\phi}(\mathbf{x}_n)$(ただし$t_{n} \in\{-1,+1\}$)の線形結合で表されることを示せ．この線形結合の係数を$\alpha_n$として，パーセプトロンの学習アルゴリズムを導き，また，$\alpha_n$を用いてパーセプトロンの予測関数を示せ．また，特徴ベクトル$\boldsymbol{\phi}(\mathbf{x})$はカーネル関数$k(\mathbf{x},\mathbf{x}^{\prime}) = \boldsymbol{\phi}({\mathbf{x}})^{\mathrm T}\boldsymbol{\phi}({\mathbf{x}^{\prime}})$の形でのみ現れることを示せ．

</div>

上巻190ページを参照する。パーセプトロン規準では、ある入力ベクトル$\mathbf{x}_n$を変換して特徴ベクトル$\boldsymbol{\phi}(\mathbf{x}_n)$を得て、以下の式で表される一般化線形モデルを構成する

$$
y(\mathbf{x}) = f(\mathbf{w}^{\mathrm T}\boldsymbol{\phi}(\mathbf{x}))
$$

ここで$f(\cdot)$はステップ関数

$$
f(a)=\left\{\begin{array}{ll}
+1 & (a>0) \\
-1 & (a<0)
\end{array}\right.
$$

で与えられる。

パーセプトロンではクラス$\mathcal{C}_1$については$\mathbf{w}^{\mathrm T}{\boldsymbol{\phi}(\mathbf{x}_n)} \gt 0$, クラス$\mathcal{C}_2$については$\mathbf{w}^{\mathrm T}{\boldsymbol{\phi}(\mathbf{x}_n)} \le 0$となるような重みベクトルを求めることが目的となる。さらに目的変数の値$t_n \in \{-1, 1\}$を使うとすべてのパターンは$\mathbf{w}^{\mathrm T}\boldsymbol{\phi}(\mathbf{x}_n)t_n \gt 0$を満たす。そして正しく分類された$\mathbf{x}_n$についての誤差は$0$、誤分類された$\mathbf{x}_n$についての誤差は$\mathbf{w}^{\mathrm T}{\boldsymbol{\phi}(\mathbf{x}_n)}$となる。

すなわち誤差関数$E_{\mathrm{P}}(\mathbf{\mathbf{w}})$は

$$
E_{\mathrm{P}}(\mathbf{w})=-\sum_{n \in \mathcal{M}} \mathrm{w}^{\mathrm{T}} \boldsymbol{\phi}_{n} t_{n} \tag{4.54}
$$

となる($\mathcal{M}$は誤分類されたすべてのパターンを表す)。この誤差関数の最小化に確率的最急降下法を用いると

$$
\mathrm{w}^{(\tau+1)}=\mathrm{w}^{(\tau)}-\eta \nabla E_{\mathrm{P}}(\mathrm{w})=\mathrm{w}^{(\tau)}+\eta \boldsymbol{\phi}_{n} t_{n} \tag{4.55}
$$

が得られる。

重みベクトルの初期値を$\mathbf{w}^{(0)}$として（$\mathbf{w}^{(0)} = \mathbf{0}$としても良い）、訓練後の重みベクトル$\mathbf{w}$はベクトル$t_n\boldsymbol{\phi}(\mathbf{x}_n)$の線形結合の形になっていることがわかる。

この線形結合の係数を$\alpha_n$とすると

$$
\mathbf{w} = \sum_{n=1}^N \alpha_n t_n \mathbf{x}_n
$$

の形で更新後の重みベクトルを表すことができる。この重みを用いたクラスラベル予測関数は

$$
\begin{aligned}
y(\mathbf{x}) &=\operatorname{sgn}\left(\mathbf{w}^{\mathrm T} \boldsymbol{\phi}(\mathbf{x})\right) \\
&=\operatorname{sgn}\left(\sum_{n=1}^{N} \alpha_{n} t_n \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)^{\mathrm T} \boldsymbol{\phi}(\mathbf{x})\right) \\
&=\operatorname{sgn}\left(\sum_{n=1}^{N} \alpha_{n} t_n k\left(\mathbf{x}_{n}, \mathbf{x}\right)\right)
\end{aligned}
$$

の形で書き表せる。これより、パーセプトロンの予測関数がカーネル関数でのみ表せていることがわかる。

また$E_P$の最小化（＝パーセプトロンの学習アルゴリズム）はすべてのパターンが$\mathbf{w}^{\mathrm T}\boldsymbol{\phi}(\mathbf{x}_n)t_n \gt 0$を満たしているので

$$
\mathbf{w}^{\mathrm T}\boldsymbol{\phi}(\mathbf{x}_n)t_n = \alpha_{n}t_{n}^2\left(
    \sum_{m=1}^{M}k(\mathbf{x}_m, \mathbf{x}_n)
    \right) \gt 0
$$

となり、グラム行列のみの双対表現で書き表すことができる。

## 演習 6.3
<div class="panel-primary">

最近傍法(2.5.2節)は，新しい入力ベクトル$\mathbf{x}$を訓練集合の中でこれに最も近い入力ベクトル$\mathbf{x}_n$を持つものと同じクラスに分類する．最も単純な場合では，距離はユークリッド距離$\| \mathbf{x} - \mathbf{x}_n\|^2$が用いられる．これをスカラー積で表すことでカーネル置換を用いて，一般的な非線形カーネルを用いた最近傍法を導け．

</div>

ユークリッド距離の2乗$\|\mathbf{x} - \mathbf{x}_n\|^2$をスカラー積（内積）$(\mathbf{x} - \mathbf{x}_n)^{\mathrm T}(\mathbf{x} - \mathbf{x}_n)$と考える。

$$
\begin{aligned}
D\left(\mathbf{x}, \mathbf{x}_{n}\right) &=\left\|\mathbf{x}-\mathbf{x}_{n}\right\|^{2} \\
&=\left(\mathbf{x}-\mathbf{x}_{n}\right)^{\mathrm T}\left(\mathbf{x}-\mathbf{x}_{n}\right) \\
&=\mathbf{x}^{\mathrm T} \mathbf{x}-2 \mathbf{x}^{\mathrm T} \mathbf{x}_{n}+\mathbf{x}_{n}^{2} \\
&=k(\mathbf{x}, \mathbf{x})+k\left(\mathbf{x}_{n}, \mathbf{x}_{n}\right)-2k\left(\mathbf{x}, \mathbf{x}_{n}\right)
\end{aligned}
$$

ここで$k(\mathbf{x}, \mathbf{x}_n) = \mathbf{x}^{\mathrm T}\mathbf{x}_n$をカーネル関数として用いた。この結果は非線形写像$\|\mathbf{x} - \mathbf{x}_n\|^2$は他の（有効な）カーネル関数で置換して表現できることを示している。

## 演習 6.4
<div class="panel-primary">

付録Cでは，要素がすべて正であるが，負の固有値をもつために，正定値ではない行列の例を紹介している．逆に，$2\times 2$行列で，すべての固有値が正であるが，少なくとも1つの負の要素をもつような行列を挙げよ．


</div>

行列の全ての固有値が正⇔正定値行列⇔任意のべクトル $\mathbf{x} \neq \mathbf{0}$に対して$\mathbf{x}^{\mathrm T} \mathbf{A} \mathbf{x}>0$
が成り立つ。

ここで負の要素を持つ行列$\left[\begin{array}{rr}2 & -1 \\ -1 & 2\end{array}\right]$の正定値性を確認する。

$$
\mathbf{x}^{\mathrm T} \mathbf{A} \mathbf{x}=\left[\begin{array}{ll}
x_{1} & x_{2}
\end{array}\right]\left[\begin{array}{rr}
2 & -1 \\
-1 & 2
\end{array}\right]\left[\begin{array}{l}
x_{1} \\
x_{2}
\end{array}\right]=2 x_{1}{ }^{2}+2 x_{2}{ }^{2}-2 x_{1} x_{2}=x_{1}{ }^{2}+x_{2}{ }^{2}+\left(x_{1}-x_{2}\right)^{2} \geqq 0
$$
さらに、$\mathbf{x}^{\mathrm T} \mathbf{A} \mathbf{x}=0$となるのは、$x_{1}=x_{2}=0$に限られるのでこの行列は正定値で、全ての固有値は正である。


## 演習 6.5
<div class="panel-primary">

有効なカーネル関数を構成するために利用できる等式
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=c k_{1}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)\tag{6.13}
$$
と
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=f(\mathbf{x}) k_{1}\left(\mathbf{x}, \mathbf{x}^{\prime}\right) f\left(\mathbf{x}^{\prime}\right)
 \tag{6.14}
$$
を確かめよ．

</div>

$(6.1)$の定義から、$k_1(\mathbf{x}, \mathbf{x}^{\prime})$が有効なカーネル関数であるならば、何らかの特徴空間への写像$\boldsymbol{\phi}(\mathbf{x})$が存在して

$$
k_1(\mathbf{x}, \mathbf{x}^{\prime}) = \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\boldsymbol{\phi}(\boldsymbol{\mathbf{x}^{\prime}})
$$

と表すことができる。そこで$c k_{1}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)$について考えると

$$
\begin{aligned}
c k_{1}\left(\mathbf{x}_{1}, \mathbf{x}^{\mathrm T}\right) &=c \boldsymbol{\phi}(\mathbf{x})^{\mathrm T} \boldsymbol{\phi}\left(\mathbf{x}^{\prime}\right) \\
&=(\sqrt{c} \boldsymbol{\phi}(\mathbf{x}))^{\mathrm T}(\sqrt{c} \boldsymbol{\phi}(\mathbf{x}))
\end{aligned}
$$

ここで新たに$\mathbf{u}(\mathbf{x}) \equiv \sqrt{c}\boldsymbol{\phi}(\mathbf{x})$と定義すれば

$$
c k_1(\mathbf{x}, \mathbf{x}^{\prime}) = \mathbf{u}(\mathbf{x})^{\mathrm T}\mathbf{u}(\mathbf{x})
$$

と書けるので、$(6.1)$の定義から$ck_1(\mathbf{x}, \mathbf{x}^{\prime})$も有効なカーネル関数である。

次に任意の関数$f(\cdot)$について（この$f(\cdot)$はスカラー値となる）、$\mathbf{v}(\mathbf{x}) \equiv f(\mathbf{x})\boldsymbol{\phi}(\mathbf{x})$を定義すると

$$
\begin{aligned}
f(\mathbf{x})k_1(\mathbf{x}, \mathbf{x}^{\prime})f(\mathbf{x}) &= f(\mathbf{x})\boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\boldsymbol{\phi}(\boldsymbol{\mathbf{x}^{\prime}})f(\mathbf{x}^{\prime}) \\
&= \left( f(\mathbf{x})\boldsymbol{\phi}(\mathbf{x}) \right)^{\mathrm T} \left( f(\mathbf{x}^{\prime})\boldsymbol{\phi}(\mathbf{x}^{\prime}) \right) \\
&= \mathbf{v}(\mathbf{x})^{\mathrm T}\mathbf{v}(\mathbf{x})
\end{aligned}
$$

となり、カーネル関数の定義から、これも有効なカーネル関数であることが示された。

## 演習 6.6
<div class="panel-primary">

有効なカーネル関数を構成するために利用できる等式
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=q\left(k_{1}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)\right) \tag{6.15}
$$
と
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\exp \left(k_{1}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)\right) \tag{6.16}
$$
を確かめよ．

</div>

まず、$q(k_1(x, x^{\prime}))$カーネル関数として有効であることを示す。

$q(\cdot)$は、非負の係数を持つ多項式なので、

$$
q(x) = \sum_j^n a_j x^j
$$

と置くことができる。よって、

$$
q(k_1(x, x^{\prime})) = \sum_j^n a_j k_1(x, x^{\prime})^j
$$

が成り立つ。

今、$(6.18)$より、$k_1(x, x^{\prime})^j$ ($0 \leq j \leq n$)は、全てカーネル関数として有効である。そして、$(6.13)$より、$a_j k_1(x, x^{\prime})^j$も、全てカーネル関数として有効である。そして、$(6.17)$より、$\sum_j^n a_j k_1(x, x^{\prime})^j$はカーネル関数として有効である。

よって、$q(k_1(x, x^{\prime}))$もまたカーネル関数として有効である。

次に、$\exp(k(x, x^{\prime}))$がカーネル関数として有効であることを示す。

テイラー展開により、$\exp(x) = \sum \frac{x^j}{j!}$である。よって、

$$
\exp(k(x, x^{\prime}))= \sum \frac{k_1(x, x^{\prime})^j}{j!}
$$

いま、$(6.18)$より、$k_1(x, x^{\prime})^j$ ($0 \leq j \leq n$)は、全てカーネル関数として有効である。そして、$(6.13)$より、$\frac{k_1(x, x^{\prime})^j}{j!}$も、全てカーネル関数として有効である。そして、$(6.17)$より、$\sum \frac{k_1(x, x^{\prime})^j}{j!}$はカーネル関数として有効である。

よって、$\exp(k(x, x^{\prime}))$もまたカーネル関数として有効である。


## 演習 6.7
<div class="panel-primary">

有効なカーネル関数を構成するために利用できる等式
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=k_{1}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)+k_{2}\left(\mathbf{x}, \mathbf{x}^{\prime}\right) \tag{6.17}
$$
と
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=k_{1}\left(\mathbf{x}, \mathbf{x}^{\prime}\right) k_{2}\left(\mathbf{x}, \mathbf{x}^{\prime}\right) \tag{6.18}
$$
を確かめよ．

</div>

$(6.17)$を示す。$(6.1)$の定義から、$k_1(\mathbf{x}, \mathbf{x}^{\prime})$, $k_2(\mathbf{x}, \mathbf{x}^{\prime})$が有効なカーネル関数であることから、何らかの特徴空間への写像$\boldsymbol{\phi}(\mathbf{x})$, $\boldsymbol{\psi}(\mathbf{x})$が存在して

$$
\begin{aligned}
k_1(\mathbf{x}, \mathbf{x}^{\prime}) &= \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\boldsymbol{\phi}(\boldsymbol{\mathbf{x}^{\prime}})\\
k_2(\mathbf{x}, \mathbf{x}^{\prime}) &= \boldsymbol{\psi}(\mathbf{x})^{\mathrm T}\boldsymbol{\psi}(\boldsymbol{\mathbf{x}^{\prime}})
\end{aligned}
$$

と表すことができる。したがって

$$
\begin{aligned}
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right) &= k_{1}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)+k_{2}\left(\mathbf{x}, \mathbf{x}^{\prime}\right) \\
&= \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\boldsymbol{\phi}(\boldsymbol{\mathbf{x}^{\prime}})+\boldsymbol{\psi}(\mathbf{x})^{\mathrm T}\boldsymbol{\psi}(\boldsymbol{\mathbf{x}^{\prime}})\\
&=\boldsymbol{\varphi}(\mathbf{x})^{\mathrm T}\boldsymbol{\varphi}(\boldsymbol{\mathbf{x}^{\prime}})
\end{aligned}
$$

ただし，$\boldsymbol\varphi(\mathbf{x})$は

$$
\boldsymbol\varphi(\mathbf{x}) = \left( \begin{array}{cc} \boldsymbol{\phi}(\mathbf{x})\\ \boldsymbol{\psi}(\mathbf{x}) \\ \end{array} \right)
$$

とする．以上により$(6.1)$の定義から$k(\mathbf{x}, \mathbf{x}^{\prime})$も有効なカーネル関数である。

次に$(6.18)$を示す。上記の方法と同様の考え方で，$k_1(\mathbf{x}, \mathbf{x}^{\prime})$, $k_2(\mathbf{x}, \mathbf{x}^{\prime})$が有効なカーネル関数であることから、何らかの特徴空間への写像$\boldsymbol{\phi}(\mathbf{x})$, $\boldsymbol{\psi}(\mathbf{x})$が存在して

$$
\begin{aligned}
k_1(\mathbf{x}, \mathbf{x}^{\prime}) &= \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\boldsymbol{\phi}(\boldsymbol{\mathbf{x}^{\prime}})\\
k_2(\mathbf{x}, \mathbf{x}^{\prime}) &= \boldsymbol{\psi}(\mathbf{x})^{\mathrm T}\boldsymbol{\psi}(\boldsymbol{\mathbf{x}^{\prime}})
\end{aligned}
$$

と表すことができる。したがって
$$
\begin{aligned}
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right) &= k_{1}\left(\mathbf{x}, \mathbf{x}^{\prime}\right)k_{2}\left(\mathbf{x}, \mathbf{x}^{\prime}\right) \\
&= \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\boldsymbol{\phi}(\boldsymbol{\mathbf{x}^{\prime}})\boldsymbol{\psi}(\mathbf{x})^{\mathrm T}\boldsymbol{\psi}(\boldsymbol{\mathbf{x}^{\prime}})\\
&=\sum_{m=1}^{M}{\phi_{m}}\left(\mathbf{x}\right){\phi_{m}}\left(\mathbf{x}^{\prime}\right)\sum_{n=1}^{N}{\psi_{n}}\left(\mathbf{x}\right){\psi_{n}}\left(\mathbf{x}^{\prime}\right)\\
&=\sum_{m=1}^{M}\sum_{n=1}^{N}{\phi_{m}}\left(\mathbf{x}\right){\psi_{n}}\left(\mathbf{x}\right){\phi_{m}}\left(\mathbf{x}^{\prime}\right){\psi_{n}}\left(\mathbf{x}^{\prime}\right)\\
&=\sum_{k=1}^{K}\varphi_k(\mathbf{x}) \varphi_k(\mathbf{x}^{\prime})\\
&=\boldsymbol{\varphi}(\mathbf{x})^{\mathrm T}\boldsymbol{\varphi}(\boldsymbol{\mathbf{x}^{\prime}})
\end{aligned}
$$
ただし$K=MN$で，$\boldsymbol\varphi(\mathbf{x})$はテンソル積
$$
\boldsymbol\varphi(\mathbf{x}) = \boldsymbol{\phi}(\mathbf{x})\otimes\boldsymbol{\psi}(\mathbf{x})
$$
である。以上によりカーネル関数の定義から、これも有効なカーネル関数であることが示された。


## 演習 6.8
<div class="panel-primary">

有効なカーネル関数を構成するために利用できる等式
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=k_{3}\left(\boldsymbol{\phi}(\mathbf{x}), \boldsymbol{\phi}\left(\mathbf{x}^{\prime}\right)\right) \tag{6.19}
$$
と
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\mathbf{x}^{\mathrm{T}} \mathbf{A} \mathbf{x}^{\prime} \tag{6.20}
$$
を確かめよ．

</div>

$(6.19)$を示す。
$k_{3}(\cdot, \cdot)$は$\mathbb{R}^{M}$で定義された有効なカーネルであり、
$(6.1)$と同様に、$\mathbf{y}, \mathbf{y}^{\prime} \in \mathbb{R}^{M}$を用いて
$$
k_{3}\left(\mathbf{y}, \mathbf{y}^{\prime}\right)=\boldsymbol{\varphi}(\mathbf{y})^{\mathrm T} \boldsymbol{\varphi}\left(\mathbf{y}^{\prime}\right)
$$
と表せる。
$$
\mathbf{y}=\boldsymbol{\phi}(\mathbf{x}), \quad \mathbf{y}^{\prime}=\boldsymbol{\phi}\left(\mathbf{x}^{\prime}\right)
$$
としたとき、
$$
\begin{aligned}
k_{3}\left(\boldsymbol{\phi}(\mathbf{x}), \boldsymbol{\phi}\left(\mathbf{x}^{\prime}\right)\right) &=\boldsymbol{\varphi}(\boldsymbol{\phi}(\mathbf{x}))^{\mathrm T} \boldsymbol{\varphi}\left(\boldsymbol{\phi}\left(\mathbf{x}^{\prime}\right)\right) \\
&=\boldsymbol{\psi}(\mathbf{x})^{\mathrm T} \boldsymbol{\psi}\left(\mathbf{x}^{\prime}\right)
\end{aligned}
$$
と表せることから$(6.19)$が示される。ただし、
$$
\boldsymbol{\psi}(\mathbf{x})=\boldsymbol{\varphi}(\boldsymbol{\phi}(\mathbf{x}))
$$
とした。

$(6.20)$を示す。
一般に、$n \times n$の実対称行列$\mathbf{A}$について、

\begin{eqnarray}
&&\mathbf{A}が半正定値行列\\
\overset{\text{def}}\iff&&\forall \mathbf{x} \in \mathbb{R}^{n}, \quad \mathbf{x}^{\mathrm T} \mathbf{A} \mathbf{x} \geq 0 \tag{6.20a}\\
\iff&&\mathbf{A}の固有値が全て非負 \tag{6.20b}\\
\iff&&ある実正方行列\mathbf{U}により\mathbf{A}=\mathbf{U}^{\mathrm T} \mathbf{U}と表せる \tag{6.20c}
\end{eqnarray}

が成り立つ。
（検索すればかなり引っかかってくるためこの同値の証明は省略します）
この$(6.20c)$を用いて、

$$
\begin{aligned}
\mathbf{x}^{\mathrm T} \mathbf{A} \mathbf{x}^{\prime} &=\mathbf{x}^{\mathrm T} \mathbf{U}^{\mathrm T} \mathbf{U} \mathbf{x}^{\prime} \\
&=(\mathbf{U} \mathbf{x})^{\mathrm T} \mathbf{U} \mathbf{x}^{\prime}\\
&=\boldsymbol{\psi}(\mathbf{x})^{\mathrm T} \boldsymbol{\psi}\left(\mathbf{x}^{\prime}\right)
\end{aligned}
$$

により$(6.20)$が示される。ただし、
$$
\boldsymbol{\psi}(\mathbf{x})=\mathbf{Ux}
$$

とした。


## 演習 6.9
<div class="panel-primary">

有効なカーネル関数を構成するために利用できる等式
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=k_{a}\left(\mathbf{x}_{a}, \mathbf{x}_{a}^{\prime}\right)+k_{b}\left(\mathbf{x}_{b}, \mathbf{x}_{b}^{\prime}\right) \tag{6.21}
$$
と
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=k_{a}\left(\mathbf{x}_{a}, \mathbf{x}_{a}^{\prime}\right) k_{b}\left(\mathbf{x}_{b}, \mathbf{x}_{b}^{\prime}\right) \tag{6.22}
$$
を確かめよ．($k_a$と$k_b$はそれぞれの特徴空間において有効なカーネル関数であるとする．)

</div>

$(6.21)$について、$(6.1)$と同様に、ある特徴空間への写像$\boldsymbol{\phi}(\mathbf{x})$と$\boldsymbol{\psi}(\mathbf{x})$を用いて

$$
\begin{aligned}
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right) &=k_{a}\left(\mathbf{x}_{a}, \mathbf{x}_{a}^{\prime}\right)+k_{b}\left(\mathbf{x}_{b}, \mathbf{x}_{b}^{\prime}\right) \\
&=\boldsymbol{\phi}\left(\mathbf{x}_{a}\right)^{\mathrm T} \boldsymbol{\phi}\left(\mathbf{x}_{a}^{\prime}\right)+\boldsymbol{\psi}\left(\mathbf{x}_{b}\right)^{\mathrm T} \boldsymbol{\psi}\left(\mathbf{x}_{b}\right) \\
&=\begin{pmatrix}\boldsymbol{\phi}\left(\mathbf{x}_{a}\right) \\ \boldsymbol{\psi}\left(\mathbf{x}_{b}\right)\end{pmatrix}^{\mathrm T}\begin{pmatrix}\boldsymbol{\phi}\left(\mathbf{x}_{a}^{\prime}\right) \\ \boldsymbol{\psi}\left(\mathbf{x}_{b}^{\prime}\right)\end{pmatrix} \\
&=\boldsymbol{\varphi}(\mathbf{x})^{\mathrm T} \boldsymbol{\varphi}\left(\mathbf{x}^{\prime}\right)
\end{aligned}
$$

と書くことができる。ここで$\displaystyle \boldsymbol{\varphi}(\mathbf{x})=\begin{pmatrix}\boldsymbol{\phi}\left(\mathbf{x}_{a}\right) \\ \boldsymbol{\psi}\left(\mathbf{x}_{b}\right)\end{pmatrix}, \boldsymbol{\varphi}\left(\mathbf{x}^{\prime}\right)=\begin{pmatrix}\boldsymbol{\phi}\left(\mathbf{x}_{a}^{\prime}\right) \\ \boldsymbol{\psi}\left(\mathbf{x}_{b}^{\prime}\right)\end{pmatrix}$と定義した。$(6.1)$よりこれも有効なカーネルである。


$(6.22)$について、上と同様に

$$
\begin{aligned}
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right) &=k_{a}\left(\mathbf{x}_{a}, \mathbf{x}_{a}^{\prime}\right) k_{b}\left(\mathbf{x}_{b}, \mathbf{x}_{b}^{\prime}\right) \\
&=\boldsymbol{\phi}\left(\mathbf{x}_{a}\right)^{\mathrm T} \boldsymbol{\phi}\left(\mathbf{x}_{a}^{\prime}\right) \boldsymbol{\psi}\left(\mathbf{x}_{b}\right)^{\mathrm T} \boldsymbol{\psi}\left(\mathbf{x}_{0}^{\prime}\right) \\
&=\sum_{m=1}^{M} \phi_{m}\left(\mathbf{x}_{a}\right) \phi_{m}\left(\mathbf{x}_{a}^{\prime}\right) \sum_{n=1}^{N} \psi_{n}\left(\mathbf{x}_{b}\right) \psi_{n}\left(\mathbf{x}_{b}^{\prime}\right) \\
&=\sum_{m=1}^{M} \sum_{n=1}^{N} \phi_{m}\left(\mathbf{x}_{a}\right) \psi_{n}\left(\mathbf{x}_{b}\right) \phi_{m}\left(\mathbf{x}_{a}^{\prime}\right) \psi_{n}\left(\mathbf{x}_{b}^{\prime}\right) \\
&=\sum_{k=1}^{K}\varphi_k({\mathbf{x}_a})\varphi_k({\mathbf{x}_b}) \\
&=\boldsymbol{\varphi}(\mathbf{x}_a)^{\mathrm T}\boldsymbol{\varphi}(\mathbf{x}_b)
\end{aligned}
$$

ただし$K=MN$で，$\boldsymbol\varphi(\mathbf{x})$はテンソル積
$$
\boldsymbol\varphi(\mathbf{x}) = \boldsymbol{\phi}(\mathbf{x})\otimes\boldsymbol{\psi}(\mathbf{x})
$$
である。以上によりカーネル関数の定義から，これも有効なカーネル関数であることが示された。

## 演習 6.10
<div class="panel-primary">

関数$f(\mathbf{x})$を学習するためのカーネルとして$k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=f(\mathbf{x}) f\left(\mathbf{x}^{\prime}\right)$が理想的であることを，このカーネルに基づく線形の学習器は，常に$f(\mathbf{x})$に比例する解を見つけることを示すことで示せ．

</div>

※「このカーネルに基づく線形の学習器は……」の部分がよくわからないですけど、線形回帰モデルで重み$\mathbf{w}$を学習するのにカーネル$k(\mathbf{x},\mathbf{x^{\prime}}) = f(\mathbf{x})f(\mathbf{x}^{\prime})$が最適で、常に$f(\mathbf{x})$に比例する解を求めることができることを示せれば良いのだろうか？

線形の学習なので、$(6.2)$の$J(\mathbf{x})$について学習済みの重みは$(6.3)$で与えられ、これを用いて新しい入力$\mathbf{x}$に対する予測$y(\mathbf{x})$は$(6.9)$になる。

$$
\begin{aligned}
y(\mathbf{x}) &=\mathbf{k}(\mathbf{x})^{\mathrm T}\left(\mathbf{K}+\lambda \mathbf{I}_{N}\right)^{-1} \mathbf{t} \\
&=\mathbf{k}(\mathbf{x})^{\mathrm T} \mathbf{a} \\
&=\sum_{n=1}^{N} a_n k\left(\mathbf{x}, \mathbf{x}_{n}\right)
\end{aligned}
$$

$k\left(\mathbf{x}, \mathbf{x}_{n}\right)$が問題文のように$f(\mathbf{x})f(\mathbf{x}_n)$で与えられるならば

$$
\begin{aligned} y(\mathbf{x})
&=\sum_{n=1}^{N} a_{n} f(\mathbf{x}) f\left(\mathbf{x}_{n}\right) \\
&=\left(\sum_{n=1}^{N} a_{n} f\left(\mathbf{x}_{n}\right)\right) f(\mathbf{x})
\end{aligned}
$$

となる。（）内はスカラー値なので、$y(\mathbf{x})$は常に$f(\mathbf{x})$に比例する解となる。

## 演習 6.11
<div class="panel-primary">

$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\exp \left(-\mathbf{x}^{\mathrm{T}} \mathbf{x} / 2 \sigma^{2}\right) \exp \left(\mathbf{x}^{\mathrm{T}} \mathbf{x}^{\prime} / \sigma^{2}\right) \exp \left(-\left(\mathbf{x}^{\prime}\right)^{\mathrm{T}} \mathbf{x}^{\prime} / 2 \sigma^{2}\right) \tag{6.25}
$$
の展開の中央の要素を，べき級数展開することによって，ガウスカーネル
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\exp \left(-\left\|\mathbf{x}-\mathbf{x}^{\prime}\right\|^{2} / 2 \sigma^{2}\right) \tag{6.23}
$$
は，無限次元の特徴ベクトルの内積で表されることを示せ．

</div>

まずは、(6.25)の展開の中央の要素を、べき級数展開する。

\begin{eqnarray}
\exp(\mathbf{x}^T \mathbf{x'}/\sigma^2) &=& \sum_{n=0}^\infty  \frac{1}{n!}
\left(\frac{\mathbf{x}^T \mathbf{x'}}{\sigma^2}\right)^n \\
&=& \sum_{n=0}^\infty  \frac{1}{n!}
\frac{(\mathbf{x}^T)^{\otimes n} \mathbf{(x')}^{\otimes n}}{\sigma^{2n} }\\
&=& \sum_{n=0}^\infty \boldsymbol{\phi}_n ( \mathbf{x})^{\mathrm T}\boldsymbol{\phi}_n(\mathbf{x'})\\
&=& \boldsymbol{\psi} ( \mathbf{x})^{\mathrm T}\boldsymbol{\psi}(\mathbf{x'})
\end{eqnarray}
ここで、$\boldsymbol{\phi}_n(\mathbf{x})$と$\boldsymbol{\phi}_n(\mathbf{x})$は以下のように定義した。

\begin{eqnarray}
\boldsymbol{\phi}_n(\mathbf{x}):&=&\frac{1}{\sqrt{n!}}\frac{\mathbf{x}^{\otimes n}}{\sigma ^n}\\
\boldsymbol{\psi}(\mathbf{x}):&=& \left\{ \boldsymbol{\phi}_0(\mathbf{x}), \boldsymbol{\phi}_1(\mathbf{x}), \dots \right\}\\
\end{eqnarray}

よって、元のカーネルは、$\boldsymbol{\varphi}(\mathbf{x}):=\exp (-\mathbf{x}^\mathrm{T} \mathbf{x}/2\sigma^2)\boldsymbol{\psi}(\mathbf{x})$を用いて$k(\mathbf{x},\mathbf{x}')=\boldsymbol{\varphi}(\mathbf{x})^\mathrm{T}\boldsymbol{\varphi}(\mathbf{x'})$と書ける。


## 演習 6.12
<div class="panel-primary">

あらかじめ固定された集合$D$のすべての部分集合$A$の空間を考え，カーネル関数
$$
k\left(A_{1}, A_{2}\right)=2^{\left| A_{1} \cap A_{2} \right|} \tag{6.27}
$$
は，写像$\boldsymbol{\phi}(A)$によって定義される$2^{|D|}$次元の特徴空間における内積であることを示せ．なお，$A$は$D$の部分集合であり，部分集合$U$で指定される$\boldsymbol{\phi}(A)$の各要素$\phi_U(A)$は，以下で与えられるとする．
$$
\phi_{U}(A)=\left\{\begin{array}{ll}
1, & U \subseteq A \text { のとき } \\
0, & \text { それ以外. }
\end{array}\right. \tag{6.95}
$$
ここで，$U \subseteq A$は，$U$は$A$の部分集合であるか，$A$そのものであることを表すとする．

</div>

$(6.1)$のカーネル関数の定義から、何らかの非線形の特徴空間への写像である$\boldsymbol{\phi}(\mathbf{x})$を用いて、この$\mathbf{x}$が集合であっても

$$
k(A_1, A_2) = 2^{|A_1 \cap A_2 |} = \boldsymbol{\phi}(A_1)^{\mathrm T} \boldsymbol{\phi}(A_2) \tag{1}
$$

という形で表すことができればよい。

まず一般に固定された集合$D$に含まれる要素の数を$|D|$として、$D$の部分集合全体の集合は要素数$2^{|D|}$となる（**べき集合**）。部分集合$A$の特徴空間への写像$\boldsymbol{\phi}(A)$は$\phi_U(A)$を構成要素としており、この部分集合$U$は同じく$2^{|D|}$個存在する。このうち、もし$\phi_U(A_1)$はもし$U$が$A_1$の部分集合であれば$1$、そうでなければ$0$となっている。

<hr>

例として$D=\{1,2,3\}, A_1=\{1,2\}, A_2 = \{1,2,3\}$を考える。これについての$(6.27)$式は

$$
k\left(A_{1}, A_{2}\right)=2^{\left|A_{1} \cap A_{2}\right|}=2^{|\{1,2\}|}=2^{2}=4
$$

となる。また、部分集合$A$の特徴空間への写像$\boldsymbol{\phi}(A)$は$2^{|D|} = 8$次元であり

$$
\boldsymbol{\phi}(A)=\left(\begin{array}{c}
\phi_{\phi}(A) \\
\phi_{\{1\}}(A) \\
\phi_{\{2\}}(A) \\
\phi_{\{3\}}(A) \\
\phi_{\{1,2\}}(A) \\
\phi_{\{2,3\}}(A) \\
\phi_{\{1,3\}}(A) \\
\phi_{\{1,2,3\}}(A)
\end{array}\right)
$$

のように構成される。$(6.95)$の定義を用いると

$$
\begin{aligned}
\boldsymbol{\phi}\left(A_{1}\right)^{\mathrm T} &= (1,1,1,0,1,0,0,0) \\
\boldsymbol{\phi}\left(A_{2}\right)^{\mathrm T} &= (1,1,1,1,1,1,1,1)
\end{aligned}
$$

である。この例では$\boldsymbol{\phi}(A_1)^{\mathrm T}\boldsymbol{\phi}(A_2) = 4$となる。

<hr>

上の例で見たように、$(1)$式について、$2^{|A_1 \cap A_2 |}$は$A_1$と$A_2$の共通の部分集合族の要素数を表している。

一方で内積$\boldsymbol{\phi}(A_1)^{\mathrm T}\boldsymbol{\phi}(A_2)$は$A_1$の部分集合であり、かつ$A_2$の部分集合となっている部分集合$U$の要素数を表していることになる。

この両者は同じ集合を表しているので

$$
k(A_1, A_2) = 2^{|A_1 \cap A_2 |}
$$

は写像$\boldsymbol{\phi}(A)$で定義される$2^{|D|}$次元の特徴空間における内積であることが示された。

※※※※※※※※※※※※
（参考）
$A$の張る線型空間においては、通常の意味での内積は定義できない。内積の定義は以下だが、そもそも$A$の集合は体ではない。（例えば、和集合$\cup$を和、積集合$\cap$を積と定義したとしても、乗法の逆元（減法）が定義されないので群をなさない。）従って、線型ベクトル空間（計量線型空間）の定義を満たさない。
\begin{eqnarray}
(A,B+C)&=&(A,B)+(A,C)\\
(A+B,C)&=&(A,C)+(B,C)\\
(kA,B)&=&k(A,B)\\
(A,kB)&=&\bar{k}(A,B)\\
(A,B)&=&\overline{(B,A)}\\
(A,A)&\geq&0\\
(A,A)&=&0となるのはA=0の時に限る。
\end{eqnarray}
上で議論したのは、$A$の非線形写像$\phi(A)$同士の内積であって、$A$同士の内積ではない。
※※※※※※※※※※※※

## 演習 6.13
<div class="panel-primary">

$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\mathbf{g}(\boldsymbol{\theta}, \mathbf{x})^{\mathrm{T}} \mathbf{F}^{-1} \mathbf{g}\left(\boldsymbol{\theta}, \mathbf{x}^{\prime}\right) \tag{6.33}
$$
で定義されるフィッシャーカーネルはパラメータベクトル$\boldsymbol{\theta}$に非線形の変換$\boldsymbol{\theta} \rightarrow \boldsymbol{\psi}(\boldsymbol{\theta})$を行っても不変であることを示せ．なお，$\boldsymbol{\psi}(\cdot)$は可逆で，かつ，微分可能であるとする．

</div>

$(6.32)$の定義から

$$
\mathbf{g}(\boldsymbol{\theta}, \mathbf{x})=\nabla_{\boldsymbol{\theta}} \ln p(\mathbf{x} \mid \boldsymbol{\theta}) \tag{6.32}
$$

そして$\mathbf{F}$はフィッシャー情報量行列

$$
\mathbf{F}=\mathbb{E}_{\mathbf{x}}\left[\mathbf{g}(\boldsymbol{\theta}, \mathbf{x}) \mathbf{g}(\boldsymbol{\theta}, \mathbf{x})^{\mathrm{T}}\right] \tag{6.34}
$$

である。

$\boldsymbol{\theta}\to \boldsymbol{\psi}(\boldsymbol{\theta})$の変換に伴い（$\boldsymbol{\psi}(\cdot)$は微分可能なので）

$$
\begin{aligned}
\mathbf{g}(\boldsymbol{\theta}, \mathbf{x}) &=\nabla_{\boldsymbol{\theta}} \ln p(\mathbf{x} \mid \boldsymbol{\theta}) \\
&=\begin{pmatrix}\frac{\partial}{\partial \theta_{1}} \\ \vdots \\\frac{\partial}{\partial \theta_{n}}\end{pmatrix} \ln p(\mathbf{x} \mid \boldsymbol{\theta}) \\
&=\begin{pmatrix}\frac{\partial \psi_{1}}{\partial \theta_{1}} & \cdots & \frac{\partial \psi_{n}}{\partial \theta_{1}} \\ \vdots & \ddots & \vdots \\ \frac{\partial \psi_{1}}{\partial \theta_{m}} & \cdots & \frac{\partial \psi_{n}}{\partial \theta_{m}}\end{pmatrix}\begin{pmatrix}\frac{\partial}{\partial \psi_{1}} \\ \vdots \\ \frac{\partial}{\partial \psi_{n}}\end{pmatrix} \ln p(\mathbf{x} \mid \boldsymbol{\psi}(\boldsymbol{\theta})) \\
&=\mathbf{M}\nabla_{\boldsymbol{\psi}}\ln p(\mathbf{x}\mid \boldsymbol{\psi}(\boldsymbol{\theta}))
\end{aligned}
$$

となる。ここで$\mathbf{M}$は$M_{ij} = \frac{\partial \psi_j}{\partial \theta_i}$を成分とするヤコビ行列である。また、簡略化のために

$$
\mathbf{g}_{\boldsymbol{\psi}} = \nabla_{\boldsymbol{\psi}}\ln p(\mathbf{x}\mid \boldsymbol{\psi}({\boldsymbol{\theta}}))
$$

とおく。これよりフィッシャー情報量行列も以下のように書き換えられる。

$$
\begin{aligned}
\mathbf{F}_{\boldsymbol{\psi}} &= \mathbb{E}_{\mathbf{x}}\left[ \mathbf{g}(\boldsymbol{\theta}, \mathbf{x}) \mathbf{g}(\boldsymbol{\theta}, \mathbf{x})^{\mathrm T} \right]\\
&=\mathbb{E}_{\mathbf{x}}\left[\mathbf{M g}_{\boldsymbol{\psi}} \mathbf{g}_{\boldsymbol{\psi}}^{\mathrm{T}} \mathbf{M}^{\mathrm{T}}\right] \\ &=\mathbf{M} \mathbb{E}_{\mathbf{x}}\left[\mathbf{g}_{\boldsymbol{\psi}} \mathbf{g}_{\boldsymbol{\psi}}^{\mathrm{T}}\right] \mathbf{M}^{\mathrm{T}}
\end{aligned}
$$

以上からこの$\mathbf{g}_{\boldsymbol{\psi}}$と$\mathbf{F}_{\boldsymbol{\psi}}$を用いて$\boldsymbol{\theta}\to \boldsymbol{\psi}(\boldsymbol{\theta})$での変換後の$(6.33)$のフィッシャーカーネルの右辺を計算すると

$$
\begin{aligned}
\mathbf{g}_{\boldsymbol{\psi}}^{\mathrm T} \mathbb{E}_{\mathbf{x}}\left[\mathbf{g}_{\boldsymbol{\psi}} \mathbf{g}_{\boldsymbol{\psi}}^{\mathrm T}\right]^{-1} \mathbf{g}_{\boldsymbol{\psi}} &= \mathbf{g}^{\mathrm T}\left(\mathbf{M}^{-1}\right)^{\mathrm T}\left(\mathbb{E}_{\mathbf{x}}\left[ \mathbf{M}^{-1}\mathbf{g}\mathbf{g}^{\mathrm T}(\mathbf{M}^{-1})^{\mathrm T} \right]\right)^{-1}\mathbf{M}^{-1}\mathbf{g} \\
&=\mathbf{g}^{\mathrm T}\left( \mathbf{M}^{-1} \right)^{\mathrm T} \left( \left( \mathbf{M}^{-1} \right)^{\mathrm T} \right)^{-1}\mathbb{E}_{\mathbf{x}}\left[\mathbf{gg}^{\mathrm T}\right]^{-1} \left( \mathbf{M}^{-1} \right)^{-1}\mathbf{M}^{-1}\mathbf{g} \\
&=\mathbf{g}^{\mathrm T}\mathbb{E}_{\mathbf{x}}\left[\mathbf{gg}^{\mathrm T}\right]^{-1}\mathbf{g} \\
&=\mathbf{g}^{\mathrm T}\mathbf{F}\mathbf{g}
\end{aligned}
$$

となる。以上から、$\boldsymbol{\theta} \rightarrow \boldsymbol{\psi}(\boldsymbol{\theta})$の変換を行っても$(6.33)$式で定義されるフィッシャーカーネルは不変であることが示された。

## 演習 6.14
<div class="panel-primary">

平均$\boldsymbol{\mu}$と共分散$\mathbf{S}$をもつガウス分布$p(\mathbf{x} \mid \boldsymbol{\mu})=\mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{S})$に対して，
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\mathbf{g}(\boldsymbol{\theta}, \mathbf{x})^{\mathrm{T}} \mathbf{F}^{-1} \mathbf{g}\left(\boldsymbol{\theta}, \mathbf{x}^{\prime}\right) \tag{6.33}
$$
で定義されるフィッシャーカーネルの具体的な形式を導け．

</div>

まずフィッシャースコア$(6.32)$について計算すると

$$
\begin{aligned}
\mathbf{g}(\boldsymbol{\mu}, \mathbf{x}) &=\nabla_{\boldsymbol{\mu}} \ln p(\mathbf{x} \mid \boldsymbol{\mu}) \\ &=\nabla_{\boldsymbol{\mu}} \ln \left(\exp \left\{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{\mathrm T} \mathbf{S}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right\}\right) \\
&=\left(-\frac{1}{2}\right)(-2) \mathbf{S}^{-1}(\mathbf{x}-\boldsymbol{\mu}) \hspace{1em} \left( \because \frac{\partial}{\partial \mathbf{s}}(\mathbf{x}-\mathbf{s})^{\mathrm T} \mathbf{W}(\mathbf{x}-\mathbf{s})=-2 \mathbf{W}(\mathbf{x}-\mathbf{s})\right)\\
&=\mathbf{S}^{-1}(\mathbf{x}-\boldsymbol{\mu})
\end{aligned}
$$

次にフィッシャー情報量行列$\mathbf{F}$は、$\mathbf{S}$が共分散行列なので対称行列であることを利用すると

$$
\begin{aligned}
\mathbf{F} &=\mathbb{E}_{\mathbf{x}}\left[\mathbf{S}^{-1}(\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^{\mathrm T}\left(\mathbf{S}^{-1}\right)^{\mathrm T}\right] \\
&=\mathbf{S}^{-1} \mathbb{E}_{\mathbf{x}}\left[(\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^{\mathrm T}\right] \mathbf{S}^{-1}\left(\because\left(\mathbf{S}^{-1}\right)^{\mathrm T}=\mathbf{S}^{-1}\right) \\
&=\mathbf{S}^{-1} \mathbf{S} \mathbf{S}^{-1}\left(\because \mathbb{E}_{\mathbf{x}}\left[(\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^{\mathrm T}\right]=\mathbf{S}\right) \\
&=\mathbf{S}^{-1}
\end{aligned}
$$

以上からフィッシャーカーネルは

$$
\begin{aligned}
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)
&= \mathbf{g}(\boldsymbol{\mu}, \mathbf{x})^{\mathrm{T}} \mathbf{F}^{-1} \mathbf{g}\left(\boldsymbol{\mu}, \mathbf{x}^{\prime}\right)\\
&=\left(\mathbf{S}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right)^{\mathrm T}\left(\mathbf{S}^{-1}\right)^{-1} \mathbf{S}^{-1}\left(\mathbf{x}^{\prime}-\boldsymbol{\mu}\right) \\
&=(\mathbf{x}-\boldsymbol{\mu})^{\mathrm T} \mathbf{S}^{-1}\left(\mathbf{x}^{\prime}-\boldsymbol{\mu}\right)
\end{aligned}
$$

となる。結果的にこの値は$(2.44)$で示された**マハラノビス距離**になっている。

## 演習 6.15
<div class="panel-primary">

$2\times 2$のグラム行列の行列式を考えて，正定値であるカーネル関数$k(x, x^{\prime})$はコーシーシュワルツの不等式

$$
k\left(x_{1}, x_{2}\right)^{2} \leqslant k\left(x_{1}, x_{1}\right) k\left(x_{2}, x_{2}\right) \tag{6.96}
$$



を満たすことを示せ．

</div>

2X2のグラム行列は次のように与えられる。
$$
\left(\begin{array}{ll}k\left(x_{1}, x_{1}\right) & k\left(x_{1}, x_{2}\right) \\ k\left(x_{2}, x_{1}\right) & k\left(x_{2}, x_{2}\right)\end{array}\right)
$$

有効なカーネル関数の場合グラム行列は半正定値なので行列式は0以上となる。(固有値
が非負のため)。上記グラム行列の行列式は
$$
k\left(x_{1}, x_{1}\right) \cdot k\left(x_{2}, x_{2}\right)-k\left(x_{1}, x_{2}\right) \cdot k\left(x_{2}, x_{1}\right) \geqq 0
$$

$$
\Leftrightarrow k\left(x_{1}, x_{2}\right)^{2} \leqq k\left(x_{1}, x_{1}\right) k\left(x_{2}, x_{2}\right)
$$

となり、(6.96)を満たすことが示された。



## 演習 6.16
<div class="panel-primary">

パラメータベクトル$\mathbf{w}$と入力のデータ集合$\mathbf{x}_1, \ldots, \mathbf{x}_N$および非線形の特徴空間への写像$\boldsymbol{\phi}(\mathbf{x})$を持つパラメトリックモデルに対し，誤差関数が$\mathbf{w}$の関数として次のように与えられるとする．
$$
J(\mathbf{w})=f\left(\mathbf{w}^{\mathbf{T}} \boldsymbol{\phi}\left(\mathbf{x}_{1}\right), \ldots, \mathbf{w}^{\mathbf{T}} \boldsymbol{\phi}\left(\mathbf{x}_{N}\right)\right)+g\left(\mathbf{w}^{\mathbf{T}} \mathbf{w}\right)
$$
ここで，$g(\cdot)$は単調増加関数であるとする．$\mathbf{w}$を，
$$
\mathbf{w}=\sum_{n=1}^{N} \alpha_{n} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)+\mathbf{w}_{\perp} \tag{6.98}
$$
という形式で書くことによって$J(\mathbf{w})$を最小化する$\mathbf{w}$の値は$n=1,\ldots,N$についての基底関数$\boldsymbol{\phi}(\mathbf{x}_n)$の線形結合で表されることを示せ．ただしすべての$n$について$\mathbf{w}_{\perp}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)=0$であるとする．

</div>

リプレゼンター定理の証明。

(6.98)式は以下のように書き換えることができる。


\begin{equation}
\mathbf{w}=\mathbf{w}_{\|}+\mathbf{w}_{\perp}
\end{equation}

\begin{equation}
\mathbf{w}_{\|}=\sum_{n=1}^{N} \alpha_{n} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)
\end{equation}

これは、$\mathbf{w}$を$\mathbf{w}_{\|}$とその直交補空間の$\mathbf{w}_{\perp}$の直和分解していることになるので、全てのnに対して$\mathbf{w}_{\perp}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)=0$が成立する。

$\mathbf{w}$を損失関数$J(\mathbf{w})$に代入すると、以下の式になる。

$$
\begin{aligned}
J(\mathbf{w})=& f\left(\left(\mathbf{w}_{\|}+\mathbf{w}_{\perp}\right)^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{1}\right), \ldots,\left(\mathbf{w}_{\|}+\mathbf{w}_{\perp}\right)^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{N}\right)\right) \\
&+g\left(\left(\mathbf{w}_{\|}+\mathbf{w}_{\perp}\right)^{\mathrm{T}}\left(\mathbf{w}_{\|}+\mathbf{w}_{\perp}\right)\right) \\
=& f\left(\mathbf{w}_{\|}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{1}\right), \ldots, \mathbf{w}_{\|}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{N}\right)\right)+g\left(\mathbf{w}_{\|}^{\mathrm{T}} \mathbf{w}_{\|}+\mathbf{w}_{\perp}^{\mathrm{T}} \mathbf{w}_{\perp}\right)
\end{aligned}
$$

損失関数の最小化を考えた場合、$\mathbf{w}_{\perp}$は正則化項の$g(\cdot)$の中にのみに存在し、定義より$g(\cdot)$は単調増加関数なので$\mathbf{w}_{\perp}$が0となる。よって

$$
\mathbf{w}=\mathbf{w}_{\|}=\sum_{n=1}^{N} \alpha_{n} \phi\left(\mathbf{x}_{n}\right)
$$

が示された。


## 演習 6.17
<div class="panel-primary">

入力に，分布$\nu(\boldsymbol{\xi})$を持つノイズがある場合の二乗和誤差関数

$$
E=\frac{1}{2} \sum_{n=1}^{N} \int\left\{y\left(\mathbf{x}_{n}+\boldsymbol{\xi}\right)-t_{n}\right\}^{2} \nu(\boldsymbol{\xi}) \mathrm{d} \boldsymbol{\xi} \tag{6.39}
$$

を考える．変分法を用いて，この誤差関数を関数$y(\mathbf{x})$について最小化し，最適な解は，基底関数として

$$
h\left(\mathbf{x}-\mathbf{x}_{n}\right)=\frac{\nu\left(\mathbf{x}-\mathbf{x}_{n}\right)}{\sum_{n=1}^{N} \nu\left(\mathbf{x}-\mathbf{x}_{n}\right)} \tag{6.41}
$$

を用いた展開

$$
y(\mathbf{x})=\sum_{n=1}^{N} t_{n} h\left(\mathbf{x}-\mathbf{x}_{n}\right) \tag{6.40}
$$

の形で与えられることを示せ．

</div>

※ 変分法（上巻の付録D）によれば、$y(\mathbf{x})$についての汎関数$E[y]$の最小化を考える。ある微小な定数$\varepsilon$と任意の関数$\eta(x)$を用いて、汎関数$E[y]$の$y(x)$に対する変分$\delta E/\delta y(x)$を次の式で定義する。

$$
E[y(x)+\varepsilon \eta(x)]=E[y(x)]+\varepsilon \int \frac{\delta E}{\delta y} \eta(x) d x+O\left(\varepsilon^{2}\right)
$$

汎関数が最大もしくは最小となる場合には関数$y(x)$の微小な変化に対して汎関数が変化しない（停留する）ことが必要条件となる。すなわち

$$
\varepsilon \int \frac{\delta E}{\delta y} \eta(x) d x = 0
$$
となる。
<hr>

以上の変分法の考えから、まず$y(\mathbf{x})\to y(\mathbf{x})+\epsilon\eta(\mathbf{x})$としたときの汎関数$E[y]$の変分を考える。

$$
\begin{align}
E[y+\varepsilon \eta]
&=\frac{1}{2} \sum_{n=1}^{N} \int\left\{y\left(\mathbf{x}_n+\boldsymbol{\xi}\right)+\varepsilon \eta\left(\mathbf{x}_n+\boldsymbol{\xi}\right)-t_{n}\right\}^{2} \nu(\boldsymbol{\xi}) d \boldsymbol{\xi} \\
&=\frac{1}{2} \sum_{n=1}^{N} \int\left[\left\{y\left(\mathbf{x}_n+\boldsymbol{\xi}\right)-t_{n}\right\}^{2}+2 \varepsilon \eta\left(\mathbf{x}_n+\boldsymbol{\xi}\right)\left\{y\left(\mathbf{x}_n+\boldsymbol{\xi}\right)-t_{n}\right\}+\varepsilon^{2} \eta^{2}\right] \nu(\boldsymbol{\xi}) d\boldsymbol{\xi} \\
&= \frac{1}{2} \sum_{n=1}^{N} \int\left[\left\{y\left(\mathbf{x}_n+\boldsymbol{\xi}\right)-t_{n}\right\}^{2} \right]\nu(\boldsymbol{\xi})d\boldsymbol{\xi} + \varepsilon \sum_{n=1}^{N}\int \eta\left(\mathbf{x}_n+\boldsymbol{\xi}\right)\left\{y\left(\mathbf{x}_n+\boldsymbol{\xi}\right)-t_{n}\right\} \nu(\boldsymbol{\xi})d\boldsymbol{\xi} +  \frac{\varepsilon^{2}}{2} \sum_{n=1}^{N} \int\eta^{2} \nu(\boldsymbol{\xi}) d\boldsymbol{\xi} \\
&=E[y] + \varepsilon\sum_{n=1}^{N}\int \eta(\mathbf{x}_n+\boldsymbol{\xi})\left\{ y(\mathbf{x}_n+\boldsymbol{\xi}) -t_n\right\}\nu(\boldsymbol{\xi})d\boldsymbol{\xi} + O(\varepsilon^2)
\end{align}
$$

以上から変分が0になる停留条件は

$$
\sum_{n=1}^{N}\int \left\{ y(\mathbf{x}_n+\boldsymbol{\xi}) -t_n\right\}\eta(\mathbf{x}_n+\boldsymbol{\xi})\nu(\boldsymbol{\xi})d\boldsymbol{\xi} = 0
$$

である。

$\eta(\mathbf{x}_n+\boldsymbol{\xi})$は任意の関数である。そのため、ディラックのデルタ関数として、積分記号を外すことを目指す。

$$
\eta(\mathbf{x}_n+\boldsymbol{\xi}) = \delta((\mathbf{x}_n+\boldsymbol{\xi})-\mathbf{z})
$$

とすると、ディラックのデルタ関数の性質より、

$$
\int f(x)\delta(x - x_0)dx = f(x_0)
$$

が成り立つ。よって、$g(x) =  \{y(\mathbf{x}_n) -t_n\} \nu(\mathbf{\xi})$として、

$$
\begin{align}
\sum_{n=1}^{N} \int\left\{y\left(\mathbf{x}_{n}+\boldsymbol{\xi}\right)-t_{n}\right\}  \delta\left(\left(
\mathbf{x}_{n}+\boldsymbol{\xi}\right)-\mathbf{z}\right) \nu(\boldsymbol{\xi}) d \boldsymbol{\xi}
&= \sum_{n=1}^{N} \int\left\{y\left(\mathbf{x}_{n}+\boldsymbol{\xi}\right)-t_{n}\right\}  \nu(\mathbf{\xi}) \delta\left(\left(\mathbf{x}_{n}+\boldsymbol{\xi}\right)-\mathbf{z}\right)d \boldsymbol{\xi} \\
&= \sum_{n=1}^{N} \int g(\mathbf{x}_n+ \boldsymbol{\xi}) \delta\left(\left(\mathbf{x}_{n}+\boldsymbol{\xi}\right)-\mathbf{z}\right)d \boldsymbol{\xi} \\
&= \sum_{n=1}^{N} g(\mathbf{z}) \\
&= \sum_{n=1}^{N}\left\{y(\mathbf{z})-t_{n}\right\} \nu\left(\mathbf{z}-\mathbf{x}_{n}\right) \\
&= 0
\end{align}
$$

なお、ディラックのデルタ関数の性質における$x = \mathbf{x}_{n}+\boldsymbol{\xi}$、$x_0 = \mathbf{z}$とした。また、最後の0は、停留条件である。

これを変形して

$$
\begin{aligned}
&y(\mathbf{z}) \sum_{n=1}^{N}\nu(\mathbf{z}-\mathbf{x}_n) = \sum_{n=1}^{N}t_n\nu(\mathbf{z}-\mathbf{x}_n) \\
&y(\mathbf{z}) = \sum_{n=1}^{N}t_n\frac{\nu(\mathbf{z}-\mathbf{x}_n)}{\sum_{n=1}^N\nu(\mathbf{z}-\mathbf{x}_n)}
\end{aligned}
$$

最後に$\mathbf{z}\to\mathbf{x}$とすれば、題意が成立する。

## 演習 6.18
<div class="panel-primary">

等方共分散をもつ，つまり，共分散行列が$\sigma^2 \mathbf{I}$ ($\mathbf{I}$は単位行列)で与えられるようなガウス基底を持つようなNadaraya-Watsonモデルを考える．入力変数$x$と，目標変数$t$はそれぞれ1次元であるとする．このとき，条件付き密度$p(t\mid x)$，条件付き期待値$\mathbb{E}[t\mid x]$，および条件付き分散$\operatorname{var}[t\mid x]$をそれぞれカーネル関数$k(x, x_n)$を用いて書け．

</div>

教科書下巻P.18の設定通り、$f(x,t)$が平均$\mathbf{0}$,分散$\sigma ^2 \mathbf{I}$の等方的なガウス分布$\mathbf{z}=(x,t)$で与えられる場合を考える。すなわち

$$
f(x, t)=\mathcal{N}\left(\mathbf{z} \mid \mathbf{0}, \sigma^{2} \mathbf{I}\right)
$$

これを用いてParzen推定法から同時分布を求める式$(6.42)$

$$
p(x, t)=\frac{1}{N} \sum_{n=1}^{N} f\left(x-x_{n}, t-t_{n}\right)
$$

とする。

条件付き確率 $\displaystyle p(t\mid x) = \frac{p(t,x)}{p(x)} = \frac{p(t,x)}{\int p(t,x)dt}$より

$$
p(t \mid x)=\frac{\sum_{n=1}^{N} \mathcal{N}\left(\left[x-x_{n}, t-t_{n}\right]^{\mathrm T} \mid \mathbf{0}, \sigma^{2} \mathbf{I}\right)}{\int \sum_{m=1}^{N} \mathcal{N}\left(\left[x-x_{m}, t-t_{m}\right]^{\mathrm T} \mid \mathbf{0}, \sigma^{2} \mathbf{I}\right) d t}
$$

となる。

分子は

$$
\begin{aligned} & \sum_{n=1}^{N} \mathcal{N}\left(\begin{pmatrix}x-x_{n} \\ t-t_{n}\end{pmatrix} \mid \mathbf{0}, \sigma^{2} \mathbf{I}\right) \\
=& \sum_{n=1}^{N}\left[\frac{1}{(2 \pi)^{2 / 2}} \frac{1}{\sqrt{\mid \sigma^{2} \mathbf{I}} \mid } \exp \left\{-\frac{1}{2}\begin{pmatrix}x-x_{n} \\ t-t_{n}\end{pmatrix}^{\mathrm T}\left(\sigma^{2} \mathbf{I}\right)^{-1}\begin{pmatrix}x-x_{n} \\ t-t_{n}\end{pmatrix}\right\}\right] \\
=& \sum_{n=1}^{N}\left[\frac{1}{2 \pi \sigma^{2}} \exp \left\{-\frac{1}{2 \sigma^{2}}\left(x-x_{n}\right)^{2}\right\} \exp \left\{-\frac{1}{2 \sigma^{2}}\left(t-t_{n}\right)^{2}\right\}\right] \\
=& \sum_{n=1}^{N}\left[\mathcal{N}\left(x-x_{n} \mid 0, \sigma^{2}\right) \cdot \mathcal{N}\left(t-t_{n} \mid 0, \sigma^{2}\right)\right]
\end{aligned}
$$

となる。途中では$\mathbf{I}$が$2\times 2$の単位行列であることから$|\sigma^2 \mathbf{I}| = (\sigma^2)^2$となることを用いた。一方分母は$t$についての周辺化を行うので

$$
\begin{aligned}
&\int \sum_{m=1}^{N} \mathcal{N}\left(\begin{pmatrix}x-x_{n} \\ t-t_{n}\end{pmatrix} \mid \mathbf{0}, \sigma^{2} \mathbf{I}\right)dt \\
=&\int \sum_{m=1}^{N}\left[\mathcal{N}\left(x-x_{n} \mid 0, \sigma^{2}\right) \cdot \mathcal{N}\left(t-t_{n} \mid 0, \sigma^{2}\right)\right] dt \\
=&\sum_{m=1}^{N}\mathcal{N}\left(x-x_{n} \mid 0, \sigma^{2}\right)\int \mathcal{N}\left(t-t_{n} \mid 0, \sigma^{2}\right) dt \\
=&\sum_{m=1}^{N}\mathcal{N}\left(x-x_{n} \mid 0, \sigma^{2}\right)
\end{aligned}
$$

となる。以上をまとめると

$$
\begin{aligned}
p(t \mid x) &=\frac{\sum_{n=1}^{N}\left[\mathcal{N}\left(x-x_{n} \mid 0, \sigma^{2}\right) \cdot \mathcal{N}\left(t-t_{n} \mid 0, \sigma^{2}\right)\right]}{\sum_{m=1}^{N} \mathcal{N}\left(x-x_{m} \mid 0, \sigma^{2}\right)} \\
&=\sum_{n=1}^{N} \frac{\mathcal{N}\left(x-x_{n} \mid 0, \sigma^{2}\right)}{\sum_{m=1}^{N} \mathcal{N}\left(x-x_{m} \mid 0, \sigma^{2}\right)} \mathcal{N}\left(t-t_{n} \mid 0, \sigma^{2}\right)
\end{aligned}
$$

ここで

$$
\begin{aligned}
g(x) &= \int_{-\infty}^{\infty} \mathcal{N}\left(\begin{pmatrix}x \\ t\end{pmatrix} \mid \mathbf{0}, \sigma^{2} \mathbf{I}\right) d t \\
&= \mathcal{N}\left( x \mid 0 , \sigma^2 \right)
\end{aligned}
$$

とすると

$$
\begin{aligned}
p(t \mid x) &=\sum_{n=1}^{N} \frac{g\left(x-x_{n}\right)}{\sum_{m=1}^{N} g\left(x-x_{m}\right)} \mathcal{N}\left(t-t_n \mid 0, \sigma^{2}\right) \\
&=\sum_{n=1}^{N} \frac{g\left(x-x_{n}\right)}{\sum_{m=1}^{N} g\left(x-x_{m}\right)} \mathcal{N}\left(t \mid t_{n}, \sigma^{2}\right) \\ & \equiv \sum_{n=1}^{N} k\left(x, x_{n}\right) \mathcal{N}\left(t \mid t_{n}, \sigma^{2}\right)
\end{aligned}
$$

となる。ここで定義したカーネル$k(x,x_n)$は和の制約

$$
\sum_{n=1}^N k(x,x_n) = 1
$$

を満たしている。

次に、条件付き期待値は定義から

$$
\begin{aligned}
\mathbb{E}[t \mid x]
&=\int t p(t \mid x) d t \\
&=\int t \sum_{n=1}^{N} k\left(x, x_{n}\right) \mathcal{N}\left(t \mid t_{n}, \sigma^{2}\right) d t \\
&=\sum_{n=1}^{N} k\left(x, x_{n}\right) \int t \mathcal{N}\left(t \mid t_{n}, \sigma^{2}\right) d t \\
&=\sum_{n=1}^{N} k(x,x_n) t_n \left( \because (1.49)式, ガウス分布の性質\right)
\end{aligned}
$$

条件付き分散は、まず$\mathbb{E}[t^2\mid x]$について

$$
\begin{aligned} \mathbb{E}\left[t^{2} \mid x\right] &=\int t^{2} p(t \mid x) d x \\ &=\int t^{2} \sum_{n=1}^{N} k\left(x, x_{n}\right) \mathcal{N}\left(t \mid t_{n}, \sigma^{2}\right) d t \\ &=\sum_{n=1}^{N} k\left(x, x_{n}\right) \int t^{2} \mathcal{N}\left(t \mid t_{n}, \sigma^{2}\right) d t \\ &=\sum_{n=1}^{N} k\left(x, x_{n}\right)\left(t_{n}^{2}+\sigma^{2}\right) \left( \because (1.50)式, ガウス分布の性質\right)
\end{aligned}
$$

これより

$$
\begin{aligned}
\operatorname{var}[t\mid x] &= \mathbb{E}[t^2\mid x] - \mathbb{E}[t\mid x]^2 \\
&= \sum_{n=1}^{N} k\left(x, x_{n}\right)\left(t_{n}^{2}+\sigma^{2}\right)-\left\{\sum_{n=1}^{N} k\left(x, x_{n}\right) t_{n}\right\}^{2}
\end{aligned}
$$

## 演習 6.19
<div class="panel-primary">

カーネル回婦の問題を別の視点から見ると，入力変数と目標変数が加法的なノイズによって影響されていると考えることができる．通常通り，各目標変数$t_n$を，点$\mathbf{z}_n$において評価された関数$y(\mathbf{z}_n)$に，ガウスノイズが加わったものとする．$\mathbf{z}_n$は直接観測されることはなく，ノイズが加わった$\mathbf{x}_{n}=\mathbf{z}_{n}+\boldsymbol{\xi}_{n}$が観測される．ここで，確率変数$\boldsymbol{\xi}$は，ある分布$g(\boldsymbol{\xi})$に従うとする．観測された集合$\left\{\mathbf{x}_{n}, t_{n}\right\}(n=1, \ldots, N)$に対して入力変数に加えられたノイズの分布で期待値を取った二乗和誤差関数
$$
E=\frac{1}{2} \sum_{n=1}^{N} \int\left\{y\left(\mathbf{x}_{n}-\boldsymbol{\xi}_{n}\right)-t_{n}\right\}^{2} g\left(\boldsymbol{\xi}_{n}\right) \mathrm{d} \boldsymbol{\xi}_{n} \tag{6.99}
$$
を考える．変分法(付録D)を用いて．$E$を関数$y(\mathbf{z})$について最小化することで，最適な$y(\mathbf{x})$は，カーネル
$$
k\left(\mathbf{x}, \mathbf{x}_{n}\right)=\frac{g\left(\mathbf{x}-\mathbf{x}_{n}\right)}{\sum_{m} g\left(\mathbf{x}-\mathbf{x}_{m}\right)} \tag{6.46}
$$
を持った，Nadaraya-Watsonカーネル回帰
$$
\begin{aligned} y(\mathbf{x})=& \frac{\sum_{n} g\left(\mathbf{x}-\mathbf{x}_{n}\right) t_{n}}{\sum_{m} g\left(\mathbf{x}-\mathbf{x}_{m}\right)} \\=& \sum_{n} k\left(\mathbf{x}, \mathbf{x}_{n}\right) t_{n} \end{aligned} \tag{6.45}
$$
の形になることを示せ．

</div>

※演習$6.17$とほぼ同じ

関数$y(\mathbf{z}_n)$について、微小な定数$\varepsilon$と任意の関数$\eta(\mathbf{z}_n)$を用いた変分法を$(6.99)$式に適用する。

$y(\mathbf{z}_n) \to y(\mathbf{z}_n) + \varepsilon\eta(\mathbf{z}_n)$の変分を考えて,

$$
\begin{aligned}
E[y+\varepsilon \eta] &=-\frac{1}{2} \sum_{n=1}^{N} \int\left\{y\left(\mathbf{x}_{n}-\mathbf{\xi}_n\right)+\varepsilon \eta\left(\mathbf{x}_{n}-\mathbf{\xi}\right)-t_{n}\right\}^{2} g\left(\mathbf{\xi}\right) d \mathbf{\xi}_n \\
&=-\frac{1}{2} \sum_{n=1}^{N} \int\left[\left\{y\left(\mathbf{x}_{n}-\mathbf{\xi}_n\right)-t_{n}\right\}^{2} +2 \varepsilon \eta\left(\mathbf{x}_{n}-\mathbf{\xi}_n\right)\left(y\left(\mathbf{x}_{n}-\mathbf{\xi}_n\right)-t_{n}\right)+\varepsilon^2\eta\left(\mathbf{x}_{n}-\mathbf{\xi}_n\right)^2\right]g(\mathbf{\xi})d\mathbf{z}_n \\
&=E[y]-\varepsilon \sum_{n=1}^{N} \int \eta\left(\mathbf{x}_{n}-\mathbf{\xi}_n \right)\left(y\left(\mathbf{x}_{n}-\mathbf{\xi}_n \right)-t_{n}\right) g\left(\mathbf{\xi}_n\right) d \mathbf{\xi}_n+O\left(\varepsilon^{2}\right)
\end{aligned}
$$

これより停留条件は

$$
\sum_{n=1}^{N} \int \eta\left(\mathbf{x}_{n}-\mathbf{\xi}_n \right)\left(y\left(\mathbf{x}_{n}-\mathbf{\xi}_n \right)-t_{n}\right) g\left(\mathbf{\xi}_n\right) d \mathbf{\xi}_n = 0
$$

となる。

ここで$\eta(\mathbf{x}_n - \mathbf{\xi}_n) = \delta(\mathbf{x}_n - \mathbf{\xi}_n- \mathbf{z})$とおくと

$$
\begin{align}
\sum_{n=1}^{N} \int \delta\left(\mathbf{x}_n-\boldsymbol{\xi}_{n}-\mathbf{z}\right)\left(y\left(\mathbf{x}_n-\boldsymbol{\xi}_{n}\right)-t_{n}\right) g\left(\mathbf{x}_n - \left( \mathbf{x}_n - \mathbf{\xi}_n \right) \right) d \mathbf{\xi}_{n}
&= \sum_{n=1}^{N} \left(y\left(\mathbf{z}\right)-t_{n}\right) g\left(\mathbf{x}_n-\mathbf{z}\right) \\
&= 0
\end{align}
$$

$y(\mathbf{z})$について解くと

$$
\begin{aligned}
&y(\mathbf{z}) \sum_{n=1}^{N} g\left(\mathbf{x}_{n}-\mathbf{z}\right)=\sum_{n=1}^{N} t_n g\left(\mathbf{x}_{n}-\mathbf{z}\right) \\
&\Leftrightarrow y(\mathbf{z}) = \sum_{n=1}^{N} t_n \frac{g\left(\mathbf{x}_{n}-\mathbf{z}\right)}{\sum_{m=1}^{N} g\left(\mathbf{x}_{m}-\mathbf{z}\right)}
\end{aligned}
$$
となる。そして、
$$
\begin{align}
y(\mathbf{z}) &= \sum_{n=1}^{N} t_n \frac{g\left(\mathbf{x}_{n}-\mathbf{z}\right)}{\sum_{m=1}^{N} g\left(\mathbf{x}_{m}-\mathbf{z}\right)} \\
&= \sum_{n=1}^{N} t_n  k(z, x_n)
\end{align}
$$

となり、$z = x$を代入すれば、題意は満たされる。

これは式$(6.46)$のカーネルを持ったNadaraya-Watsonモデルとなっており、さらにこのカーネルは和の制約

$$
\sum_{n=1}^N k(\mathbf{x},\mathbf{x}_n) = \frac{\sum_{n=1}^N g\left(\mathbf{x}_{n}-\mathbf{x}\right)}{\sum_{m=1}^{N} g\left(\mathbf{x}_{m}-\mathbf{x}\right)} = 1
$$

を満たしている。

## 演習 6.20
<div class="panel-primary">

$$
m\left(\mathbf{x}_{N+1}\right)=\mathbf{k}^{\mathrm{T}} \mathbf{C}_{N}^{-1} \mathbf{t} \tag{6.66}
$$
と
$$
\sigma^{2}\left(\mathbf{x}_{N+1}\right)=c-\mathbf{k}^{\mathrm{T}} \mathbf{C}_{N}^{-1} \mathbf{k} \tag{6.67}
$$
の結果を確認せよ．

</div>

上巻P.82の2.3.1 条件付きガウス分布を参照。多変数ガウス分布の重要な特性で、2つの変数集合の同時分布がガウス分布に従うならば、一方の変数集合が与えられたときの、もう一方の集合の条件付き分布もガウス分布となる。さらに、どちらの変数集合の周辺分布も同様にガウス分布になる。

今、訓練集合として入力$\mathbf{x}_{1}, \ldots, \mathbf{x}_{N}$と、対応する$\mathbf{t}_N = (t_1, \ldots, t_N)^{\mathrm T}$が与えられているときに、新しい入力ベクトルに対する目標変数$t_{N+1}$を予測する。$t_1,\ldots,t_{N+1}$の同時分布は

$$
p(\mathbf{t}_{N+1}) = \mathcal{N}\left(\mathbf{t}_{N+1} \mid \mathbf{0}, \mathbf{C}_{N+1}\right) \tag{6.64}
$$

のガウス分布で、$t_1,\ldots,t_{N}$の同時分布は

$$
p(\mathbf{t})=\int p(\mathbf{t} \mid \mathbf{y}) p(\mathbf{y}) \mathrm{d} \mathbf{y}=\mathcal{N}(\mathbf{t} \mid \mathbf{0}, \mathbf{C}) \tag{6.61}
$$

のガウス分布で与えられている。

条件付き分布$p(t_{N+1}\mid \mathbf{t})$を求めたいので、2.3.1.条件付きガウス分布の定理$(2.81), (2.82)$に書かれている、条件付き分布$p(\mathbf{x}_a\mid \mathbf{x}_b)$の平均と共分散が

$$
\begin{aligned}
\boldsymbol{\boldsymbol{\mu}}_{a \mid b}=\boldsymbol{\mu}_{a}+\mathbf{\Sigma}_{a b} \mathbf{\Sigma}_{b b}^{-1}\left(\mathbf{x}_{b}-\boldsymbol{\mu}_{b}\right) \\ \mathbf{\Sigma}_{a \mid b}=\mathbf{\Sigma}_{a a}-\mathbf{\Sigma}_{a b} \mathbf{\Sigma}_{b b}^{-1} \mathbf{\Sigma}_{b a}
\end{aligned}
$$

で与えられることを利用するために、$\mathbf{x}_a \to t_{N+1}$, $\mathbf{x}_b \to \mathbf{t}$とする。この設定より$\boldsymbol{\mu}_a \to 0$, $\boldsymbol{\mu}_b \to \mathbf{0}$とする。ここで、共分散行列$\mathbf{C}_{N+1}$はこのように設定した影響で

$$
\mathbf{C}_{N+1}=\begin{pmatrix}c & \mathbf{k}^{\mathrm{T}} \\ \mathbf{k} & \mathbf{C}_{N}\end{pmatrix}
$$

と書き表されることに注意する。これから

$$
\left(\begin{array}{ll}\mathbf{\Sigma}_{a a} & \mathbf{\Sigma}_{a b} \\ \mathbf{\Sigma}_{b a} & \mathbf{\Sigma}_{b b}\end{array}\right)=
\begin{pmatrix}c & \mathbf{k}^{\mathrm{T}} \\ \mathbf{k} & \mathbf{C}_{N}\end{pmatrix}
$$

となる。

以上から、

$$
m(\mathbf{x}_{N+1}) = \mathbf{0} + \mathbf{k}^{\mathrm T}\mathbf{C}_N^{-1}(\mathbf{t}-\mathbf{0}) = \mathbf{k}^{\mathrm T}\mathbf{C}_N^{-1}\mathbf{t} \tag{6.66}
$$

$$
\sigma^{2}(\mathbf{x}_{N+1}) = c - \mathbf{k}^{\mathrm T}\mathbf{C}_N^{-1}\mathbf{k} \tag{6.67}
$$

を得る。

## 演習 6.21
<div class="panel-primary">

固定された非線形の基底関数を使ってカーネル関数が定義されたガウス過程による回帰モデルを考え，その予測分布が3.3.2節で得られたベイス線形回帰モデルに対する結果
$$
p(t \mid \mathbf{x}, \mathbf{t}, \alpha, \beta)=\mathcal{N}\left(t \mid \mathbf{m}_{N}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x}), \sigma_{N}^{2}(\mathbf{x})\right) \tag{3.58}
$$
と同じになることを示せ．両方のモデルがガウス予測分布を持つことに注意する，つまり条件付き期待値と条件付き分散がそれぞれ等しくなることを示せばよい．条件付き期待値については，行列に関する等式(C.6)を条件付き分散については(C.7)を用いよ．

</div>

(3.58)の$m_{N}$,$\mathbf{S}_{N}$はそれぞれ
$$
\mathbf{m}_{N}=\beta\mathbf{S}_{N}\mathbf{\Phi}^{T}\mathbf{t}\tag{3.53}
$$
$$
\mathbf{S}_{N}^{-1}=\alpha\mathbf{I}+\beta\mathbf\Phi\mathbf{\Phi}^{T}\tag{3.54}
$$
と表される.
(C.6), (C.7)はそれぞれ

$$
(\mathbf{I}+\mathbf{A}\mathbf{B})^{-1}\mathbf{A}=\mathbf{A}(\mathbf{I}+\mathbf{B}\mathbf{A})^{-1}\tag{C.6}
$$
$$
(\mathbf{A}+\mathbf{B}\mathbf{D}^{-1}\mathbf{C})^{-1}=\mathbf{A}^{-1}-\mathbf{A}^{-1}\mathbf{B}(\mathbf{D}+\mathbf{C}\mathbf{A}^{-1}\mathbf{B})^{-1}\mathbf{C}\mathbf{A}^{-1}\tag{C.7}
$$

である．
固定された非線形の基底関数を$\boldsymbol{\phi}$とおくと(6.62)より
$$
\begin{aligned}
\mathbf{C}_{N}&=\mathbf{K}+\beta^{-1}\mathbf{I}_{N}\\
&=\alpha^{-1}\mathbf{\Phi\Phi^T}+\beta^{-1}\mathbf{I}_{N}
\end{aligned}
$$
$$
\begin{aligned}
\mathbf{k}&=(k(x_{1}, x_{N+1}),k(x_{2},x_{N+1}), ...,k(x_{N},x_{N+1}))^{T}\\
&=((\phi(x_{1})^{T}\phi(x_{N+1}),(\phi(x_{2})^{T}\phi(x_{N+1}),...,(\phi(x_{N})^{T}\phi(x_{N+1})))^{T}\\
&=\alpha^{-1}\mathbf{\Phi}\mathbf{\phi}(x_{N+1})
\end{aligned}
$$
$$
c = k(\mathbf{x}_{N+1},\mathbf{x}_{N+1})+\beta^{-1}
$$
これらを(6.66)に代入して
$$
\begin{aligned}
m(\mathbf{x}_{N+1})&=\mathbf{k}^{T}\mathbf{C}_{N}^{-1}\mathbf{t}\\
&=\left\{\alpha^{-1}\mathbf{\Phi}\mathbf{\phi}(\mathbf{x}_{N+1})\right\}^{T}(\alpha^{-1}\mathbf{\Phi\Phi^T}+\beta^{-1}\mathbf{I}_{N})^{-1}\mathbf{t}\\
&=\mathbf{\phi}(\mathbf{x}_{N+1})^{T}\alpha^{-1}\mathbf{\Phi}^{T}\alpha\beta(\beta\mathbf{\Phi\Phi^T}+\alpha\mathbf{I}_{N})^{-1}\mathbf{t}\\
&=\mathbf{\phi}(\mathbf{x}_{N+1})^{T}\beta(\beta\mathbf{\Phi^T\Phi}+\alpha\mathbf{I}_{N})^{-1}\mathbf{\Phi}^{T}\mathbf{t}\\
&=\mathbf{\phi}(\mathbf{x}_{N+1})^{T}(\beta\mathbf{S}_{N}\mathbf{\Phi}^{T}\mathbf{t})\\
&=\mathbf{\phi}(\mathbf{x}_{N+1})^{T}\mathbf{m}_{N}
\end{aligned}
$$
となり(3.58)の結果と一致する．3行目から4行目にかけての式変形において(C.6)の右辺から左辺を導く操作を行なった．次に共分散行列について(6.67)に代入して
$$
\begin{aligned}
\sigma^{2}(\mathbf{x}_{N+1})&=c-\mathbf{k}^{T}\mathbf{C}_{N}^{-1}\mathbf{k}\\
&=\left\{\mathbf\phi(x_{N+1})^{T}\mathbf\phi(x_{N+1})+\beta^{-1}\right\}-\left\{\alpha^{-1}\mathbf{\Phi}\mathbf{\phi}(x_{N+1})\right\}^{T}(\alpha^{-1}\mathbf{\Phi\Phi^T}+\beta^{-1}\mathbf{I}_{N})^{-1}\alpha^{-1}\mathbf{\Phi}\mathbf{\phi}(x_{N+1})\\
&=\beta^{-1}+\mathbf\phi(x_{N+1})^{T}\mathbf\phi(x_{N+1})-\mathbf{\phi}(\mathbf{x}_{N+1})^{T}\alpha^{-1}\mathbf{\Phi}^{T}\left\{\alpha\beta（\beta\mathbf{\Phi\Phi^T}+\alpha\mathbf{I}_{N})^{-1}\right\}\alpha^{-1}\mathbf{\Phi}\mathbf{\phi}(x_{N+1})\\
&=\beta^{-1}+\mathbf{\phi}(\mathbf{x}_{N+1})^{T}\left\{\mathbf{I}-\alpha^{-1}\beta\mathbf{\Phi}^{T}（\beta\mathbf{\Phi\Phi^T}+\alpha\mathbf{I}_{N})^{-1}\mathbf{\Phi}\right\}\mathbf{\phi}(\mathbf{x}_{N+1})\\
&=\beta^{-1}+\mathbf{\phi}(\mathbf{x}_{N+1})^{T}\left\{\mathbf{I}-\alpha^{-1}\beta（\beta\mathbf{\Phi^T\Phi}+\alpha\mathbf{I}_{N})^{-1}\mathbf{\Phi}^{T}\mathbf{\Phi}\right\}\mathbf{\phi}(\mathbf{x}_{N+1})\\
&=\beta^{-1}+\mathbf{\phi}(\mathbf{x}_{N+1})^{T}\left\{\alpha^{-1}(\mathbf{I+\alpha^{-1}\beta\mathbf{\Phi}^T}\mathbf{\Phi})^{-1}\right\}\mathbf{\phi}(\mathbf{x}_{N+1})\\
&=\beta^{-1}+\mathbf{\phi}(\mathbf{x}_{N+1})^{T}(\alpha\mathbf{I}+\beta\mathbf{\Phi}^T\mathbf{\Phi})^{-1}\mathbf{\phi}(\mathbf{x}_{N+1})\\
&=\beta^{-1}+\mathbf{\phi}(\mathbf{x}_{N+1})^{T}\mathbf{S}_{N}\mathbf{\phi}(\mathbf{x}_{N+1})\\
&=\sigma^{2}_{N}(\mathbf{x})
\end{aligned}
$$
となり(3.58)    の結果と一致している．なお4行目から5行目の式変形には(C.6)を右辺から左辺を導く形で用いた．また5行目から6行目の式変形には(C.7)を右辺から左辺を導く形で用いた．8行目から9行目は(3.59)を用いた．


## 演習 6.22
<div class="panel-primary">

$N$個の入力ベクトル$\mathbf{x}_{1}, \ldots, \mathbf{x}_{N}$を持つ訓練集合と，$L$個の入力ベクトル$\mathbf{x}_{N+1}, \ldots, \mathbf{x}_{N+L}$を持つテスト集合があるような回帰問題を考える．また, 関数$t(\mathbf{x})$上の事前分布としてガウス過程を考える．$t(\mathbf{x}_1),\ldots, t(\mathbf{x}_N)$が与えられたときの，$t(\mathbf{x}_{N+1}),\ldots, t(\mathbf{x}_{N+L})$の同時予測分布を導け．この分布の，ある$t_j$($N+1 \leq j \leq N+L$とする)についての周辺分布を考えたとき，それは通常のガウス過程による回帰の結果
$$
m\left(\mathbf{x}_{N+1}\right)=\mathbf{k}^{\mathrm{T}} \mathbf{C}_{N}^{-1} \mathbf{t} \tag{6.66}
$$
と
$$
\sigma^{2}\left(\mathbf{x}_{N+1}\right)=c-\mathbf{k}^{\mathrm{T}} \mathbf{C}_{N}^{-1} \mathbf{k} \tag{6.67}
$$
に一致することを示せ．

</div>

求めたいのは$t(\mathbf{x}_1),\ldots, t(\mathbf{x}_N)$が与えられたときの$t(\mathbf{x}_{N+1}),\ldots, t(\mathbf{x}_{N+L})$の同時予測分布なので、$p\left(\mathbf{t}_{N+1 \ldots N+L} \mid \mathbf{t}_{1 \ldots N}\right)$の形である。ここで

$$
\begin{aligned}
\mathbf{t}_{1 \ldots N}=\mathbf{t}_{N}=\left(t_{1}, \ldots, t_{N}\right)^{\mathrm T} \\
\mathbf{t}_{N+1 \ldots {N+L}}=\left(t_{N+1}, \cdots, t_{N+L}\right)^{\mathrm T}
\end{aligned}
$$

と記述する。

$(6.61)$からすべての$n=1, \ldots, N+L$について

$$
p(\mathbf{t}) = \mathcal{N}(\mathbf{t}\mid \mathbf{0}, \mathbf{C})
$$

であり、これを2.3.1節にしたがって$\mathbf{t}$を$\mathbf{t}_{1\ldots N}$と$\mathbf{t}_{N+1\ldots N+L}$に分割する。すなわち

$$
\mathbf{t}=\begin{pmatrix}
\mathbf{t}_{N+1 ,\cdots, N+L} \\
\mathbf{t}_{1, \ldots, N}
\end{pmatrix}
$$

の形にする。これに対応する共分散行列$\mathbf{C}$も

$$
\mathbf{C}=\begin{pmatrix}
\mathbf{C}_{a a} & \mathbf{C}_{a b} \\
\mathbf{C}_{b a} & \mathbf{C}_{b b}
\end{pmatrix}=\begin{pmatrix}
\mathbf{C}_{N+1, \ldots, N+L} & \mathbf{K}^{\mathrm T} \\
\mathbf{K} & \mathbf{C}_{1,\ldots,N}
\end{pmatrix}
$$

のように分割する。ここで$\mathbf{K}$は$N\times L$行列で

$$
\mathbf{K}=\begin{pmatrix}
k\left(\mathbf{x}_{1}, \mathbf{x}_{N+1}\right) & \cdots & k\left(\mathbf{x}_{1}, \mathbf{x}_{N+L}\right) \\
\vdots & \ddots & \vdots \\
k\left(\mathbf{x}_{N}, \mathbf{x}_{N+1}\right) & \cdots & k\left(\mathbf{x}_{N}, \mathbf{x}_{N+L}\right)
\end{pmatrix}
$$

で定義される。

これらを用いて$(2.81)$と$(2.82)$を用いて条件付き分布$p\left(\mathbf{t}_{N+1,\ldots,N+L} \mid \mathbf{t}_{N}\right)$の平均と共分散を求めると

$$
\begin{array}{l}
\boldsymbol{\mu}_{a b}=\mathbf{0}+\mathbf{K}^{\mathrm T} \mathbf{C}_{N}^{-1} \mathbf{t}_{N} \\
\mathbf{\Sigma}_{a \mid b}=\mathbf{C}_{N+1,\ldots,N+L}-\mathbf{K}^{\mathrm T} \mathbf{C}_{N}^{-1} \mathbf{K}
\end{array}
$$

となるので、以上から

$$
\begin{aligned}
p\left(\mathbf{t}_{N+1,\ldots,N+L} \mid \mathbf{t}_{N}\right) &= \mathcal{N}\left(\mathbf{t}_{N+1,\ldots,N+L} \mid \boldsymbol{\mu}_{a \mid b}, \mathbf{\Sigma}_{a \mid b}\right) \\
&= \mathcal{N}\left(\mathbf{t}_{N+1,\ldots,N+L} \mid \mathbf{K}^{\mathrm T} \mathbf{C}_{N}^{-1} \mathbf{t}_{N}, \mathbf{C}_{N+1,\ldots,N+L}-\mathbf{K}^{\mathrm T} \mathbf{C}_{N}^{-1} \mathbf{K}\right)
\end{aligned}
$$

となる。

また、この分布のある$t_j\ (N+1 \le j \le N+L)$についての周辺分布は$p(t_j \mid \mathbf{t}_N)$となり、これはP.19の議論と同様に

$$
\mathbf{C} = \begin{pmatrix}
c & \mathbf{k}^{\mathrm T} \\
\mathbf{k} & \mathbf{C}_N
\end{pmatrix}
$$

となる($c = k(\mathbf{x}_j, \mathbf{x}_{N+1}) + \beta^{-1}$)。

これは結局$(6.66)$と$(6.67)$と同様に

$$
p\left(t_{j} \mid \mathbf{t}_{N}\right)=\mathcal{N}\left(t_{j} \mid m\left(\mathbf{x}_{N+1}\right), \sigma^{2}\left(\mathbf{x}_{N+1}\right)\right)
$$
となり、ここで

$$
\begin{aligned}
m\left(\mathbf{x}_{N+1}\right) &= \mathbf{k}^{\mathrm{T}} \mathbf{C}_{N}^{-1} \mathbf{t}_N \\
\sigma^{2}\left(\mathbf{x}_{N+1}\right) &= c-\mathbf{k}^{\mathrm{T}} \mathbf{C}_{N}^{-1} \mathbf{k}
\end{aligned}
$$

と一致する。

## 演習 6.23
<div class="panel-primary">

ガウス過程による回帰モデルで目標変数$\mathbf{t}$の次元が$D$であるようなものを考える．入力ベクトル$\mathbf{x}_1, \ldots, \mathbf{x}_N$を持つ訓練集合と，対応する目標変数の値の集合$\mathbf{t}_1,\ldots,\mathbf{t}_N$が与えられたときの，テストデータ$\mathbf{x}_{N+1}$に対する$\mathbf{t}_{N+1}$の条件付き分布を導け．

</div>

6.4.2節で議論されていた条件付き分布を$\mathbf t_{N}=\left(\begin{array}{c}t_{1} \\
\vdots \\
t_{N}
\end{array}\right)$から$\mathbf T_{N}=\left(\begin{array}{cccc}t_{11} & \cdots & t_{1D} \\ \vdots & & \vdots \\ t_{N 1} & \cdots & t_{N D}\end{array}\right)$に拡張する。

(6.61)式と同様に、
$$
p\left(\mathbf T_{N}\right)=\mathcal{N}\left(\mathbf T_{N} \mathbf \mid \mathbf{O}, \mathbf{C}_{N}\right)
$$
とあらわせる。
ただし、$\mathbf{C}_{N}$は$C\left(x_{n}, x_{m}\right)=k\left(x_{n}, x_{m}\right)+\beta^{-1} \delta_{n m}$を要素として持つ共分散行列。
(2.81)式、(2.82)式を用いて、(6.66)式,(6.67)式と同様に、
$$
\mathbf m\left(x_{N+1}\right)=\left(\mathbf k^{\mathrm T} \mathbf C_{N}^{-1} \mathbf T_{N}\right)^{\mathrm T}\\
\sigma^{2}\left(x_{N+1}\right)=c-\mathbf{k}^{\mathrm T} \mathbf{C}_{N}^{-1} \mathbf k
$$
と算出されるような条件付き分布
$$
p\left(\mathbf t_{N+1} \mid \mathbf T_{N}\right)=\mathcal{N}\left(\mathbf t_{N+1} \mid \mathbf m\left(x_{N+1}\right), \sigma\left(x_{N+1}\right) \mathbf{I}_{N}\right)
$$
を導ける。
## 演習 6.24
<div class="panel-primary">

対角行列$\mathbf{W}$で，その要素が$0 \lt W_{ii} \lt 1$を満たすものは，正定値であることを示せ．また，2つの正定値行列の和は，やはり正定値になることを示せ．

</div>

正定値であることの定義から、任意のベクトル$\mathbf{x}$（$\mathbf{x} \neq \mathbf{0}$）に対して$\mathbf{x}^{\mathrm T}\mathbf{Wx} \gt 0$が成立していることを示せば良い。また$\mathbf{W}$は対角行列なので

$$
\begin{aligned}
\mathbf{x}^{\mathrm T}\mathbf{Wx} &= \sum_{i} \sum_{j} x_{i} W_{i j} x_{j} \\
&= \sum_{i}x_i^2 W_{ii} \hspace{1em} (\because W_{ij}=0\ \textrm{ if }\ i \neq j )
\end{aligned}
$$

である。ここで問題設定から$0 \lt W_{ii} \lt 1$であり、全ての$i$について$x_i^2 W_{ii} \geq 0$が成り立つ。また、$\mathbf{x} \neq \mathbf{0}$より少なくとも1つの$i$について$x_i^2 \neq 0$であり、$\mathbf{x}^{\mathrm T}\mathbf{Wx} \gt 0$が常に成立する。したがって、$\mathbf{W}$は正定値であることが示された。

また任意の正定値行列$\mathbf{A}_1$と$\mathbf{A}_2$と、この2つの和である行列$\mathbf{A} = \mathbf{A}_1+\mathbf{A}_2$について、これらは定義から$\mathbf{x}^{\mathrm T}\mathbf{A}_{1}\mathbf{x} \gt 0$, $\mathbf{x}^{\mathrm T}\mathbf{A}_{2}\mathbf{x} \gt 0$となっていることから、$\mathbf{x}^{\mathrm T}\mathbf{A}\mathbf{x}$について計算すると

$$
\mathbf{x}^{\mathrm T}\mathbf{A}\mathbf{x} = \mathbf{x}^{\mathrm T}(\mathbf{A}_{1} + \mathbf{A}_{2})\mathbf{x} = \mathbf{x}^{\mathrm T}\mathbf{A}_{1}\mathbf{x} + \mathbf{x}^{\mathrm T}\mathbf{A}_{2}\mathbf{x} \gt 0
$$

となる。以上から、任意の2つの正定置行列の和は正定値行列であることが示された。

## 演習 6.25
<div class="panel-primary">

ニュートンーラフソン法の公式
$$
\mathbf{w}^{(\text {new})}=\mathbf{w}^{(\text {old})}-\mathbf{H}^{-1} \nabla E(\mathbf{w}) \tag{4.92}
$$
を用いて，ガウス過程による分類モデルに対する，事後分布のモード$\mathbf{a}_N^{\star}$を求めるための逐次更新の公式
$$
\mathbf{a}_{N}^{\text {new }}=\mathbf{C}_{N}\left(\mathbf{I}+\mathbf{W}_{N} \mathbf{C}_{N}\right)^{-1}\left\{\mathbf{t}_{N}-\boldsymbol{\sigma}_{N}+\mathbf{W}_{N} \mathbf{a}_{N}\right\} \tag{6.83}
$$
を導け．

</div>

$(6.81)$と$(6.82)$式をニュートンーラフソン法に当てはめる。

$$
\nabla \Psi\left(\mathbf{a}_{N}\right)=\mathbf{t}_{N}-\boldsymbol{\sigma}_{N}-\mathbf{C}_{N}^{-1} \mathbf{a}_{N} \tag{6.81}
$$

ここで$\boldsymbol{\sigma}_N$は要素$\sigma(a_n)=\frac{1}{1+e^{-a_n}}$を持つベクトルである。これの$\mathbf{a}_n$での2階微分が$(6.82)$式で

$$
\nabla \nabla \Psi\left(\mathbf{a}_{N}\right)=-\mathbf{W}_{N}-\mathbf{C}_{N}^{-1} \tag{6.82}
$$

である。ここで$\mathbf{W}_N$は$\sigma(a_n)(1-\sigma(a_n))$を要素に持つ対角行列である。

ニュートンーラフソン法に当てはめると、

$$
\mathbf{a}^{\text {new}}=\mathbf{a}_{N}^{\text {old}}-\left(\nabla \nabla \Psi\left(\mathbf{a}_{N}^{\text {old}}\right)\right)^{-1}\left(\nabla \Psi\left(\mathbf{a}_{N}^{\text {old}}\right)\right)
$$

となるので、これを展開していく。

$$
\begin{aligned}
\mathbf{a}_{N}^{\text{new}} &=\mathbf{a}_{N}^{\text{old}}+\left(\mathbf{W}_{N}+\mathbf{C}_{N}^{-1}\right)^{-1}\left(\mathbf{t}_{N}-\boldsymbol{\sigma}_{N}-\mathbf{C}_{N}^{-1} \mathbf{a}_{N}^{\text{old}}\right) \\
&=\left(\mathbf{W}_{N}+\mathbf{C}_{N}^{-1}\right)^{-1}\left\{\left(\mathbf{W}_{N}+\mathbf{C}_{N}^{-1}\right) \mathbf{a}_{N}^{\text{old}}+\mathbf{t}_{N}-\boldsymbol{\sigma}_{N}-\mathbf{C}_{N}^{-1} \mathbf{a}_{N}^{\text{old}}\right\} \\
&=\left(\mathbf{W}_{N}+\mathbf{C}_{N}^{-1}\right)^{-1}\left(\mathbf{W}_{N} \mathbf{a}_{N}^{\text{old}}+\mathbf{t}_{N}-\boldsymbol{\sigma}_{N}\right) \\
&=\mathbf{C}_{N} \mathbf{C}_{N}^{-1}\left(\mathbf{W}_{N}+\mathbf{C}_{N}^{-1}\right)^{-1}\left(\mathbf{t}_{N}-\boldsymbol{\sigma}_{N}+\mathbf{W}_{N} \mathbf{a}_{N}^{\text{old}}\right) \\
&=\mathbf{C}_{N}\left(\mathbf{W}_{N} \mathbf{C}_{N}+\mathbf{I}\right)^{-1}\left(\mathbf{t}_{N}-\boldsymbol{\sigma}_{N}+\mathbf{W}_{N} \mathbf{a}_{N}^{\text{old}}\right) \quad \left(\because \mathbf{A}^{-1} \mathbf{B}^{-1}=\mathbf{BA}^{-1}\right) \\
&=\mathbf{C}_{N}\left(\mathbf{I}+\mathbf{W}_{N} \mathbf{C}_{N}\right)^{-1}\left(\mathbf{t}_{N}-\boldsymbol{\sigma}_{N}+\mathbf{W}_{N} \mathbf{a}_{N}^{\text{old}}\right)
\end{aligned}
$$

以上より、逐次更新式$(6.83)$が求まった。

## 演習 6.26
<div class="panel-primary">

$$
p(\mathbf{y})=\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \boldsymbol{\mu}+\mathbf{b}, \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}\right) \tag{2.115}
$$
の結果を用いて，ガウス過程による分類モデルに対する事後分布$p(a_{N+1}\mid \mathbf{t}_N)$の平均
$$
\mathbb{E}\left[a_{N+1} \mid \mathbf{t}_{N}\right] =\mathbf{k}^{\mathrm{T}}\left(\mathbf{t}_{N}-\boldsymbol{\sigma}_{N}\right) \tag{6.87}
$$
と分散
$$
\operatorname{var}\left[a_{N+1} \mid \mathbf{t}_{N}\right] =c-\mathbf{k}^{\mathrm{T}}\left(\mathbf{W}_{N}^{-1}+\mathbf{C}_{N}\right)^{-1} \mathbf{k} \tag{6.88}
$$
を導け．

</div>

(2.113)式$\Leftrightarrow$(6.86)式、(2.114)式$\Leftrightarrow$(6.78)式の対応関係は、
$$
\begin{aligned}
\mathbf{x} &= \mathbf{a}_N\\
\mathbf{y} &= \mathbf{a}_{N+1}\\
\mathbf{A} &= \mathbf{k}^{\mathrm T} \mathbf{C}_N^{-1}\\
\boldsymbol{\mu} &= \mathbf{a}_N^\star = \mathbf{C}_N (\mathbf{t}_N -\boldsymbol{\sigma}_N)\\
\mathbf{b} &= \mathbf{0} \\
\mathbf{L}^{-1} &= c - \mathbf{k}^{\mathrm T} \mathbf{C}_N^{-1}\mathbf{k} \\
\mathbf{\Lambda} ^{-1} &= \mathbf{H}^{-1} =
\left( \mathbf{W}_N + \mathbf{C}_N^{-1} \right) ^{-1}
\end{aligned}
$$

となる。（ニュートン-ラフソン法の下で、$\mathbf{a}_N$は$\mathbf{a}^\star_N$で置き換えられた。）

従って、(2.115)式における平均値$\mathbf{A} \boldsymbol{\mu} + \mathbf{b}$は、

$$
\begin{aligned}
&\mathbf{A} \boldsymbol{\mu} + \mathbf{b} \\
=&\ (\mathbf{k}^{\mathrm T} \mathbf{C}_N^{-1})
\{\mathbf{C}_N (\mathbf{t}_N -\boldsymbol{\sigma}_N)\}+ \mathbf{0} \\
=&\ \mathbf{k}^{\mathrm T} (\mathbf{t}_N -\boldsymbol{\sigma}_N)
\end{aligned}
$$

次に、(2.115)式における分散$\mathbf{L}^{-1}+\mathbf{A}\mathbf{\Lambda}^{-1}\mathbf{A}^{\mathrm{T}}$は、

$$
\begin{aligned}
&\mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1}\mathbf{A}^{\mathrm{T}} \\
=&\ (c - \mathbf{k}^{\mathrm T} \mathbf{C}_N^{-1}\mathbf{k})
+
\left( \mathbf{k}^{\mathrm T} \mathbf{C}_N ^{-1}\right)
\left( \mathbf{W}_N + \mathbf{C}_N^{-1} \right)^{-1}
\left( \mathbf{k}^{\mathrm T} \mathbf{C}_N ^{-1}\right)^{\mathrm T} \\
=&\ (c - \mathbf{k}^{\mathrm T} \mathbf{C}_N^{-1}\mathbf{k})
+
\mathbf{k}^{\mathrm T} \mathbf{C}_N ^{-1}
\left( \mathbf{W}_N + \mathbf{C}_N^{-1} \right)^{-1}
 \mathbf{C}_N^{-1}  \mathbf{k}
\end{aligned}
$$

となる。なお、転置の逆行列が逆行列の転置に一致すること、および$\mathbf{C}_N$が対称行列であることを用いた。ここで、

$$
\begin{aligned}
&\mathbf{C}_N ^{-1}
\left( \mathbf{W}_N + \mathbf{C}_N^{-1} \right)^{-1} \\
=&\left[ ( \mathbf{W}_N + \mathbf{C}_N^{-1} ) \mathbf{C}_N \right]^{-1} \\
=&\left( \mathbf{W}_N \mathbf{C}_N + \mathbf{I} \right)^{-1}
\end{aligned}
$$

なので、分散は、

$$
\begin{aligned}
& \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1}\mathbf{A}^{\mathrm{T}} \\
=&\ (c - \mathbf{k}^{\mathrm T} \mathbf{C}_N^{-1}\mathbf{k})
+
\mathbf{k}^{\mathrm T} \left( \mathbf{W}_N \mathbf{C}_N +\mathbf{I}\right)^{-1}
 \mathbf{C}_N^{-1}  \mathbf{k}\\
=&\ c + \mathbf{k}^{\mathrm T} \left\{
-\mathbf{I}+ \left( \mathbf{W}_N \mathbf{C}_N +\mathbf{I}\right)^{-1}
\right\}
\mathbf{C}_N^{-1}  \mathbf{k}\\
\end{aligned}
$$
となる。ここで、

$$
\begin{aligned}
& \left\{ -\mathbf{I}+\left( \mathbf{W}_N \mathbf{C}_N +\mathbf{I}\right)^{-1} \right\} \mathbf{C}_N^{-1}\\
=& \left\{\left[ -( \mathbf{W}_N \mathbf{C}_N +\mathbf{I}) +\mathbf{I}\right]
\left( \mathbf{W}_N \mathbf{C}_N +\mathbf{I}\right)^{-1} \right\} \mathbf{C}_N^{-1}\\
=& \left\{-\mathbf{W}_N \mathbf{C}_N
\left( \mathbf{W}_N \mathbf{C}_N +\mathbf{I}\right)^{-1} \right\} \mathbf{C}_N^{-1}\\
=& -\mathbf{W}_N \mathbf{C}_N
\left\{\left( \mathbf{W}_N \mathbf{C}_N +\mathbf{I}\right)^{-1}  \mathbf{C}_N^{-1}\right\}\\
=& -\mathbf{W}_N \mathbf{C}_N
\left\{ \mathbf{C}_N \left( \mathbf{W}_N \mathbf{C}_N +\mathbf{I}\right)\right\} ^{-1}\\
=& - \{(\mathbf{W}_N \mathbf{C}_N )^{-1}\}^{-1}
\left\{ \mathbf{C}_N \left( \mathbf{W}_N \mathbf{C}_N +\mathbf{I}\right)\right\} ^{-1}\\
=& -\left\{ \mathbf{C}_N \left( \mathbf{W}_N \mathbf{C}_N +\mathbf{I}\right) (\mathbf{W}_N \mathbf{C}_N )^{-1}
\right\} ^{-1}\\
=& - \left\{ \mathbf{C}_N (\mathbf{I}+  (\mathbf{W}_N \mathbf{C}_N )^{-1} )
\right\} ^{-1}\\
=&- \left( \mathbf{C}_N + \mathbf{C}_N \mathbf{C}_N ^{-1} \mathbf{W}_N ^{-1}
\right) ^{-1}\\
=&- \left( \mathbf{C}_N + \mathbf{W}_N ^{-1}
\right) ^{-1}\\
\end{aligned}
$$

なので、分散は、

$$
\begin{aligned}
&\mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1}\mathbf{A}^{\mathrm{T}} \\
=\ &c - \mathbf{k}^{\mathrm T} \left( \mathbf{C}_N + \mathbf{W}_N ^{-1}
\right) ^{-1}
\mathbf{k}\\
\end{aligned}
$$

## 演習 6.27
<div class="panel-primary">

(難問)ガウス過程による分類モデルのラプラス近似による対数尤度関数
$$
\ln p\left(\mathbf{t}_{N} \mid \theta\right)=\Psi\left(\mathbf{a}_{N}^{\star}\right)-\frac{1}{2} \ln \left|\mathbf{W}_{N}+\mathbf{C}_{N}^{-1}\right|+\frac{N}{2} \ln (2 \pi) \tag{6.90}
$$
を導け．また
$$
\frac{\partial \ln p\left(\mathbf{t}_{N} \mid \theta\right)}{\partial \theta_{j}}= \frac{1}{2} \mathbf{a}_{N}^{\star \mathrm{T}} \mathbf{C}_{N}^{-1} \frac{\partial \mathbf{C}_{N}}{\partial \theta_{j}} \mathbf{C}_{N}^{-1} \mathbf{a}_{N}^{\star} -\frac{1}{2} \operatorname{Tr}\left[\left(\mathbf{I}+\mathbf{C}_{N} \mathbf{W}_{N}\right)^{-1} \mathbf{W}_{N} \frac{\partial \mathbf{C}_{N}}{\partial \theta_{j}}\right] \tag{6.91}
$$
と
$$
\begin{aligned}
  -&\frac{1}{2} \sum_{n=1}^{N} \frac{\partial \ln \left|\mathbf{W}_{N}+\mathbf{C}_{N}^{-1}\right|}{\partial a_{n}^{\star}} \frac{\partial a_{n}^{\star}}{\partial \theta_{j}} \\
  =-&\frac{1}{2} \sum_{n=1}^{N}\left[\left(\mathbf{I}+\mathbf{C}_{N} \mathbf{W}_{N}\right)^{-1} \mathbf{C}_{N}\right]_{n n} \sigma_{n}^{\star}\left(1-\sigma_{n}^{\star}\right)\left(1-2 \sigma_{n}^{\star}\right) \frac{\partial a_{n}^{\star}}{\partial \theta_{j}}
\end{aligned} \tag{6.92}
$$
および
$$
\frac{\partial a_{n}^{\star}}{\partial \theta_{j}}=\left(\mathbf{I}+\mathbf{W}_{N} \mathbf{C}_{N}\right)^{-1} \frac{\partial \mathbf{C}_{N}}{\partial \theta_{j}}\left(\mathbf{t}_{N}-\boldsymbol{\sigma}_{N}\right) \tag{6.94}
$$
を対数尤度の勾配を用いて表せ．

</div>

(4.135)式
$$
Z=\int f(\mathbf{z}) \mathrm{d} \mathbf{z}\\
=f(z_{0})\int exp(-\frac{1}{2} (\mathbf{z}- \mathbf{z_{0}}) A (\mathbf{z}- \mathbf{z_{0}})) \mathrm{d} \mathbf{z}\\
=f(z_{0})\frac{(2\pi)^{\frac{M}{2}}}{|A|^\frac{1}{2}} \\
Z=p(\mathbf{t_{N}}\mid\theta), f(\mathbf{z})=p(\mathbf{t_{N}}\mid\mathbf{a_{N}})p(\mathbf{a_{N}}\mid\theta), \mathbf{z}=\mathbf{a_{N}}と置くと、\\
p(\mathbf{t_{N}}\mid\theta)=p(\mathbf{t_{N}}\mid\mathbf{a^*_{N}})p(\mathbf{a^*_{N}}\mid\theta)\frac{(2\pi)^{\frac{N}{2}}}{|H|^\frac{1}{2}}\\
=exp(\Psi\left(\mathbf{a}_{N}^{\star}\right))\frac{(2\pi)^{\frac{N}{2}}}{|H|^\frac{1}{2}}\\
両辺の対数を取って、\\
\ln p\left(\mathbf{t}_{N} \mid \theta\right)=\Psi\left(\mathbf{a}_{N}^{\star}\right)-\frac{1}{2} \ln \left|\mathbf{W}_{N}+\mathbf{C}_{N}^{-1}\right|+\frac{N}{2} \ln (2 \pi)
$$

※


