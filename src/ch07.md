# PRML第7章演習問題解答

<head>
<style>
  div.panel-primary {
	border: 1px solid #000;
    margin: 10px 5px;
    padding: 16px 10px 0px;
  }
</style>
</head>

## 演習 7.1
<div class="panel-primary">

今，入力ベクトルのデータ集合$\{\mathbf{x}_n\}$とそれに対応する目標値$t_n \in \{-1, 1\}$が与えられ，かつ，それぞれのクラス分布をカーネル関数$k(\mathbf{x}, \mathbf{x}^{\prime})$を用いてParzen推定法(2.5.1節参照)でモデル化したとするそれぞれのクラスの事前確率が等しいとしたとき，誤分類率が最も小さくなる分類規則を求めよ．またカーネルが$k(\mathbf{x}, \mathbf{x}^{\prime}) = \mathbf{x}^{\mathrm T}\mathbf{x}^{\prime}$どという形で表される場合，分類規則は単に重心との距離が近い方のクラスを新しい入力ベクトルに割り当てる， という形になることを示せ．最後にカーネルが$k(\mathbf{x}, \mathbf{x}^{\prime}) = \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\boldsymbol{\phi}(\mathbf{x}^{\prime})$という形の場合は，分類規則は特徴空間$\boldsymbol{\phi}(\mathbf{x})$において最も重心が近いクラスを割り当てることに等しいことを示せ．

</div>

式 (2.249) に従い、$p(\mathbf{x}|t)$を
\begin{align}
p(\mathbf{x}|t) \propto
\begin{cases}
\frac{1}{N_{+1}} \sum_{t = +1} k(\mathbf{x}, \mathbf{x}_n) \ \ t = +1\\
\frac{1}{N_{-1}} \sum_{t = -1} k(\mathbf{x}, \mathbf{x}_n) \ \ t = -1
\end{cases}
\end{align}
と書ける。各クラスの事前確率が等しいと仮定するので、事後確率$p(t|\mathbf{x})$は
\begin{align}
p(t|\mathbf{x}) \propto
\begin{cases}
\frac{1}{N_{+1}} \sum_{t = +1} k(\mathbf{x}, \mathbf{x}_n) \ \ t = +1 \\
\frac{1}{N_{-1}} \sum_{t = -1} k(\mathbf{x}, \mathbf{x}_n) \ \ t = -1
\end{cases}
\end{align}
となる。新しい$\mathbf{x}^{\star}$を分類するには、$p(t|\mathbf{x}^{\star})$を最大化する$t^{\star}$を探せばいいので、
\begin{align}
t^{\star} =
\begin{cases}
+1\ \ \mathrm{if}\ \ \frac{1}{N_{+1}} \sum_{t = +1} k(\mathbf{x}^{\star}, \mathbf{x}_n) \geq \frac{1}{N_{-1}} \sum_{t = -1} k(\mathbf{x}^{\star}, \mathbf{x}_n) \\
-1\ \ \mathrm{if}\ \ \frac{1}{N_{+1}} \sum_{t = +1} k(\mathbf{x}^{\star}, \mathbf{x}_n) \leq \frac{1}{N_{-1}} \sum_{t = -1} k(\mathbf{x}^{\star}, \mathbf{x}_n)
\end{cases}
\end{align}
ここで$k(\mathbf{x}, \mathbf{x}') = \mathbf{x}^{T}\mathbf{x}'$のとき、
\begin{align}
\frac{1}{N_{+1}} \sum_{t = +1} k(\mathbf{x}, \mathbf{x}_n) & = \frac{1}{N_{+1}} \sum_{t = +1} \mathbf{x}^{T}\mathbf{x}_n \\
&= \frac{1}{N_{+1}} \sum_{i = 1}^{N_{+1}} x_1 (x_{n1} + x_{n2} + \cdots + x_{nd}) + \cdots + x_d (x_{n1} + x_{n2} + \cdots + x_{nd}) \\
&= x_1 (\bar{x}_{+1,1} + \bar{x}_{+1,2} + \cdots + \bar{x}_{+1,d}) + \cdots + x_d (\bar{x}_{+1,1} + \bar{x}_{+1,2} + \cdots + \bar{x}_{+1,d}) \\
&= \mathbf{x}^{T}\mathbf{\bar{x}}_{+1}
\end{align}
同様に、
\begin{align}
\frac{1}{N_{+1}} \sum_{t = -1} k(\mathbf{x}, \mathbf{x}_n) = \mathbf{x}^{T}\mathbf{\bar{x}}_{-1}
\end{align}
よって、上記の分類規則は
\begin{align}
t^{\star} =
\begin{cases}
+1\ \ \mathrm{if}\ \ \mathbf{x}^{T}\mathbf{\bar{x}}_{+1} \geq \mathbf{x}^{T}\mathbf{\bar{x}}_{-1} \\
-1\ \ \mathrm{if}\ \ \mathbf{x}^{T}\mathbf{\bar{x}}_{+1} \leq \mathbf{x}^{T}\mathbf{\bar{x}}_{-1}
\end{cases}
\end{align}
となる。
$k(\mathbf{x}, \mathbf{x}') = \phi(\mathbf{x})^{T}\phi(\mathbf{x}')$としたときも、同様の計算により、
\begin{align}
t^{\star} =
\begin{cases}
+1\ \ \mathrm{if}\ \ \phi(\mathbf{x})^{T}\bar{\phi}(\mathbf{x})_{+1} \geq \phi(\mathbf{x})^{T}\bar{\phi}(\mathbf{x})_{-1} \\
-1\ \ \mathrm{if}\ \ \phi(\mathbf{x})^{T}\bar{\phi}(\mathbf{x})_{+1} \leq \phi(\mathbf{x})^{T}\bar{\phi}(\mathbf{x})_{-1}
\end{cases}
\end{align}
ここで$\bar{\phi}(\mathbf{x})_{+1} = \frac{1}{N_{+1}} \sum_{n = 1}^{N_{+1}} \phi(\mathbf{x}_n)$、$\bar{\phi}(\mathbf{x})_{-1} = \frac{1}{N_{+1}} \sum_{n = 1}^{N_{-1}} \phi(\mathbf{x}_n)$




## 演習 7.2
<div class="panel-primary">

制約式
$$
t_{n}\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)+b\right) \geqslant 1, \quad n=1, \ldots, N \tag{7.5}
$$
において，右辺の$1$を任意の正数$\gamma$で置き換えても，マージン最大の超平面は変化しないことを示せ．

</div>

$$
t_{n}\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)+b\right) \geqslant \gamma, \quad n=1, \cdots, N \tag{7.5.a}
$$
と置き換えると、
$\mathbf{w}^{\prime}=\frac{\mathbf{w}}{\gamma}, \quad b^{\prime}=\frac{b}{\gamma} .$として
$$
t_{n}\left(\mathbf{w}^{\prime\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)+b^{\prime}\right) \geqslant 1, \quad n=1, \ldots, N \tag{7.5.b}
$$
と書け、マージンは
$$
\min _{n} \frac{\left[t_{n}\left(\mathbf{w}^{\prime\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)+b^{\prime}\right)\right]}{\|\mathbf{w}^{\prime}\|}
=\min _{n} \frac{\left[t_{n}\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)+b\right)\right]}{\|\mathbf{w}\|}
$$
と変化しない。

## 演習 7.3
<div class="panel-primary">

データ空間の次元数によらず各クラスに一つずつデータが存在すれば， 2つのデータ点だけから成るデータ集合でマージン最大の超平面を決定できることを示せ．

</div>

※

各クラスに一つずつデータ点が与えられたとき，その2点を $\mathbf{x}_1\in\mathit{C}_+ (t_1 = +1)$， $\mathbf{x}_2\in\mathit{C}_- (t_2 = -1)$ とすると以下の制約式のもとで式(7.6)を解くことでマージンを最大化する超平面が得られる

$$
arg\,min_{\mathbf{w}, b}\,\frac{1}{2}||\mathbf{w}||^2\tag{7.6}
$$

$$
\mathbf{w}^T\mathbf{x}_1+b= +1\tag{1}
$$

$$
\mathbf{w}^T\mathbf{x}_2+b= -1\tag{2}
$$

(7.6)式をラグランジュ乗数$\lambda$と$\eta$を用いて解くと

$$
arg\,min_{\mathbf{w}, b}\,\left\{\frac{1}{2}||\mathbf{w}||^2+\lambda(\mathbf{w}^T\mathbf{x}_1+b-1)+\eta(\mathbf{w}^T\mathbf{x}_2+b+1)\right\}\tag{3}
$$

(3)式の$\mathbf{w}$とbについて微分した式を0とおくと

$$
0=\mathbf{w}+\lambda\mathbf{x}_1+\eta\mathbf{x}_2\tag{4}
$$
$$
0=\lambda+\eta\tag{5}
$$

が得られ，(4)，(5)式から

$$
\mathbf{w}=\lambda(\mathbf{x}_2-\mathbf{x}_1)\tag{6}
$$

また(1)，(2)式からbは

$$
2b=-\mathbf{w}^T(\mathbf{x}_1+\mathbf{x}_2)
$$

であり，これと(6)式と合わせて

$$
\begin{aligned}
b=&-\frac{\lambda}{2}(\mathbf{x}_1-\mathbf{x}_2)^T(\mathbf{x}_1+\mathbf{x}_2)\\=&-\frac{\lambda}{2}(\mathbf{x}_1^T\mathbf{x}_1-\mathbf{x}_2^T\mathbf{x}_2)
\end{aligned}
$$

のように求まり，マージンを最大化する超平面が定まる．


## 演習 7.4
<div class="panel-primary">

マージン最大の超平面のマージン$\rho$は，以下の式を満たすことを示せ．

$$
\frac{1}{\rho^2}= \sum_{n=1}^N a_n \tag{7.123}
$$

ただし$\{a_n\}$は

$$
\widetilde{L}(\mathbf{a})=\sum_{n=1}^{N} a_{n}-\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} a_{n} a_{m} t_{n} t_{m} k\left(\mathbf{x}_{n}, \mathbf{x}_{m}\right) \tag{7.10}
$$

を制約条件

$$
a_{n} \geqslant 0, \hspace{2em} n=1, \ldots, N \tag{7.11}
$$

$$
\sum_{n=1}^{N} a_{n} t_{n}=0 \tag{7.12}
$$

の下で解いて得られる解とする．

</div>

今、定義と、(7, 2)より、
\begin{align}
\rho = \frac{t_n y(x_n)}{||\mathbf{w}||} = \frac{t_n (\mathbf{w}^T \phi(\mathbf{x}_n)+b)}{||\mathbf{w}||}
\end{align}
である。今、分子と分母を定数倍すると、ある$\mathbf{w}^{\star}$において、
\begin{align}
\rho = \frac{1}{||\mathbf{w}^{\star}||}
\end{align}
が成り立つ。よって、$\frac{1}{\rho^2} = ||\mathbf{w}^{\star}||^2$であり、$||\mathbf{w}^{\star}||^2 = \sum_{n=1}^N a_n$を証明すれば題意は満たされる。今、(7, 10)より
\begin{align}
\widetilde{L}(\mathbf{a}) &= \sum_n a_n - \frac{1}{2}\sum_n \sum_m a_n a_m t_n t_m k(\mathbf{x}_n, \mathbf{x}_m)　\\
 &= \sum_n a_n - \frac{1}{2}\sum_n a_n t_n \phi(\mathbf{x}_n) \sum_m  a_m  t_m  \phi(\mathbf{x}_m)　\\
&= \sum_n a_n - \frac{1}{2}||\mathbf{w^{\star}}||^2 &(\because　(7, 8))
\end{align}
また、ラグランジュ乗数法の定義より。
\begin{align}
\widetilde{L}(a) = L(\mathbf{w^{\star}}, b, \mathbf{a} ) = \frac{1}{2}||\mathbf{w}^{\star}||^2 &(\because (7.7))
\end{align}
よって、$\sum_n a_n - \frac{1}{2}||\mathbf{w^{\star}}||^2 = \frac{1}{2}||\mathbf{w}^{\star}||^2$であり、整理すると、題意が導かれる。

## 演習 7.5
<div class="panel-primary">

前問における$\rho$および$\{a_n\}$は，次の式を満たすことを示せ．

$$
\frac{1}{\rho^{2}}=2 \widetilde{L}(\mathbf{a}) \tag{7.124}
$$

ここで，$\widetilde{L}(\mathbf{a})$は

$$
\widetilde{L}(\mathbf{a})=\sum_{n=1}^{N} a_{n}-\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N} a_{n} a_{m} t_{n} t_{m} k\left(\mathbf{x}_{n}, \mathbf{x}_{m}\right) \tag{7.10}
$$

で定義される関数である同様に以下の関係が成り立つことを示せ．

$$
\frac{1}{\rho^{2}}= \|\mathbf{w}\|^2 \tag{7.125}
$$

</div>

本問については、7.4ですでに示されている.

\begin{align}
(
\because
\widetilde{L}(a) = \frac{1}{2}||\mathbf{w}'||^2 ,
\rho = \frac{1}{||\mathbf{w}'||}
)
\end{align}

※

## 演習 7.6
<div class="panel-primary">

出力値が$t\in\{-1, 1\}$であるロジスティック回帰モデルについて考える．

$$
y(\mathbf{x})=\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x})+b \tag{7.1}
$$

という形の$y(\mathbf{x})$を用いて，$p(t=1|y) = \sigma(y)$とすると，対数尤度(の符号を反転したもの)に2乗ノルムの正則化項を加えたものは

$$
\sum_{n=1}^{N} E_{\mathrm{LR}}\left(y_{n} t_{n}\right)+\lambda\|\mathbf{w}\|^{2} \tag{7.47}
$$

という形を取ることを示せ．ただし

$$
E_{\mathrm{LR}}(y t)=\ln (1+\exp (-y t)) \tag{7.48}
$$

である．

</div>

ロジスティック回帰モデルはinputデータに対して各クラスの事後確率を求め
最も高い確率のクラスに分類する手法。

各クラスの事後確率はロジスティックシグモイド関数として以下のように書ける。

入力データがクラス1である確率：$p(t=1 \mid y)=\sigma(y)$
入力データがクラス2である確率：$p(t=-1 \mid y)=1-\sigma(y)=\sigma(-y)$
$※\sigma(y)=\frac{1}{1+e^{-y}}　y(\mathbf{x})=\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x})+b$

学習データとして、各学習データがi.i.dの$\mathcal{D}=\left\{\left(t_{1}, \mathbf{x}_{n}\right), \ldots,\left(t_{N}, \mathbf{x}_{N}\right)\right\}$が与えられると
最適パラメータ($\mathbf{w},b$)は以下の尤度を最大化することで得られる。

$$p(\mathcal{D})=\prod_{t_{n}=1} \sigma\left(y_{n}\right) \prod_{t_{n^{\prime}}=-1} \sigma\left(-y_{n^{\prime}}\right)=\prod_{n=1}^{N} \sigma\left(t_{n} y_{n}\right)　※y_{n}=y\left(\mathbf{x}_{n}\right) , t_{n} \in\{-1,1\}$$

これは、各学習データが正分類される確率を全データに対して掛け合わせたものを表しており
正しく正分類されているほど、尤度は大きくなる。負の対数尤度を取ると以下となる。

$$\begin{aligned}-\ln p(\mathcal{D}) &=-\ln \prod_{n=1}^{N} \sigma\left(t_{n} y_{n}\right) \\ &=\sum_{n=1}^{N} \ln \sigma\left(t_{n} y_{n}\right)^{\mathrm{-1}}  \\ &=\sum_{n=1}^{N} \ln \left(1+\exp \left(-t_{n} y_{n}\right)\right) \end{aligned}$$

これに、$\lambda\|\mathbf{w}\|^{2}$を加えると(7.47)という形を取る。


## 演習 7.7
<div class="panel-primary">

SVM回帰モデルのラグランジュ関数

$$
\begin{aligned}
L=&\ C \sum_{n=1}^{N}\left(\xi_{n}+\widehat{\xi}_{n}\right)+\frac{1}{2}\|\mathbf{w}\|^{2}-\sum_{n=1}^{N}\left(\mu_{n} \xi_{n}+\widehat{\mu}_{n} \widehat{\xi}_{n}\right) \\
&-\sum_{n=1}^{N} a_{n}\left(\epsilon+\xi_{n}+y_{n}-t_{n}\right)-\sum_{n=1}^{N} \widehat{a}_{n}\left(\epsilon+\widehat{\xi}_{n}-y_{n}+t_{n}\right) .
\end{aligned} \tag{7.56}
$$

について考える．$(7.56)$の $\mathbf{w}, b, \xi_{n}, \widehat{\xi}_{n}$に対する偏微分をそれぞれ零とおき，その結果を代入することで双対ラグランジュ関数

$$
\begin{aligned}
\widetilde{L}(\mathbf{a}, \widehat{\mathbf{a}})=&-\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N}\left(a_{n}-\widehat{a}_{n}\right)\left(a_{m}-\widehat{a}_{m}\right) k\left(\mathbf{x}_{n}, \mathbf{x}_{m}\right) \\ &-\epsilon \sum_{n=1}^{N}\left(a_{n}+\widehat{a}_{n}\right)+\sum_{n=1}^{N}\left(a_{n}-\widehat{a}_{n}\right) t_{n}
\end{aligned} \tag{7.61}
$$

が得られることを示せ．

</div>

(7.56)に$y(\mathbf{x})=\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x})+b$を代入して、スラック変数を分解すると以下の式が得られる。

$$\begin{aligned} L=& \sum_{n=1}^{N} C \xi_{n}+\sum_{n=1}^{N} C\widehat{\xi}_{n}+\frac{1}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w}-\sum_{n=1}^{N}\left(\mu_{n} \xi_{n}+\widehat{\mu}_{n} \widehat{\xi}_{n}\right) \\ &-\sum_{n=1}^{N} a_{n}\left(\epsilon+\xi_{n}+\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)+b-t_{n}\right) \\ &-\sum_{n=1}^{N} \widehat{a}_{n}\left(\epsilon+\widehat{\xi}_{n}-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)-b+t_{n}\right) \qquad (*)\end{aligned}$$

ラグランジュ関数(7.56)の$\mathbf{w}, b, \xi_{n}, \widehat{\xi}_{n}$に対する偏微分をそれぞれ零とおくことで
以下の式が得られる。

$$
\begin{aligned}
&\frac{\partial L}{\partial \mathrm{w}}=0 \Rightarrow \mathrm{w}=\sum_{n=1}^{N}\left(a_{n}-\widehat{a}_{n}\right) \phi\left(\mathrm{x}_{n}\right)\qquad (7.57)\\
&\frac{\partial L}{\partial b}=0 \Rightarrow \sum_{n=1}^{N}\left(a_{n}-\widehat{a}_{n}\right)=0\qquad (7.58)\\
&\frac{\partial L}{\partial \xi_{n}}=0 \Rightarrow a_{n}+\mu_{n}=C\qquad (7.59)\\
&\frac{\partial L}{\partial \widehat{\xi}_{n}}=0 \Rightarrow \widehat{a}_{n}+\widehat{\mu}_{n}=C\qquad (7.60)\
\end{aligned}
$$

(*)式に、(7.57) , (7.59) , (7.60)を代入すると以下の式となる。

$$
\begin{aligned}
L=& \sum_{n=1}^{N}\left(a_{n}+\mu_{n}\right) \xi_{n}+\sum_{n=1}^{N}\left(\widehat{a}_{n}+\widehat{\mu}_{n}\right) \widehat{\xi}_{n} \\
&+\frac{1}{2} \sum_{n=1}^{N} \sum_{m=1}^{N}\left(a_{n}-\widehat{a}_{n}\right)\left(a_{m}-\widehat{a}_{m}\right) \phi\left(\mathbf{x}_{n}\right)^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{m}\right)-\sum_{n=1}^{N}\left(\mu_{n} \xi_{n}+\widehat{\mu}_{n} \widehat{\xi}_{n}\right) \\
&-\sum_{n=1}^{N}\left(a_{n} \xi_{n}+\widehat{a}_{n} \widehat{\xi}_{n}\right)-\epsilon \sum_{n=1}^{N}\left(a_{n}+\widehat{a}_{n}\right)+\sum_{n=1}^{N}\left(a_{n}-\widehat{a}_{n}\right) t_{n} \\
&-\sum_{n=1}^{N} \sum_{m=1}^{N}\left(a_{n}-\widehat{a}_{n}\right)\left(a_{m}-\widehat{a}_{m}\right) \phi\left(\mathbf{x}_{n}\right)^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{m}\right)-b \sum_{n=1}^{N}\left(a_{n}-\widehat{a}_{n}\right) .
\end{aligned}
$$

この式の第1,2項は、第4,5項と丁度打ち消しあう。また、式(7.58)により最後の項は0になるので
まとめると(7.61)が得られる。


## 演習 7.8
<div class="panel-primary">

7.1.4節で議論した SVM回帰モデルについて，$\xi_{n} \gt 0$が成り立つ訓練データ点については$a_n = C$，同様に$\widehat{\xi}_{n} \gt 0$が成り立つ訓練データ点については$\widehat{a}_{n} = C$が成立することを示せ．

</div>

※(7.67),(7.68)から明らか。

## 演習 7.9
<div class="panel-primary">

RVM回帰モデルについて，重みに対する事後確率分布の平均および共分散が

\begin{align}
\mathbf{m}&=\beta \mathbf{\Sigma} \Phi^{\mathrm{T}} \mathbf{t} \tag{7.82} \\
\mathbf{\Sigma}&=\left(\mathbf{A}+\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \tag{7.83}
\end{align}

で与えられることを示せ．

</div>

(7.79)式の直後の段落に記載された仮定（各$w_i$の事前確率分布の平均が0、分散が$\alpha_i$）により、(3.49)〜(3.51)式と(7.81)〜(7.83)式との対応関係は、
\begin{eqnarray}
\mathbf{m}_0 &=& \mathbf{0} \\
\mathbf{S}_0 &=& \rm{diag}(\alpha_i^{-1}) \\
\mathbf{m}_N &=& \mathbf{m} \rm{\ \ :definition} \\
\mathbf{S}_N &=& \mathbf{\Sigma} \rm{\ \ :definition}
\end{eqnarray}

である。従って、
\begin{eqnarray}
\mathbf{m}
&=& S_N \left( S_0^{-1}\mathbf{m}_0 + \beta \mathbf{\Phi}^\mathrm{T} \mathbf{t} \right) \\
&=& \beta \Sigma \Phi ^\mathrm{T} \mathbf{t}
\end{eqnarray}

\begin{eqnarray}
\Sigma ^{-1} &=& \left( \rm{diag}(\alpha_i^{-1} )\right) ^{-1} + \beta \Phi^\mathrm{T} \Phi \\
&=& \rm{diag}(\alpha_i) + \beta \Phi^\mathrm{T} \Phi \\
\Sigma &=& \left( A + \beta \Phi^\mathrm{T} \Phi \right) ^{-1}
\end{eqnarray}
となる。ただし、$A=$diag$(\alpha _i)$と定義した。

## 演習 7.10
<div class="panel-primary">

RVM回帰モデルについて周辺化尤度関数の式

$$
\begin{aligned} \ln p(\mathbf{t} \mid \mathbf{X}, \boldsymbol{\alpha}, \beta) &=\ln \mathcal{N}(\mathbf{t} \mid \mathbf{0}, \mathbf{C}) \\ &=-\frac{1}{2}\left\{N \ln (2 \pi)+\ln |\mathbf{C}|+\mathbf{t}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t}\right\} \end{aligned} \tag{7.85}
$$

を，

$$
p(\boldsymbol{t} \mid \mathbf{X}, \boldsymbol{\alpha}, \beta)=\int p(\mathbf{t} \mid \mathbf{X}, \mathbf{w}, \beta) p(\mathbf{w} \mid \boldsymbol{\alpha}) \mathrm{d} \mathbf{w} \tag{7.84}
$$

の$\mathbf{w}$に対する積分を実行することで導け．(指数に現れる2次式を平方完成するとよい．)

</div>

$$
\begin{aligned}
p(\mathbf{t} \mid \mathbf{X}, \boldsymbol{\alpha}, \beta) &=\int p(\mathbf{t} \mid \mathbf{X}, \mathbf{\mathbf{w}}, \beta) p(\mathbf{w} \mid \alpha) d \mathbf{w} \\
&=\int \prod_{n=1}^{N} \mathcal{N}\left(t_{n} \mid \mathbf{w}^{\mathrm T} \boldsymbol{\phi}(\mathbf{x}), \beta^{-1}\right) \prod_{i=1}^{M} \mathcal{N}\left(w_{i} \mid 0, \alpha_{i}^{-1}\right) d \mathbf{w} \\
&=\int\left(\frac{\beta}{2 \pi}\right)^{\frac{N}{2}} \prod_{n=1}^{N} \exp \left\{-\frac{\left(t_{n}-\mathbf{w}^{\mathrm T} \boldsymbol{\phi}(\mathbf{x})\right)^{2}}{2 \beta^{-1}}\right\}\left(\frac{1}{2 \pi}\right)^{\frac{M}{2}} \prod_{i=1}^{M} \alpha_{i}^{\frac{1}{2}} \exp \left\{-\frac{w_{i}^{2}}{2 \alpha_{i}^{-1}}\right\} d \mathbf{w} \\
&=\left(\frac{\beta}{2 \pi}\right)^{\frac{N}{2}}\left(\frac{1}{2 \pi}\right)^{\frac{M}{2}} \prod_{i=1}^{M} \alpha_{i}^{\frac{1}{2}} \int \exp \left\{-\frac{\beta}{2}\|\mathbf{t}-\mathbf{\Phi} \mathbf{w}\|^{2}-\frac{1}{2} \mathbf{w}^{\mathrm T} \mathbf{A} \mathbf{w}\right\} d \mathbf{w}
\end{aligned}
$$

ここで$\mathbf{A} = \operatorname{diag}(\alpha_i)$である。指数部分を整理すると

$$
\begin{aligned}
-\frac{\beta}{2}\|\mathbf{t}-\mathbf{\Phi} \mathbf{w}\|^{2}-\frac{1}{2} \mathbf{w}^{\mathrm T} \mathbf{A} \mathbf{w} &=-\frac{1}{2}\left\{\beta\left(\mathbf{t}^{\mathrm T} \mathbf{t}-2 \mathbf{t}^{\mathrm T} \mathbf{\Phi} \mathbf{w}+\mathbf{w}^{\mathrm T} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi} \mathbf{w}\right)+\mathbf{w}^{\mathrm T} \mathbf{A} \mathbf{w}\right\} \\
&=-\frac{1}{2}\left\{\mathbf{w}^{\mathrm T}\left(\mathbf{A}+\beta \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right) \mathbf{w}-2 \beta \mathbf{t}^{\mathrm T} \mathbf{\Phi} \mathbf{w}+\beta \mathbf{t}^{\mathrm T} \mathbf{t}\right\} \\
&=-\frac{1}{2}\left\{(\mathbf{w}-\mathbf{m})^{\mathrm T} \mathbf{\Sigma}^{-1}(\mathbf{w}-\mathbf{m})+\beta \mathbf{t}^{\mathrm T} \mathbf{t}-\mathbf{m}^{\mathrm T} \mathbf{\Sigma}^{-1} \mathbf{m}\right\}
\end{aligned}
$$

ここで$(3.49)$の平方完成にならって$\mathbf{\Sigma} = \left(\mathbf{A}+\beta \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right)^{-1}$, $\mathbf{m} = \beta \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T}\mathbf{t}$とした。これより$\mathbf{w}$についての積分が行えるので

$$
\begin{aligned}
p(\mathbf{t} \mid \mathbf{X}, \boldsymbol{\alpha}, \beta) &=\left(\frac{\beta}{2 \pi}\right)^{\frac{N}{2}}\left(\frac{1}{2 \pi}\right)^{\frac{M}{2}} \prod_{i=1}^{M} \alpha_{i}^{\frac{1}{2}} \int \exp \left\{-\frac{1}{2}\left\{(\mathbf{w}-\mathbf{m})^{\mathrm T} \mathbf{\Sigma}^{-1}(\mathbf{w}-\mathbf{m})+\beta \mathbf{t}^{\mathrm T} \mathbf{t}-\mathbf{m}^{\mathrm T} \mathbf{\Sigma}^{-1} \mathbf{m}\right\}\right\} d \mathbf{w} \\
&=\left(\frac{\beta}{2 \pi}\right)^{\frac{N}{2}}\left(\frac{1}{2 \pi}\right)^{\frac{M}{2}} \prod_{i=1}^{M} \alpha_{i}^{\frac{1}{2}}\left(2\pi\right)^{\frac{M}{2}}|\mathbf{\Sigma}|^{\frac{1}{2}} \exp \left\{-\frac{1}{2}\left(\beta \mathbf{t}^{\mathrm T} \mathbf{t}-\mathbf{m}^{\mathrm T} \mathbf{\Sigma}^{-1} \mathbf{m}\right)\right\}
\end{aligned}
$$

となる。そしてさらに$\mathbf{t}$について再度指数部分を整理すると

$$
\begin{aligned}
-\frac{1}{2}\left(\beta \mathbf{t}^{\mathrm{T}} \mathbf{t}-\mathbf{m}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \mathbf{m}\right) &= - \frac{1}{2}\left(\beta \mathbf{t}^{\mathrm{T}} \mathbf{t}-\beta \mathbf{t}^{\mathrm{T}} \mathbf{\Phi} \mathbf{\Sigma} \mathbf{\Sigma}^{-1} \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm{T}} \mathbf{t} \beta\right) \\
&= -\frac{1}{2} \mathbf{t}^{\mathrm{T}}\left(\beta \mathbf{I}-\beta \mathbf{\Phi} \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm{T}} \beta\right) \mathbf{t} \\
&= -\frac{1}{2} \mathbf{t}^{\mathrm{T}}\left(\beta \mathbf{I}-\beta \mathbf{\Phi}\left(\mathbf{A}+\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm{T}} \beta\right) \mathbf{t} \\
&= -\frac{1}{2}  \mathbf{t}^{\mathrm T}\left(\left(\beta^{-1} \mathbf{I}\right)^{-1}-\left(\beta^{-1} \mathbf{I}\right)^{-1} \mathbf{\Phi}\left(\mathbf{A}+\mathbf{\Phi}^{\mathrm T}\left(\beta^{-1} \mathbf{I}\right)^{-1} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm T}\left(\beta^{-1} \mathbf{I}\right)^{-1}\right) \mathbf{t} \\
&= -\frac{1}{2} \mathbf{t}^{\mathrm{T}}\left(\beta^{-1} \mathbf{I}+\mathbf{\Phi} \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}}\right)^{-1} \mathbf{t} \hspace{1em} (\because \textrm{Woodburyの公式}, \textrm{(C.7)})\\
&= -\frac{1}{2} \mathbf{t}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t}
\end{aligned}
$$

となる。ただし、最後で$\mathbf{C} = \beta^{-1} \mathbf{I}+\mathbf{\Phi} \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}}$とした。以上から対数を取ることで

$$
\begin{aligned}
\ln p(\mathbf{t} \mid \mathbf{X}, \boldsymbol{\alpha}, \beta) &= \frac{N}{2}(\ln\beta -\ln (2\pi)) + \frac{1}{2}\ln |\mathbf{\Sigma}| + \frac{1}{2}\sum_{i=1}^{M}\ln \alpha_i -\frac{1}{2} \mathbf{t}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t} \\
&= -\frac{N}{2}\ln(2\pi) + \frac{N}{2}\ln \beta +\frac{1}{2}\ln\left| \mathbf{\Sigma} \right| + \frac{1}{2}\sum_{i=1}^{M}\ln \alpha_i -\frac{1}{2} \mathbf{t}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t} \\
&= -\frac{N}{2}\ln(2\pi) - \frac{1}{2}\ln |\mathbf{C}| -\frac{1}{2} \mathbf{t}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t}
\end{aligned}
$$

となり、展開することで$(7.85)$式を得られる。

※ 最後の式変形部分について、$\displaystyle \frac{N}{2}\ln \beta + \frac{1}{2}\ln |\mathbf{\Sigma}| + \frac{1}{2}\sum_{i=1}^{M}\ln \alpha_i = -\frac{1}{2}\ln \left|\beta^{-1} \mathbf{I}+\mathbf{\Phi} \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}}\right|$を示す。

これは$\displaystyle \ln \left(\beta^{N} \cdot |\mathbf{\Sigma}| \cdot \prod_{i=1}^{M} \alpha_{i} \right) = \ln \left|\beta^{-1} \mathbf{I}+\mathbf{\Phi} \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}}\right|^{-1}$を示せれば良い。

$$
\begin{aligned}
\ln \left(\beta^{N} \cdot |\mathbf{\Sigma}| \cdot \prod_{i=1}^{M} \alpha_{i}\right) &=\ln (|\beta \mathbf{I}| |\mathbf{A}| |\mathbf{\Sigma}|) \quad (\because |\beta\mathbf{I}| = \beta^N, |\mathbf{A}||\mathbf{B}| = |\mathbf{B}||\mathbf{A}|)\\
&=\ln \left(\left|(\beta \mathbf{I})^{-1}\right|^{-1}\left|\mathbf{A}^{-1}\right|^{-1}\left|\mathbf{A}+\mathbf{\Phi}^{\mathrm{T}}(\beta \mathbf{I}) \mathbf{\Phi}\right|^{-1}\right) \quad (\because(\mathrm{C}. 3)) \\
&=\ln \left|(\beta \mathbf{I})^{-1} \mathbf{A}^{-1}\left(\mathbf{A}+\mathbf{\Phi}^{\mathrm{T}}(\beta \mathbf{I}) \mathbf{\Phi}\right)\right|^{-1} \quad (\because(\mathrm{C}. 12)) \\
&=\ln\left|(\beta \mathbf{I})^{-1}\left(\mathbf{I}+\mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}}(\beta \mathbf{I}) \mathbf{\Phi}\right)\right|^{-1} \\
&=\ln \left|(\beta \mathbf{I})^{-1}\left(\mathbf{I}+\left(\mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}}\right)^{\mathrm{T}}((\beta \mathbf{I}) \mathbf{\Phi})^{\mathrm{T}}\right)\right|^{-1}\quad (\because(\mathrm{C} .14)) \\
&=\ln \left|(\beta \mathbf{I})^{-1}\left(\mathbf{I}+\mathbf{\Phi} \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}}(\beta \mathbf{I})\right)\right|^{-1} \quad \left(\because\left(\mathbf{A}^{-1}\right)^{\mathrm{T}}=\mathbf{A}^{-1}\right) \\
&=\ln \left|\left(\mathbf{I}+\mathbf{\Phi} \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}}(\beta \mathbf{I})\right)(\beta \mathbf{I})^{-1}\right|^{-1} \quad \left(\because |\mathbf{AB}| = |\mathbf{BA}|\right) \\
&=\ln \left|\beta^{-1} \mathbf{I}+\mathbf{\Phi} \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}}\right|^{-1}
\end{aligned}
$$

## 演習 7.11
<div class="panel-primary">

前問を，

$$
p(\mathbf{y})=\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \boldsymbol{\mu}+\mathbf{b}, \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathrm{T}}\right) \tag{2.115}
$$

の結果を用いて解け．

</div>

※$(2.115)$式に代入するだけで求まる。

演習7.10の途中式から

$$
\begin{aligned}
p(\mathbf{t} \mid \mathbf{X}, \boldsymbol{\alpha}, \beta) &=\int p(\mathbf{t} \mid \mathbf{X}, \mathbf{w}, \beta) p(\mathbf{w} \mid \alpha) d \mathbf{w} \\
&=\int \prod_{n=1}^{N} \mathcal{N}\left(t_{n} \mid \mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x}), \beta^{-1}\right) \prod_{i=1}^{M} \mathcal{N}\left(w_{i} \mid 0, \alpha_{i}^{-1}\right) d \mathbf{w} \\
&=\int \mathcal{N} \left( \mathbf{t} \mid \mathbf{\Phi w}, \beta^{-1}\mathbf{I} \right) \mathcal{N} \left( \mathbf{w} \mid \mathbf{0}, \mathbf{A}^{-1} \right) d\mathbf{w}
\end{aligned}
$$

$(2.115)$式を使って周辺化すると

$$
\begin{aligned}
p(\mathbf{t} \mid \mathbf{X}, \boldsymbol{\alpha}, \beta) &= \mathcal{N}\left(\mathbf{t} \mid \mathbf{\Phi 0}+\left(\beta^{-1} \mathbf{I}\right)+\mathbf{\Phi} \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm T}\right) \\
&=\mathcal{N}\left(\mathbf{t} \mid \mathbf{0}, \mathbf{C}\right)
\end{aligned}
$$

となるので、$(7.85)$式が求められた。

## 演習 7.12
<div class="panel-primary">

RVM回帰モデルについて周辺化対数尤度

$$
\begin{aligned} \ln p(\mathbf{t} \mid \mathbf{X}, \boldsymbol{\alpha}, \beta) &=\ln \mathcal{N}(\mathbf{t} \mid \mathbf{0}, \mathbf{C}) \\ &=-\frac{1}{2}\left\{N \ln (2 \pi)+\ln |\mathbf{C}|+\mathbf{t}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t}\right\} \end{aligned} \tag{7.85}
$$

を直接最大化すると，更新式

$$
\alpha_{i}^{\text {new }}=\frac{\gamma_{i}}{m_{i}^{2}} \tag{7.87}
$$

および

$$
\left(\beta^{\text {new}}\right)^{-1}=\frac{\|\mathbf{t}-\Phi \mathbf{m}\|^{2}}{N-\sum_{i} \gamma_{i}} \tag{7.88}
$$

が得られることを示せ．ただし$\gamma_i$は

$$
\mathbf{\Sigma}=\left(\mathbf{A}+\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \tag{7.83}
$$

で定義される共分散行列$\mathbf{\Sigma}$の$i$番目の対角成分を用いて

$$
\gamma_i = 1-\alpha_i \Sigma_{ii} \tag{7.89}
$$

で与えられるものとする．

</div>

※$\mathbf{\Phi},\mathbf{\Phi}^{\mathrm T},\mathbf{\Sigma}$はそれぞれ$M\times N,N\times M, N\times N$行列、$\mathbf{t}, \mathbf{m}$はそれぞれ$M, N$次元ベクトルである。

演習 7.10または7.11の結果から

$$
\ln p(\mathbf{t} \mid \mathbf{X}, \alpha, \beta)=\frac{N}{2} \ln \beta-\frac{N}{2} \ln (2 \pi)+\frac{1}{2} \ln |\mathbf{\Sigma}|+\frac{1}{2} \sum_{i=1}^{M} \ln \alpha_{i}-\frac{1}{2} \mathbf{t}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t}
$$

となる。次にテキスト58ページのように、この対数尤度の微分を$0$とする。

まず$\alpha_i$について偏微分するが、準備として$\mathbf{I}_{ii}$を$ii$成分のみ$1$で残りを$0$とする行列とする。これを用いて上式第3項の$\alpha_i$についての偏微分は

$$
\begin{aligned}
\frac{\partial}{\partial \alpha_{i}} \ln |\mathbf{\Sigma}| &=-\frac{\partial}{\partial \alpha_{i}} \ln \left|\mathbf{\Sigma}^{-1}\right| \\
&=-\operatorname{Tr}\left[\mathbf{\Sigma} \frac{\partial \mathbf{\Sigma}^{-1}}{\partial \alpha_{i}}\right] \quad(\because \textrm{(C.22)}) \\
&=-\operatorname{Tr}\left[\mathbf{\Sigma} \frac{\partial}{\partial \alpha_{i}}\left(\mathbf{A}+\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)\right] \\
&=-\operatorname{Tr}\left[\mathbf{\Sigma} \mathbf{I}_{i i}\right] \\
&=-\Sigma_{i i}
\end{aligned}
$$

第5項の$\alpha_i$についての偏微分は、$\mathbf{\Sigma}$が対称行列であることと$\mathbf{\Sigma} = \left(\mathbf{A}+\beta \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right)^{-1}$, $\mathbf{m} = \beta \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T}\mathbf{t}$を利用して

$$
\begin{aligned}
\frac{\partial}{\partial \alpha_{i}}\left(\mathbf{t}^{\mathrm{T}} \mathbf{C} \mathbf{t}\right) &=\frac{\partial}{\partial \alpha_{i}}\left(\beta \mathbf{t}^{\mathrm{T}} \mathbf{t}-\mathbf{m}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \mathbf{m}\right) \quad (\because 演習7.10)\\
&=-\frac{\partial}{\partial \alpha_{i}}\left(\mathbf{m}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \mathbf{m}\right) \\
&=-\frac{\partial}{\partial \alpha_{i}}\left(\beta \mathbf{t}^{\mathrm{T}} \mathbf{\Phi} \mathbf{\Sigma} \mathbf{\Sigma}^{-1} \beta \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm{T}} \mathbf{t}\right) \quad (\because \mathbf{m} = \beta \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm{T}} \mathbf{t})\\
&=-\frac{\partial}{\partial \alpha_{i}}\left(\beta^{2} \mathbf{t}^{\mathrm{T}} \mathbf{\Phi} \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm{T}} \mathbf{t}\right) \\
&=-\operatorname{Tr}\left[ \left( \frac{\partial}{\partial \mathbf{\Sigma}^{-1}} \beta^{2} \mathbf{t}^{\mathrm{T}} \mathbf{\Phi} \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm{T}} \mathbf{t} \right)^{\mathrm T} \frac{\partial \mathbf{\Sigma}^{-1}}{\partial \alpha_i}\right] \quad (\because \textrm{Matrix Cookbook (137)}) \\
&=-\operatorname{Tr}\left[\beta^{2}\left(-\mathbf{\Sigma}\left(\mathbf{\Phi}^{\mathrm T} \mathbf{t}\right)\left(\mathbf{\Phi}^{\mathrm T} \mathbf{t}\right)^{\mathrm T} \mathbf{\Sigma}\right)^{\mathrm T} \mathbf{I}_{i i}\right] \quad (\because \textrm{Matrix Cookbook (61)}) \\
&=\operatorname{Tr}\left[\left(\mathbf{mm}^{\mathrm T}\right)^{\mathrm T} \mathbf{I}_{i i}\right] \\
&=m_{i}^2
\end{aligned}
$$

となる。$m_i$は$(7.82)$で定義される事後平均$\mathbf{m}$の$i$番目の要素である。また途中の式変形で[Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)に掲載されている行列の微分の公式を用いた。

$$
\frac{\partial}{\partial \alpha_{i}}\ln p(\mathbf{t} \mid \mathbf{X}, \alpha, \beta) = -\frac{1}{2}\Sigma_{ii} + \frac{1}{2\alpha_i}-\frac{1}{2}m_i^{2}
$$

これを$0$として移項すると

$$
\begin{aligned}
& \alpha_{i} m_{i}^{2} = 1-\alpha_{i} \Sigma_{i i} \\
& \therefore \alpha_{i} = \frac{1-\alpha_{i} \Sigma_{i i}}{m_{i}^{2}}=\frac{\gamma_{i}}{m_{i}^{2}}
\end{aligned}
$$

これが求める$\alpha_i^{\textrm{new}}$となる。

同様にして$\beta$について偏微分する。$\ln p$の第3項について

$$
\begin{aligned}
\frac{\partial}{\partial \beta}\ln | \mathbf{\Sigma} | &= -\frac{\partial}{\partial \beta} \ln \left|\mathbf{\Sigma}^{-1}\right| \\
&=-\operatorname{Tr}\left[\mathbf{\Sigma} \frac{\partial \mathbf{\Sigma}^{-1}}{\partial \beta}\right] \\
&=-\operatorname{Tr}\left[\mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right] \end{aligned}
$$

第5項について

$$
\begin{aligned}
\frac{\partial}{\partial \beta}\left(\mathbf{t}^{\mathrm T} \mathbf{C} \mathbf{t}\right) &=\frac{\partial}{\partial \beta}\left(\beta \mathbf{t}^{\mathrm T} \mathbf{t}-\mathbf{m}^{\mathrm T} \mathbf{C} \mathbf{m}\right) \\
&=\mathbf{t}^{\mathrm T} \mathbf{t}-\frac{\partial}{\partial \beta}\left(\beta^{2} \mathbf{t}^{\mathrm T} \mathbf{\Phi} \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T} \mathbf{t}\right) \\
&=\mathbf{t}^{\mathrm T} \mathbf{t}-2 \beta\left(\mathbf{t}^{\mathrm T} \mathbf{\Phi} \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T} \mathbf{t}\right)-\beta^{2} \frac{\partial}{\partial \beta}\left(\mathbf{t}^{\mathrm T} \mathbf{\Phi} \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T} \mathbf{t}\right) \\
&=\mathbf{t}^{\mathrm T} \mathbf{t}-2 \mathbf{t}^{\mathrm T} \mathbf{\Phi} \mathbf{m}-\beta^{2} \operatorname{Tr}\left[\frac{\partial}{\partial \mathbf{\Sigma}^{-1}}\left(\left(\mathbf{\Phi}^{\mathrm T} \mathbf{t}\right)^{\mathrm T} \mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T} \mathbf{t}\right)^{\mathrm T} \frac{\partial \mathbf{\Sigma}^{-1}}{\partial \beta}\right] \\
&=\mathbf{t}^{\mathrm T} \mathbf{t}-2 \mathbf{t}^{\mathrm T} \mathbf{\Phi} \mathbf{m}+\beta^{2} \operatorname{Tr}\left[\mathbf{\Sigma}\left(\mathbf{\Phi}^{\mathrm T} \mathbf{t}\right)(\mathbf{\Phi}^{\mathrm T} \mathbf{t})^{\mathrm T} \mathbf{\Sigma} \cdot\left(\mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right)\right] \\
&=\mathbf{t}^{\mathrm T} \mathbf{t}-2 \mathbf{t}^{\mathrm T} \mathbf{\Phi} \mathbf{m}+\operatorname{Tr}\left[\mathbf{m} \mathbf{m}^{\mathrm T} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right] \\
&=\mathbf{t}^{2} \mathbf{t}-2 \mathbf{t}^{\mathrm T} \mathbf{\Phi} \mathbf{m}+\operatorname{Tr}\left[\mathbf{m}^{\mathrm T} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi} \mathbf{m}\right] \\
&=\mathbf{t}^{2} \mathbf{t}-2 \mathbf{t}^{\mathrm T} \mathbf{\Phi} \mathbf{m}+(\mathbf{\Phi} \mathbf{m})^{\mathrm T} \mathbf{\Phi} \mathbf{m} \\
&=\|\mathbf{t}-\mathbf{\Phi} \mathbf{m}\|^{2}
\end{aligned}
$$

これより、

$$
\frac{\partial}{\partial \beta}\ln p(\mathbf{t} \mid \mathbf{X}, \alpha, \beta)=\frac{1}{2}\left(\frac{N}{\beta}-\operatorname{Tr}\left[\mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right]-\|\mathbf{t}-\mathbf{\Phi} \mathbf{m}\|^{2}\right)
$$

となる。このうち$\operatorname{Tr}\left[\mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right]$について

$$
\begin{aligned}
\mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi} &=\mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}+\beta^{-1} \mathbf{\Sigma} \mathbf{A}-\beta^{-1} \mathbf{\Sigma} \mathbf{A} \\
&=\mathbf{\Sigma}\left(\beta \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}+\mathbf{A}\right) \beta^{-1}-\beta^{-1} \mathbf{\Sigma} \mathbf{A} \\
&=\mathbf{I} \beta^{-1}-\beta^{-1} \Sigma \mathbf{A} \\
&=\beta^{-1}(\mathbf{I}-\mathbf{\Sigma} \mathbf{A})
\end{aligned}
$$

となるので、

$$
\begin{aligned}
\ & \frac{\partial}{\partial \beta}\ln p(\mathbf{t} \mid \mathbf{X}, \alpha, \beta) = 0 \\
\Leftrightarrow &\ \frac{1}{2}\left(\frac{N}{\beta}-\operatorname{Tr}\left[\mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right]-\|\mathbf{t}-\mathbf{\Phi} \mathbf{m}\|^{2}\right) = 0 \\
\Leftrightarrow &\ \beta^{-1} = \frac{\|\mathbf{t}-\mathbf{\Phi} \mathbf{m}\|^{2}}{N-\operatorname{Tr}(\mathbf{I}-\mathbf{\Sigma A})}=\frac{\|\mathbf{t}-\mathbf{\Phi} \mathbf{m}\|^{2}}{N-\sum_{i} \gamma_{i}}
\end{aligned}
$$

これが$\left(\beta^{\text {new}}\right)^{-1}$となる。

※ $\alpha_{i}^{\text {new }}$も$\left(\beta^{\text {new}}\right)^{-1}$も1つ前の$\alpha_i, \beta^{-1}$の値に依存しているので、これら超パラメータの学習はP.58に書かれているように、適当な初期値を決めてから更新していき、適当な収束条件が満たされるまで繰り返される。

## 演習 7.13
<div class="panel-primary">

本文では，RVM回帰モデルについて，

$$
\begin{aligned} \ln p(\mathbf{t} \mid \mathbf{X}, \boldsymbol{\alpha}, \beta) &=\ln \mathcal{N}(\mathbf{t} \mid \mathbf{0}, \mathbf{C}) \\ &=-\frac{1}{2}\left\{N \ln (2 \pi)+\ln |\mathbf{C}|+\mathbf{t}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t}\right\} \end{aligned} \tag{7.85}
$$

の周辺化尤度の最大化から，更新式

$$
\alpha_{i}^{\text {new }}=\frac{\gamma_{i}}{m_{i}^{2}} \tag{7.87}
$$

および

$$
\left(\beta^{\text {new}}\right)^{-1}=\frac{\|\mathbf{t}-\Phi \mathbf{m}\|^{2}}{N-\sum_{i} \gamma_{i}} \tag{7.88}
$$

を導いた．超パラメータの事前分布を

$$
\operatorname{Gam}(\tau \mid a, b)=\frac{1}{\Gamma(a)} b^{a} \tau^{a-1} e^{-b \tau} \tag{B.26}
$$

の形のガンマ分布に変更したときの$\boldsymbol{\alpha}$と$\beta$に対する更新式を，同様に事後確率$p(\mathbf{t}, \boldsymbol{\alpha}, \beta \mid \mathbf{X})$を$\boldsymbol{\alpha}$と$\beta$に対して最大化することで導出せよ．

</div>

題意により、$\mathbf{\alpha}_i$と$\beta$の事前分布を以下のように定める。ここで、全ての$\alpha_i$についてパラメータ$a,b$は共通とした。（本文では$\alpha$が確率変数ではないので、$i$に応じて異なるパラメータにしないと関連度自動決定の議論に繋がらないが、$\alpha$を確率変数とみなすことで、各$i$について同一のパラメータを採用することができる。）

\begin{aligned}
p(\alpha_i) = \operatorname{Gam}(\alpha_i \mid a, b) = \frac{1} {\Gamma(a)} b^{a} \alpha_i{}^{a-1} e^{-b \alpha_i} \\
p(\beta) = \operatorname{Gam}(\beta \mid \tilde{a}, \tilde{b}) = \frac{1}{\Gamma(\tilde{a})} \tilde{b}^{\tilde{a}} \beta^{\tilde{a}-1} e^{-\tilde{b} \beta}
\end{aligned}

尤度関数$p(\mathbf{t}, \mathbf{\alpha}, \beta \mid \mathbf{X} ) = p(\mathbf{t} \mid \mathbf{X}, \mathbf{\alpha}, \beta) \prod_i p(\alpha_i) p(\beta)$を最大化する$\mathbf{\alpha}$と$\beta$を求める。

対数尤度関数は、以下の通り。

\begin{aligned}
\ln p(\mathbf{t}, \mathbf{\alpha}, \beta \mid \mathbf{X} )
=&
\frac{N}{2} \ln \beta-\frac{N}{2} \ln (2 \pi)+\frac{1}{2} \ln |\mathbf{\Sigma}|+\frac{1}{2} \sum_{j=1}^{M} \ln \alpha_{j}-\frac{1}{2} \mathbf{t}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t} \\
&+
\sum_{j=1}^M \left\{ a \ln b + (a-1)\ln \alpha_j - b \alpha_j - \ln \Gamma (a) \right\} \\
&+
\left\{ \tilde{a} \ln \tilde{b} + (\tilde{a}-1)\ln \beta - \tilde{b} \beta - \ln \Gamma (\tilde{a}) \right\}
\end{aligned}

対数尤度関数を$\alpha_i$と$\beta$で偏微分する。１行目の偏微分は演習(7.12)に登場する式変形を参照。

\begin{aligned}
\frac{\partial}{\partial \alpha_i} \ln p(\mathbf{t}, \mathbf{\alpha}, \beta \mid \mathbf{X} )
=& \left( -\frac{1}{2}\Sigma_{ii} + \frac{1}{2\alpha_i}-\frac{1}{2}m_i^{2} \right) + \left( \frac{a-1}{\alpha_i} - b \right) \\
=& -\frac{1}{2} \frac{1 - \gamma_i}{\alpha_i} + \frac{1}{2\alpha_i}-\frac{1}{2}m_i^{2} + \frac{a-1}{\alpha_i} - b \\
=& \frac{1}{2 \alpha_i} \left\{ -(1 - \gamma_i) + 1 + 2(a-1) \right\} - \frac{1}{2} (m_i^2 + 2b)\\
=& \frac{1}{2 \alpha_i} \left( \gamma_i + 2a -2 \right) - \frac{1}{2} (m_i^2 + 2b)
\end{aligned}
右辺$=0$を解いて、
\begin{aligned}
\alpha_i =\frac{\gamma_i + 2a -2}{m_i^2 + 2b}
\end{aligned}

となる。でも、公式解答は

\begin{aligned}
\alpha_i =\frac{\gamma_i + 2a -2}{m_i^2 - 2b}
\end{aligned}

となっている・・・。次に、

\begin{aligned}
\frac{\partial}{\partial \beta} \ln p(\mathbf{t}, \mathbf{\alpha}, \beta \mid \mathbf{X} )
=& \frac{1}{2}\left(\frac{N}{\beta}-\operatorname{Tr}\left[\mathbf{\Sigma} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right]-\|\mathbf{t}-\mathbf{\Phi} \mathbf{m}\|^{2}\right) +(\tilde{a} -1)\frac{1}{\beta}-\tilde{b} \\
=& \frac{1}{2}\left(\frac{N}{\beta}-\frac{\sum_i \gamma_i}{\beta}-\|\mathbf{t}-\mathbf{\Phi} \mathbf{m}\|^{2}\right) +(\tilde{a} -1)\frac{1}{\beta}-\tilde{b} \\
=& \frac{1}{2}\left\{ \frac{ N-\sum_i \gamma_i +2(\tilde{a}-1) }{\beta} - \left( \|\mathbf{t}-\mathbf{\Phi} \mathbf{m}\|^{2} + 2\tilde{b} \right) \right\}
\end{aligned}
右辺$=0$を解いて、
\begin{aligned}
\beta^{-1} =\frac
{\|\mathbf{t}-\mathbf{\Phi} \mathbf{m}\|^{2} + 2\tilde{b}}
{2\tilde{a}-2+N-\sum_i \gamma_i}
\end{aligned}
となる。でも、公式解答は

\begin{aligned}
\beta^{-1} =\frac
{\|\mathbf{t}-\mathbf{\Phi} \mathbf{m}\|^{2} + 2\tilde{b}}
{\tilde{a}+2+N-\sum_i \gamma_i}
\end{aligned}
となっている・・・。
## 演習 7.14
<div class="panel-primary">

RVM回帰モデルの予測確率分布が

$$
\begin{aligned} p\left(t \mid \mathbf{x}, \mathbf{X}, \mathbf{t}, \alpha^{\star}, \beta^{\star}\right) &=\int p\left(t \mid \mathbf{x}, \mathbf{w}, \beta^{\star}\right) p\left(\mathbf{w} \mid \mathbf{X}, \mathbf{t}, \boldsymbol{\alpha}^{\star}, \beta^{\star}\right) \mathrm{d} \mathbf{w} \\ &=\mathcal{N}\left(t \mid \mathbf{m}^{\mathrm{T}} \phi(\mathbf{x}), \sigma^{2}(\mathbf{x})\right) \end{aligned} \tag{7.90}
$$

で与えられることを示せ．また，その予測分布の分散が

$$
\sigma^{2}(\mathbf{x})=\left(\beta^{\star}\right)^{-1}+\boldsymbol{\phi}(\mathbf{x})^{\mathrm{T}} \mathbf{\Sigma} \boldsymbol{\phi}(\mathbf{x}) \tag{7.91}
$$

で与えられることも示せ．ここで，$\mathbf{\Sigma}$は

$$
\mathbf{\Sigma} = \left(\mathbf{A}+\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \tag{7.83}
$$

において$\alpha = \alpha^{\star}$および$\beta = \beta^{\star}$としたものである．

</div>

$(7.76)$式,$(7.81)$式から

\begin{aligned} p\left(t \mid \mathbf{x}, \mathbf{w}, \beta^{\star}\right)  &=\mathcal{N}\left(t \mid \mathbf{w}^{\mathrm{T}} \phi(\mathbf{x}), (\beta^{\star})^{-1}\right )
 \\ &
\end{aligned}
\begin{aligned} p\left(\mathbf{w} \mid \mathbf{X}, \mathbf{t}, \boldsymbol{\alpha}^{\star}, \beta^{\star}\right)&=\mathcal{N}\left(\mathbf{w} \mid \mathbf{m}, \mathbf{\Sigma} \right )
 \\ &
\end{aligned}
$$
(2.115)式において、A\mathbf{x}を\mathbf{w}^{\mathrm{T}} \phi(\mathbf{x})に、L^{-1}を(\beta^{\star})^{-1}に、\muを\mathbf{m}に、\Lambda^{-1}を\Sigmaに置き換えると、
$$
\begin{aligned} p\left(t \mid \mathbf{x}, \mathbf{X}, \mathbf{t}, \alpha^{\star}, \beta^{\star}\right) &=\int p\left(t \mid \mathbf{x}, \mathbf{w}, \beta^{\star}\right) p\left(\mathbf{w} \mid \mathbf{X}, \mathbf{t}, \boldsymbol{\alpha}^{\star}, \beta^{\star}\right) \mathrm{d} \mathbf{w} \\ &=\mathcal{N}\left(t \mid \mathbf{m}^{\mathrm{T}} \phi(\mathbf{x}), \left(\beta^{\star}\right)^{-1}+\boldsymbol{\phi}(\mathbf{x})^{\mathrm{T}} \mathbf{\Sigma} \boldsymbol{\phi}(\mathbf{x})\right) \end{aligned}
となる。


## 演習 7.15
<div class="panel-primary">

$$
|\mathbf{C}| =\left|\mathbf{C}_{-i}\right|\left(1+\alpha_{i}^{-1} \boldsymbol{\varphi}_{i}^{\mathrm{T}} \mathbf{C}_{-i}^{-1} \boldsymbol{\varphi}_{i}\right) \tag{7.94}
$$

および

$$
\mathbf{C}^{-1} =\mathbf{C}_{-i}^{-1}-\frac{\mathbf{C}_{-i}^{-1} \boldsymbol{\varphi}_{i} \boldsymbol{\varphi}_{i}^{\mathrm{T}} \mathbf{C}_{-i}^{-1}}{\alpha_{i}+\boldsymbol{\varphi}_{i}^{\mathrm{T}} \mathbf{C}_{-i}^{-1} \boldsymbol{\varphi}_{i}} \tag{7.95}
$$

を用いて，周辺化尤度

$$
\begin{aligned} \ln p(\mathbf{t} \mid \mathbf{X}, \boldsymbol{\alpha}, \beta) &=\ln \mathcal{N}(\mathbf{t} \mid \mathbf{0}, \mathbf{C}) \\ &=-\frac{1}{2}\left\{N \ln (2 \pi)+\ln |\mathbf{C}|+\mathbf{t}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t}\right\} \end{aligned} \tag{7.85}
$$

が

$$
L(\boldsymbol{\alpha})=L\left(\boldsymbol{\alpha}_{-i}\right)+\lambda\left(\alpha_{i}\right) \tag{7.96}
$$

の形に変形できることを示せ．ただし$\lambda(\alpha_n)$および品質/疎性パラメータはそれぞれ

$$
\lambda\left(\alpha_{i}\right)=\frac{1}{2}\left[\ln \alpha_{i}-\ln \left(\alpha_{i}+s_{i}\right)+\frac{q_{i}^{2}}{\alpha_{i}+s_{i}}\right] \tag{7.97}
$$

$$
s_{i}=\boldsymbol{\varphi}_{i}^{\mathrm{T}} \mathbf{C}_{-i}^{-1} \boldsymbol{\varphi}_{i} \tag{7.98}
$$

$$
q_{i}=\boldsymbol{\varphi}_{i}^{\mathrm{T}} \mathbf{C}_{-i}^{-1} \mathbf{t} \tag{7.99}
$$

で定義されているとする．

</div>

$(7.94)$式は

$$
\begin{aligned}
|\mathbf{C}| &= \left| \mathbf{C}_{-i}\left(\mathbf{I}+\alpha_{i}^{-1} \mathbf{C}_{-i}^{-1} \varphi_{i} \varphi_{i}^{\mathrm T}\right)\right| \\
&= \left| \mathbf{C}_{-i} \right| \left| \mathbf{I}+\alpha_{i}^{-1} \mathbf{C}_{-i}^{-1} \varphi_{i} \varphi_{i}^{\mathrm T}\right| \\
&=\left|\mathbf{C}_{-i}\right|\left(1+\alpha_{i}^{-1}\left(\mathbf{C}_{-i}^{-1} \varphi_{i}\right)^{\mathrm T} \varphi_{i}\right) \quad (\because (\textrm{C}. 15)) \\
&=\left|\mathbf{C}_{-i}\right|\left(1+\alpha_{i}^{-1} \varphi_{i}^{\mathrm T} \mathbf{C}_{-i}^{-1} \varphi_{i}\right) \quad \left(\because \mathbf{C}_{-i}^{-1} = \left( \mathbf{C}_{-i}^{-1} \right)^{\mathrm T} \right)
\end{aligned}
$$

$(7.95)$式はWoodburyの公式を用いて求められる。
$$
\begin{aligned}
\left(\mathbf{C}_{-i}+\alpha_{i}^{-1} \varphi_{i} \varphi_{i}^{\mathrm T}\right)^{-1}
&=\mathbf{C}_{-i}^{-1}-\mathbf{C}_{-i}^{-1} \varphi_{i}\left(\alpha_{i} \mathbf{I}+\varphi_{i}^{\mathrm T} \mathbf{C}_{-i}^{-1} \varphi_{i}\right)^{-1} \varphi_{i}^{\mathrm T} \mathbf{C}_{-i}^{-1} \\
&=\mathbf{C}_{-i}^{-1}-\frac{\mathbf{C}_{-i}^{-1} \varphi_{i} \varphi_{i}^{\mathrm T} \mathbf{C}_{-i}^{-1}}{\alpha_{i}+\varphi_{i}^{\mathrm T} \mathbf{C}_{-i}^{-1} \varphi_{i}}
\end{aligned}
$$

これらを用いて対数周辺尤度$\displaystyle \ln p(\mathbf{t} \mid \mathbf{X}, \boldsymbol{\alpha}, \beta) =-\frac{1}{2}\left\{N \ln (2 \pi)+\ln |\mathbf{C}|+\mathbf{t}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t}\right\}$を計算すると

$$
\begin{aligned}
L(\boldsymbol{\alpha})=&-\frac{1}{2}\left\{N \ln (2 \pi)+\ln |\mathbf{C}|+\mathbf{t}^{\mathrm T} \mathbf{C}^{-1} \mathbf{t}\right\} \\
=&-\frac{1}{2}\left\{N \ln (2 \pi)+\ln \left(\left|\mathbf{C}_{-i}\right|\left(1+\alpha_{i}^{-1} \varphi_{i}^{\mathrm T} \mathbf{C}_{-1}^{-1} \varphi_{i}\right)\right)+\mathbf{t}^{\mathrm T}\left(\mathbf{C}_{-i}^{-1} - \frac{\mathbf{C}_{-i}^{-1} \varphi_{i} \varphi_{i}^{\mathrm T} \mathbf{C}_{-i}^{-1}}{\alpha_{i}+\varphi_{i}^{\mathrm T} \mathbf{C}_{-i}^{-1} \varphi_{i}}\right) \mathbf{t}\right\} \\
=&-\frac{1}{2}\left\{N \ln (2 \pi)+\ln \left(\left|\mathbf{C}_{-i}\right|\left(1+\alpha_{i}^{-1} s_{i}\right)\right)+\mathbf{t}^{\mathrm T} \mathbf{C}_{-i}^{-1} \mathbf{t} - \frac{q_{i}^{2}}{\alpha_{i}+s_{i}}\right\} \\
&(\because q_{i}^{2}=q_{i}^{\mathbf{T}}q_{i}=(\varphi_{i}^{\mathbf{T}}\mathbf{C}_{-i}^{-1}\mathbf{t})^{\mathbf{T}}(\varphi_{i}^{\mathbf{T}}\mathbf{C}_{-i}^{-1}\mathbf{t})=\mathbf{t}^{\mathbf{T}}(\mathbf{C}_{-i}^{-1})^{\mathbf{T}}\varphi_{i}\varphi_{i}^{\mathbf{T}}\mathbf{C}_{-i}^{-1}\mathbf{t})
\\
=&-\frac{1}{2}\left\{N \ln (2 \pi)+\ln |\mathbf{C}_{-i}|+\mathbf{t}^{\mathrm T} \mathbf{C}_{-i}^{-1} \mathbf{t} \right\} -\frac{1}{2} \ln \left(\frac{\alpha_{i}+s_{i}}{\alpha_{i}}\right) + \frac{1}{2} \frac{q_{i}^{2}}{\alpha_{i}+s_{i}} \\
=&\ L(\boldsymbol{\alpha}_{-i})+\frac{1}{2}\left[\ln \alpha_{i}-\ln \left(\alpha_{i}+s_{i}\right)+\frac{q_{i}{ }^{2}}{\alpha_{i}+s_{i}}\right] \\
=&\ L(\boldsymbol{\alpha}_{-i})+\lambda(\alpha_i)
\end{aligned}
$$

以上より、$(7.96)$式が導出された。

## 演習 7.16
<div class="panel-primary">

超パラメータ$\alpha_i$に対して， RVM回帰モデルの周辺化対数尤度

\begin{align}
\lambda(\alpha_{i}) = \frac{1}{2}[ \ln \alpha_i - \ln(\alpha_i + s_i) +\frac{q_i^2}{\alpha_1 + s_i} ]
\end{align}

の2階微分を取ることで，

$$
\alpha_{i}=\frac{s_{i}^{2}}{q_{i}^{2}-s_{i}} \tag{7.101}
$$

で与えられる停留点が周辺化尤度の極大値であることを示せ．

</div>

$\lambda(\alpha_i)$を一階微分すると、
\begin{align}
\frac{\partial \lambda(\alpha_i)}{\partial \alpha_i} = \frac{\alpha_i^{-1}s_i^2 - (q_i^2 -s_i)}{2(\alpha_i + s_i )^2}
\end{align}
である。よって、その分子が0をとるとき、$\alpha_i$は極値をとる。よって、

\begin{align}
&\alpha_i^{-1}s_i^2 - (q_i^2 -s_i) = 0 \\
&\Rightarrow \alpha_i =\frac{s_i^2}{q_i^2 - s_i}
\end{align}

次に、２階微分は以下になる。
\begin{align}
\frac{\partial^2 \lambda(\alpha_i)}{\partial^2 \alpha_i} = \frac{1}{2}[-\frac{1}{\alpha_i^2}+\frac{1}{(\alpha_i+s_i)^2 }+\frac{2q_i^2}{(\alpha_i+s_i)^3} ]
\end{align}

次に、２階微分に$\alpha_i =\frac{s_i^2}{q_i^2 - s_i}$を代入した際に、0未満であれば、その$\alpha_i$は極大値であることが明らかになる。
\begin{align}
 \frac{1}{2}[-\frac{1}{\alpha_i^2}+\frac{1}{(\alpha_i+s_i)^2 }+\frac{2q_i^2}{(\alpha_i+s_i)^3} ] &=
\frac{1}{2}[-\frac{1}{(\frac{s_i^2}{q_i^2 - s_i})^2}+\frac{1}{(\frac{s_i^2}{q_i^2 - s_i}+s_i)^2 }+\frac{2q_i^2}{(\frac{s_i^2}{q_i^2 - s_i}+s_i)^3} ] \\
&= \frac{1}{2}[-\frac{(q_i^2 - s_i)^2}{s_i^4}+\frac{(q_i^2 - s_i)^2}{s_i^2 q_i^4}+\frac{2(q_i^2 - s_i)^3}{s_i^3 q_i^4}] \\
&= -\frac{1}{2}{(q_i^2 - s_i)^4}{q_i^4 s_i^2} < 0 &(\because q_i^2 - s_i > 0)
\end{align}
よって、$\alpha_i =\frac{s_i^2}{q_i^2 - s_i}$において極大値を取る。

## 演習 7.17
<div class="panel-primary">

\begin{align}
\boldsymbol{\Sigma}&=\left(\mathbf{A}+\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \tag{7.83} \\
\mathbf{C}&=\beta^{-1} \mathbf{I}+\mathbf{\Phi} \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}} \tag{7.87}\\
\left(\mathbf{A}+\mathbf{B D}^{-1} \mathbf{C}\right)^{-1}&=\mathbf{A}^{-1}-\mathbf{A}^{-1} \mathbf{B}\left(\mathbf{D}+\mathbf{C A}^{-1} \mathbf{B}\right)^{-1} \mathbf{C A}^{-1} \tag{C.7}
\end{align}

を用いて，

\begin{align}
Q_{i}&=\boldsymbol{\varphi}_{i}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t} \tag{7.102} \\
S_{i}&=\boldsymbol{\varphi}_{i}^{\mathrm{T}} \mathbf{C}^{-1} \boldsymbol{\varphi}_{i} \tag{7.103}
\end{align}

で定義される$Q_n, S_n$が，

\begin{align}
Q_{i}=\beta \boldsymbol{\varphi}_{i}^{\mathrm{T}} \mathbf{t}-\beta^{2} \boldsymbol{\varphi}_{i}^{\mathrm{T}} \boldsymbol{\Phi} \boldsymbol{\Sigma} \boldsymbol{\Phi}^{\mathrm{T}} \mathbf{t} \tag{7.106} \\
S_{i}=\beta \boldsymbol{\varphi}_{i}^{\mathrm{T}} \boldsymbol{\varphi}_{i}-\beta^{2} \boldsymbol{\varphi}_{i}^{\mathrm{T}} \boldsymbol{\Phi} \boldsymbol{\Sigma} \boldsymbol{\Phi}^{\mathrm{T}} \boldsymbol{\varphi}_{i} \tag{7.107}
\end{align}

に変形できることを示せ．

</div>

※
(7.102)式に(7.87)式を代入して

$$
\begin{aligned}
Q_{i}&=\boldsymbol{\varphi}_{i}^{\mathrm{T}} \mathbf{C}^{-1} \mathbf{t}\\
&=\boldsymbol{\varphi}_{i}^{T}(\beta^{-1} \mathbf{I}+\mathbf{\Phi} \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}})^{-1}\mathbf{t}\\
&=\boldsymbol{\varphi}_{i}^{T}\left\{\beta\mathbf{I}-\beta^{2}\mathbf{\Phi}(\mathbf{A}^{-1}+\beta\mathbf{\Phi}^{T}\mathbf{\Phi})^{-1}\mathbf{\Phi}^{T}\right\}\mathbf{t}\\
&=\beta\boldsymbol{\varphi}_{i}^{T}\mathbf{t}-\beta^{2}\boldsymbol{\varphi}_{i}^{T}\mathbf{\Phi}(\mathbf{A}^{-1}+\beta\mathbf{\Phi}^{T}\mathbf{\Phi})^{-1}\mathbf{\Phi}^{T}\mathbf{t}\\
&=\beta\boldsymbol{\varphi}_{i}^{T}\mathbf{t}-\beta^{2}\boldsymbol{\varphi}_{i}^{T}\mathbf{\Phi}\boldsymbol{\Sigma}\mathbf{\Phi}^{T}\mathbf{t}\\
\end{aligned}
$$

よって(7.106)式が得られる．2行目から3行目への式変形に(C.7)式を用い，4行目から5行目の式変形で(7.83)式を用いた．
また$\mathbf{t}$を$\boldsymbol{\varphi}_i$として上記と同様の計算を行うことで$S_i$についての式(7.107)が求まる．


## 演習 7.18
<div class="panel-primary">

RVM分類モデルの対数事後確率分布

$$
\begin{aligned}
\ln p(\mathbf{w} \mid \mathbf{t}, \boldsymbol{\alpha})&=\ln \{p(\mathbf{t} \mid \mathbf{w}) p(\mathbf{w} \mid \boldsymbol{\alpha})\}-\ln p(\mathbf{t} \mid \alpha) \\
&=\sum_{n=1}^{N}\left\{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right\}-\frac{1}{2} \mathbf{w}^{\mathrm{T}} \mathbf{A} \mathbf{w}+\text { const. }
\end{aligned} \tag{7.109}
$$

の勾配ベクトルおよびへシアン行列は

\begin{align}
\nabla \ln p(\mathbf{w} \mid \mathbf{t}, \boldsymbol{\alpha}) &=\boldsymbol{\Phi}^{\mathrm{T}}(\mathbf{t}-\mathbf{y})-\mathbf{A} \mathbf{w} \tag{7.110} \\
\nabla \nabla \ln p(\mathbf{w} \mid \mathbf{t}, \boldsymbol{\alpha}) &=-\left(\Phi^{\mathrm{T}} \mathbf{B} \Phi+\mathbf{A}\right) \tag{7.111}
\end{align}

で与えられることを示せ．

</div>

$p(\mathbf{w} \mid \boldsymbol{\alpha})$は$(7.80)$から

$$
\begin{aligned}
p(\mathbf{w} \mid \boldsymbol{\alpha}) &=\prod_{i=1}^{M} \mathcal{N}\left(w_{i} \mid 0, \alpha_{i}^{-1}\right) \\
\ln p(\mathbf{w} \mid \boldsymbol{\alpha}) &=\sum_{i=1}^{M} \ln \left[\left(\frac{\alpha_{i}}{2 \pi}\right)^{\frac{1}{2}} \exp \left\{-\frac{\alpha_{i} w_{i}^{2}}{2}\right\}\right]=-\frac{1}{2} \mathbf{w}^{\mathrm T} \mathbf{Aw} + \textrm{const.}
\end{aligned}
$$

である。

$p(\mathbf{t} \mid \mathbf{w})$は$(4.90)$式のクロスエントロピー誤差関数$E(\mathbf{w})$の符号を反転させたもの

$$
\ln p(\mathbf{t} \mid \mathbf{w})=\sum_{n=1}^{N}\left\{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right\}
$$

である。

演習4.13と同様に$\ln p(\mathbf{t} \mid \mathbf{w})$の$\mathbf{w}$についての勾配は

$$
\begin{aligned}
\nabla_{\mathbf{w}} \ln p &=\frac{\partial \ln p}{\partial y_{n}} \frac{\partial y_{n}}{\partial a_{n}} \nabla_{\mathbf{w}} a_{n} \\
\frac{\partial \ln p}{\partial y_{n}} &=\sum_{n=1}^{N}\left(\frac{t_{n}}{y_{n}}-\frac{1-t_{n}}{1-y_{n}}\right) \\ &=\sum_{n=1}^{N} \frac{t_{n}-y_{n}}{y_{n}\left(1-y_{n}\right)} \\
\frac{\partial y_{n}}{\partial a_{n}} &= \sigma\left(a_{n}\right)\left(1-\sigma\left(a_{n}\right)\right)=y_{n}\left(1-y_{n}\right) \\
\nabla_{\mathbf{w}} a_{n}&=\boldsymbol{\phi}_{n}
\end{aligned}
$$

よって

$$
\begin{aligned}
\nabla_{\mathbf{w}} \ln p(\mathbf{w} \mid \mathbf{t}, \boldsymbol{\alpha}) &=\sum_{n=1}^{N}\left(t_{n}-y_{n}\right) \boldsymbol{\phi}_{n}-\frac{1}{2} \cdot 2 \mathbf{Aw} \\ &=\mathbf{\Phi}^{\mathrm T}(\mathbf{t}-\mathbf{y})-\mathbf{Aw}
\end{aligned}
$$

ヘッセ行列は

$$
\begin{aligned}
\nabla_{\mathbf{w}}\left(\mathbf{\Phi}^{\mathrm T}(\mathbf{t}-\mathbf{y})-\mathbf{Aw}\right) &=-\sum_{n=1}^{N}\left(\frac{\partial y_{n}}{\partial a_{n}} \nabla_{\mathbf{w}} a_{n}\right) \boldsymbol{\phi}_{n}^{\mathrm T}-\mathbf{A}^{\mathrm T} \\
&=-\sum_{n=1}^{N} y_{n}\left(1-y_{n}\right) \boldsymbol{\phi}_{n} \boldsymbol{\phi}_{n}^{\mathrm T}-\mathbf{A} \\
&=-\left(\mathbf{\Phi}^{\mathrm T} \mathbf{B} \mathbf{\Phi}+\mathbf{A}\right)
\end{aligned}
$$

となる。

## 演習 7.19
<div class="panel-primary">

RVM分類モデルにおいて，周辺尤度関数の近似式

$$
\begin{aligned}
p(\mathbf{t} \mid \boldsymbol{\boldsymbol{\alpha} }) &=\int p(\mathbf{t} \mid \mathbf{w}) p(\mathbf{w} \mid \boldsymbol{\boldsymbol{\alpha} }) \mathrm{d} \mathbf{w} \\
& \simeq p\left(\mathbf{t} \mid \mathbf{w}^{\star}\right) p\left(\mathbf{w}^{\star} \mid \boldsymbol{\boldsymbol{\alpha} }\right)(2 \pi)^{M / 2}|\mathbf{\Sigma}|^{1 / 2}
\end{aligned} \tag{7.114}
$$

を最大化すると，超パラメータの更新式

$$
\alpha_{i}^{\text {new }}=\frac{\gamma_{i}}{\left(w_{i}^{\star}\right)^{2}} \tag{7.116}
$$

が得られることを示せ．

</div>

$\mathbf{w}^{\star}$を用いると条件付き確率$(4.89)$、事前分布$(7.80)$はそれぞれ

$$
\begin{aligned}
p\left(\mathbf{t} \mid \mathbf{w}^{\star}\right) &= \prod_{n=1}^{N} y_{n}^{t_n}\left(1-y_{n}\right)^{1-t_{n}} \\
p\left(\mathbf{w}^{\star} \mid \boldsymbol{\alpha} \right) &= \prod_{i=1}^{M} \mathcal{N} \left(w_{i}^{*} \mid 0, \alpha_{i}^{-1}\right) = \left(\frac{1}{2 \pi}\right)^{\frac{M}{2}} \prod_{i=1}^{M} \alpha_{i}^{\frac{1}{2}} \exp \left\{-\frac{\alpha_{i}{w_{i}^{\star}}^{2}}{2}\right\}
\end{aligned}
$$

であるから$(7.114)$式の対数をとって対数周辺化尤度を求めると

$$
\begin{aligned}
\ln p(\mathbf{t} \mid \boldsymbol{\alpha} ) &=
\ln p\left(\mathbf{t} \mid \mathbf{w}^{\star}\right)+\ln p\left(\mathbf{w}^{\star} \mid \boldsymbol{\alpha} \right)+\frac{M}{2} \ln (2 \pi)+\frac{1}{2}\ln |\mathbf{\Sigma}| \\
&=\sum_{n=1}^{N}\left\{t_{n} \ln y_{n}^{*}+\left(1-t_{n}\right) \ln \left(1-y_{n}^{*}\right)\right\} \\
& -\frac{1}{2} \sum_{i=1}^{M} \alpha_{i} w_{i}^{*^{2}}+\frac{1}{2} \sum_{i=1}^{N} \ln \alpha_{i}-\frac{M}{2} \ln (2 \pi)+\frac{M}{2} \ln (2 \pi)+\frac{1}{2} \ln |\mathbf{\Sigma}| \\
&=\left[\sum_{n=1}^{N}\left\{t_{n} \ln y_{n}{ }^{*}+\left(1-t_{n}\right) \ln \left(1-y_{n}{ }^{*}\right)\right\}\right]-\frac{1}{2} \sum_{i=1}^{M} \alpha_{i} w_{i}^{*^{2}}+\frac{1}{2} \sum_{i=1}^{N} \ln \alpha_{i}+\frac{1}{2} \ln |\mathbf{\Sigma}|
\end{aligned}
$$

$\alpha_i$についての微分を$0$とすると、今$\mathbf{w} = \mathbf{w}^{\star}$で固定されているので、$y_n^{\star} = \sigma(a_n) = \sigma({\mathbf{w}^{\star}}^{\mathrm T}\boldsymbol{\phi}_n)$も固定されている。つまり$[\ ]$以外の項について微分を取れば良い。

$$
\begin{aligned}
\frac{\partial}{\partial \alpha_{i}}\left[-\frac{1}{2} \sum_{i=1}^{M} \alpha_{i} w_{i}^{*^{2}}+\frac{1}{2} \sum_{i=1}^{M} \ln \alpha_{i}+\frac{1}{2} \ln |\mathbf{\Sigma}|\right]=0 \\
-\frac{1}{2}\left(w_{i}^{*}\right)^{2}+\frac{1}{2} \frac{\partial}{\partial \alpha_{i}}\left(\ln \alpha_{i}\right)+\frac{1}{2} \frac{\partial}{\partial \alpha_{i}} \ln |\mathbf{\Sigma}|=0 \\
-\frac{1}{2}\left(w_{i}^{*}\right)^{2}+\frac{1}{2 \alpha_{i}}-\frac{1}{2} \Sigma_{i i}=0 \quad (\because 演習 7.12)
\end{aligned}
$$

以上から$(7.115)$式が得られた。これに$\gamma_i = 1 - \alpha_{i} \Sigma_{ii}$を導入すれば

$$
\begin{aligned}
\alpha_{i}\left(w_{i}^{*}\right)^{2} &= 1-\alpha_{i} \Sigma_{i i}=\gamma_{i} \\ \therefore \ \alpha_{i} &= \frac{\gamma_{i}}{\left(w_{i}^{*}\right)^{2}}
\end{aligned}
$$

これが$\alpha_{i}$の更新式となり$\displaystyle \left( \alpha_{i}^{\textrm {(new)}} \leftarrow \frac{\gamma_{i}}{\left(w_{i}^{*}\right)^{2}} \right)$、$(7.87)$と同一である。
