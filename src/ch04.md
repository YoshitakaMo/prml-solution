# PRML第4章演習問題解答

<head>
<style>
  div.panel-primary {
	border: 1px solid #000;
    margin: 10px 5px;
    padding: 16px 10px 0px;
  }
</style>
</head>

## 演習 4.1
<div class="panel-primary">

データ$\{ \mathbf{x}_n\}$の集合が与えられ，**凸包**(convex hull)とは以下の式で与えられるすべての点の集合であると定義することができる．

$$
\mathbf{x} = \sum_{n}\alpha_n \mathbf{x}_n \tag{4.156}
$$

ここで$\alpha_n \geq 0$であり，$\sum_n \alpha_n = 1$である．第2のデータ$\{ \mathbf{y}_n\}$の集合とそれに対応する凸包を考える．もしすべてのデータ$\mathbf{x}_n$に対し$\hat{\mathbf{w}}^{\mathrm{T}}\mathbf{x}_n + w_0 > 0$を満たし，すべてのデータ$\mathbf{y}_n$に対し$\hat{\mathbf{w}}^{\mathrm{T}}\mathbf{y}_n + w_0 < 0$を満たすベクトル$\hat{\mathbf{w}}$とスカラー$w_0$が存在するなら，定義によりこれら2つのデータの集合は線形分離可能である．それらの凸包が重なる場合，2つのデータの集合は線形分離可能ではないことを示せ．また，逆に2つのデータの集合が線形分離可能な場合，それらの凸包が重ならないことを示せ．

</div>

※[凸包 (Wikipedia)](https://ja.wikipedia.org/wiki/%E5%87%B8%E5%8C%85)とは与えられた集合を含む最小の凸集合である。例えば$X$がユークリッド平面内の有界な点集合のとき、その凸包は直観的には$X$をゴム膜で包んだときにゴム膜が作る図形として視認することができる。つまり「与えられたデータを包み込む最小の集合」なので、凸包が2つのデータ集合間で重なっている場合には、線形分離不可能であるイメージが直観的に理解できる。矛盾を示す際には、両方にまたがる交差領域を使う。

「**それらの凸包が重なる場合，2つのデータの集合は線形分離可能ではないことを示せ**」について

凸包が重なるというのは、2つのデータ集合$\{\mathbf{x}_n\}$と$\{\mathbf{y}_m\}$が存在し、それぞれについての凸包のintersect（交差）領域が存在することを意味する。そしてそのようなintersect上の点$\mathbf{z}$を

$$
\mathbf{z}=\sum_{n} \alpha_{n} \mathbf{x}_{n}=\sum_{m} \beta_{m} \mathbf{y}_{m}
$$

のように書くことができる。ここで$\beta_m$はすべての$m$について$\beta_m \geq 0$であり、$\sum_m \beta_m = 1$を満たす。

ここで、凸包が重なる場合であっても2つのデータ集合$\{\mathbf{x}_n\}$と$\{\mathbf{y}_m\}$が線形分離可能であると仮定する。このとき、2つのintersect上に存在する点$\mathbf{z}$について

$$
\begin{aligned}
    \hat{\mathbf{w}}^{\mathrm T}\mathbf{z} + w_0 &= \sum_n \alpha_n \hat{\mathbf{w}}^{\mathrm T}\mathbf{x}_n + w_0 \\
    &= \sum_n \alpha_n \hat{\mathbf{w}}^{\mathrm T}\mathbf{x}_n + \underbrace{\left( \sum_n\alpha_n \right)}_{1}w_0 \\
    &= \sum_n \alpha_n \left( \hat{\mathbf{w}}^{\mathrm T}\mathbf{x}_n + w_0 \right) > 0 \quad (\because \hat{\mathbf{w}}^{\mathrm T}\mathbf{x}_n + w_0 > 0)
\end{aligned}
$$

一方で

$$
\begin{aligned}
    \hat{\mathbf{w}}^{\mathrm T}\mathbf{z} + w_0 &= \sum_m \beta_m \hat{\mathbf{w}}^{\mathrm T}\mathbf{y}_m + w_0 \\
    &= \sum_m \beta_m \hat{\mathbf{w}}^{\mathrm T}\mathbf{y}_m + \underbrace{\left( \sum_m \beta_m \right)}_{1}w_0 \\
    &= \sum_m \beta_m \left( \hat{\mathbf{w}}^{\mathrm T}\mathbf{y}_m + w_0 \right) < 0 \quad (\because \hat{\mathbf{w}}^{\mathrm T}\mathbf{y}_m + w_0 < 0)
\end{aligned}
$$

となる。これは矛盾するので仮定は誤りであり、凸包が重なる場合には2つのデータ集合$\{\mathbf{x}_n\}$と$\{\mathbf{y}_m\}$は線形分離可能ではないことが示された。また、この対偶を取ることで「**2つのデータの集合が線形分離可能な場合，それらの凸包が重ならない**」ことも自動的に示される。

## 演習 4.2
<div class="panel-primary">

二乗和誤差関数
$$
E_{D}(\widetilde{\mathbf{W}})=\frac{1}{2} \operatorname{Tr}\left\{(\widetilde{\mathbf{X}} \widetilde{\mathbf{W}}-\mathbf{T})^{\mathrm{T}}(\widetilde{\mathbf{X}} \widetilde{\mathbf{W}}-\mathbf{T})\right\} \tag{4.15}
$$
の最小化を考える．学習データ集合におけるすべての目的変数ベクトルが線形制約

$$
\mathbf{a}^{\mathrm{T}} \mathbf{t}_{n}+b=0 \tag{4.157}
$$

を満たすと仮定する．ここで，$\mathbf{t}_n$は$(4.15)$における行列$\mathbf{T}$の$n$番目の行に相当する．この制約の結果として，最小二乗解
$$
\mathbf{y}(\mathbf{x})=\widetilde{\mathbf{W}}^{\mathrm{T}} \widetilde{\mathbf{x}}=\mathbf{T}^{\mathrm{T}}\left(\tilde{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\widetilde{\mathbf{x}},\quad \text{where}\quad \tilde{\mathbf{X}}^{\dagger} = \left(\tilde{\mathbf{X}}^{\mathrm{T}} \tilde{\mathbf{X}}\right)^{-1} \tilde{\mathbf{X}}^{\mathrm{T}} \tag{4.17}
$$
によって与えられるモデルの予測$\mathbf{y}(\mathbf{x})$の要素もまたこの制約を満たす，つまり，以下の式を満たすことを示せ．

$$
\mathbf{a}^{\mathrm{T}} \mathbf{y}(\mathbf{x})+b=0 \tag{4.158}
$$

上記を示すため，パラメータ$w_0$がバイアスとしての役割を持つように，基底関数が$\phi_0(\mathbf{x})=0$であると仮定する．

</div>

まずそれぞれの行列の形を確認する。

$$
\mathbf{\widetilde{X}}=\begin{pmatrix}
1 & \mathbf{x}_1^{\mathrm{T}} \\
1 & \mathbf{x}_2^{\mathrm{T}} \\
\vdots & \vdots \\
1 & \mathbf{x}_n^{\mathrm{T}} \\
\end{pmatrix} = \begin{pmatrix}\mathbf{1} & \mathbf{X}\end{pmatrix},\quad
\mathbf{\widetilde{W}}=\begin{pmatrix}
w_{10} & w_{20} & \cdots & w_{K0} \\
\mathbf{w}_1 & \mathbf{w}_2 & \cdots & \mathbf{w}_K \\
\end{pmatrix}= \begin{pmatrix}\mathbf{w}_0^{\mathrm{T}} \\ \mathbf{W}\end{pmatrix}
$$
$\mathbf{\widetilde{X}}$は$N\times (D+1)$行列, $\mathbf{\widetilde{W}}$は$(D+1)\times K$行列である。$\mathbf{w}_0$はバイアスパラメータのベクトルである。また$\mathbf{1}$は$N$次元の全要素が$1$の列ベクトルを表しており、ダミー入力$x_0 = 1$を明示的に表示するために入れている。

$\mathbf{T}$は$N\times K$行列で
$$
\mathbf{T}=\begin{pmatrix}
\mathbf{t}_1^{\mathrm{T}} \\
\mathbf{t}_2^{\mathrm{T}} \\
\vdots \\
\mathbf{t}_n^{\mathrm{T}}
\end{pmatrix}
$$
と書ける。

これらからバイアスパラメータとダミー入力を分離して$(4.15)$の誤差二乗和関数$E_D(\widetilde{\mathbf{W}})$を表すと

$$
\begin{aligned}
E_{D}(\widetilde{\mathbf{W}})&=\frac{1}{2} \operatorname{Tr}\left\{(\widetilde{\mathbf{X}} \widetilde{\mathbf{W}}-\mathbf{T})^{\mathrm{T}}(\widetilde{\mathbf{X}} \widetilde{\mathbf{W}}-\mathbf{T})\right\} \\
&=\frac{1}{2} \operatorname{Tr}\left\{
    (\mathbf{XW}+\mathbf{1}\mathbf{w}_0^{\mathrm{T}}-\mathbf{T})^{\mathrm{T}}(\mathbf{XW}+\mathbf{1}\mathbf{w}_0^{\mathrm{T}}-\mathbf{T})
    \right\}
\end{aligned}
$$

となる。

$E_D(\widetilde{\mathbf{W}})$の最小化のためにまず$\mathbf{w}_0$について微分すると

$$
\begin{aligned}
    \frac{\partial E_D}{\partial \mathbf{w}_0} &= \frac{1}{2}\cdot 2\left( \mathbf{XW}+\mathbf{1}\mathbf{w}_0^{\mathrm{T}}-\mathbf{T} \right)^{\mathrm{T}}\mathbf{1} \\
    &=\left( \mathbf{XW} - \mathbf{T} \right)^{\mathrm{T}}\mathbf{1} + \mathbf{w}_0 \mathbf{1}^{\mathrm{T}}\mathbf{1} \\
    &=\left( \mathbf{XW} - \mathbf{T} \right)^{\mathrm{T}}\mathbf{1} + N\mathbf{w}_0
\end{aligned}
$$
これが$\mathbf{0}$となれば良いので
$$
\begin{aligned}
\mathbf{w}_0 &= \frac{1}{N}\left( \mathbf{T} - \mathbf{XW} \right)^{\mathrm{T}}\mathbf{1} \\
&= \frac{1}{N}\mathbf{T}^{\mathrm{T}}\mathbf{1} - \frac{1}{N}\mathbf{W}^{\mathrm{T}}\mathbf{X}^{\mathrm{T}}\mathbf{1} \\
&:= \overline{\mathbf{t}}-\mathbf{W}^{\mathrm{T}}\overline{\mathbf{x}}
\end{aligned}
$$
ここで、以降の略記のために$\displaystyle \overline{\mathbf{t}} = \frac{1}{N}\mathbf{T}^{\mathrm{T}}\mathbf{1},\quad \overline{\mathbf{x}} = \frac{1}{N}\mathbf{X}^{\mathrm{T}}\mathbf{1}$とした。

$E_D(\widetilde{\mathbf{W}})$の$\mathbf{w}_0$をこれで書き換えると
$$
\begin{aligned}
E_{D}(\mathbf{W}) &= \frac{1}{2} \operatorname{Tr}\left\{(\mathbf{X} \mathbf{W}+\overline{\mathbf{T}}-\overline{\mathbf{X}} \mathbf{W}-\mathbf{T})^{\mathrm{T}}(\mathbf{X} \mathbf{W}+\overline{\mathbf{T}}-\overline{\mathbf{X}} \mathbf{W}-\mathbf{T})\right\} \\
&= \frac{1}{2} \operatorname{Tr}\left\{\left((\mathbf{X} - \overline{\mathbf{X}})\mathbf{W} - (\mathbf{T} - \overline{\mathbf{T}})\right)^{\mathrm{T}}\left((\mathbf{X} - \overline{\mathbf{X}})\mathbf{W} - (\mathbf{T} - \overline{\mathbf{T}})\right)\right\}
\end{aligned}
$$
ここで略記のために$\displaystyle \overline{\mathbf{T}}=\mathbf{1} \overline{\mathbf{t}}^{\mathrm{T}}, \quad \overline{\mathbf{X}}=\mathbf{1} \overline{\mathbf{x}}^{\mathrm{T}}$とした。

これについても$\mathbf{W}$について微分して$\mathbf{0}$をとると
$$
\frac{\partial E_{D}}{\partial \mathbf{W}} = \frac{1}{2}\cdot 2 (\mathbf{X} - \overline{\mathbf{X}})^{\mathrm{T}}\left((\mathbf{X} - \overline{\mathbf{X}})\mathbf{W} - (\mathbf{T} - \overline{\mathbf{T}})\right) = \mathbf{0}
$$
$\widehat{\mathbf{X}}=\mathbf{X}-\overline{\mathbf{X}}, \widehat{\mathbf{T}}=\mathbf{T}-\overline{\mathbf{T}}$と書き直すと
$$
\mathbf{W}=\left(\widehat{\mathbf{X}}^{\mathrm{T}} \widehat{\mathbf{X}}\right)^{-1} \widehat{\mathbf{X}}^{\mathrm{T}} \widehat{\mathbf{T}}=\widehat{\mathbf{X}}^{\dagger} \widehat{\mathbf{T}}
$$
となる。

以上から、新しい入力$\mathbf{x}^{*}$が得られたときの予測値$\mathbf{y}(\mathbf{x}^{*})$は

$$
\begin{aligned}
\mathbf{y}\left(\mathbf{x}^{*}\right) &= \widetilde{\mathbf{W}}^{\mathrm{T}} \widetilde{\mathbf{x}^{*}}\\
&= \begin{pmatrix}\mathbf{w}_0 & \mathbf{W}^{\mathrm{T}}\end{pmatrix} \begin{pmatrix}1 \\ \mathbf{x}^{*}\end{pmatrix} \\
&=\mathbf{W}^{\mathrm{T}} \mathbf{x}^{\star}+\mathbf{w}_{0} \\
&=\mathbf{W}^{\mathrm{T}} \mathbf{x}^{\star}+\overline{\mathbf{t}}-\mathbf{W}^{\mathrm{T}} \overline{\mathbf{x}} \\
&=\overline{\mathbf{t}}+\widehat{\mathbf{T}}^{\mathrm{T}}\left(\widehat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right)
\end{aligned}
$$
となる。

一方、$(4.157)$式の線形制約条件は$\overline{\mathbf{t}}$について適用すると、以下のように書き直せる
$$
\begin{aligned}
\mathbf{a}^{\mathrm{T}}\overline{\mathbf{t}} + b &= \frac{1}{N}(\mathbf{a}^{\mathrm{T}}\mathbf{T}^{\mathrm{T}}\mathbf{1} + Nb) \\
&= \frac{1}{N}\sum_{n=1}^N(\mathbf{a}^{\mathrm{T}}\mathbf{t}_n + b) \\
&= 0
\end{aligned}
$$
すなわち$\mathbf{a}^{\mathrm{T}}\overline{\mathbf{t}} = -b$が得られる。また、$\mathbf{a}^{\mathrm{T}}\mathbf{t}_n + b = 0$を$\mathbf{T}^{\mathrm{T}}$について適用すると
$$
\mathbf{a}^{\mathrm{T}}\mathbf{T}^{\mathrm{T}} = \begin{pmatrix} -b & -b & \cdots & -b \end{pmatrix} = -b\mathbf{1}^{\mathrm{T}}
$$
となる。

以上を用いて、$(4.157)$式の線形制約が成立しているとき、モデルの予測$\mathbf{y}(\mathbf{x}^{*})$について$(4.158)$の左辺を計算してみると
$$
\begin{aligned}
\mathbf{a}^{\mathrm{T}} \mathbf{y}\left(\mathbf{x}^{\star}\right) + b &=\mathbf{a}^{\mathrm{T}} \overline{\mathbf{t}}+\mathbf{a}^{\mathrm{T}} \widehat{\mathbf{T}}^{\mathrm{T}}\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) + b \\
&= \mathbf{a}^{\mathrm{T}} \widehat{\mathbf{T}}^{\mathrm{T}}\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \\
&= \mathbf{a}^{\mathrm{T}} (\mathbf{T} - \overline{\mathbf{T}})^{\mathrm{T}}\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \\
&= (\mathbf{a}^{\mathrm{T}}\mathbf{T}^{\mathrm{T}} - \mathbf{a}^{\mathrm{T}}\overline{\mathbf{T}}^{\mathrm{T}})\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \\
&= (\mathbf{a}^{\mathrm{T}}\mathbf{T}^{\mathrm{T}} - \mathbf{a}^{\mathrm{T}}\overline{\mathbf{t}}\mathbf{1}^{\mathrm{T}})\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \\
&= (-b\mathbf{1}^{\mathrm{T}} + b\mathbf{1}^{\mathrm{T}})\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \\
&= \mathbf{0}^{\mathrm{T}} \left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \\
&= 0
\end{aligned}
$$
よって、$(4.158)$式の$\mathbf{a}^{\mathrm{T}} \mathbf{y}\left(\mathbf{x}^{\star}\right) + b = 0$が示された。

> 目標値の分布が超平面に載るという拘束条件下で誤差関数を最小二乗和にしてクラス分類すると、その予測値の分布も超平面上に載るということである。

## 演習 4.3
<div class="panel-primary">

演習問題4.2の結果を拡張し，多次元線形制約が目的変数ベクトルによって同時に満たされる場合，同じ制約が線形モデルの最小二乗予測によっても満たされることを示せ．

</div>

演習問題4.2の仮定を拡張する。
「多次元線形制約が目的変数ベクトルによって同時に満たされる場合」というのは、$(4.157)$式の線形制約が

$$
\mathbf{A}\mathbf{t}_n+\mathbf{b} = \mathbf{0}
$$

となることを表す。ここで、$\mathbf{A}$は行列で$\mathbf{b}$はベクトルであり、$\mathbf{A}$と$\mathbf{b}$の各行が1つの線形制約を表すようなものである。つまり$l$を多次元線形制約の次元数として

$$
\mathbf{A} = \begin{pmatrix}
\mathbf{a}_1^{\mathrm T} \\
\mathbf{a}_2^{\mathrm T} \\
\vdots \\
\mathbf{a}_l^{\mathrm T}
\end{pmatrix}, \quad
\mathbf{b} = \begin{pmatrix}
b_1 \\
b_2 \\
\vdots \\
b_l
\end{pmatrix}
$$

となる。

題意「同じ制約が線形モデルの最小二乗予測によっても満たされることを示せ」は、$\mathbf{A}\mathbf{t}_n+\mathbf{b} = \mathbf{0}$が成り立つ中で$\mathbf{A}\mathbf{y}(\mathbf{x}^{\star})+\mathbf{b}$が$\mathbf{0}$となることを示すことなので、これを演習問題4.2の最後の式に当てはめると

$$
\begin{aligned}
    \mathbf{A}^{\mathrm{T}} \mathbf{y}\left(\mathbf{x}^{\star}\right) + \mathbf{b} &=\mathbf{A}^{\mathrm{T}} \overline{\mathbf{t}}+\mathbf{A}^{\mathrm{T}} \widehat{\mathbf{T}}^{\mathrm{T}}\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) + \mathbf{b} \\
    &= \mathbf{A}^{\mathrm{T}} \widehat{\mathbf{T}}^{\mathrm{T}}\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \\
    &= \mathbf{A}^{\mathrm{T}} (\mathbf{T} - \overline{\mathbf{T}})^{\mathrm{T}}\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \\
    &= (\mathbf{A}^{\mathrm{T}}\mathbf{T}^{\mathrm{T}} - \mathbf{A}^{\mathrm{T}}\overline{\mathbf{T}}^{\mathrm{T}})\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \\
    &= (\mathbf{A}^{\mathrm{T}}\mathbf{T}^{\mathrm{T}} - \mathbf{A}^{\mathrm{T}}\overline{\mathbf{t}}\mathbf{1}^{\mathrm{T}})\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \\
    &= (-\mathbf{b}\mathbf{1}^{\mathrm{T}} + \mathbf{b}\mathbf{1}^{\mathrm{T}})\left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \\
    &= \mathbf{0}^{\mathrm{T}} \left(\hat{\mathbf{X}}^{\dagger}\right)^{\mathrm{T}}\left(\mathbf{x}^{\star}-\overline{\mathbf{x}}\right) \\
    &= \mathbf{0}
\end{aligned}
$$

よって$\mathbf{A}\mathbf{y}(\mathbf{x}^{\star})+\mathbf{b} = \mathbf{0}$となるので、題意が示された。

## 演習 4.4
<div class="panel-primary">

制約$\mathbf{w}^{\mathrm{T}}\mathbf{w}=1$を満たすようにラグランジュ乗数を利用し，
$$
m_{2}-m_{1}=\mathbf{w}^{\mathrm{T}}\left(\mathbf{m}_{2}-\mathbf{m}_{1}\right) \tag{4.22}
$$
によって与えられるクラス分離規準を$\mathbf{w}$に関して最大化すれば$\mathbf{w} \propto (\mathbf{m}_2-\mathbf{m}_1)$となることを示せ．

</div>


$\mathbf{w}^\mathrm{T}\mathbf{w} =1$の制約の下でラグランジュの未定乗数法を用いると

$$
\begin{aligned}
    L &= \mathbf{w}^\mathrm{T}( \mathbf{m}_2-\mathbf{m}_1)+\lambda(\mathbf{w}^\mathrm{T}\mathbf{w}-1) \\
    \frac{\partial L}{\partial \mathbf{w}}&= \mathbf{m}_2-\mathbf{m}_1+2\lambda \mathbf{w}
\end{aligned}
$$
これが$\mathbf{0}$になる点を考えると

$$
\mathbf{w} = -\frac{1}{2\lambda}(\mathbf{m}_2-\mathbf{m}_1)
$$
となり$\mathbf{w}\propto(\mathbf{m}_2-\mathbf{m}_1)$であることが示せた。

## 演習 4.5
<div class="panel-primary">

$$
y = \mathbf{w}^{\mathrm T}\mathbf{x} \tag{4.20}
$$
$$
m_k = \mathbf{w}^{\mathrm T}\mathbf{m}_k \tag{4.23}
$$
と
$$
s_{k}^{2}=\sum_{n \in \mathcal{C}_{k}}\left(y_{n}-m_{k}\right)^{2} \tag{4.24}
$$
を使って，フィッシャーの判別規準
$$
J(\mathbf{w})=\frac{\left(m_{2}-m_{1}\right)^{2}}{s_{1}^{2}+s_{2}^{2}} \tag{4.25}
$$
が
$$
J(\mathbf{w})=\frac{\mathbf{w}^{\mathrm{T}} \mathbf{S}_{\mathrm{B}} \mathbf{w}}{\mathbf{w}^{\mathrm{T}} \mathbf{S}_{\mathrm{W}} \mathbf{w}} \tag{4.26}
$$
の形で書けることを示せ．

</div>

※$\mathbf{S}_{\mathrm{B}}$と$\mathbf{S}_{\mathrm{W}}$の定義はそれぞれ$(4.27)$と$(4.28)$で与えられる。

$(4.27)$と$(4.28)$より
$$
\begin{aligned}
    \mathbf{S}_{\mathrm{B}}&=\left(\mathbf{m}_{2}-\mathbf{m}_{1}\right)\left(\mathbf{m}_{2}-\mathbf{m}_{1}\right)^{\mathrm{T}} \\
    \mathbf{S}_{\mathrm{W}}&=\sum_{n \in \mathcal{C}_{1}}\left(\mathbf{x}_{n}-\mathbf{m}_{1}\right)\left(\mathbf{x}_{n}-\mathbf{m}_{1}\right)^{\mathrm{T}}+\sum_{n \in \mathcal{C}_{2}}\left(\mathbf{x}_{n}-\mathbf{m}_{2}\right)\left(\mathbf{x}_{n}-\mathbf{m}_{2}\right)^{\mathrm{T}}
\end{aligned}
$$
である。これを用いて$(4.25)$を変形していくと

$$
\begin{aligned}
    J(\mathbf{w}) &=\frac{\left(m_{2}-m_{1}\right)^{2}}{s_{1}^{2}+s_{2}^{2}} \\
    &=\frac{\left(\mathbf{w}^{\mathrm{T}}\left(\mathbf{m}_{2}-\mathbf{m}_{1}\right)\right)^{2}}{\sum_{n \in \mathcal{C}_{1}}\left(\mathbf{w}^{\mathrm{T}}\left(\mathbf{x}_{n}-\mathbf{m}_{1}\right)\right)^{2}+\sum_{n \in \mathcal{C}_{2}}\left(\mathbf{w}^{\mathrm{T}}\left(\mathbf{x}_{n}-\mathbf{m}_{2}\right)\right)^{2}} \\
    &=\frac{\left(\mathbf{w}^{\mathrm{T}}\left(\mathbf{m}_{2}-\mathbf{m}_{1}\right)\right)\left(\left(\mathbf{m}_{2}-\mathbf{m}_{1}\right)^{\mathrm{T}} \mathbf{w}\right)}{\sum_{n \in \mathcal{C}_{1}}\left(\mathbf{w}^{\mathrm{T}}\left(\mathbf{x}_{n}-\mathbf{m}_{1}\right)\left(\mathbf{x}_{n}-\mathbf{m}_{1}\right)^{\mathrm{T}} \mathbf{w}\right)+\sum_{n \in \mathcal{C}_{2}}\left(\mathbf{w}^{\mathrm{T}}\left(\mathbf{x}_{n}-\mathbf{m}_{2}\right)\left(\mathbf{x}_{n}-\mathbf{m}_{2}\right)^{\mathrm{T}} \mathbf{w}\right)} \\
    &=\frac{\mathbf{w}^{\mathrm{T}} \mathbf{S}_{\mathrm B} \mathbf{w}}{\mathbf{w}^{\mathrm{T}}\left\{\sum_{n \in \mathcal{C}_{1}}\left(\mathbf{x}_{n}-\mathbf{m}_{1}\right)\left(\mathbf{x}_{n}-\mathbf{m}_{1}\right)^{\mathrm{T}}+\sum_{n \in \mathcal{C}_{2}}\left(\mathbf{x}_{n}-\mathbf{m}_{2}\right)\left(\mathbf{x}_{n}-\mathbf{m}_{2}\right)^{\mathrm{T}}\right\} \mathbf{w}} \\
    &=\frac{\mathbf{w}^{\mathrm{T}} \mathbf{S}_{\mathrm B} \mathbf{w}}{\mathbf{w}^{\mathrm{T}} \mathbf{S}_{\mathrm W} \mathbf{w}}
\end{aligned}
$$

となり$(4.26)$式が得られる。

## 演習 4.6
<div class="panel-primary">

$$
\mathbf{S}_{\mathbf{B}}=\left(\mathbf{m}_{2}-\mathbf{m}_{1}\right)\left(\mathbf{m}_{2}-\mathbf{m}_{1}\right)^{\mathrm{T}} \tag{4.27}
$$
で与えられるクラス間共分散行列と
$$
\mathbf{S}_{\mathbf{W}}=\sum_{n \in \mathcal{C}_{1}}\left(\mathbf{x}_{n}-\mathbf{m}_{1}\right)\left(\mathbf{x}_{n}-\mathbf{m}_{1}\right)^{\mathrm{T}}+\sum_{n \in \mathcal{C}_{2}}\left(\mathbf{x}_{n}-\mathbf{m}_{2}\right)\left(\mathbf{x}_{n}-\mathbf{m}_{2}\right)^{\mathrm{T}} \tag{4.28}
$$
で与えられるクラス内共分散行列のそれぞれの定義と，
$$
w_0 = -\mathbf{w}^{\mathrm T}\mathbf{m} \tag{4.34}
$$
$$
\mathbf{m}=\frac{1}{N} \sum_{n=1}^{N} \mathbf{x}_{n}=\frac{1}{N}\left(N_{1} \mathbf{m}_{1}+N_{2} \mathbf{m}_{2}\right) \tag{4.36}
$$
および4.1.5節で述べた目的値を使って，二乗和誤差関数を最小化する表現
$$
\sum_{n=1}^{N}\left(\mathbf{w}^{\mathrm{T}} \mathbf{x}_{n}+w_{0}-t_{n}\right) \mathbf{x}_{n}=0 \tag{4.33}
$$
が
$$
\left(\mathbf{S}_{\mathbf{W}}+\frac{N_{1} N_{2}}{N} \mathbf{S}_{\mathbf{B}}\right) \mathbf{w}=N\left(\mathbf{m}_{1}-\mathbf{m}_{2}\right) \tag{4.37}
$$
の形で書けることを示せ．

</div>

※ 簡単な代数演算をして……と書かれてあるが、計算量的には簡単じゃない。まず$\displaystyle \sum_{n=1}^N t_n\mathbf{x}_n = N(\mathbf{m}_1-\mathbf{m}_2)$となることを利用して、残りの部分で$(\cdot)\mathbf{w}$の式に書き直せるようにスカラー部分とベクトル部分を入れ替える技（($\mathbf{a}^{\mathrm{T}}\mathbf{b})\mathbf{c} = \mathbf{c}\mathbf{b}^{\mathrm{T}}\mathbf{a}$）を使う。

まず$(4.33)$の$\displaystyle \sum_{n=1}^N t_n\mathbf{x}_n$について変形すると
$$
\begin{aligned}
\sum_{n=1}^N t_n\mathbf{x}_n &= \frac{N}{N_1}\sum_{n \in \mathcal{C}_1}\mathbf{x}_n - \frac{N}{N_2}\sum_{n \in \mathcal{C}_2}\mathbf{x}_n \\
&=\frac{N}{N_1}(N_1\mathbf{m}_1)-\frac{N}{N_2}(N_2\mathbf{m}_2) \\
&=N(\mathbf{m}_1 - \mathbf{m}_2)
\end{aligned}
$$
となる。これは$(4.37)$式の右辺になっているので、
$$
\sum_{n=1}^{N}\left(\mathbf{w}^{\mathrm{T}} \mathbf{x}_{n}+w_{0}\right)\mathbf{x}_{n} = \left(\mathbf{S}_{\mathrm{W}}+\frac{N_{1} N_{2}}{N} \mathbf{S}_{\mathrm{B}}\right) \mathbf{w}
$$
となることを示せれば良い。
$$
\begin{aligned}
    \sum_{n=1}^{N}\left(\mathbf{w}^{\mathrm{T}} \mathbf{x}_{n}+w_{0}\right) \mathbf{x}_{n}&=\sum_{n=1}^{N}\left(\mathbf{w}^{\mathrm{T}} \mathbf{x}_{n}-\mathbf{w}^{\mathrm{T}} \mathbf{m}\right) \mathbf{x}_{n} \\
    &=\sum_{n=1}^{N}\left\{\left(\mathbf{x}_{n} \mathbf{x}_{n}^{\mathrm{T}}-\mathbf{x}_{n} \mathbf{m}^{\mathrm{T}}\right) \mathbf{w}\right\} \\
    &= \sum_{n \in \mathcal{C}_{1}}\left\{\left(\mathbf{x}_{n} \mathbf{x}_{n}^{\mathrm{T}}-\mathbf{x}_{n} \mathbf{m}^{\mathrm{T}}\right) \mathbf{w}\right\} + \sum_{m \in \mathcal{C}_{2}}\left\{\left(\mathbf{x}_{m} \mathbf{x}_{m}^{\mathrm{T}}-\mathbf{x}_{m} \mathbf{m}^{\mathrm{T}}\right) \mathbf{w}\right\} \\
    &=\left(\sum_{n \in \mathcal{C}_{1}} \mathbf{x}_{n} \mathbf{x}_{n}^{\mathrm{T}}-N_{1} \mathbf{m}_{1} \mathbf{m}^{\mathrm{T}}\right) \mathbf{w} + \left(\sum_{m \in \mathcal{C}_{2}} \mathbf{x}_{m} \mathbf{x}_{m}^{\mathrm{T}}-N_{2} \mathbf{m}_{2} \mathbf{m}^{\mathrm{T}}\right) \mathbf{w} \\
    &=\left(\sum_{n \in \mathcal{C}_{1}} \mathbf{x}_{n} \mathbf{x}_{n}^{\mathrm{T}}+\sum_{m \in \mathcal{C}_{2}} \mathbf{x}_{m} \mathbf{x}_{m}^{\mathrm{T}}-\left(N_{1} \mathbf{m}_{1}+N_{2} \mathbf{m}_{2}\right) \mathbf{m}^{\mathrm{T}}\right) \mathbf{w}
\end{aligned}
$$
となる。

ここで、$(4.27)$と$(4.28)$の展開した形を用意しておく。$(4.27)$は

$$
\begin{aligned}
    \mathbf{S}_{\mathrm{B}} &= \left(\mathbf{m}_{2}-\mathbf{m}_{1}\right)\left(\mathbf{m}_{2}-\mathbf{m}_{1}\right)^{\mathrm{T}} \\
    &=\mathbf{m}_2\mathbf{m}_2^{\mathrm{T}}-\mathbf{m}_2\mathbf{m}_1^{\mathrm{T}}-\mathbf{m}_1\mathbf{m}_2^{\mathrm{T}}+\mathbf{m}_1\mathbf{m}_1^{\mathrm{T}}
\end{aligned}
$$

となり、$(4.28)$は

\begin{aligned}
    \mathbf{S}_{\mathrm W} = \sum_{k=1}^2 \sum_{i \in \mathcal{C}_{k}}\left(\mathbf{x}_{i}-\mathbf{m}_{k}\right)\left(\mathbf{x}_{i}-\mathbf{m}_{k}\right)^{\mathrm{T}} &= \sum_{k=1}^2 \sum_{i \in \mathcal{C}_{k}}\left(\mathbf{x}_{i} \mathbf{x}_{i}^{\mathrm{T}}-\mathbf{x}_{i} \mathbf{m}_{k}^{\mathrm{T}}-\mathbf{m}_{k} \mathbf{x}_{i}^{\mathrm{T}}+\mathbf{m}_{k} \mathbf{m}_{k}^{\mathrm{T}}\right) \\
    &=\sum_{k=1}^2  \left( \sum_{i \in \mathcal{C}_{k}} \mathbf{x}_{i} \mathbf{x}_{i}^{\mathrm{T}}-N_{k} \mathbf{m}_{k} \mathbf{m}_{k}^{\mathrm{T}}\right) \\
    &=\sum_{n \in \mathcal{C}_{1}} \mathbf{x}_{n} \mathbf{x}_{n}^{\mathrm{T}} + \sum_{m \in \mathcal{C}_{2}} \mathbf{x}_{m} \mathbf{x}_{m}^{\mathrm{T}} - N_1\mathbf{m}_{1} \mathbf{m}_{1}^{\mathrm{T}} - N_2\mathbf{m}_{2} \mathbf{m}_{2}^{\mathrm{T}}
\end{aligned}

であるから、これらを利用して書き直すと
$$
\begin{aligned}
    &\quad \left(\sum_{n \in \mathcal{C}_{1}} \mathbf{x}_{n} \mathbf{x}_{n}^{\mathrm{T}}+\sum_{m \in \mathcal{C}_{2}} \mathbf{x}_{m} \mathbf{x}_{m}^{\mathrm{T}}-\left(N_{1} \mathbf{m}_{1}+N_{2} \mathbf{m}_{2}\right) \mathbf{m}^{\mathrm{T}}\right) \mathbf{w} \\
    &= \left(\mathbf{S}_{\mathbf{w}}+N_{1} \mathbf{m}_{1} \mathbf{m}_{1}^{\mathrm{T}}+N_{2} \mathbf{m}_{2} \mathbf{m}_{2}^{\mathrm{T}}-\left(N_{1} \mathbf{m}_{1}+N_{2} \mathbf{m}_{2}\right) \frac{1}{N}\left(N_{1} \mathbf{m}_{1}+N_{2} \mathbf{m}_{2}\right)^{\mathrm{T}}\right) \mathbf{w} \\
    &= \left(\mathbf{S}_{\mathbf{w}}+\left(N_{1}-\frac{N_{1}^{2}}{N}\right) \mathbf{m}_{1} \mathbf{m}_{1}^{\mathrm{T}}-\frac{N_{1} N_{2}}{N}\left(\mathbf{m}_{1} \mathbf{m}_{2}^{\mathrm{T}}+\mathbf{m}_{2} \mathbf{m}_{1}^{\mathrm{T}}\right) +\left(N_{2}-\frac{N_{2}^{2}}{N}\right) \mathbf{m}_{2} \mathbf{m}_{2}^{\mathrm{T}}\right) \mathbf{w} \\
    &= \left(\mathbf{S}_{\mathbf{w}}+\frac{\left(N_{1}+N_{2}\right) N_{1}-N_{1}^{2}}{N} \mathbf{m}_{1} \mathbf{m}_{1}^{\mathrm{T}}-\frac{N_{1} N_{2}}{N}\left(\mathbf{m}_{1} \mathbf{m}_{2}^{\mathrm{T}}+\mathbf{m}_{2} \mathbf{m}_{1}^{\mathrm{T}}\right) +\frac{\left(N_{1}+N_{2}\right) N_{2}-N_{2}^{2}}{N} \mathbf{m}_{2} \mathbf{m}_{2}^{\mathrm{T}}\right) \mathbf{w} \\
    &= \left(\mathbf{S}_{\mathbf{w}}+\frac{N_{1} N_{2}}{N}\left(\mathbf{m}_{1} \mathbf{m}_{1}^{\mathrm{T}}-\mathbf{m}_{1} \mathbf{m}_{2}^{\mathrm{T}}-\mathbf{m}_{2} \mathbf{m}_{1}^{\mathrm{T}}+\mathbf{m}_{2} \mathbf{m}_{2}^{\mathrm{T}}\right)\right) \mathbf{w} \\
    &= \left(\mathbf{S}_{\mathbf{w}}+\frac{N_{1} N_{2}}{N} \mathbf{S}_{\mathrm{B}}\right) \mathbf{w}
\end{aligned}
$$
となる。
以上から、$(4.33)$式が
$$
\left(\mathbf{S}_{\mathbf{W}}+\frac{N_{1} N_{2}}{N} \mathbf{S}_{\mathbf{B}}\right) \mathbf{w}=N\left(\mathbf{m}_{1}-\mathbf{m}_{2}\right) \tag{4.37}
$$
と書き直せることが示された。

## 演習 4.7
<div class="panel-primary">

ロジスティックシグモイド関数
$$
\sigma(a) = \frac{1}{1+ \exp(-a)} \tag{4.59}
$$
が$\sigma(-a) = 1-\sigma(a)$を満たすことを示せ．また，その逆関数が$\sigma^{-1}(y) = \ln {y/(1-y)}$で与えられることを示せ．

</div>

$$\begin{aligned} \sigma(-a) &= \frac{1}{1+\exp(a)} \\&= \frac{1}{1+\left\{ 1/\exp(-a) \right\}} \\&= \frac{\exp(-a)}{\exp(-a)+1} \\&= 1-\frac{1}{1+\exp(-a)} \\&= 1-\sigma(a) \end{aligned}$$

逆関数について$y=\sigma(a)$としたとき

$$y=\frac{1}{1+\exp(-a)}$$
$$\begin{aligned} \exp(-a)&=\frac{1}{y}-1 \\&= \frac{1-y}{y} \end{aligned}$$
$$a=\ln(\frac{y}{1-y})$$

よって

$$\sigma^{-1}(y)=\ln(\frac{y}{1-y})$$

## 演習 4.8
<div class="panel-primary">

$$
\begin{aligned}
p\left(\mathcal{C}_{1} \mid \mathbf{x}\right) &=\frac{p\left(\mathbf{x} \mid \mathcal{C}_{1}\right) p\left(\mathcal{C}_{1}\right)}{p\left(\mathbf{x} \mid \mathcal{C}_{1}\right) p\left(\mathcal{C}_{1}\right)+p\left(\mathbf{x} \mid \mathcal{C}_{2}\right) p\left(\mathcal{C}_{2}\right)} \\
&=\frac{1}{1+\exp (-a)}=\sigma(a)
\end{aligned} \tag{4.57}
$$
と
$$
a=\ln \frac{p\left(\mathbf{x} \mid \mathcal{C}_{1}\right) p\left(\mathcal{C}_{1}\right)}{p\left(\mathbf{x} \mid \mathcal{C}_{2}\right) p\left(\mathcal{C}_{2}\right)} \tag{4.58}
$$
を使って，ガウス確率密度分布を用いた2クラス生成モデルにおけるクラスの事後確率に対する
$$
p\left(\mathcal{C}_{1} \mid \mathbf{x}\right)=\sigma\left(\mathbf{w}^{\mathrm{T}} \mathbf{x}+w_{0}\right) \tag{4.65}
$$
の結果を導出せよ．また，パラメータ$\mathbf{w}$と$w_0$に対する結果
$$
\mathbf{w} =\mathbf{\Sigma}^{-1}\left(\boldsymbol{\mu}_{1}-\boldsymbol{\mu}_{2}\right) \tag{4.66}
$$
と
$$
w_{0} =-\frac{1}{2} \boldsymbol{\mu}_{1}^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_{1}+\frac{1}{2} \boldsymbol{\mu}_{2}^{\mathrm{T}} \boldsymbol{\Sigma}^{-1} \boldsymbol{\mu}_{2}+\ln \frac{p\left(\mathcal{C}_{1}\right)}{p\left(\mathcal{C}_{2}\right)} \tag{4.67}
$$
を検証せよ．

</div>

※
$(4.65)$式を$(4.57)$, $(4.58)$, それから

$$
p\left(\mathbf{x} \mid \mathcal{C}_{k}\right)=\frac{1}{(2 \pi)^{D / 2}} \frac{1}{|\mathbf{\Sigma}|^{1 / 2}} \exp \left\{-\frac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}_{k}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\mathbf{x}-\boldsymbol{\mu}_{k}\right)\right\} \tag{4.64}
$$
を利用して変形していく。
$$
\begin{aligned}
p(\mathcal{C}_1\mid \mathbf{x}) &= \sigma\left(\ln \frac{p\left(\mathbf{x}\mid \mathcal{C}_{1}\right) p\left(\mathcal{C}_{1}\right)}{p\left(\mathbf{x} \mid \mathcal{C}_{2}\right) p\left(\mathcal{C}_{2}\right)}\right) \\
&=\sigma\left(\ln \frac{\exp \left\{ -\frac{1}{2} \left(\mathbf{x}-\boldsymbol{\mu}_{1}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\mathbf{x}-\boldsymbol{\mu}_{1}\right)\right\}}{\exp \left\{ -\frac{1}{2}  \left(\mathbf{x}-\boldsymbol{\mu}_{2}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\mathbf{x}-\boldsymbol{\mu}_{2}\right)\right\}}+\ln \frac{p\left(\mathcal{C}_{1}\right)}{p\left(\mathcal{C}_{2}\right)}\right) \\
&=\sigma\left(-\frac{1}{2}\left\{\left(\mathbf{x}-\boldsymbol{\mu}_{1}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\mathbf{x}-\boldsymbol{\mu}_{1}\right)\right\}+\frac{1}{2}\left\{\left(\mathbf{x}-\boldsymbol{\mu}_{2}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\mathbf{x}-\boldsymbol{\mu}_{2}\right)\right\}+\ln \frac{p\left(\mathcal{C}_{1}\right)}{p\left(\mathcal{C}_{2}\right)}\right)\\
&=\sigma\left(\boldsymbol{\mu}_{1}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \mathbf{x}-\frac{1}{2} \boldsymbol{\mu}_{1}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \boldsymbol{\mu}_{1}-\boldsymbol{\mu}_{2}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \mathbf{x}+\frac{1}{2} \boldsymbol{\mu}_{2}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \boldsymbol{\mu}_{2}+\ln \frac{p\left(\mathcal{C}_{1}\right)}{p\left(\mathcal{C}_{2}\right)}\right)\\
&=\sigma\left(\left((\mathbf{\Sigma}^{-1})^{\mathrm{T}}\left(\boldsymbol{\mu}_{1}-\boldsymbol{\mu}_{2}\right)\right)^{\mathrm{T}} \mathbf{x}-\frac{1}{2} \boldsymbol{\mu}_{1}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \boldsymbol{\mu}_{1}+\frac{1}{2} \boldsymbol{\mu}_{2}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \boldsymbol{\mu}_{2}+\ln \frac{p\left(\mathcal{C}_{1}\right)}{p\left(\mathcal{C}_{2}\right)}\right)
\end{aligned}
$$

$(\mathbf{\Sigma}^{-1})^{\mathrm{T}} = \mathbf{\Sigma}^{-1}$なので、$\mathbf{w} = \mathbf{\Sigma}^{-1}(\boldsymbol{\mu}_1 - \boldsymbol{\mu}_2) \quad (4.66)$と$\displaystyle w_0 = -\frac{1}{2} \boldsymbol{\mu}_{1}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \boldsymbol{\mu}_{1}+\frac{1}{2} \boldsymbol{\mu}_{2}^{\mathrm{T}} \mathbf{\Sigma}^{-1} \boldsymbol{\mu}_{2}+\ln \frac{p\left(\mathcal{C}_{1}\right)}{p\left(\mathcal{C}_{2}\right)} \quad (4.67)$の定義を用いると,

$$
p(\mathcal{C}_1\mid \mathbf{x}) = \sigma(\mathbf{w}^{\mathrm T}\mathbf{x}+w_0) \tag{4.65}
$$

と書ける。

## 演習 4.9
<div class="panel-primary">

クラスの事前確率$p(\mathcal{C}_k)=\pi_k$と一般的なクラスの条件付き確率密度$p(\boldsymbol{\phi}\mid \mathcal{C}_k)$によって定義される$K$クラス分類問題の生成モデルを考える．ここで$\boldsymbol{\phi}$は入力特徴ベクトルである．学習データ集合$\{ \boldsymbol{\phi}_n, \mathbf{t}_n \}$が与えられたと仮定する．ここで，$n=1,\ldots,N$であり，$\mathbf{t}_n$は，1-of-$K$符号化法を使う長さ$K$の2値目的変数ベクトルである．つまり，パターン$n$のクラスが$\mathcal{C}_k$である場合，2値目的変数ベクトルは構成要素$t_{nj} = I_{jk}$を持つ．データがこのモデルから独立に抽出されると仮定すると，その事前確率に対する最尤解が以下の式で与えられることを示せ．

$$
\pi_k = \frac{N_k}{N} \tag{4.159}
$$

ここで，$N_k$はクラス$\mathcal{C}_k$に割り当てられるデータの個数である．

</div>

※ 「パターン$n$のクラスが$\mathcal{C}_k$である場合，2値目的変数ベクトルは構成要素$t_{nj} = I_{jk}$を持つ」というのは、$j=k$であれば$t_{nk} = I_{kk} = 1$, $j\neq k$であれば$0$ということである。

2クラス分類のときの尤度関数

$$
p(\mathbf{t},\mathbf{X}\mid \pi, \boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\mathbf{\Sigma}) = \prod_{n=1}^{N}[\pi \mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_1,\mathbf{\Sigma})]^{t_n}[(1-\pi)\mathcal{N}(\mathbf{x}_n \mid \boldsymbol{\mu}_2,\mathbf{\Sigma})]^{1-t_n}  \tag{4.71}
$$

を多クラスに拡張したい。

$$
\mathbf{I}_k=\begin{pmatrix}
0 \\
0 \\
\vdots \\
1 \\
\vdots \\
0 \\
\end{pmatrix}
$$


という$k$番目の要素が$1$で他が$0$であるベクトルを用いて、多クラスの尤度関数$p(\{\mathbf{t}_n, \boldsymbol{\phi}_n \}\mid \{ \pi_k \} )$は以下のようになる。（$\mathbf{I}_k^\mathrm{T}\mathbf{t}_n = t_{nk}$である。）

$$
\begin{aligned}
    p(\{\mathbf{t}_n, \boldsymbol{\phi}_n \}\mid \{ \pi_k \} ) = \prod_{n=1}^{N}\prod_{k=1}^{K}[\pi_k p(\boldsymbol{\phi}_n\mid \mathcal{C}_k)]^{\mathbf{I}_k^\mathrm{T}\mathbf{t}_n}
\end{aligned}
$$

最大化するにあたって、これの対数尤度関数をとる。


$$
\begin{aligned}
    \ln p(\{\mathbf{t}_n, \boldsymbol{\phi}_n \}\mid \{ \pi_k \} ) = \sum_{n=1}^{N}\sum_{k=1}^{K}\mathbf{I}_k ^\mathrm{T}\mathbf{t}_n  (\ln\pi_k + p(\boldsymbol{\phi}_n\mid \mathcal{C}_k))
\end{aligned}
$$

ここで、最大化に関係する部分だけ取り出し、$\sum_{k=1}^{K} \pi_k = 1$であることを考慮してラグランジュの未定乗数法を用いる(あまり本来の使用法と違う気もしますが使えるので)と

$$
\begin{aligned}
    L = \sum_{n=1}^{N}\sum_{k=1}^{K}\mathbf{I}_k ^\mathrm{T}\mathbf{t}_n \ln\pi_k -\lambda \left\{ \left(\sum_{k=1}^{K}\pi_k \right) -1 \right\} \\
\end{aligned}
$$

すべての$k$における$\pi_k$と$\lambda$で$L$を微分し、$0$を取ると

$$
\begin{aligned}
    \frac{\partial L}{\partial \pi_k} &= \sum_{n=1}^{N}\frac{\mathbf{I}_k ^\mathrm{T} \mathbf{t}_n}{\pi_k}-\lambda = 0 \\
    \frac{\partial L}{\partial \lambda} &= \left( \sum_{k=1}^{K}\pi_k \right) -1 = 0
\end{aligned}
$$

となる。上式の分母を払うと

$$
\lambda\pi_k = \sum_{n=1}^N \mathbf{I}_k ^\mathrm{T} \mathbf{t}_n
$$

ここで$\sum_{n=1}^N \mathbf{I}_k ^\mathrm{T} \mathbf{t}_n$は定義からクラス$\mathcal{C}_k$に割り当てられるデータの個数$N_k$であるから、$\pi_k = N_k/\lambda$となる。

一方、すべての$k$について足し合わせると

$$
\begin{aligned}
    \sum_{k=1}^K \lambda\pi_k &= \sum_{k=1}^K \sum_{n=1}^N \mathbf{I}_k ^\mathrm{T} \mathbf{t}_n \\
    \lambda \left( \sum_{k=1}^K \pi_k \right) &= \sum_{k=1}^K N_k \\
    \lambda &= N\quad \left(\because \frac{\partial L}{\partial \lambda} = 0から \left( \sum_{k=1}^{K}\pi_k \right) = 1 \right)
\end{aligned}
$$

より

$$
\pi_k = \frac{N_k}{N}
$$
となり、示された。

## 演習 4.10
<div class="panel-primary">

演習問題4.9の分類モデルを考え，クラスの条件付き確率密度が共通の共分散行列を持つガウス分布によって与えられる，つまり，以下の式が成立すると仮定する．

$$
p\left(\boldsymbol{\phi} \mid \mathcal{C}_{k}\right)=\mathcal{N}\left(\boldsymbol{\phi} \mid \boldsymbol{\mu}_{k}, \mathbf{\Sigma} \right) \tag{4.160}
$$

クラス$\mathcal{C}_k$のガウス分布の平均に対する最尤解が以下の式で与えられることを示せ．

$$
\boldsymbol{\mu}_k=\frac{1}{N_{k}}\sum_{n=1}^N t_{nk}\boldsymbol{\phi}_n  \tag{4.161}
$$

これは，クラス$\mathcal{C}_k$に割り当てられる特徴ベクトルの平均を表す．同様に，共通の共分散に対する最尤解が，以下の式で与えられることを示せ．

$$
\mathbf{\Sigma}=\sum_{k=1}^{K} \frac{N_{k}}{N} \mathbf{S}_{k} \tag{4.162}
$$

ここで

$$
\mathbf{S}_{k}=\frac{1}{N_{k}} \sum_{n=1}^{N} t_{nk}\left(\boldsymbol{\phi}_{n}-\boldsymbol{\mu}_{k}\right)\left(\boldsymbol{\phi}_{n}-\boldsymbol{\mu}_{k}\right)^{\mathrm{T}} \tag{4.163}
$$

である．よって，$\mathbf{\Sigma}$は，各クラスのデータの共分散の重み付き平均で与えられる．重み付け係数はクラスの事前確率で与えられる．

</div>

※ 演習問題4.9での$p(\boldsymbol{\phi}_n\mid \mathcal{C}_k)$に正規分布$\displaystyle \mathcal{N}(\boldsymbol{\phi}_n \mid \boldsymbol{\mu}_k,\mathbf{\Sigma}) = \frac{1}{(2\pi)^{D/2}|\mathbf{\Sigma}|^{1/2}} \exp\left\{ -\frac{1}{2}(\boldsymbol{\phi}_n - \boldsymbol{\mu}_k)^{\mathrm T}\mathbf{\Sigma}^{-1}(\boldsymbol{\phi}_n - \boldsymbol{\mu}_k) \right\}$を仮定した場合になる。

演習問題4.9を利用すると尤度関数は
$$
p\left(\left\{\mathbf{t}_{n}, \boldsymbol{\phi}_{n}\right\} \mid\left\{\pi_{k}\right\}\right)=\prod_{n=1}^{N} \prod_{k=1}^{K}\left[\pi_{k} \mathcal{N}(\boldsymbol{\phi}_n \mid \boldsymbol{\mu}_k,\mathbf{\Sigma})p(\mathcal{C}_k)\right]^{t_{nk}}
$$
であり、対数尤度関数は$\boldsymbol{\mu}_k$と$\mathbf{\Sigma}$に依存する項を抜き出して残りを$\textrm{const.}$とすると
$$
\begin{aligned}
    \ln p\left(\left\{\mathbf{t}_{n}, \boldsymbol{\phi}_{n}\right\} \mid\left\{\pi_{k}\right\}\right) &=\sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk}\left(
    \ln \pi_k +\ln \mathcal{N}(\boldsymbol{\phi}_n \mid \boldsymbol{\mu}_k,\mathbf{\Sigma})+\ln p(\mathcal{C}_k)\right) \\
    &=-\frac{1}{2}\sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk}\left(
        \ln |\mathbf{\Sigma}| + (\boldsymbol{\phi}_n - \boldsymbol{\mu}_k)^{\mathrm T}\mathbf{\Sigma}^{-1}(\boldsymbol{\phi}_n - \boldsymbol{\mu}_k)
        \right) + \textrm{const.}
\end{aligned}
$$
$\boldsymbol{\mu}_k$についての微分を$0$とすると(※ここで$k$は特定の値をとる)
$$
\begin{aligned}
\frac{\partial}{\partial \boldsymbol{\mu}_{k}} \ln p
&=-\frac{1}{2} \sum_{n=1}^{N}\left\{t_{n k} \frac{\partial}{\partial \boldsymbol{\mu}_{k}}\left(\boldsymbol{\phi}_{n}-\boldsymbol{\mu}_{k}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\boldsymbol{\phi}_{n}-\boldsymbol{\mu}_{k}\right)\right\} \\
&=\sum_{n=1}^{N}\left\{t_{n k} \mathbf{\Sigma}^{-1}\left(\boldsymbol{\phi}_{n}-\boldsymbol{\mu}_{k}\right)\right\} \\
&=\mathbf{\Sigma}^{-1} \sum_{n=1}^{N}\left\{t_{n k}\left(\boldsymbol{\phi}_{n}-\boldsymbol{\mu}_{k}\right)\right\} \\
&=\mathbf{\Sigma}^{-1}\left(\sum_{n=1}^{N} t_{n k} \boldsymbol{\phi}_{n}-\sum_{n=1}^{N} t_{n k} \boldsymbol{\mu}_{k}\right) \\
&=\mathbf{\Sigma}^{-1}\left(\sum_{n=1}^{N} t_{n k} \boldsymbol{\phi}_{n}-N_{k} \boldsymbol{\mu}_{k}\right) = 0
\end{aligned}
$$
以上から
$$
\boldsymbol{\mu}_{k}=\frac{1}{N_{k}} \sum_{n=1}^{N} t_{n k} \boldsymbol{\phi}_{n} \tag{4.161}
$$
が求まる。

続いて$\mathbf{S}_k$をうまく使えるように対数尤度関数を変形すると、
$$
\begin{aligned}
\ln p &=\sum_{n=1}^{N} \sum_{k=1}^{K}\left\{t_{n k} \ln \mathcal{N}\left(\boldsymbol{\phi}_{n} \mid \boldsymbol{\mu}_{k}, \mathbf{\Sigma}\right)\right\} \\
&=\sum_{n=1}^{N} \sum_{k=1}^{K}\left\{t_{n k}\left[-\frac{1}{2} \ln |\mathbf{\Sigma}|-\frac{1}{2}\left(\boldsymbol{\phi}_{n}-\boldsymbol{\mu}_{k}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\boldsymbol{\phi}_{n}-\boldsymbol{\mu}_{k}\right)\right]\right\}+\mathrm{const.} \\
&=-\frac{1}{2} \sum_{n=1}^{N} \sum_{k=1}^{K} t_{n k} \ln |\mathbf{\Sigma}|-\frac{1}{2} \sum_{n=1}^{N} \sum_{k=1}^{K} t_{n k}\left(\boldsymbol{\phi}_{n}-\boldsymbol{\mu}_{k}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\boldsymbol{\phi}_{n}-\boldsymbol{\mu}_{k}\right)+\mathrm{const.} \\
&=-\frac{N}{2} \ln |\mathbf{\Sigma}|-\frac{1}{2} \sum_{n=1}^{N} \sum_{k=1}^{K} t_{n k}\left(\boldsymbol{\phi}_{n}-\boldsymbol{\mu}_{k}\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1}\left(\boldsymbol{\phi}_{n}-\boldsymbol{\mu}_{k}\right)+\mathrm{const.} \\
&=-\frac{N}{2} \ln |\mathbf{\Sigma}|-\frac{1}{2} \sum_{n=1}^{N} \sum_{k=1}^{K} t_{n k} \operatorname{Tr}\left[\mathbf{\Sigma}^{-1}\left(\boldsymbol{\phi}_{n}-\boldsymbol{\mu}_{k}\right)\left(\boldsymbol{\phi}_{n}-\boldsymbol{\mu}_{k}\right)^{\mathrm{T}}\right]+\mathrm{const.} \\
&=-\frac{N}{2} \ln |\mathbf{\Sigma}|-\frac{1}{2} \sum_{k=1}^{K}\left( N_k \sum_{n=1}^{N} \frac{1}{N_k} t_{nk} \operatorname{Tr}\left[\mathbf{\Sigma}^{-1}\left(\boldsymbol{\phi}_{n}-\boldsymbol{\mu}_{k}\right)\left(\boldsymbol{\phi}_{n}-\boldsymbol{\mu}_{k}\right)^{\mathrm{T}}\right]\right) +\mathrm{const.} \\
&=-\frac{N}{2} \ln |\mathbf{\Sigma}|-\frac{1}{2} \sum_{k=1}^{K} N_k \left( \operatorname{Tr}\left[ \mathbf{\Sigma}^{-1} \frac{1}{N_k} \sum_{n=1}^{N} t_{nk} \left(\boldsymbol{\phi}_{n}-\boldsymbol{\mu}_{k}\right)\left(\boldsymbol{\phi}_{n}-\boldsymbol{\mu}_{k}\right)^{\mathrm{T}}\right]\right) +\mathrm{const.} \\
&=-\frac{N}{2} \ln |\mathbf{\Sigma}|-\frac{1}{2} \sum_{k=1}^K N_k \operatorname{Tr}\left[\mathbf{\Sigma}^{-1} \mathbf{S}_k\right]+\mathrm{const.}
\end{aligned}
$$

$\mathbf{\Sigma}$についての微分をとって$0$とする。[演習問題2.34](../PRML%E3%81%AE%E6%BC%94%E7%BF%92%E5%95%8F%E9%A1%8C%E8%A7%A3%E7%AD%94%E9%9B%86%20%E7%AC%AC2%E7%AB%A0%202.61%E3%81%BE%E3%81%A7#%E6%BC%94%E7%BF%92%202.34)と同様に変形していくと、

$$
\begin{aligned}
\frac{\partial}{\partial \mathbf{\Sigma}}\ln p &= -\frac{N}{2}(\mathbf{\Sigma}^{-1})^{\mathrm T} - \frac{1}{2}\sum_{k=1}^K N_k \frac{\partial}{\partial \mathbf{\Sigma}}\operatorname{Tr}\left[\mathbf{\Sigma}^{-1} \mathbf{S}_k\right] \\
&= -\frac{N}{2}(\mathbf{\Sigma}^{-1})^{\mathrm T} - \frac{1}{2}\sum_{k=1}^K N_k (\mathbf{\Sigma}^{-1} \mathbf{S}_k \mathbf{\Sigma}^{-1})^{\mathrm T} = 0
\end{aligned}
$$

転置をとって移項すると

$$
\mathbf{\Sigma}^{-1} = \sum_{k=1}^K \frac{N_k}{N} \mathbf{\Sigma}^{-1} \mathbf{S}_k \mathbf{\Sigma}^{-1}
$$

左と右からそれぞれ$\mathbf{\Sigma}$をかければ
$$
\mathbf{\Sigma} = \sum_{k=1}^K \frac{N_k}{N}\mathbf{S}_k \tag{4.162}
$$
を得る。

## 演習 4.11
<div class="panel-primary">

各々が$L$の離散状態を取ることのできる$M$個の要素を持つ特徴ベクトル$\boldsymbol{\phi}$に対する$K$クラスの分類問題を考える．1-of-$L$符号化法によって成分の値は表現されるとする．さらに，クラス$\mathcal{C}_k$に対し$\boldsymbol{\phi}$の$M$個の成分が独立であり，クラスの条件付き確率密度は特徴ベクトルの要素に分解できると仮定する．クラスの事後確率を記述しているソフトマックス関数の引数に現れる

$$
a_{k}=\ln \left(p\left(\mathbf{x} \mid \mathcal{C}_{k}\right) p\left(\mathcal{C}_{k}\right)\right) \tag{4.63}
$$

によって与えられる量$a_k$が$\boldsymbol{\phi}$の成分の線形関数であることを示せ．これが，8.2.2節で議論されるナイーブベイズモデルの一例であることに注意せよ．

</div>

要素が$L$個のベクトル$\mathbf{x}$が$M$個並んだ以下のような行列$\boldsymbol{\phi}$を考える

$$
\boldsymbol{\phi}=\begin{pmatrix}
\mathbf{x}_1 \\
\mathbf{x}_2 \\
\vdots \\
\mathbf{x}_M \\
\end{pmatrix}
$$
また、

$$
\begin{aligned}
    p(\boldsymbol{\phi} \mid \mathcal{C}_k) = \prod_{m=1}^{M} \prod_{l=1}^{L} \mu_{kml}^{\phi_{ml}}
\end{aligned}
$$
である。ここで、$\mu$はそのデータがクラス$\mathcal{C}_k$に属する確率を表している。

$$
\sum_{l} \mu_{kml} = 1
$$

である。
これを用いて

$$
\begin{aligned}
    a_k &= \ln \left(\prod_{m=1}^{M}\prod_{l=1}^{L}\mu_{kml}^{x_{ml}}\right) + \ln p(\mathcal{C}_k) \\
    &= \sum_{m=1}^{M}\sum_{l=1}^{L}x_{ml} \ln \mu_{kml}+ \ln p(\mathcal{C}_k) \\
    &= \sum_{m=1}^{M}x_m ^\mathrm{T} \begin{pmatrix}
     \ln \mu_{km1} \\
     \ln \mu_{km2} \\
     \vdots \\
     \ln \mu_{kmL} \\
  \end{pmatrix}
    + \ln p(\mathcal{C}_k)
\end{aligned}
$$
以上により$a_k$が$\boldsymbol{\phi}$の成分の線形関数であることが示された。


## 演習 4.12
<div class="panel-primary">

$$
\sigma(a) = \frac{1}{1+ \exp(-a)} \tag{4.59}
$$
で定義されるロジスティックシグモイド関数の微分に対する関係
$$
\frac{d\sigma}{da} = \sigma(1-\sigma) \tag{4.88}
$$
を検証せよ．

</div>


$$
\begin{aligned}
    \frac{ \mathrm{d} }{ \mathrm{d} a}  \sigma(a) &=  \frac{ \mathrm{d} }{ \mathrm{d} a} \frac{1}{1+ \exp(-a)} \\
    &= - e^{-a} \frac{-1}{\left(1+ \exp(-a)\right)^2} \\
    &= \frac{1}{1+ \exp(-a)} \frac{e^{-a}}{1+ \exp(-a)} \\
    &= \sigma(1-\sigma)
\end{aligned}
$$

## 演習 4.13
<div class="panel-primary">

ロジスティックシグモイドの微分に対する結果
$$
\frac{d\sigma}{da} = \sigma(1-\sigma) \tag{4.88}
$$
を使って，ロジスティック回帰モデルに対する誤差関数
$$
E(\mathbf{w})=-\ln p(\mathsf{t} \mid \mathbf{w})=-\sum_{n=1}^{N}\left\{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right\} \tag{4.90}
$$
の微分が，
$$
\nabla E(\mathbf{w})=\sum_{n=1}^{N}\left(y_{n}-t_{n}\right) \phi_{n} \tag{4.91}
$$
で与えられることを示せ．

</div>

$E$は$y_n$の関数、$y_n$は$a_n$の関数、$a_n$は$\mathbf{w}$の関数なので、連鎖律を利用する。

すなわち$\nabla E(\mathbf{w}) = \frac{\partial E}{\partial y_n}\frac{\partial y_n}{\partial a_n}\nabla a_n $ としてそれぞれの項を求めれば良い。

<br>

$E(\mathbf{w})=-\sum_{n=1}^{N}\left\{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right\}$ より

$$
\begin{aligned}
\frac{\partial E}{\partial y_n} &=-\sum_{n=1}^{N}\left( \frac{t_{n}}{y_{n}}-\frac{1-t_{n}}{1-y_{n}} \right)\\
&=-\sum_{n=1}^{N}\left(\frac{t_{n}\left(1-y_{n}\right)-y_{n}\left(1-t_{n}\right)}{y_{n}\left(1-y_{n}\right)}\right) \\
&=\sum_{n=1}^{N} \frac{y_{n}-t_{n}}{y_{n}\left(1-y_{n}\right)}
\end{aligned} \tag{1}
$$
<br>


$y_n=\sigma(a_n)$ より $\frac{d\sigma}{da} = \sigma(1-\sigma)$を利用して

$$
\frac{\partial y_{n}}{\partial a_{n}}=\frac{\partial \sigma\left(a_{n}\right)}{\partial a_{n}}=\sigma\left(a_{n}\right)\left(1-\sigma\left(a_{n}\right)\right)=y_{n}\left(1-y_{n}\right) \tag{2}
$$

<br>

$a_n = \mathbf{w}^{\mathrm T}\boldsymbol{\phi}_n$より

$$
\nabla a_n = \boldsymbol{\phi}_n \tag{3}
$$

<br>


よって(1), (2), (3)より

$$
\nabla E(\mathbf{w}) = \frac{\partial E}{\partial y_n}\frac{\partial y_n}{\partial a_n}\nabla a_n = \sum_{n=1}^{N}(y_n - t_n)\boldsymbol{\phi}_n
$$

となり、$(4.91)$式が示された。

<br>

## 演習 4.14
<div class="panel-primary">

線形分離可能なデータ集合に対し，ロジスティック回帰モデルの最尤解が，クラスを分離する決定境界$\mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}(\mathbf{x})=0$を満足するベクトル$\mathbf{w}$に対し，その値を$\infty$とすることで得られることを示せ．

</div>

データ集合$\{ \boldsymbol{\phi}_n, t_n\}$, $t_n \in \{0, 1\}$について、もしデータ集合が線形分離可能ならば、2つのクラスを分ける決定境界の超平面は$\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x})=0$で与えられ、
$$
\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}_{n} \left\{\begin{aligned}
\geq 0 & \text { if } t_{n}=1 \\
<0 & \text { otherwise } (t_n = 0)
\end{aligned}\right.
$$
となる。

一方、最尤法で尤度の負の対数をとって誤差関数$(4.90)$を定義すると

$$
E(\mathbf{w})=-\ln p(\mathsf{t} \mid \mathbf{w})=-\sum_{n=1}^{N}\left\{t_{n} \ln y_{n}+\left(1-t_{n}\right) \ln \left(1-y_{n}\right)\right\} \tag{4.90}
$$

この勾配は

$$
\nabla E(\mathbf{w})=\sum_{n=1}^{N}\left(y_{n}-t_{n}\right) \phi_{n} \tag{4.91}
$$

すなわち、すべての$n$について$y_n = \sigma(\mathbf{w}^{\mathrm T}\boldsymbol{\phi}_n)=t_n$のときに最小化される。
   ここで、$t_n \in \{ 0, 1\}$なので$y_n \in \{ 0, 1\}$になるが、これはシグモイド関数の両端、つまり$\mathbf{w}^{\mathrm T}\boldsymbol{\phi}_n = \pm \infty$のときに相当する。$\boldsymbol{\phi}_n$は固定なので、$\mathbf{w}$の大きさが$\infty$の時に相当する。このとき$E(\mathbf{w})$は最小化されロジスティック回帰モデルの最尤解が得られることになる。

## 演習 4.15
<div class="panel-primary">

$$
\mathbf{H}=\nabla \nabla E(\mathbf{w})=\sum_{n=1}^{N} y_{n}\left(1-y_{n}\right) \boldsymbol{\phi}_{n} \boldsymbol{\phi}_{n}^{\mathrm{T}}=\mathbf{\Phi}^{\mathrm{T}} \mathbf{R} \mathbf{\Phi} \tag{4.97}
$$
で与えられるロジスティック回帰モデルのヘッセ行列$\mathbf{H}$が正定値行列であることを示せ．ここで$\mathbf{R}$は，要素を$y_n(1-y_n)$とする対角行列であり，$y_n$は，入力ベクトル$\mathbf{x}_n$に対するロジスティック回帰モデルの出力である．したがって，誤差関数は$\mathbf{w}$の凸関数であり，唯一の最小解を持つことを示せ．

</div>


ヘッセ行列$\mathbf{H}$が正定値行列であるためには任意の$\mathbf{x}$に対して$\mathbf{x}^\mathrm{T} \mathbf{\Phi}^\mathrm{T} \mathbf{R} \mathbf{\Phi} \mathbf{x}>0$が成り立つことを確認すれば良い。

ここで、$M×1$ベクトルである$\mathbf{z}= \mathbf{\Phi}\mathbf{x}$について考えると、
$\mathbf{\Phi}$はフルランクであるため$\mathbf{z}\neq \mathbf{0}$であることがわかる。

よって、$\mathbf{x}^\mathrm{T} \mathbf{\Phi}^\mathrm{T} \mathbf{R} \mathbf{\Phi} \mathbf{x} = \mathbf{z}^\mathrm{T} \mathbf{R} \mathbf{z}$となるが、$\mathbf{R}$の要素$y_n(1-y_n)$は$0<y_n<1$であることから全ての対角成分が正である対角行列なので常に$\mathbf{z}^\mathrm{T} \mathbf{R} \mathbf{z} > 0$が成り立つ。

以上よりヘッセ行列$\mathbf{H}$が正定値行列であることが確認できた。
また、$\mathbf{H}=\nabla \nabla E(\mathbf{w})$であるので$\mathbf{H}$が正定値であれば誤差関数は唯一の最小解を持つ。

## 演習 4.16
<div class="panel-primary">

$t=0$または$t=1$に対応する2クラスの1つに属することが知られている各観測値$\mathbf{x}_n$における2値分類問題を考える．このとき，学習データがときどき間違ったラベルを付けられるため，学習データの収集手順は完全なものではないと仮定する．すべてのデータ$\mathbf{x}_n$に対し，クラスラベルの値$t_n$を与える代わりに，$t_n = 1$となる確率を表現する値$\pi_n$を与える．確率モデル$p(t=1\mid \boldsymbol{\phi})$が与えられた場合そのようなデータ集合に適切な対数尤度関数を記述せよ．

</div>

※尤度関数の解釈を問う問題？

もし完全にラベル付け$t_n$が間違っていない場合、$p_n = p(t_n = 1 \mid \boldsymbol{\phi}(\mathbf{x}_n))$とすると、ある観測点$\mathbf{x}_n$についての尤度関数は
$$
p(t_n\mid \boldsymbol{\phi}(\mathbf{x}_n)) = p_n^{t_n}(1-p_n)^{1-t_n}
$$
すべてのデータ$\{\mathsf{t}, \boldsymbol{\phi}(\mathbf{x}_n)\}$に対する尤度関数と対数尤度関数は
$$
p(\mathsf{t}\mid\boldsymbol{\phi}(\mathbf{x}_n)) = \prod_{n=1}^{N}p_n^{t_n}(1-p_n)^{1-t_n}
$$
$$
\ln p(\mathsf{t}\mid\boldsymbol{\phi}(\mathbf{x}_n)) = \sum_{n=1}^{N}\{t_n \ln p_n + (1-t_n)\ln(1-p_n)\}
$$
となる。この式を考えると、$t_n=1$のときに対数尤度関数を$\ln p_n$だけ増加させ、$t_n=0$のときには$\ln(1-p_n)$だけ増加させると解釈することができる。この解釈に基づくと、確率$\pi_n$で$t_n=1$となる（確率$1-\pi_n$で$t_n=0$となる）とき、対数尤度関数は

$$
\begin{aligned}
    \ln p(\mathsf{t}\mid\boldsymbol{\phi}(\mathbf{x}_n)) &= \sum_{n=1}^{N}\{\pi_n \ln p_n + (1-\pi_n)\ln(1-p_n)\} \\
    &=\sum_{n=1}^{N}\{\pi_n \ln p(t_n = 1 \mid \boldsymbol{\phi}(\mathbf{x}_n)) + (1-\pi_n)\ln(1-p(t_n = 1 \mid \boldsymbol{\phi}(\mathbf{x}_n)))\}
\end{aligned}
$$

で与えられることになる。

## 演習 4.17
<div class="panel-primary">

ソフトマックス活性化関数
$$
p\left(\mathcal{C}_{k} \mid \boldsymbol{\phi}\right)=y_{k}(\boldsymbol{\phi})=\frac{\exp \left(a_{k}\right)}{\sum_{j} \exp \left(a_{j}\right)} \tag{4.104}
$$
の微分が，
$$
\frac{\partial y_{k}}{\partial a_{j}}=y_{k}\left(I_{k j}-y_{j}\right) \tag{4.106}
$$
によって与えられることを示せ．$I_{kj}$は単位行列の要素である。ここで，$a_k$は
$$
a_k = \mathbf{w}_k^{\mathrm T}\boldsymbol{\phi} \tag{4.105}
$$
によって定義される．

</div>

※4.3.4 多クラスロジスティック回帰を参照。$k=j$と$k\neq j$のときに分けて考える。

$(i)$ $k\neq j$のとき
$$
\begin{aligned}
    \frac{\partial y_k}{\partial a_j} &= \frac{\partial}{\partial a_j}\left( \frac{e^{a_k}}{\sum_j \exp(a_j)} \right) \\
    &= -\frac{e^{a_k}e^{a_j}}{\left( \sum_j \exp(a_j) \right)^2} \\
    &=-y_k y_j
\end{aligned}
$$

$(ii)$ $k = j$のとき
$$
\begin{aligned}
    \frac{\partial y_j}{\partial a_j} &= \frac{\partial}{\partial a_j}\left( \frac{e^{a_j}}{\sum_j \exp(a_j)} \right) \\
    &= \frac{e^{a_j}}{\sum_j \exp(a_j)} - \frac{e^{a_j}\cdot e^{a_j}}{\left(\sum_j \exp(a_j) \right)^2} \\
    &=y_j(1-y_j)
\end{aligned}
$$

これらをまとめると
$$
\frac{\partial y_k}{\partial a_j} = y_k(I_{kj}-y_j) \tag{4.106}
$$

となる。ここで、$I_{kj}$は単位行列の$kj$成分である。

## 演習 4.18
<div class="panel-primary">

ソフトマックス活性化関数の微分の結果
$$
\frac{\partial y_{k}}{\partial a_{j}}=y_{k}\left(I_{k j}-y_{j}\right) \tag{4.106}
$$
を使って，交差エントロピー誤差
$$
E\left(\mathbf{w}_{1}, \ldots, \mathbf{w}_{K}\right)=-\ln p\left(\mathbf{T} \mid \mathbf{w}_{1}, \ldots, \mathbf{w}_{K}\right)=-\sum_{n=1}^{N} \sum_{k=1}^{K} t_{n k} \ln y_{n k} \tag{4.108}
$$
の勾配が
$$
\nabla_{\mathbf{w}_{j}} E\left(\mathbf{w}_{1}, \ldots, \mathbf{w}_{K}\right)=\sum_{n=1}^{N}\left(y_{n j}-t_{n j}\right) \boldsymbol{\phi}_{n} \tag{4.109}
$$
で与えられることを示せ．

</div>

※演習問題4.17を利用する。

まず前提として行列$\mathbf{T}$は、「クラス$\mathcal{C}_k$に属する特徴ベクトル$\boldsymbol{\phi}_n$に対する目的変数ベクトル$\mathbf{t}_n$が、$k$番目の要素で$1$で残りはすべて$0$となる1-of-$K$符号化法」で記述されるような行列（教科書P.208）である。

$\nabla_{\mathbf{w}_j}E$について微分のchain ruleを使う。
$E$がすでに$y_{nk}(=y_k(\boldsymbol{\phi}_n))$の関数となっていることを用いると

$$
\begin{aligned}
\nabla_{\mathbf{w}_{j}} E(\mathbf{w}_1,\ldots,\mathbf{w}_K)
&=\frac{\partial E}{\partial y_{nk}}\frac{\partial y_{nk}}{\partial a_j}\nabla_{\mathbf{w}_{j}} a_j \\
&=-\sum_{n=1}^{N} \sum_{k=1}^{K} \frac{t_{nk}}{y_{nk}}y_{nk}(I_{kj}-y_{nj})\boldsymbol{\phi}_n \\
&=-\sum_{n=1}^{N} \sum_{k=1}^{K} t_{nk}(I_{kj}-y_{nj})\boldsymbol{\phi}_n \\
&=\sum_{n=1}^{N} \left\{ y_{nj}\left( \sum_{k=1}^{K} t_{nk} \right) - \left( \sum_{k=1}^{K} t_{nk}I_{kj} \right) \right\}\boldsymbol{\phi}_n
\end{aligned}
$$

ここで、「前提」より$\sum_{k=1}^{K}t_{nk}=1$となり、$k=j$の場合$I_{kj}=1$で、$k\neq j$で$I_{kj}=0$であることを用いると
$$
\begin{aligned}
\nabla E_{\mathbf{w}_{j}}(\mathbf{w}_1,\ldots,\mathbf{w}_K)
&=\sum_{n=1}^{N} \left\{ y_{nj}\left( \sum_{k=1}^{K} t_{nk} \right) - \left( \sum_{k=1}^{K} t_{nk}I_{kj} \right) \right\}\boldsymbol{\phi}_n \\
&=\sum_{n=1}^{N} (y_{nj}-t_{nj})\boldsymbol{\phi}_n
\end{aligned}
$$
となり、$(4.109)$式が与えられた。

## 演習 4.19
<div class="panel-primary">

4.3.5節で定義したプロビット回帰モデルに対し対数尤度の勾配および，対応するヘッセ行列を求めよ．これらは，IRLSを使って，プロビット回帰モデルのようなモデルを学習するために必要とされる量である．

</div>

※まずプロビット回帰モデルの定義と尤度関数を確認する。微分計算は結構大変で、演習問題4.13をうまく利用して進めていく。ヘッセ行列は勾配をもう一度偏微分することで求まる。

プロビット回帰モデルは
$$
p(t_n=1\mid a_n) = y_n = \Phi(a_n) = \int_{-\infty}^{a_n}\mathcal{N}(\theta\mid 0,1)d\theta
$$
で表現するモデルである。ここで、$a_n = \mathbf{w}^{\mathrm T}\boldsymbol{\phi_n}$である。

尤度関数は

$$
p(\mathsf{t}\mid \mathbf{w}) = \prod_{n=1}^{N}y_n^{t_n}(1-y_n)^{1-t_n}
$$

なので負の対数尤度(cross entropy誤差関数)は$(4.90)$と同形で
$$
E(\mathbf{w}) = -\sum_{n=1}^N \{ t_n\ln y_n + (1-t_n)\ln(1-y_n) \}
$$
を与える。

演習問題4.13と同様にして
$$
\begin{aligned}
    \frac{\partial E(\mathbf{w})}{\partial y_{n}} &=\sum_{n=1}^{N} \frac{y_{n}-t_{n}}{y_{n}\left(1-y_{n}\right)} \\
    \nabla_{\mathbf{w}} a_n &= \boldsymbol{\phi}_n
\end{aligned}
$$
であり、
$$
\frac{\partial \Phi(a_n)}{\partial a_n}=\frac{\partial}{\partial a_n} \int_{-\infty}^{a_n} \mathcal{N}(\theta \mid 0,1) \mathrm{d} \theta=\mathcal{N}(a_n \mid 0,1)
$$
となることから$\nabla_{\mathbf{w}} E(\mathbf{w})$を求めると

$$
\begin{aligned}
    \nabla_{\mathbf{w}} E(\mathbf{w})&=\sum_{n=1}^{N} \frac{\partial E_{n}}{\partial y_{n}} \frac{\partial y_{n}}{\partial a_{n}} \nabla_{\mathbf{w}} a_{n} \\
    &=\sum_{n=1}^{N} \frac{y_{n}-t_{n}}{y_{n}\left(1-y_{n}\right)} \mathcal{N}\left(a_{n} \mid 0,1\right) \boldsymbol{\phi}_{n} \\
    &= \sum_{n=1}^{N} \frac{y_{n}-t_{n}}{y_{n}\left(1-y_{n}\right)} \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{a_n^2}{2}\right) \boldsymbol{\phi}_{n}
\end{aligned}
$$

次にこれを用いてヘッセ行列$\nabla_{\mathbf{w}} \nabla_{\mathbf{w}} E(\mathbf{w})$を求める。ベクトルをベクトルで微分するとき、$\boldsymbol{\phi}_n\to\boldsymbol{\phi}_n^{\mathrm T}$にしておく。

$$
\begin{aligned}
\nabla_{\mathbf{w}} \nabla_{\mathbf{w}} E(\mathbf{w}) &=\nabla_{\mathbf{w}} \sum_{n=1}^{N} \frac{y_{n}-t_{n}}{y_{n}\left(1-y_{n}\right)} \mathcal{N}\left(a_{n} \mid 0,1\right) \boldsymbol{\phi}_{n}^{\mathrm{T}} \\
&=\sum_{n=1}^{N}\left\{\left[\nabla_{\mathbf{w}} \frac{y_{n}-t_{n}}{y_{n}\left(1-y_{n}\right)}\right] \mathcal{N}\left(a_{n} \mid 0,1\right) + \frac{y_{n}-t_{n}}{y_{n}\left(1-y_{n}\right)} \left[\nabla_{\mathbf{w}} \mathcal{N}\left(a_{n} \mid 0,1\right)\right]\right\} \boldsymbol{\phi}_{n}^{\mathrm{T}} \\
&=\sum_{n=1}^{N}\left\{\left(\frac{\partial}{\partial y_{n}} \frac{y_{n}-t_{n}}{y_{n}\left(1-y_{n}\right)}\right) \frac{\partial y_{n}}{\partial a_{n}} \nabla_{\mathbf{w}} a_{n} \mathcal{N}\left(a_{n} \mid 0,1\right)+\frac{y_{n}-t_{n}}{y_{n}\left(1-y_{n}\right)}\left(\frac{\partial}{\partial a_{n}} \mathcal{N}\left(a_{n} \mid 0,1\right)\right) \nabla_{\mathbf{w}} a_{n}\right\} \boldsymbol{\phi}_{n}^{\mathrm{T}} \\
&=\sum_{n=1}^{N}\left\{\frac{y_{n}^{2}+t_{n}-2 y_{n} t_{n}}{y_{n}^{2}\left(1-y_{n}\right)^{2}} \mathcal{N}\left(a_{n} \mid 0,1\right)^{\mathrm{2}} \boldsymbol{\phi}_{n}+\frac{y_{n}-t_{n}}{y_{n}\left(1-y_{n}\right)}\left(-a_{n}\right) \mathcal{N}\left(a_{n} \mid 0,1\right) \boldsymbol{\phi}_{n}\right\} \boldsymbol{\phi}_{n}^{\mathrm{T}} \\
&=\sum_{n=1}^{N}\left\{\frac{y_{n}^{2}+t_{n}-2 y_{n} t_{n}}{y_{n}\left(1-y_{n}\right)}\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{a_n^2}{2}\right)-a_{n}\left(y_{n}-t_{n}\right)\right\} \frac{e^{-\frac{a_n^2}{2}}}{\sqrt{2\pi}y_{n}\left(1-y_{n}\right)}\boldsymbol{\phi}_{n}\boldsymbol{\phi}_{n}^{\mathrm{T}}
\end{aligned}
$$

> 公式解答例は$e$の肩に乗るべき係数が$1/2$ずつ間違えてる。

## 演習 4.20
<div class="panel-primary">

$$
\nabla_{\mathbf{w}_{k}} \nabla_{\mathbf{w}_{j}} E\left(\mathbf{w}_{1}, \ldots, \mathbf{w}_{K}\right)=\sum_{n=1}^{N} y_{n k}\left(I_{k j}-y_{n j}\right) \boldsymbol{\phi}_{n} \boldsymbol{\phi}_{n}^{\mathrm{T}} \tag{4.110}
$$
で定義される多クラスロジスティック回帰問題に対するヘッセ行列が半正定値行列であることを示せ．この問題における非退化ヘッセ行列のサイズは，$MK \times MK$であることに注意されたい．ここで，$M$はパラメータ数であり，$K$はクラス数である．半正定値性を証明するため，長さ$MK$の任意のベクトル$\mathbf{u}$に関して，積$\mathbf{u}^{\mathrm{T}}\mathbf{H}\mathbf{u}$を考え，イェンセンの不等式を適用する．

</div>

※多クラスロジスティック回帰問題でのヘッセ行列は、その$jk$成分が$(4.110)$式で与えられる$M\times M$の部分行列から構成される。つまり、
$$
\mathbf{H}=\begin{pmatrix}
\mathbf{H}_{11} & \mathbf{H}_{12} & \cdots & \mathbf{H}_{1K} \\
\mathbf{H}_{21} & \mathbf{H}_{22} & \cdots & \mathbf{H}_{2K} \\
\vdots & \vdots & & \vdots \\
\mathbf{H}_{K 1} & \mathbf{H}_{K 2} & \cdots & \mathbf{H}_{KK}
\end{pmatrix}, \quad \mathbf{H}_{jk} = \sum_{n=1}^{N}y_{nk}(I_{kj}-y_{nj})\underbrace{\boldsymbol{\phi}_n\boldsymbol{\phi}_n^{\mathrm T}}_{M\times M}
$$
となっている。よって、$\mathbf{H}$は$MK \times MK$の行列である。

このため、ベクトル$\mathbf{u}$も$MK$次元である必要があり、$M$次元ベクトル$\mathbf{u}_j(1\le j \le K)$について
$$
\mathbf{u} = \begin{pmatrix}
\mathbf{u}_{1} \\ \mathbf{u}_{2} \\ \vdots \\ \mathbf{u}_{K}
\end{pmatrix}
$$
となる。

演習問題4.15と同じように、任意のベクトル$\mathbf{u}$について$\mathbf{u}^{\mathrm T}\mathbf{Hu} \ge 0$であることを示せば良い（半正定値行列なので$\ge 0$を示す）

要素について注目し計算していくと

$$
\begin{aligned}
\mathbf{u}^{\mathrm T}\mathbf{Hu} &= \sum_{k}\sum_{j}\mathbf{u}_{j}^{\mathrm T} \mathbf{H}_{jk}\mathbf{u}_{k} \\
&= \sum_{k}\sum_{j}\mathbf{u}_{j}^{\mathrm T} \sum_n y_{nk}(I_{kj}-y_{nj})\boldsymbol{\phi}_n\boldsymbol{\phi}_n^{\mathrm T} \mathbf{u}_{k} \\
&= \sum_{k}\sum_{j}\sum_n \mathbf{u}_{j}^{\mathrm T} y_{nk}(I_{kj}-y_{nj})\boldsymbol{\phi}_n\boldsymbol{\phi}_n^{\mathrm T} \mathbf{u}_{k} \\
&= \sum_{k}\sum_{j}\sum_n\left( \mathbf{u}_{j}^{\mathrm T} y_{nk}I_{kj}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^{\mathrm T} \mathbf{u}_{k} - \mathbf{u}_{j}^{\mathrm T} y_{nk}y_{nj}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^{\mathrm T} \mathbf{u}_{k} \right) \\
&= \sum_{k}\sum_n\left( \mathbf{u}_{k}^{\mathrm T} y_{nk}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^{\mathrm T} \mathbf{u}_{k} - \sum_{j} \mathbf{u}_{j}^{\mathrm T} y_{nk}y_{nj}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^{\mathrm T} \mathbf{u}_{k} \right) (\because I_{kj}=1\textrm{ when } k=j, \textrm{ otherwise } 0)\\
&= \sum_{k}\sum_n\left\{ y_{nk}(\mathbf{u}_{k}^{\mathrm T} \boldsymbol{\phi}_n)^2 - y_{nk} \sum_{j} y_{nj} \mathbf{u}_{j}^{\mathrm T} \boldsymbol{\phi}_n\boldsymbol{\phi}_n^{\mathrm T} \mathbf{u}_{k} \right\} \\
&= \sum_n\left\{\sum_k y_{nk}(\mathbf{u}_{k}^{\mathrm T} \boldsymbol{\phi}_n)^2 - \sum_k y_{nk} \sum_{j} y_{nj} \sum_{k,j} \mathbf{u}_{j}^{\mathrm T} \boldsymbol{\phi}_n\boldsymbol{\phi}_n^{\mathrm T} \mathbf{u}_{k} \right\} \\
&= \sum_n\left\{\sum_k y_{nk}(\mathbf{u}_{k}^{\mathrm T} \boldsymbol{\phi}_n)^2 - \left(\sum_j y_{nj}\mathbf{u}_{j}^{\mathrm T} \boldsymbol{\phi}_n \right) \left(\sum_k y_{nk}\mathbf{u}_{k}^{\mathrm T} \boldsymbol{\phi}_n \right) \right\} \\
&= \sum_n\left\{\sum_k y_{nk}(\mathbf{u}_{k}^{\mathrm T} \boldsymbol{\phi}_n)^2 - \left( \sum_k y_{nk} \mathbf{u}_{k}^{\mathrm T} \boldsymbol{\phi}_n \right)^2
\right\} \\
\end{aligned}
$$

ここで、$0\le y_{nk} \le 1, \sum_{k}y_{nk}=1$であり、[イェンセンの不等式](https://ja.wikipedia.org/wiki/%E3%82%A4%E3%82%A7%E3%83%B3%E3%82%BB%E3%83%B3%E3%81%AE%E4%B8%8D%E7%AD%89%E5%BC%8F)を凸関数$f(x)=x^2$に対して適用する。$x = \mathbf{u}_{k}^{\mathrm T} \boldsymbol{\phi}_n$とすれば
$$
\sum_k y_{nk}(\mathbf{u}_{k}^{\mathrm T} \boldsymbol{\phi}_n)^2 \ge \left( \sum_k y_{nk} \mathbf{u}_{k}^{\mathrm T} \boldsymbol{\phi}_n \right)^2
$$
が成立する。以上から$\mathbf{u}^{\mathrm T}\mathbf{Hu} \ge 0$となり、多クラスロジスティック回帰問題に対するヘッセ行列$\mathbf{H}$が半正定値行列であることが示された。

## 演習 4.21
<div class="panel-primary">

プロビット関数の逆関数
$$
\Phi(a)=\int_{-\infty}^{a} \mathcal{N}(\theta \mid 0,1) \mathrm{d} \theta \tag{4.114}
$$
とerf関数
$$
\operatorname{erf}(a)=\frac{2}{\sqrt{\pi}} \int_{0}^{a} \exp \left(-\theta^{2}\right) \mathrm{d} \theta \tag{4.115}
$$
が
$$
\Phi(a)=\frac{1}{2}\left\{1+\operatorname{erf}\left(\frac{a}{\sqrt{2}}\right)\right\} \tag{4.116}
$$
によって関連付けられることを示せ．

</div>

$$
\begin{aligned}
    \mathbf{\Phi}(a) &= \int_{-\infty}^{a} \mathcal{N}(\theta \mid 0,1)d\theta\\
    &=\int_{-\infty}^{0} \mathcal{N}(\theta \mid 0,1)d\theta + \int_{0}^{a} \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{\theta^2}{2}\right)d\theta \\
    &= \frac{1}{2} + \int_{0}^{a} \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{\theta^2}{2}\right)d\theta \quad \left(\because\quad \int_{-\infty}^{\infty} \mathcal{N}(\theta \mid 0,1)d\theta = 1 \right)
\end{aligned}
$$
ここで$\displaystyle \frac{\theta}{\sqrt{2}} = x$とおくと
$$
\begin{aligned}
    \mathbf{\Phi}(a) &= \frac{1}{2}+\frac{1}{\sqrt{\pi}}\int_{0}^{\frac{a}{\sqrt{2}}} \exp(-x^2)dx \\
    &= \frac{1}{2}\left\{1+\mathrm{erf}\left(\frac{a}{\sqrt{2}}\right)\right\}
\end{aligned}
$$

となり、$(4.116)$式が示された。

## 演習 4.22
<div class="panel-primary">

$$
\begin{aligned}
Z &=\int f(\mathbf{z}) \mathrm{d} \mathbf{z} \\
& \simeq f\left(\mathbf{z}_{0}\right) \int \exp \left\{-\frac{1}{2}\left(\mathbf{z}-\mathbf{z}_{0}\right)^{\mathrm{T}} \mathbf{A}\left(\mathbf{z}-\mathbf{z}_{0}\right)\right\} \mathrm{d} \mathbf{z} \\
&=f\left(\mathbf{z}_{0}\right) \frac{(2 \pi)^{M / 2}}{|\mathbf{A}|^{1 / 2}}
\end{aligned} \tag{4.135}
$$
を使って，ラプラス近似の下での対数モデルエビデンスに対する表現
$$
\ln p(\mathcal{D}) \simeq \ln p\left(\mathcal{D} \mid \theta_{\mathrm{MAP}}\right)+\underbrace{\ln p\left(\theta_{\mathrm{MAP}}\right)+\frac{M}{2} \ln (2 \pi)-\frac{1}{2} \ln |\mathbf{A}|}_{\text {Occam 係数 }} \tag{4.137}
$$
を導出せよ．

</div>

P.216参照。$(4.136)$式を$(4.135)$式に代入すると

$$
\begin{aligned}
    p(\mathcal{D}) &=\int p(\mathcal{D} \mid \boldsymbol{\theta}) p(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta} \\
    & \simeq p\left(\mathcal{D} \mid \boldsymbol{\theta}_{\mathrm{MAP}}\right) p\left(\boldsymbol{\theta}_{\mathrm{MAP}}\right) \\
    & \int \exp \left\{-\frac{1}{2}\left(\boldsymbol{\theta}-\boldsymbol{\theta}_{\mathrm{MAP}}\right) \mathbf{A}\left(\boldsymbol{\theta}-\boldsymbol{\theta}_{\mathrm{MAP}}\right)\right\} \mathrm{d} \boldsymbol{\theta} \\
    &=p\left(\mathcal{D} \mid \boldsymbol{\theta}_{\mathrm{MAP}}\right) p\left(\boldsymbol{\theta}_{\mathrm{MAP}}\right) \frac{(2 \pi)^{M / 2}}{|\mathbf{A}|^{1 / 2}}
\end{aligned}
$$

これより、両辺の対数を取ると
$$
\ln p(\mathcal{D}) \simeq \ln p\left(\mathcal{D} \mid \boldsymbol{\theta}_{\mathrm{MAP}}\right)+\underbrace{\ln p\left(\theta_{\mathrm{MAP}}\right)+\frac{M}{2} \ln (2 \pi)-\frac{1}{2} \ln |\mathbf{A}|}_{\text {Occam 係数 }} \tag{4.137}
$$
が得られる。

## 演習 4.23
<div class="panel-primary">

この演習問題では$(4.137)$で与えられるモデルエビデンスに対し，ラプラス近似から始めて，BICの結果
$$
\ln p(\mathcal{D}) \simeq \ln p\left(\mathcal{D} \mid \theta_{\mathrm{MAP}}\right)-\frac{1}{2} M \ln N \tag{4.139}
$$
を導出する．パラメータ上での事前確率が$p(\theta)=\mathcal{N}\left(\boldsymbol{\theta} \mid \mathbf{m}, \mathbf{V}_{0}\right)$形式のガウス分布のとき，ラプラス近似の下での対数モデルエビデンスが以下の式となることを示せ．

$$
\ln p(\mathcal{D}) \simeq \ln p\left(\mathcal{D} \mid \boldsymbol{\theta}_{\mathrm{MAP}}\right)-\frac{1}{2}\left(\boldsymbol{\theta}_{\mathrm{MAP}}-\mathbf{m}\right)^{\mathrm{T}} \mathbf{V}_{0}^{-1}\left(\boldsymbol{\theta}_{\mathrm{MAP}}-\mathbf{m}\right)-\frac{1}{2} \ln |\mathbf{H}|+\text {const.}
$$

ここで，$\mathbf{H}$は$\boldsymbol{\theta}_{\mathrm{MAP}}$で評価された負の対数尤度$\ln p(\mathcal{D}|\boldsymbol{\theta})$の2階微分の行列である．事前確率が広い幅を持っている，つまり，$\mathbf{V}_{0}^{-1}$が小さく，上式右辺第2項が無視できると仮定する．さらに，$\mathbf{H}$が各データ点に対応する項の和で書けるように独立同時分布(i.i.d)の場合を考える．その場合，対数モデルエビデンスが近似的にBIC表現$(4.139)$の式で書けることを示せ．

</div>

$(4.137)$式における$\mathbf{A}$は事後分布の負の対数の2階微分であるヘッセ行列であり、
$$
\mathbf{A} = -\nabla\nabla \ln p(\mathcal{D} \mid \boldsymbol{\theta}_{\mathrm{MAP}})p(\boldsymbol{\theta}_{\mathrm{MAP}}) = -\nabla\nabla\ln p(\boldsymbol{\theta}_{\mathrm{MAP}} \mid \mathcal{D})
$$
である。ここで、問題文から$\mathbf{H}$は$\boldsymbol{\theta}_{\mathrm{MAP}}$で評価された負の対数尤度$\ln p(\mathcal{D} \mid \boldsymbol{\theta}_{\mathrm{MAP}})$の2階微分の行列なので
$$
\mathbf{H} = -\nabla\nabla\ln p(\mathcal{D} \mid \boldsymbol{\theta}_{\mathrm{MAP}})
$$
であり、この2式から
$$
\mathbf{A} = \mathbf{H} -\nabla\nabla\ln p(\boldsymbol{\theta}_{\mathrm{MAP}})
$$
となる。今、$p(\boldsymbol{\theta}) = \mathcal{N}(\boldsymbol{\theta}\mid \mathbf{m},\mathbf{V}_0)$とすると

$$
\begin{aligned}
    -\nabla\nabla\ln p(\boldsymbol{\theta}_{\mathrm{MAP}}) &= -\nabla\nabla \ln \left[ \frac{1}{(2\pi)^{M/2}|\mathbf{V}_0|^{1/2}} \exp\left\{ -\frac{1}{2} (\boldsymbol{\theta}_{\mathrm{MAP}}-\mathbf{m})^{\mathrm T} \mathbf{V}_0^{-1} (\boldsymbol{\theta}_{\mathrm{MAP}}-\mathbf{m}) \right\} \right] \\
    &=\mathbf{V}_0^{-1}
\end{aligned}
$$
なので、$\mathbf{A} = \mathbf{H} + \mathbf{V}_0^{-1}$となる。

問題文の通り、もし事前分布が十分に広い幅を持っていれば（またはデータの数$N$が多ければ）$\mathbf{V}_0^{-1}$は$\mathbf{H}$に比べ十分に小さくなるので$\mathbf{A} \simeq \mathbf{H}$と近似できる。$(4.137)$式にこれを代入すれば

$$
\ln p(\mathcal{D}) \simeq \ln p\left(\mathcal{D} \mid \theta_{\mathrm{MAP}}\right)-\frac{1}{2}\left(\theta_{\mathrm{MAP}}-\mathbf{m}\right)^{\mathrm{T}} \mathbf{V}_{0}^{-1}\left(\theta_{\mathrm{MAP}}-\mathbf{m}\right)-\frac{1}{2} \ln |\mathbf{H}|+\text { const. }
$$

が得られる。

再び$\boldsymbol{\theta}$の事前分布が平坦であるという仮定を用いると、上式の第1項に対して第2項を無視できる。またデータ点がi.i.d.であると仮定すると、ヘッセ行列を各データ点からの寄与$\mathbf{H}_n$の和で表すことができる。また、その和をデータ点の数$N$と各データ点からの寄与の平均値$\hat{\mathbf{H}}$を用いて表すこともできる。
$$
\mathbf{H} = \sum_{n=1}^N \mathbf{H}_n = N\hat{\mathbf{H}}, \quad \hat{\mathbf{H}} = \frac{1}{N}\sum_{n=1}^N \mathbf{H}_n
$$
すると、$\mathbf{H}$は$M \times M$（$M$は$\boldsymbol{\theta}$のパラメータ数）の行列で、非退化（フルランク）であると仮定するなら
$$
\ln |\mathbf{H}| = \ln |N\hat{\mathbf{H}}| = \ln(N^M |\hat{\mathbf{H}}|) = M\ln N + \ln |\hat{\mathbf{H}}|
$$

第2項は$\ln N$と比較して$O(1)$なので無視できる(ここよくわからない)ので、最終的に$(4.139)$式

$$
\ln p(\mathcal{D}) \simeq \ln p\left(\mathcal{D} \mid \theta_{\mathrm{MAP}}\right)-\frac{1}{2} M \ln N \tag{4.139}
$$

のように粗く近似することができる。これが **ベイズ情報量規準 (BIC)** である。

## 演習 4.24
<div class="panel-primary">

2.3.2節からの結果を利用して，パラメータ$\mathbf{w}$の事後確率分布がガウス分布である場合のロジスティック回帰モデルの周辺化に対する結果
$$
p\left(\mathcal{C}_{1} \mid \mathbf{t}\right)=\int \sigma(a) p(a) \mathrm{d} a=\int \sigma(a) \mathcal{N}\left(a \mid \mu_{a}, \sigma_{a}^{2}\right) \mathrm{d} a \tag{4.151}
$$
を導け．

</div>

※P.218 4.5.2 予測分布のところで、「デルタ関数は$\mathbf{w}$に線形制約を科し，$\boldsymbol{\phi}$に直交するすべての方向に積分することで，同時分布$q(\mathbf{w})$から周辺分布を形成することに留意して，$p(a)$を評価することができる．」という記述がある。そこで、$M$次元空間を回転させ、$\boldsymbol{\phi}$に平行な$\mathbf{w}$の成分を$w_{\parallel}$とし、$\boldsymbol{\phi}$に直交する$M-1$個の成分をまとめて$\mathbf{w}_{\perp}$とする。すなわち

$$
a = \mathbf{w}^{\mathrm T}\boldsymbol{\phi} = w_{\parallel}\|\boldsymbol{\phi}\|,\quad \textrm{where}\ \mathbf{w}^{\mathrm T} = (w_{\parallel}, \mathbf{w}_{\perp})
$$

とする。これを用いて$(4.147)$式に代入すると
$$
\begin{aligned}
    \int \sigma\left(\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\right) q(\mathbf{w}) \mathrm{d} \mathbf{w}
    &=\iint \sigma\left(w_{\parallel}\|\boldsymbol{\phi}\|\right) q\left(w_{\parallel}, \mathbf{w}_{\perp}\right) \mathrm{d} w_{\parallel} \mathrm{d} \mathbf{w}_{\perp} \\
    &=\int \sigma\left(w_{\parallel}\|\boldsymbol{\phi}\|\right) \int q\left(w_{\parallel}, \mathbf{w}_{\perp} \right) \mathrm{d}\mathbf{w}_{\perp} \mathrm{d} w_{\parallel} \\
    &=\int \sigma\left(w_{\parallel}\|\boldsymbol{\phi}\|\right) q\left(w_{\|}\right) \mathrm{d} w_{\parallel} \\
    &=\int \sigma(a) q(w_{\|}) \mathrm{d} w_{\parallel}
\end{aligned}
$$

ここで、同時確率分布$q(w_{\parallel}, \mathbf{w}_{\perp})$はガウス分布なので、2.3.2節の内容から、周辺分布$q(w_{\parallel})$もガウス分布である。すなわち

$$
q(w_{\parallel}) = \mathcal{N}(w_{\parallel}\mid \mu, \sigma^2)
$$
のような形で記述することができる。

上式は$w_{\parallel}$についてのスカラー量になっているので、行列形式で書くために単位ベクトル

$$
\mathbf{e} = \frac{\boldsymbol{\phi}}{\|\boldsymbol{\phi}\|}
$$
を導入すると、$\mu = \mathbf{e}^{\mathrm T}\mathbf{m}_{\mathrm{MAP}}, \sigma^2 = \mathbf{e}^{\mathrm T}\mathbf{S}_{\mathrm{MAP}}\mathbf{e}$となるような$\mathbf{m}_{\mathrm{MAP}}, \mathbf{S}_{\mathrm{MAP}}$が存在する。

$$
q(w_{\parallel}) = \mathcal{N}(w_{\parallel}\mid \mu, \sigma^2) = \|\boldsymbol{\phi}\|\mathcal{N}(a\mid \boldsymbol{\phi}^{\mathrm T}\mathbf{m}_{\mathrm{MAP}}, \boldsymbol{\phi}^{\mathrm T}\mathbf{S}_{\mathrm{MAP}}\boldsymbol{\phi})
$$

また、$a = w_{\parallel}\|\boldsymbol{\phi}\|$より$\|\boldsymbol{\phi}\|\mathrm{d}w_{\parallel} = \mathrm{d}a$を利用すると

$$
\begin{aligned}
    \int \sigma(a) q(w_{\|}) \mathrm{d} w_{\parallel} &= \|\boldsymbol{\phi}\| \int \sigma(a) \mathcal{N}(a\mid \boldsymbol{\phi}^{\mathrm T}\mathbf{m}_{\mathrm{MAP}}, \boldsymbol{\phi}^{\mathrm T}\mathbf{S}_{\mathrm{MAP}}\boldsymbol{\phi})\mathrm{d}w_{\parallel} \\
    &= \int \sigma(a) \mathcal{N}(a\mid \boldsymbol{\phi}^{\mathrm T}\mathbf{m}_{\mathrm{MAP}}, \boldsymbol{\phi}^{\mathrm T}\mathbf{S}_{\mathrm{MAP}}\boldsymbol{\phi})\mathrm{d}a
\end{aligned}
$$

最後に$\mu_a = \boldsymbol{\phi}^{\mathrm T}\mathbf{m}_{\mathrm{MAP}}, \sigma^{2}_{a} = \boldsymbol{\phi}^{\mathrm T}\mathbf{S}_{\mathrm{MAP}}\boldsymbol{\phi}$とすれば、$(4.151)$式が導かれる。

## 演習 4.25
<div class="panel-primary">

$$
\sigma(a)=\frac{1}{1+\exp (-a)} \tag{4.59}
$$
で定義されるロジスティックシグモイド関数$\sigma(a)$をスケールパラメータを持つプロビット関数の逆関数$\mathrm{\Phi}(\lambda a)$で近似するとする．ここで，$\mathrm{\Phi}(a)$は
$$
\Phi(a)=\int_{-\infty}^{a} \mathcal{N}(\theta \mid 0,1) \mathrm{d} \theta \tag{4.114}
$$
で定義される．2つの関数の微分が$a=0$で等しいように$\lambda$を選ぶ場合，$\lambda^2 = \pi /8$となることを示せ．

</div>

※大学入試にありそうな問題。

ロジスティックシグモイド$\sigma(a)$の$a=0$での微分$\displaystyle \left. \frac{d\sigma}{da}\right|_{a=0}$とプロビット関数の逆関数$\Phi(\lambda a)$の$a=0$での微分$\displaystyle \left. \frac{d\Phi(\lambda a)}{da}\right|_{a=0}$が一致すれば良い。

$$
\left. \frac{d\sigma}{da} \right|_{a=0} = \left. \frac{e^{-a}}{(1+e^{-a})^2}\right|_{a=0} = \frac{1}{4} \tag{1}
$$

$$
\begin{aligned}
    \left. \frac{d\Phi(\lambda a)}{da}\right|_{a=0} &= \left. \frac{d(\lambda a)}{da} \frac{d}{d(\lambda a)}\int_{-\infty}^{\lambda a} \mathcal{N}(\theta\mid 0, 1)d\theta\right|_{a=0} \\
    &= \left. \lambda \mathcal{N}(\lambda a\mid 0, 1)\right|_{a=0} \\
    &= \left. \frac{\lambda}{\sqrt{2\pi}} \exp\left( -\frac{(\lambda a)^2}{2} \right)\right|_{a=0} \\
    &= \frac{\lambda}{\sqrt{2\pi}}
\end{aligned}\tag{2}
$$

$(1), (2)$が一致するので、

$$
\lambda = \frac{\sqrt{2\pi}}{4},\quad \lambda^2 = \frac{\pi}{8}
$$

となる。

## 演習 4.26
<div class="panel-primary">

この演習問題で，ガウス分布とプロビット関数の逆関数のたたみ込みに対する関係
$$
\int \Phi(\lambda a) \mathcal{N}\left(a \mid \mu, \sigma^{2}\right) \mathrm{d} a=\Phi\left(\frac{\mu}{\left(\lambda^{-2}+\sigma^{2}\right)^{1 / 2}}\right) \tag{4.152}
$$
を証明する．この証明を行うため，$\mu$に関する左辺の微分が右辺の微分に等しいことを示せ．また，$\mu$に関して両辺を積分して，積分定数が消えることを示せ．左辺を微分する前に，$a$での積分を$z$での積分で置き換えるように変数変換$a=\mu+\sigma z$を導入すれば便利である．関係$(4.152)$の左辺を微分すれば解析的に評価可能な$z$に関するガウス分布の積分が得られる．

</div>

※右辺の$\mu$についての微分は簡単なのでそちらから求める。

$$
\frac{d}{d \mu} \Phi\left(\frac{\mu}{\left(\lambda^{-2}+\sigma^{2}\right)^{1 / 2}}\right)=\frac{1}{\sqrt{2 \pi}} \frac{1}{\left(\lambda^{-2}+\sigma^{2}\right)^{1 / 2}} \exp \left\{-\frac{\mu^{2}}{2\left(\lambda^{-2}+\sigma^{2}\right)}\right\} \tag{1}
$$

次に左辺について、左辺の式を$f(a)$とおき、誘導に従って$a = \mu + \sigma\lambda$を導入すると

$$
\begin{aligned}
f(a)&=\int \Phi(\lambda a) \mathcal{N}\left(a \mid \mu, \sigma^{2}\right) d a \\
&=\int \Phi(\lambda(\mu+\sigma z)) \mathcal{N}\left(\mu+\sigma z \mid \mu, \sigma^{2}\right) \sigma d z \quad (\because da = \sigma dz)\\
&=\int \Phi(\lambda \mu+\lambda \sigma z) \frac{1}{\left(2 \pi \sigma^{2}\right)^{1/2}} \exp \left\{-\frac{1}{2} z^{2}\right\} \sigma d z
\end{aligned}
$$

$f(a)$に対し$\mu$についての微分を実行する。ここで

$$
\frac{\partial}{\partial \mu}\Phi(\lambda(\mu + \sigma z)) = \frac{\partial (\lambda(\mu + \sigma z))}{\partial\mu}\frac{\partial \Phi(\lambda(\mu + \sigma z))}{\partial (\lambda(\mu + \sigma z))} = \lambda \mathcal{N}(\lambda(\mu + \sigma z))
$$

であることに注意すると

$$
\begin{aligned}
    \frac{\partial f}{\partial \mu} &=\int \lambda \mathcal{N}(\lambda \mu+\lambda \sigma z) \frac{1}{\sqrt{2 \pi}} \exp \left\{-\frac{1}{2} z^{2}\right\} d z \\
    &=\frac{\lambda}{2 \pi} \int \exp \left\{-\frac{1}{2} z^{2}-\frac{\lambda^{2}}{2}(\mu+\sigma z)^{2}\right\} d z
\end{aligned}
$$

ここで、積分記号の中がガウス分布の形になるように$z$について平方完成を行うと、積分記号の中は

$$
\begin{aligned}
    -\frac{1}{2}\left(z^{2}+\lambda^{2}(\mu+\sigma z)^{2}\right) &=-\frac{1}{2}\left\{\left(1+\lambda^{2} \sigma^{2}\right) z^{2}+2 \lambda^{2} \mu \sigma z+\lambda^{2} \mu^{2}\right\} \\
    &=-\frac{1+\lambda^{2} \sigma^{2}}{2}\left\{\left(z+\frac{\lambda^{2} u \sigma}{1+\lambda^{2} \sigma^{2}}\right)^{2}-\frac{\lambda^{4} \mu^{2} \sigma^{2}}{\left(1+\lambda^{2} \sigma^{2}\right)^{2}}+\frac{\lambda^{2} \mu^{2}}{1+\lambda^{2} \sigma^{2}}\right\} \\
    &=-\frac{1+\lambda^{2} \sigma^{2}}{2}\left(z+\frac{\lambda^{2} \mu \sigma}{1+\lambda^{2} \sigma^{2}}\right)^{2}-\frac{\lambda^{2} u^{2}}{2\left(1+\lambda^{2} \sigma^{2}\right)}
\end{aligned}
$$

となる。さらに途中でガウス分布の正規化定数$\displaystyle \int \exp \left(-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right) dz = \sqrt{2\pi \sigma^2}$を利用することで

$$
\begin{aligned}
    \frac{\partial f}{\partial \mu} &=\frac{\lambda}{2 \pi} \int \exp \left\{-\frac{1+\lambda^{2} \sigma^{2}}{2}\left(z+\frac{\lambda^{2} \mu \sigma}{1+\lambda^{2} \sigma^{2}}\right)^{2}-\frac{\lambda^{2} u^{2}}{2\left(1+\lambda^{2} \sigma^{2}\right)}\right\} d z \\
    &=\frac{\lambda}{2 \pi} \exp \left(-\frac{\lambda^{2} u^{2}}{2\left(1+\lambda^{2} \sigma^{2}\right)}\right) \underbrace{\int \exp \left\{-\frac{1+\lambda^{2} \sigma^{2}}{2}\left(z+\frac{\lambda^{2} \mu \sigma}{1+\lambda^{2} \sigma^{2}}\right)^{2}\right\}}_{\textrm{Gaussian integral }} d z \\
    &=\frac{\lambda}{2 \pi} \exp \left(-\frac{\lambda^{2} u^{2}}{2\left(1+\lambda^{2} \sigma^{2}\right)}\right) \sqrt{\frac{2 \pi}{1+\lambda^{2} \sigma^{2}}} \\
    &=\frac{\lambda}{\sqrt{2 \pi}} \frac{1}{\left(1+\lambda^{2} \sigma^{2}\right)^{1 / 2}} \exp \left(\frac{-\lambda^{2} \mu^{2}}{2\left(1+\lambda^{2} \sigma^{2}\right)}\right) \\
    &=\frac{1}{\sqrt{2 \pi}} \frac{1}{\left(\lambda^{-2}+\sigma^{2}\right)^{1 / 2}} \exp \left\{-\frac{\mu^{2}}{2\left(\lambda^{-2}+\sigma^{2}\right)}\right\}
\end{aligned} \tag{2}
$$

$(1)$, $(2)$の結果から両辺の$\mu$に関する微分が等しいことが示された。

また、$(4.152)$の両辺について$\mu \to -\infty$を考えると両辺とも$0$となる。これは積分定数が$0$であることを示し、微分形が一致することと合わせると、$(4.152)$式の等号が成立することが示された。

