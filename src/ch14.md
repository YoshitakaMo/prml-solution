## 演習 14.1

モデルの集合$p\left(\mathbf{t} \mid \mathbf{x}, \mathbf{z}_{h}, \boldsymbol{\theta}_{h}, h\right)$を考える。ただし、$\mathbf{x}$は入力ベクトル、$\mathbf{t}$は目標ベクトル、$h$はモデルのインデックス、$\mathbf{z}_h$はモデル$h$の潜在変数、そして$\boldsymbol{\theta}_{h}$はモデル$h$のパラメータ集合とする。そして、モデルの事前確率を$p(h)$とし、訓練集合として$\mathbf{X}=\left\{\mathbf{x}_{1}, \ldots, \mathbf{x}_{N}\right\}$と$\mathbf{T}=\left\{\mathbf{t}_{1}, \ldots, \mathbf{t}_{N}\right\}$を与える。潜在変数とモデルインデックスを周辺化した予測分布$p(\mathbf{t} \mid \mathbf{x},\mathbf{X},\mathbf{T})$を求める式を書き下せ。この式を利用して、異なるモデルのベイズ平均化と、一つのモデル内での潜在変数を利用することの違いを強調せよ。

----

※かなり難解なのであっているかわかりません。公式解答例ではベイズモデル平均化とモデル結合をまとめて書いた形で書き下していますが、ここでは分けて考えることにします。

P.372の議論参照。目的はモデルの結合とベイズモデル平均化の違いを理解することである。

問題文の設定から、モデルの集合を$p\left(\mathbf{t} \mid \mathbf{x}, \mathbf{z}_{h}, \boldsymbol{\theta}_{h}, h\right)$としている。モデル$h=1,2,\ldots,H$について、それぞれ$\mathbf{z}_{h}$という潜在変数と$\boldsymbol{\theta}_{h}$というパラメータ集合（例えば混合ガウスモデルでは平均$\boldsymbol{\mu}$と分散$\mathbf{\Sigma}$など）が存在している。このモデルが訓練集合$(\mathbf{\mathbf{x}_{n}},\mathbf{t}_{n})$と新しい入力ベクトル$\mathbf{x}$とその目標ベクトル$\mathbf{t}$のペア$(\mathbf{x}, \mathbf{t})$を生成している。

(1) ベイズモデル平均化の場合

これは$h$という変数がモデルの不確実性を表し、真の1つのモデルからデータ集合が生成されるので、有向グラフを書くと

![](https://i.imgur.com/LKL41Na.png)


となる。これについて有向分離を用いると

$$
\begin{aligned}
p(\mathbf{t} \mid \mathbf{x}, \mathbf{X}, \mathbf{T}) &=\sum_{h} \sum_{\mathbf{z}_{h}} \int p\left(\mathbf{t}, h, \mathbf{z}_{h}, \boldsymbol{\theta}_{h} \mid \mathbf{x}, \mathbf{X}, \mathbf{T}\right) d \boldsymbol{\theta}_{h} \quad (\because 加法定理)\\
&=\sum_{h} \sum_{\mathbf{z}_{h}} \int p\left(\mathbf{t} \mid h, \mathbf{z}_{h}, \boldsymbol{\theta}_{h}, \mathbf{x}, \mathbf{X}, \mathbf{T}\right) p\left(h, \mathbf{z}_{h}, \boldsymbol{\theta}_{h} \mid  \mathbf{x}, \mathbf{X}, \mathbf{T}\right) d \boldsymbol{\theta}_{h} \quad (\because 乗法定理)\\
&=\sum_{h} \sum_{\mathbf{z}_{h}} \int p\left(\mathbf{t} \mid h, \mathbf{z}_{h}, \boldsymbol{\theta}_{h}, \mathbf{x}\right) p\left(h, \mathbf{z}_{h}, \boldsymbol{\theta}_{h} \mid \mathbf{X}, \mathbf{T}\right) d \boldsymbol{\theta}_{h}\quad (\because 有向分離)\\
\end{aligned}
$$

この$p\left(h, \mathbf{z}_{h}, \boldsymbol{\theta}_{h} \mid \mathbf{X}, \mathbf{T}\right)$について

$$
\begin{aligned}
p\left(h, \mathbf{z}_{h}, \boldsymbol{\theta}_{h} \mid \mathbf{X}, \mathbf{T}\right) &=\frac{p\left(h, \mathbf{z}_{h}, \boldsymbol{\theta}_{h}, \mathbf{X}, \mathbf{T}\right)}{p(\mathbf{X}, \mathbf{T})} \\
& \propto p\left(\mathbf{X}, \mathbf{T} \mid h, \mathbf{z}_{h}, \boldsymbol{\theta}_{h}\right) p\left(h, \mathbf{z}_{h}, \boldsymbol{\theta}_{h}\right) \quad (\because 乗法定理)\\
&=p\left(\mathbf{X}, \mathbf{T} \mid h, \mathbf{z}_{h}, \boldsymbol{\theta}_{h}\right) p(h) p\left(\mathbf{z}_{h}\right) p\left(\boldsymbol{\theta}_{h}\right)
\end{aligned}
$$

まとめると、

$$
p(\mathbf{t} \mid \mathbf{x}, \mathbf{X}, \mathbf{T})=\frac{1}{p(\mathbf{X},\mathbf{T})}\sum_{h=1}^{H} p(h) \sum_{\mathbf{z}_{h}} p\left(\mathbf{z}_{h}\right) \int p\left(\mathbf{t} \mid \mathbf{x}, \boldsymbol{\theta}_{h}, \mathbf{z}_{h}, h\right) p\left(\mathbf{X}, \mathbf{T} \mid h, \mathbf{z}_{h}, \boldsymbol{\theta}_{h}\right)p(\boldsymbol{\theta}_{h})d \boldsymbol{\theta}_{h}
$$

(2) モデル結合の場合

1つのモデル$h$につき、観測された点$\mathbf{t}_{n}$ごとに対応する$\mathbf{z}_{h}$が存在することが大きな違い。よって、$h$個のモデルでは$n\times h$個の潜在変数$\mathbf{z}_{hn}$が存在することになる。有向グラフはこんな感じになると思われる。

![](https://i.imgur.com/Kq0POzk.png)

$$
\begin{aligned}
p(\mathbf{t} \mid \mathbf{x}, \mathbf{X}, \mathbf{T}) &= \sum_{h_{n}} \sum_{h} \sum_{\mathbf{z}_{hn}} \sum_{\mathbf{z}_{h}} \int p\left(\mathbf{t}, \boldsymbol{\theta}_{h}, h_{n}, h,\mathbf{z}_{hn}, \mathbf{z}_{h} \mid \mathbf{x}, \mathbf{X}, \mathbf{T}\right) d \boldsymbol{\theta}_{h} \\
&=\sum_{h_{n}} \sum_{h} \sum_{\mathbf{z}_{hn}} \sum_{\mathbf{z}_{h}} \int p\left(\mathbf{t} \mid \boldsymbol{\theta}_{h}, h_{n}, h,\mathbf{z}_{hn}, \mathbf{z}_{h}, \mathbf{x}, \mathbf{X}, \mathbf{T}\right) \\
&\phantom{=\sum_{h_{n}} \sum_{h} \sum_{\mathbf{z}_{hn}} \sum_{\mathbf{z}_{h}} \int}p\left(\boldsymbol{\theta}_{h}, h_{n}, h,\mathbf{z}_{hn}, \mathbf{z}_{h} \mid \mathbf{x}, \mathbf{X}, \mathbf{T}\right) d \boldsymbol{\theta}_{h} \\
&=\sum_{h_{n}} \sum_{h} \sum_{\mathbf{z}_{hn}} \sum_{\mathbf{z}_{h}} \int p\left(\mathbf{t} \mid \mathbf{x}, \boldsymbol{\theta}_{h}, \mathbf{z}_{h}, h \right) \\
&\phantom{=\sum_{h_{n}} \sum_{h} \sum_{\mathbf{z}_{hn}} \sum_{\mathbf{z}_{h}} \int}p\left(\boldsymbol{\theta}_{h}, h_{n}, h,\mathbf{z}_{hn}, \mathbf{z}_{h} \mid \mathbf{X}, \mathbf{T}\right) d \boldsymbol{\theta}_{h} \quad (\because 有向分離)\\
\end{aligned}
$$

この$p\left(\boldsymbol{\theta}_{h}, h_{n}, h,\mathbf{z}_{hn}, \mathbf{z}_{h} \mid \mathbf{X}, \mathbf{T}\right) d \boldsymbol{\theta}_{h}$について

$$
\begin{aligned}
p\left(\boldsymbol{\theta}_{h}, h_{n}, h,\mathbf{z}_{hn}, \mathbf{z}_{h} \mid \mathbf{X}, \mathbf{T}\right)  &= \frac{p\left(\boldsymbol{\theta}_{h}, h_{n}, h,\mathbf{z}_{hn}, \mathbf{z}_{h}, \mathbf{X}, \mathbf{T}\right)}{p(\mathbf{X},\mathbf{T})} \\
&\propto p\left(\boldsymbol{\theta}_{h}, h_{n}, h,\mathbf{z}_{hn}, \mathbf{z}_{h}, \mathbf{X}, \mathbf{T}\right) \\
&=p\left(\mathbf{X}, \mathbf{T} \mid h_{n}, h, \boldsymbol{\theta}_{h},\mathbf{z}_{hn},\mathbf{z}_{h}\right) p\left( h_{n}, h, \boldsymbol{\theta}_{h},\mathbf{z}_{hn},\mathbf{z}_{h} \right) \\
&=p\left(\mathbf{X}, \mathbf{T} \mid h_{n},\mathbf{z}_{hn}, \boldsymbol{\theta}_{h}\right) p\left(h_{n}\right) p(h) p\left(\boldsymbol{\theta}_{h}\right) p(\mathbf{z}_{hn}) p\left(\mathbf{z}_{h}\right) \\
\end{aligned}
$$

以上をまとめると

$$
\begin{aligned}
p(\mathbf{t} \mid \mathbf{x}, \mathbf{X}, \mathbf{T}) &= \frac{1}{p(\mathbf{X},\mathbf{T})}\sum_{h=1}^{H} p(h) \sum_{\mathbf{z}_{h}}p\left(\mathbf{z}_{h}\right)\int p\left(\mathbf{t} \mid \mathbf{x}, \boldsymbol{\theta}_{h}, \mathbf{z}_{h}, h\right) \\
&\phantom{\frac{1}{p(\mathbf{X},\mathbf{T})}\sum_{h=1}^{H} p(h) \sum_{\mathbf{z}_{h}}p\left(\mathbf{z}_{h}\right)\int}\left\{\sum_{h_{n}=1}^{H} \sum_{z_{hn}} p\left(\mathbf{X}, \mathbf{T} \mid h_{n}, \mathbf{z}_{hn}, \boldsymbol{\theta}_{h}\right) p\left(\mathbf{z}_{hn}\right) p\left(h_{n}\right) p\left(\boldsymbol{\theta}_{h}\right)\right\} d \boldsymbol{\theta}_{h}
\end{aligned}
$$

## 演習 14.2

単純なコミッティモデルの二乗和誤差$E_{\textrm{AV}}$の期待値は

$$
E_{\mathrm{AV}}=\frac{1}{M} \sum_{m=1}^{M} \mathbb{E}_{\mathbf{x}}\left[\epsilon_{m}(\mathbf{x})^{2}\right] \tag{14.10}
$$

で定義され、コミッティそれ自体の誤差の期待値は

$$
\begin{aligned} E_{\mathrm{COM}} &=\mathbb{E}_{\mathbf{x}}\left[\left\{\frac{1}{M} \sum_{m=1}^{M} y_{m}(\mathbf{x})-h(\mathbf{x})\right\}^{2}\right] \\ &=\mathbb{E}_{\mathbf{x}}\left[\left\{\frac{1}{M} \sum_{m=1}^{M} \epsilon_{m}(\mathbf{x})\right\}^{2}\right] \end{aligned} \tag{14.11}
$$

で与えられる。個別の誤差が

$$
\mathbb{E}_{\mathbf{x}}\left[\epsilon_{m}(\mathbf{x})\right]=0 \tag{14.12}
$$

と

$$
\mathbb{E}_{\mathbf{x}}\left[\epsilon_{m}(\mathbf{x}) \epsilon_{l}(\mathbf{x})\right]=0, \quad m \neq l \tag{14.13}
$$

を満足することを仮定して、

$$
E_{\mathrm{COM}}=\frac{1}{M} E_{\mathrm{AV}} \tag{14.14}
$$

の結果を導け。

----
$(14.11)$より

$$
\begin{aligned}
E_{\text {COM }} &=\mathbb{E}_{\mathbf{x}}\left[\left\{\frac{1}{M} \sum_{m=1}^{M} \epsilon_{m}(\mathbf{x})\right\}^{2}\right] \\
&=\frac{1}{M^{2}} \mathbb{E}_{\mathbf{x}}\left[\left(\epsilon_{1}(\mathbf{x})+\cdots \cdots \epsilon_{M}(\mathbf{x})\right)^{2}\right] \\
&=\frac{1}{M^{2}} \mathbb{E}_{\mathbf{x}}\left[\epsilon_{1}^{2}(\mathbf{x})+\cdots \epsilon_{M}^{2}(\mathbf{x})+\epsilon_{1}(\mathbf{x}) \epsilon_{2}(\mathbf{x})+\cdots+\epsilon_{M}(\mathbf{x}) \epsilon_{M-1}(\mathbf{x})\right] \\
&=\frac{1}{M^{2}}\left\{\mathbb{E}_{\mathbf{x}}\left[\epsilon_{1}^{2}(\mathbf{x})\right]+\cdots \mathbb{E}_{\mathbf{x}}\left[\epsilon_{M}^{2}(\mathbf{x})\right]+\mathbb{E}_{\mathbf{x}}\left[\epsilon_{1}(\mathbf{x}) \epsilon_{2}(\mathbf{x})\right]+\cdots \mathbb{E}_{\mathbf{x}}\left[\epsilon_{M}(\mathbf{x}) \epsilon_{M-1}(\mathbf{x})\right]\right\} \\
&=\frac{1}{M^{2}} \sum_{m=1}^{M} \mathbb{E}_{\mathbf{x}}\left[\epsilon_{m}^{2}(\mathbf{x})\right] \qquad(\because (14.13)) \\
&=\frac{1}{M} E_{\mathrm{AV}}\qquad(14.10)
\end{aligned}
$$

よって$(14.14)$が導かれた。

## 演習 14.3

イェンセンの不等式

$$
f\left(\sum_{i=1}^{M} \lambda_{i} x_{i}\right) \leqslant \sum_{i=1}^{M} \lambda_{i} f\left(x_{i}\right) \tag{1.115}
$$

を利用することで、$f(x) = x^2$という凸関数について、

$$
E_{\mathrm{AV}}=\frac{1}{M} \sum_{m=1}^{M} \mathbb{E}_{\mathbf{x}}\left[\epsilon_{m}(\mathbf{x})^{2}\right] \tag{14.10}
$$

で与えられる単純なコミッティモデルのメンバーの二乗和誤差の平均期待値$E_{\textrm{AV}}$、そして

$$
\begin{aligned} E_{\mathrm{COM}} &=\mathbb{E}_{\mathbf{x}}\left[\left\{\frac{1}{M} \sum_{m=1}^{M} y_{m}(\mathbf{x})-h(\mathbf{x})\right\}^{2}\right] \\ &=\mathbb{E}_{\mathbf{x}}\left[\left\{\frac{1}{M} \sum_{m=1}^{M} \epsilon_{m}(\mathbf{x})\right\}^{2}\right] \end{aligned} \tag{14.11}
$$

で与えられるコミッティそれ自体の誤差$E_{\textrm{COM}}$の期待値について以下が成り立つことを示せ.

$$
E_{\textrm{COM}} \leq E_{\textrm{AV}} \tag{14.54}
$$

----
イェンセンの不等式

$$
f\left(\sum_{i=1}^{M} \lambda_{i} x_{i}\right) \leqslant \sum_{i=1}^{M} \lambda_{i} f\left(x_{i}\right)
$$

ただし、$f$は凸関数において

$$
\begin{gathered}
\lambda_{i}=\frac{1}{M},x_{i}=\epsilon_{i}(x), f(x)=x^{2} \\
\left(\sum_{i=1}^{M} \frac{1}{M} \epsilon_{i}(x)\right)^{2} \leqq \sum_{i=1}^{M} \frac{1}{M} \epsilon_{i}^{2}(x)
\end{gathered}
$$

を得る。

両辺に$p(x)$をかけて$x$で積分すると

$$
\int p(x)\left(\sum_{i=1}^{M} \frac{1}{M} \epsilon_{i}(x)d x\right) ^{2}\leqq \int p(x) \sum_{i=1}^{M} \frac{1}{M} \epsilon_{i}(x)^{2} d x
$$

となる。これより

$$
\begin{aligned}
&\mathbb{E}_{\mathbf{x}}\left[\left(\sum_{i=1}^{M} \frac{1}{M} \epsilon_{i}(x)\right)^{2}\right] \leqq \mathbb{E}_{\mathbf{x}}\left[\sum_{i=1}^{M} \frac{1}{M} \epsilon_{i}(x)^{2}\right] \\
&\therefore \quad \mathbb{E}_{\mathbf{x}}\left[\left(\frac{1}{M} \sum_{i=1}^{M} \epsilon_{i}(x)\right)^{2}\right] \leqq \frac{1}{M} \sum_{i=1}^{M} \mathbb{E}_{\mathbf{x}}\left[\epsilon_{i}(x)^{2}\right]
\end{aligned}
$$

(14.10),(14.11)を入れて

$$
E_{\operatorname{COM}} \leqq E_{A V}
$$


## 演習 14.4

イェンセンの不等式

$$
f\left(\sum_{i=1}^{M} \lambda_{i} x_{i}\right) \leqslant \sum_{i=1}^{M} \lambda_{i} f\left(x_{i}\right) \tag{1.115}
$$

を利用することで、前の演習問題で得られた

$$
E_{\textrm{COM}} \leq E_{\textrm{AV}} \tag{14.54}
$$

の結果が、二乗和誤差以外の任意の$y$の凸関数となる誤差関数$E(y)$においても成り立つことを示せ。

----
イェンセンの不等式により、

$$
\begin{aligned}
E_{\mathrm{AV}} &:= \frac{1}{M} \sum_{m=1}^{M} \mathbb{E}_{\mathbf{x}}\left[E(y(\mathbf{x}))\right] \\
&= \mathbb{E}_{\mathbf{x}}\left[ \sum_{m=1}^{M} \frac{1}{M} E(y(\mathbf{x}))\right] \\
&\geq \mathbb{E}_{\mathbf{x}}\left[ E\left(\sum_{m=1}^{M} \frac{1}{M} y(\mathbf{x})\right)\right] \\
&=: E_{\mathrm{COM}}
\end{aligned}
$$

となる。なお、一般の誤差関数に対する$E_{\mathrm AV}$と$E_{\mathrm COM}$は、$(14.10)$式と$(14.11)$式に則して定義した。

## 演習 14.5

構成要素のモデルに一様でない重み付けをしたコミッティとして

$$
y_{\operatorname{COM}}(\mathbf{x})=\sum_{m=1}^{M} \alpha_{m} y_{m}(\mathbf{x}) \tag{14.55}
$$

を考える。予測$y_{\textrm{COM}}(\mathbf{x})$を理にかなった限界内に収まることを保証するため、次式のように個々の$\mathbf{x}$の値がコミッティのメンバーの最小値と最大値の間に制約されることを要求したい

$$
y_{\min }(\mathbf{x}) \leqslant y_{\operatorname{COM}}(\mathbf{x}) \leqslant y_{\max }(\mathbf{x}) \tag{14.56}
$$

この制約の必要十分条件が、係数$\alpha_{m}$が次式を満たすことであることを示せ。

$$
\alpha_{m} \geqslant 0, \quad \sum_{m=1}^{M} \alpha_{m}=1 \tag{14.57}
$$

----
まず(14.57)が(14.56)の十分条件であることを示す。
ある$y_m(\mathbf{x})$の集合を考え(14.57)を満たす範囲で$a_m$を変化させて得られる$y_{\mathrm{COM}}(\mathbf{x})$を考える。

$y_{\mathrm{COM}}(\mathbf{x})$が最大となるのは$y_k(\mathbf{x})\geq y_m(\mathbf{x})$となる$k$で$\alpha_{k}=1$、$k$以外で $\alpha_{m}=0$ となるときで，このとき $y_{\mathrm{COM}}(\mathbf{x})=y_{\max }(\mathbf{x})$である。
最小値についても同様に考えることで示すことができる。

その他の場合の$\boldsymbol{\alpha}$について

$$
y_{\min }(\mathbf{x})<y_{\mathrm{COM}}(\mathbf{x})<y_{\max }(\mathbf{x})
$$

を示す。$y_{\mathrm{COM}}(\mathbf{x})$は以下の式を満たすような$y_m(\mathbf{x})$の凸結合である。

$$
\forall m: y_{\min }(\mathbf{x}) \leqslant y_{m}(\mathbf{x}) \leqslant y_{\max }(\mathbf{x})
$$

したがって(14.57)は(14.56)の十条件である。

次に(14.57)が(14.56)の必要条件であることを示す。これは(14.56)が(14.57)の十分条件であることを示せば良い。

(14.56)を満たす任意の$\left\{y_{m}(\mathbf{x})\right\}$ を選んだとき(14.57)が成り立つことを示す。

$\alpha_k$が$\mathbf{\alpha}$で最小の値を取るとき，すなわち$k \neq m$のkについて$\alpha_{k} \leqslant \alpha_{m}$であるとき

このとき $y_{k}(\mathbf{x})=1$ かつ全ての$m \neq k$に対して $y_{m}(\mathbf{x})=0$である場合を考える. このときコミッティの中で最小の要素は $y_{\min }(\mathbf{x})=0$ である一方で $y_{\mathrm{COM}}(\mathbf{x})=\alpha_{k}$である。
したがって(14.56)から$\alpha_{k} \geqslant 0$であることがわかる。 また $\alpha_{k}$ は $\alpha$ の中で最小の値を選んでいるので任意の$\alpha$について $\alpha \geqslant 0$が成り立つことがわかる。 同様に, 全ての$m$について$y_{m}(\mathbf{x})=1$である場合を考える。このとき $y_{\min }(\mathbf{x})=y_{\max }(\mathbf{x})=1$であり,  $y_{\mathrm{COM}}(\mathbf{x})=\sum_{m} \alpha_{m}$となる. このとき(14.56)から $\sum_{m} \alpha_{m}=1$となることがわかる.

以上から(14.56)が(14.57)の十分条件であることがわかる。




## 演習 14.6

誤差関数

$$
\begin{aligned} E &=e^{-\alpha_{m} / 2} \sum_{n \in \mathcal{T}_{m}} w_{n}^{(m)}+e^{\alpha_{m} / 2} \sum_{n \in \mathcal{M}_{m}} w_{n}^{(m)} \\ &=\left(e^{\alpha_{m} / 2}-e^{-\alpha_{m} / 2}\right) \sum_{n=1}^{N} w_{n}^{(m)} I\left(y_{m}\left(\mathbf{x}_{n}\right) \neq t_{n}\right)+e^{-\alpha_{m} / 2} \sum_{n=1}^{N} w_{n}^{(m)} \end{aligned} \tag{14.23}
$$

を$\alpha_m$について微分することにより、AdaBoostアルゴリズムにおけるパラメータ$\alpha_m$が

$$
\alpha_{m}=\ln \left\{\frac{1-\epsilon_{m}}{\epsilon_{m}}\right\} \tag{14.17}
$$

を利用して更新されることを示せ。ここで$\epsilon_{m}$は

$$
\epsilon_{m}=\frac{\sum_{n=1}^{N} w_{n}^{(m)} I\left(y_{m}\left(\mathbf{x}_{n}\right) \neq t_{n}\right)}{\sum_{n=1}^{N} w_{n}^{(m)}} \tag{14.16}
$$

で定義される。

----
$$
E=\left(e^{\frac{α_{m}}{2}}-e^{-\frac{α_{m}}{2}}\right) \sum_{n=1}^{N} \omega_{n}^{(m)} I\left(y_{m}\left(\mathbf{x}_{n}\right) \neq t_{n}\right)+e^{-\frac{αm}{2}} \sum_{n=1}^{N} \omega_{n}^{(m)}
$$

$E$を最小化する$\alpha_{m}$は

$$
0=\frac{\partial E}{\partial \alpha_{m}}=\left(\frac{1}{2} e^{\frac{\alpha_{m}}{2}}+\frac{1}{2} e^{-\frac{\alpha_{m}}{2}}\right) \sum_{n=1}^{N} w_{n}^{(m)} I\left(y_{m}\left(\mathbf{x}_{n}\right) \neq t_{n}\right) - \frac{1}{2} e^{-\frac{α_{m}}{2}} \sum_{n=1}^{N} w_{n}^{(m)}
$$

これより

$$
0=\left(e^{\alpha_{m}}+1\right) \sum_{n=1}^{N} w_{n}^{(m)} I\left(y_{n}\left(\mathbf{x}_{n}\right) \neq t_{n}\right)-\sum_{n=1}^{N} w_{n}^{(m)}
$$

$$
e^{\alpha_{m}} \sum_{n=1}^{N} w_{n}^{(m)} I\left(y_{m}\left(\mathbf{x}_{n}\right) \neq t_{n}\right)=\sum_{n=1}^{N} w_{n}^{(m)}-\sum_{n=1}^{N} w_{n}^{(m)} I\left(y_{m}\left(\mathbf{x}_{n}\right) \neq t_{n}\right)
$$

$$
\begin{aligned}
e^{\alpha_{m}}&=\frac{\sum_{n=1}^{N} w_{n}^{(m)}}{\sum_{n=1}^{N} w_{n}^{(m)} I\left(y_{m}\left(\mathbf{x}_{n}\right) \neq t_{n}\right)}-1 \\
&=\frac{1}{\epsilon_{m}}-1 \\
&=\frac{1-\epsilon_{m}}{\epsilon_{m}}
\end{aligned}
$$

$$
\alpha_{m}=\ln \left\{\frac{1-\epsilon_{m}}{\epsilon_{m}}\right\}
$$

## 演習 14.7

$$
\mathbb{E}_{\mathbf{x}, t}[\exp \{-t y(\mathbf{x})\}]=\sum_{t} \int \exp \{-t y(\mathbf{x})\} p(t \mid \mathbf{x}) p(\mathbf{x}) \mathrm{d} \mathbf{x} \tag{14.27}
$$

で与えられる指数誤差関数の期待値を、すべての可能な関数$y(\mathbf{x})$について変分最小化すれば、最小化関数は

$$
y(\mathbf{x})=\frac{1}{2} \ln \left\{\frac{p(t=1 \mid \mathbf{x})}{p(t=-1 \mid \mathbf{x})}\right\} \tag{14.28}
$$

で与えられることを示せ。

----
$$
\begin{aligned}
\mathbb{E}_{\mathbf{x}, t}[\exp \{-t y(\mathbf{x})\}]&=\sum_{t} \int \exp \{-t y(\mathbf{x})\} p(t \mid \mathbf{x}) p(\mathbf{x}) d \mathbf{x} \\
&=\int \sum_{t} \exp \{-t y(\mathbf{x})\} p(t \mid \mathbf{x}) p(\mathbf{x}) d \mathbf{x}
\end{aligned}
$$

ここで、$\mathbb{E}_{x, t}[\exp \{-t y(\mathbf{x})\}]$の$y$についての停留条件は(D.8)で与えられる。

$$
\begin{aligned}
0&=\frac{\partial G}{\partial y} \\
 &=\sum_{t}(-t) \exp \{-t y(\mathbf{x})\} p(t \mid \mathbf{x}) p(\mathbf{x}) \\
 &=\{-\exp \{-y(\mathbf{x})\} p(t=1 \mid \mathbf{x})+\exp \{y(\mathbf{x})\} p(t=-1 \mid \mathbf{x}] p(\mathbf{x})
\end{aligned}
$$

$$
\therefore \quad \exp \{-y(\mathbf{x})\} p(t=1 \mid \mathbf{x})=\exp \{y(\mathbf{x})\} p(t=-1 \mid \mathbf{x})
$$

これより

$$
\exp \{2 y(\mathbf{x})\}=\frac{p(t=1 \mid \mathbf{x})}{p(t=-1 \mid \mathbf{x})}
$$

よって

$$
y(\mathbf{x})=\frac{1}{2} \ln \frac{p(t=1 \mid \mathbf{x})}{p(t=-1 \mid \mathbf{x})}
$$

## 演習 14.8

AdaBoostアルゴリズムで最小化される指数誤差関数

$$
E=\sum_{n=1}^{N} \exp \left\{-t_{n} f_{m}\left(\mathbf{x}_{n}\right)\right\} \tag{14.20}
$$

が、いかなる望ましい性質を持つ(well-behaved)確率モデルの対数尤度にも対応しないことを示せ。このためには、対応する条件付き分布$p(t\mid \mathbf{x})$を正しく正規化できないことを示せばよい。

----
一例として、誤差関数と負の対数尤度関数を対応付けると、次のような比例関係で定式化できる

$$
\begin{aligned}
\exp (-E) &=\prod_{n=1}^{N} \exp \left(-\exp \left\{-t_{n} f_{m}\left(\mathbf{x}_{n}\right)\right\}\right) \\
p\left(t_{n} \mid \mathbf{x}_{n}\right) & \propto \exp \left(-\exp \left\{-t_{n} f_{m}\left(\mathbf{x}_{n}\right)\right\}\right)
\end{aligned}
$$

二値変数$t_{n}=\{-1, 1\}$について和を取って分配関数は

$$
Z=\exp \left(-\exp \left\{f_{m}\left(\mathbf{x}_{n}\right)\right\}\right)+\exp \left(-\exp \left\{-f_{m}\left(\mathbf{x}_{n}\right)\right\}\right)
$$

で表せる。これは$f_{m}(\mathbf{x})$に依存しており、正規化された$p\left(t_{n} \mid \mathbf{x}_{n}\right)$は最早$\exp \left(-\exp \left\{-t_{n} f_{m}\left(\mathbf{x}_{n}\right)\right\}\right)$ に比例する関数系では表現できない。

## 演習 14.9

ブースティングのスタイルによる

$$
f_{m}(\mathbf{x})=\frac{1}{2} \sum_{l=1}^{m} \alpha_{l} y_{l}(\mathbf{x}) \tag{14.21}
$$

の形の加算モデルの二乗和誤差関数の逐次最小化では、新しい個別のベース分類器は、直前のモデルから得られた残留誤差$t_{n}-f_{m-1}(\mathbf{x}_{n})$によるフィッティングとなることを示せ。

----
2乗和誤差関数は（1.2）より

$$
E=\frac{1}{2} \sum_{n=1}^{N}\left\{f_{m}\left(x_{n}, \theta\right)-t_{n}\right\}^{2}
$$

ここで、(14.21)より

$$
f_{m}(x, \theta)=\frac{1}{2} \sum_{l=1}^{m} \alpha_{l} y_{l}\left(x, \theta_{l}\right)
$$

とする。

$y_{1} \sim y_{m-1}, \alpha_{1} \sim \alpha_{m-1}$を固定して、$y_{m}, \alpha_{m}$ついてEを逐次最適化する。

Eの式で$y_{m}, \alpha_{m}$を抜き出すと

$$
E=\frac{1}{2} \sum_{n=1}^{N}\left(f_{m-1}+\frac{1}{2} \alpha_{m} y_{m}-t_{n}\right)^{2}
$$

これを最小化する$y_{m}$は

$$
f_{m-1}+\frac{1}{2} \alpha_{m} y_{m}-t_{n}=0
$$

$$
\therefore \quad y_{m}=\frac{2}{\alpha_{m}}\left(t_{n}-f_{m-1}\right)
$$

で与えられる。

## 演習 14.10

訓練集合の値$\{t_{n}\}$との間の二乗和誤差が最小となる単一の予測値$t$を得ようとするなら、$t$の最適解は$\{t_{n}\}$の平均値で与えられることを検証せよ。

----
訓練集合の値との二乗和誤差は

$$
E=\sum_{\{t_{n}\}}\left(t-t_{n}\right)^{2}
$$

$E$を最小にする$t$は

$$
0=\frac{\partial E}{\partial t}=\sum_{\{t_{n}\}} 2\left(t-t_{n}\right)
$$

で与えられる。これを解けば

$$
\therefore t=\frac{1}{N^{\prime}} \sum_{\{t_{n}\}} t_{n}
$$

を得る。ここで、$N^{\prime}$は訓練集合の値$\{t_{n}\}$の個数を表している。すなわち、最適な$t$の値は$\{t_{n}\}$の平均値となることが示された。


## 演習 14.11

クラス$\mathcal{C}_{1}$からの$400$個のデータ点と、クラス$\mathcal{C}_{2}$からの$400$個のデータ点からなるデータ集合を考える。木モデルAは、最初の葉ノード($\mathcal{C}_{1}$を予測)に$(300,100)$を割り当て、2番目の葉ノード($\mathcal{C}_{2}$を予測)には$(100,300)$を割り当てるように分割すると仮定する。ここで、$(n,m)$は$n$個の点が$\mathcal{C}_{1}$に割り当てられ、$m$個の点が$\mathcal{C}_{2}$に割り当てられることを示す。同様にして、二つ目の木モデルBでは、それぞれ$(200,400)$と$(200,0)$に分割するものとする。二つの木の誤判別率を評価し、それらが等しいことを示せ。同様に、 二つの木に対する枝刈り基準

$$
C(T)=\sum_{\tau=1}^{|T|} Q_{\tau}(T)+\lambda|T| \tag{14.31}
$$

が、交差エントロピーの場合

$$
Q_{\tau}(T)=-\sum_{k=1}^{K} p_{\tau k} \ln p_{\tau k} \tag{14.32}
$$

とジニ係数の場合

$$
Q_{\tau}(T)=\sum_{k=1}^{K} p_{\tau k}\left(1-p_{\tau k}\right) \tag{14.33}
$$

の両者において、木Aよりも木Bにおいて値が小さくなることを示せ。

----
性能を示す誤分類の関数によって結果が異なることを示す、簡単な計算問題。まず、誤判別率は、

$$
\begin{aligned}
R_A = \frac{100+100}{400+400} = \frac{1}{4} \\
R_B= \frac{0+200}{400+400} = \frac{1}{4}
\end{aligned}
$$

次に交差エントロピーを用いた枝刈基準は、(14.31)と(14.32)を用いて

$$
\begin{aligned}
&C_{\textrm{Xent}}(T_A) = -2 \left(\frac{100}{400}\ln \frac{100}{400} +\frac{300}{400}\ln \frac{300}{400}\right)+ 2\lambda \simeq 1.12 + 2\lambda \\
&C_{\textrm{Xent}}(T_B) = -\frac{400}{400}\ln \frac{400}{400} -\frac{200}{400}\ln \frac{200}{400}-\frac{0}{400}\ln \frac{0}{400}-\frac{200}{400}\ln \frac{200}{400}+ 2\lambda \simeq 0.69 + 2\lambda
\end{aligned}
$$

よって、Bの方が小さい。

同様に、ジニ係数の場合について,

$$
\begin{aligned}
&C_{\textrm{Gini}}(T_A) = 2 \left(\frac{300}{400}\left(1 - \frac{300}{400} \right)+\frac{100}{400}\ln \frac{100}{400}\right)+ 2\lambda =\frac{3}{4} + 2\lambda \\
&C_{\textrm{Gini}}(T_B) = \frac{400}{400}\left(1- \frac{400}{400}\right) +\frac{200}{400}\left(1- \frac{200}{400}\right) +\frac{0}{400}\left(1- \frac{0}{400}\right) +\frac{200}{400}\left(1- \frac{200}{400}\right) = \frac{1}{2}+2\lambda
\end{aligned}
$$

よって、Bの方が小さい。

## 演習 14.12

14.5.1節の線形回帰の混合モデルについての結果を、ベクトル$\mathbf{t}$で記述される複数の目標値の場合に拡張せよ。これを行うためは3.1.5節の結果を利用せよ。

----
(3.32)式のようにtを多次元にした場合、(14.34)にあたる式は

$$
p(\mathbf{t} \mid \boldsymbol{\theta})=\sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(\mathbf{t} \mid \mathbf{W}^{\mathrm{T}} \boldsymbol{\phi}, \beta^{-1} \mathbf{I}\right)
$$

(14.35)-(14.37), $Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text {old }}\right)$を同様に変えていき、(14.39)は

$$
Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text {old }}\right)=\sum_{n=1}^{N} \gamma_{n k}\left\{-\frac{\beta}{2}\left\|\mathbf{t}_{n}-\mathbf{W}^{\mathrm{T}} \boldsymbol{\phi}_{n}\right\|^{2}\right\}+\text { const. }
$$

となる。
同様に(14.42)は

$$
\mathbf{W}_{k}=\left(\boldsymbol{\Phi}^{\mathrm{T}} \mathbf{R}_{k} \boldsymbol{\Phi}\right)^{-1} \boldsymbol{\Phi}^{\mathrm{T}} \mathbf{R}_{k} \mathbf{T}
$$

(14.43)は

$$
Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\text {old }}\right)=\sum_{n=1}^{N} \sum_{k=1}^{K} \gamma_{n k}\left\{\frac{D}{2} \ln \beta-\frac{\beta}{2}\left\|\mathbf{t}_{n}-\mathbf{W}^{\mathrm{T}} \boldsymbol{\phi}_{n}\right\|^{2}\right\}
$$

となり、(14.44)が

$$
\frac{1}{\beta}=\frac{1}{N D} \sum_{n=1}^{N} \sum_{k=1}^{K} \gamma_{n k}\left\|\mathbf{t}_{n}-\mathbf{W}^{\mathrm{T}} \boldsymbol{\phi}_{n}\right\|^{2}
$$

となる。

## 演習 14.13

線形回帰モデルの混合における完全データに対する対数尤度関数が

$$
\ln p(\mathbf{t}, \mathbf{Z} \mid \boldsymbol{\theta})=\sum_{n=1}^{N} \sum_{k=1}^{K} z_{n k} \ln \left\{\pi_{k} \mathcal{N}\left(t_{n} \mid \mathbf{w}_{k}^{\mathrm{T}} \boldsymbol{\phi}_{n}, \beta^{-1}\right)\right\} \tag{14.36}
$$

で与えられることを検証せよ。

----
$(14.34)$の混合分布から出発して、9.2節で示したガウス分布の混合分布と同じステップを踏むことになる。

$(9.10)$,$(9.11)$と同様に

$$
\begin{aligned}
&p(\mathbf{z} \mid \boldsymbol{\theta})=\prod_{k=1}^{K} \pi_{k}^{z_{k}} \\
&p(t \mid \mathbf{z},\boldsymbol{\theta})=\prod_{k=1}^{K} \mathcal{N}\left(t \mid \mathbf{w}_{k}^{\mathrm{T}} \boldsymbol{\phi}_{n}, \beta^{-1}\right)^{z_{k}}
\end{aligned}
$$

である。これより

$$
\begin{aligned}
p(t, \mathbf{z} \mid \boldsymbol{\theta}) &=p(t \mid \mathbf{z}, \boldsymbol{\theta}) p(\mathbf{z} \mid \boldsymbol{\theta}) \\
&=\prod_{k=1}^{K} \mathcal{N}\left(t \mid \mathbf{w}_{k}^{\mathrm{T}} \boldsymbol{\phi}_{n}, \beta^{-1}\right)^{z_{k}} \prod_{k=1}^{K} \pi_{k}^{z_{k}} \\
&=\prod_{k=1}^{K}\left\{\pi_{k} \mathcal{N}\left(t \mid \mathbf{w}_{k}^{\mathrm{T}} \boldsymbol{\phi}_{n}, \beta^{-1}\right)\right\}^{z_{k}}
\end{aligned}
$$

二値潜在変数の集合$\mathbf{Z} = \{\mathbf{z}_{n}\}$を導入し、各データ点$n$に対して$\mathbf{z}_{n}$は$z_{n k} \in\{0,1\}$であるとする。$\left(t_{1}, \mathbf{z}_{1}\right) \cdots\left(t_{N}, \mathbf{z}_{N}\right)$はi.i.dなので

$$
\begin{aligned}
p(\mathbf{t}, \mathbf{Z} \mid \boldsymbol{\theta}) &=\prod_{n=1}^{N} p\left(t_{n}, \mathbf{z}_{n} \mid \boldsymbol{\theta}\right) \\
&=\prod_{n=1}^{N} \prod_{k=1}^{K}\left\{\pi_{k} \mathcal{N}\left(t_{n} \mid \mathbf{w}_{k}^{\mathrm{T}} \boldsymbol{\phi}_{n}, \beta^{-1}\right)\right\}^{z_{n k}} \end{aligned}
$$

両辺の対数をとって

$$
\ln p(\mathbf{t}, \mathbf{Z} \mid \boldsymbol{\theta})=\sum_{n=1}^{N} \sum_{k=1}^{K} z_{n k} \ln \left\{\pi_{k} \mathcal{N}\left(t_{n} \mid \mathbf{w}_{k}^{\mathrm{T}} \boldsymbol{\phi}_{n}, \beta^{-1}\right)\right\} \quad \tag{14.36}
$$


## 演習 14.14

ラグランジュ未定乗数法(付録E)の技術を利用し、EMアルゴリズムによる最尤推定で訓練される線形回帰モデルの混合では、Mステップで行う混合係数を再推定する式が

$$
\pi_{k}=\frac{1}{N} \sum_{n=1}^{N} \gamma_{n k} \tag{14.38}
$$

で与えられることを示せ.

----
ラグランジアンは、

$$
\begin{aligned}
L &=Q\left(\boldsymbol{\theta}, \boldsymbol{\theta}^{\rm{old}}\right)+\lambda\left(\sum_{k=1}^{K} \pi_{k}-1\right) \\
&=\sum_{n=1}^{N} \sum_{k=1}^{K} \gamma_{n k}\left\{\ln \pi_{k}+\ln \mathcal{N}\left(t_{n} \mid \mathbf{w}_{k}^{\mathrm{T}} \boldsymbol{\phi}_{n}, \beta^{-1}\right)\right\}+\lambda\left(\sum_{k=1}^{K} \pi_{k}-1\right)
\end{aligned}
$$

$\pi_{k}$について停留条件

$$
\begin{aligned}
&0=\frac{\partial L}{\partial \pi_{k}}=\sum_{n=1}^{N} \gamma_{n k} \frac{1}{\pi_{k}}+\lambda \\
&\therefore \pi_{k}=-\frac{1}{\lambda} \sum_{n=1}^{N} \gamma_{n k}
\end{aligned}
$$

$\lambda$について停留条件

$$
0=\frac{\partial L}{\partial \lambda}=\sum_{k=1}^{K} \pi_{k}-1
$$

より、

$$
1=\sum_{k=1}^{K} \pi_{k}=\sum_{k=1}^{K} \left( -\frac{1}{\lambda} \sum_{n=1}^{N} \gamma_{n k} \right)
$$

$$
\begin{aligned}
\therefore\quad \lambda&=-\sum_{n=1}^{N} \sum_{k=1}^{K} \gamma_{n k} \\
&=-\sum_{n=1}^{N} \underbrace{\sum_{k=1}^{K} \frac{\pi_{k} \mathcal{N}\left(t_{n} \mid \mathbf{w}_{k}^{\mathrm{T}} \boldsymbol{\phi}_{n}, \beta^{-1}\right)}{\sum_{j} \pi_{j} \mathcal{N}\left(t_{n} \mid \mathbf{w}_{j}^{\mathrm{T}} \boldsymbol{\phi}_{n}, \beta^{-1}\right)}}_{=1} \quad (\because(14.37))\\
&=-N
\end{aligned}
$$

よって、

$$
\pi_{k}=\frac{1}{N} \sum_{n=1}^{N} \gamma_{n k} \tag{14.38}
$$

を得る。

## 演習 14.15

すでに述べたように、回帰問題において二乗損失関数を用いれば、新しい入力ベクトルに対する目標変数の最適な予測値は予測分布の条件付き平均で与えられる。14.5.1節で議論した線形回帰モデルの混合に対しては、条件付き平均が構成要素分布ごとの平均を線形結合した形で与えられることを示せ。ただし、もしも目標データの条件付き分布が多峰性なら、条件付き平均の予測性能は低くなることに注意せよ。

----
※「すでに述べたように、回帰問題において二乗損失関数を用いれば、新しい入力ベクトルに対する目標変数の最適な予測値は予測分布の条件付き平均で与えられる。」これは1.5.5節や第3章の冒頭で述べている。

線形回帰モデルの混合の場合について考える。条件付き平均は$\mathbb{E}[t\mid \boldsymbol{\theta}]$と表される。ここで、$\boldsymbol{\theta}$は重みパラメータ$\mathbf{w}_{k}$で支配される$K$個の線形回帰モデル内のすべての適応パラメータの集合，すなわち$\mathbf{w}=\{\mathbf{w}_{k}\}$，$\boldsymbol{\pi} = \{\pi_{k}\}$および共通の精度パラメータ$\beta$をまとめて表している。$p(t\mid \boldsymbol{\theta})$は

$$
p(t \mid \boldsymbol{\theta})=\sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(t \mid \mathbf{w}_{k}^{\mathrm{T}} \boldsymbol{\phi}, \beta^{-1}\right) \tag{14.34}
$$

で与えられる。

新しい入力ベクトル$\boldsymbol{\widehat{\phi}}$が与えられたときの予測分布$p(t \mid \widehat{\boldsymbol{\phi}}, \boldsymbol{\boldsymbol{\theta}})$を求める。有向グラフは下図の通り。

![](https://i.imgur.com/yFvquyR.png)

$$
\begin{aligned}
p(t \mid \widehat{\boldsymbol{\phi}}, \boldsymbol{\boldsymbol{\theta}})&=\sum_{\mathbf{\widehat{z}}} p(t, \mathbf{\widehat{z}} \mid \widehat{\boldsymbol{\phi}}, \boldsymbol{\pi}, \beta, \mathbf{W}) \\
&=\sum_{\mathbf{\widehat{z}}} p(t \mid \mathbf{\widehat{z}}, \widehat{\boldsymbol{\phi}}, \boldsymbol{\pi}, \beta, \mathbf{W}) p(\mathbf{\widehat{z}} \mid \widehat{\boldsymbol{\phi}}, \boldsymbol{\pi}, \beta, \mathbf{W}) \\
&=\sum_{\mathbf{\widehat{z}}} p(t \mid \mathbf{\widehat{z}}, \widehat{\boldsymbol{\phi}}, \beta, \mathbf{W}) p(\mathbf{\widehat{z}} \mid \boldsymbol{\pi}) (\because \text { 有向分離) }\\
&=\sum_{\mathbf{\widehat{z}}} p(t \mid \mathbf{\widehat{z}}, \widehat{\boldsymbol{\phi}}, \beta, \mathbf{W}) \left(\prod_{k=1}^{K} \pi_{k}^{\hat{z_{k}}}\right) (\because \text { 演習14.13) }\\
&=\sum_{\mathbf{\widehat{z}}} \prod_{k=1}^{K} \pi_{k}^{\hat{z_{k}}} \mathcal{N}\left(t \mid \mathbf{w}_{k}^{\mathrm{T}} \widehat{\boldsymbol{\phi}}, \beta^{-1}\right)^{\hat{z_{k}}}  (\because \text { 演習14.13) }\\
&=\sum_{\mathbf{\widehat{z}}} \prod_{k=1}^{K}\left(\pi_{k} \mathcal{N}\left(t \mid \mathbf{w}_{k}^{\mathrm{T}} \widehat{\boldsymbol{\phi}}, \beta^{-1}\right)\right)^{\hat{z_{k}}} \\
&=\sum_{j=1}^{K} \prod_{k=1}^{K}\left(\pi_{k} \mathcal{N}\left(t \mid \mathbf{w}_{k}^{\mathrm{T}} \widehat{\boldsymbol{\phi}}, \beta^{-1}\right)\right)^{\delta_{k j}}(\because \text { 演習9.3; 1-of-K 符号化法) } \\
&=\sum_{k=1}^{K} \pi_{k} \mathcal{N}\left(t \mid \mathbf{w}_{k}^{\mathrm{T}} \widehat{\boldsymbol{\phi}}, \beta^{-1}\right)
\end{aligned}
$$

よって

$$
\begin{aligned}
\mathbb{E}[t \mid \widehat{\boldsymbol{\phi}}, \boldsymbol{\theta}] &=\int t p(t \mid \widehat{\boldsymbol{\phi}}, \boldsymbol{\theta}) d t \\
&=\sum_{k=1}^{K} \pi_{k} \int t \mathcal{N}\left(t \mid \mathbf{w}_{k}^{\mathrm{T}} \widehat{\boldsymbol{\phi}}, \beta^{-1}\right) d t \\
&=\sum_{k=1}^{K} \pi_{k} \mathbb{E}\left[t \mid \widehat{\boldsymbol{\phi}}, \mathbf{w}_{k}, \beta \right]
\end{aligned}
$$

の形となる。これは、「新しい入力ベクトルに対する目標変数の最適な予測値（→条件付き平均）が構成要素分布ごとの平均を線形結合した形で与えられる」ことを表している。

## 演習 14.16

14.5.2節で議論したロジスティック回帰の混合モデルを、$C \geq 2$のクラスを表現できるソフトマックス分類器の混合に拡張せよ。最尤推定を通じて、このモデルのパラメータを決定するためのEMアルゴリズムを書き下せ。

----
14.5.2節の議論（2クラス回帰）を、多クラス回帰に拡張する。4.3.2節の議論を4.3.4節の議論に拡張する流れと同様に解けばよい。

$$
\begin{aligned}
p(\mathbf{t} | \mathbf{w}) = \prod_{n=1}^N y_{n}^{t_n}(1-y_n)^{1-t_n}
\end{aligned} \tag{4.89}
$$

の多クラス版として

$$
\begin{aligned}
p(\mathbf{T} | \mathbf{w}_1,\cdots ,\mathbf{w}_K) = \prod_{n=1}^N \prod_{k=1}^K y_{nk}^{t_{nk}}
\end{aligned} \tag{4.107}
$$

に拡張したのと同様に、

$$
\begin{aligned}
p(\mathbf{t}|\boldsymbol\theta) = \prod_{n=1}^N \left( \sum_{k=1}^K \pi_k y_k^t (1-y_k)^{1-t} \right)
\end{aligned}\tag{14.46}
$$

の多クラス版は、

$$
\begin{aligned}
p(\mathbf{T}|\boldsymbol\Theta)
= \prod_{n=1}^N \left( \sum_{k=1}^K \pi_k \prod _{c=1}^C y_{kc}^{t_c} \right)
\end{aligned}
$$

と書ける。ここで、$c$はクラスラベルを表す。（4章ではクラスラベルを$k$で表していたが、14章では$k$を既に混合要素のラベルとして使っているので、問題文で別の文字ラベルが指定されている。）
教科書(14.46)-(14.49)式と同じ議論を行うことで、

$$
\begin{aligned}
Q(\boldsymbol\theta, \boldsymbol\theta ^{\rm old})
= \sum_{n=1}^N \sum_{k=1}^K \gamma_{nk} \ \left[ \ln \pi_k + t_n \ln y_{nk} + (1-t_n) \ln (1-y_{nk}) \right]
\end{aligned} \tag{14.49}
$$

に対応する完全データ対数尤度関数の多クラス版は、$\mathbf{t}$が1-of-K符号なので

$$
\begin{aligned}
Q(\boldsymbol\Theta, \boldsymbol\Theta ^{\rm old}) = \sum_{n=1}^N \sum_{k=1}^K \gamma_{nk} \ \left[ \ln \pi_k + \sum_{c=1}^C t_{nc} \ln y_{nkc} \right]
\end{aligned}
$$

と書ける。（教科書(4.108)式の交差エントロピーに相当する。）

Mステップのうち、$\boldsymbol\pi$の最大化は混合ロジスティックモデルと同様で、(14.50)式そのものである。
次に、Mステップの$\mathbf{W}_1, \cdots ,\mathbf{W}_K$での最大化を考える。ここで、$\mathbf{W}_k = (\mathbf{w}_{k1}, \cdots ,\mathbf{w}_{kC} )$を指す。

(14.51)、(14.52)式に相当する勾配$Q=0$の式とヘッシアンの式は、

$$
\begin{aligned}
\nabla _{\mathbf{w}_{kc}} Q &=\sum_{n=1}^N \gamma_{nk} (t_{nc} - y_{nkc} ) \phi_n = 0\\
\mathbf{H}_k &= -\nabla _{w_{kc}} \nabla_{w_{kc'}} Q = \sum_{n=1}^N \gamma_{nk} y_{nkc} (I_{cc'} - y_{nkc'}) \phi_n \phi_n^{\rm T}
\end{aligned}
$$

となる。（(4.109)式や(14.51)式の導出と同様。ちなみに、14章の$E$は、4章とは$E$と逆符号で定義されていることに注意。この解答では14章の符号の定義に合わせた。）これは解析的に解けないので、(14.51)式と同様に、反復的な手法（IRLSアルゴリズムなど）を用いて解くことになる。

## 演習 14.17

条件付き分布の混合モデル$p(t \mid \mathbf{x})$として次式を考える。

$$
p(t \mid \mathbf{x})=\sum_{k=1}^{K} \pi_{k} \psi_{k}(t \mid \mathbf{x}) \tag{14.58}
$$

ここでは、各々の混合構成要素$\psi_{k}(t\mid \mathbf{x})$自体が混合モデルである。この2レベルの階層的混合が、標準的な1レベルの混合モデルと等価であることを示せ。ここでさらに、階層的モデルにおける両方のレベルの混合係数は$\mathbf{x}$についての任意の関数であると考える。この階層的モデルも再び、$\mathbf{x}$に依存した混合係数を持つ1レベルモデルと等価であることを示せ。最後に、階層的混合の両方のレベルの混合係数が線形分類（ロジスティックもしくはソフトマックス）モデルに制限される場合を考える。一般的に、階層的混合が混合係数に線形分類モデルを用いた1レベルの混合では表現できないことを示せ。
ヒント：このためには、一つの反例を示せば十分である。そこで、混合係数が線形ロジスティックモデルである二つの構成要素の混合で、それらのうちの一つの構成要素それ自体が二つの構成要素の混合であると考える。そして、これが線形ソフトマックスモデルで決定される混合係数をもつ3つの構成要素による単一レベルの混合では表現できないことを示せばよい。

----
ゲート関数が定数の場合

$(14.58)$における$\psi_{k}(t \mid \mathbf{x})$が混合モデルで表せるので

$$
\psi_{k}(t \mid \mathbf{x})=\sum_{m=1}^{M} \lambda_{m k} \phi_{m k}(t \mid \mathbf{x})
$$

と書くことができる。$(14.58)$に代入して

$$
\begin{aligned}
p(t \mid \mathbf{x}) &=\sum_{k=1}^{K} \pi_{k} \sum_{m=1}^{M} \lambda_{m k} \phi_{m k}(t \mid \mathbf{x}) \\
&=\sum_{k=1}^{K} \sum_{m=1}^{M} \pi_{k} \lambda_{m k} \phi_{m k}(t \mid \mathbf{x})
\end{aligned}
$$

$\pi_{k} \lambda_{m k}= \eta_{l}$とおいて添字を振り直すことで

$$
p(t \mid \mathbf{x})=\sum_{l=1}^{L} \eta_{l} \phi_{l}(t \mid \mathbf{x})
$$

と書くことができ1レベルモデルと等価であることがわかる。

ゲート関数が$\mathbf{x}$についての任意の関数の場合

$\pi_{k}$と$\lambda_{m k}$が$x$に依存する場合でも非負であり、$x$のあらゆる値に対して合計が1になるという制約のもとで1レベルの混合モデルで書くことができる。

ゲート関数が線形分類モデルの場合

問題文のヒントに従った混合モデルを構成することで示す。

![](https://i.imgur.com/p2PMfoU.png)


図12の左図に示すような木構造の階層的な混合モデルを考える。ルートレベルでは、これは2つの要素を持つ混合モデルである。混合係数は線形ロジスティック回帰モデルによって与えられ、入力に依存する。左のサブツリーは局所条件付き密度モデル$\psi_{1}(t \mid \mathbf{x})$に対応する。右のサブツリーでは、ルートからの構造が複製され、両方のサブツリーが局所条件付き密度モデル、$\psi_{2}(t \mid \mathbf{x})$および$\psi_{3}(t \mid \mathbf{x})$を含むという違いがある。
結果として得られる混合モデルは、以下の混合係数で(14.58)の形式で書くことができる。

$$
\begin{aligned}
&\pi_{1}(\mathbf{x})=\sigma\left(\mathbf{v}_{1}^{\mathrm{T}} \mathbf{x}\right) \\
&\pi_{2}(\mathbf{x})=\left(1-\sigma\left(\mathbf{v}_{1}^{\mathrm{T}} \mathbf{x}\right)\right) \sigma\left(\mathbf{v}_{2}^{\mathrm{T}} \mathbf{x}\right) \\
&\pi_{3}(\mathbf{x})=\left(1-\sigma\left(\mathbf{v}_{1}^{\mathrm{T}} \mathbf{x}\right)\right)\left(1-\sigma\left(\mathbf{v}_{2}^{\mathrm{T}} \mathbf{x}\right)\right)
\end{aligned}
$$

ここで、$\sigma(-)$は(4.59)で定義され、$\mathbf{v}_1$, $\mathbf{v}_2$はロジスティック回帰モデルのパラメータ・ベクトルである。式より$\pi_{1}(\mathbf{x})$は$\mathbf{v}_2$に依存しない。これは混合係数が1レベルのソフトマックスモデルを使用してモデル化された場合，すなわち以下の式でゲート関数が書ける場合には当てはまらない。

$$
\pi_{k}(\mathbf{x})=\frac{e^{\mathbf{u}_{k}^{\mathrm{T}} \mathbf{x}}}{\sum_{j}^{3} e^{\mathbf{u}_{j}^{\mathrm{T}} \mathbf{x}}}
$$

ここで、$\pi_{k}(\mathbf{x})$に対応するパラメータ$\mathbf{u}_k$は、分母を通して他の混合係数$\pi_{j\neq k}(\mathbf{x})$にも影響する。これにより、線形ソフトマックスモデルと比較して、入力空間上の混合係数のモデル化において、階層的モデルは異なる特性を持つことになる。図12の右図に例を示すが、赤線は入力空間における混合係数が等しい境界を表している。この境界線は2本の直線で形成されており、図12左図の2つのロジスティックユニットに相当する。ソフトマックスモデルで入力空間を分割すると、3本の直線が1点で結ばれ、例えばPRMLの図4.3の赤線のような形になるが、線形3クラスソフトマックスモデルでは図12右パネルのような境界を実装できないことに注意されたい。

![](https://i.imgur.com/7pFB89n.png)

