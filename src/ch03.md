# PRML第3章演習問題解答

<head>
<style>
  div.panel-primary {
	border: 1px solid #000;
    margin: 10px 5px;
    padding: 16px 10px 0px;
  }
</style>
</head>

## 演習 3.1
<div class="panel-primary">

tanh関数とロジステイックシグモイド関数

$$
\sigma(a)=\frac{1}{1+\exp (-a)} \tag{3.6}
$$

は次のように関係付けられることを示せ．

$$
\tanh(a) = 2\sigma(2a)-1 \tag{3.100}
$$

さらに，次の形のロジステイックシグモイド関数の線形結合

$$
y(x, \mathbf{w})=w_{0}+\sum_{j=1}^{M} w_{j} \sigma\left(\frac{x-\mu_{j}}{s}\right) \tag{3.101}
$$

は次の形の$\tanh$関数の線形結合

$$
y(x, \mathbf{u})=u_{0}+\sum_{j=1}^{M} u_{j} \tanh \left(\frac{x-\mu_{j}}{2 s}\right) \tag{3.102}
$$

と等価であることを示し，新しいパラメータ$\{ u_0, \ldots, u_M\}$ともとのパラメータ$\{ w_0, \ldots, w_M\}$を関係付ける式を求めよ．

</div>

双曲線関数$\sinh$と$\cosh$関数を使うと
$$
\sinh a = \frac{e^a - e^{-a}}{2},\quad \cosh a = \frac{e^a + e^{-a}}{2}
$$
であるから、
$$
\tanh a = \frac{\sinh a}{\cosh a} = \frac{e^a - e^{-a}}{e^a + e^{-a}}
$$
である。これと$(3.100)$式の右辺を計算すると
$$
2\sigma(2a) -1 = \frac{2}{1+e^{-2a}}-1 = \frac{1-e^{-2a}}{1+e^{-2a}} = \frac{e^a - e^{-a}}{e^a + e^{-a}}
$$
となるので、$\tanh a = 2\sigma(2a) -1$が示された。

また$(3.100)$の関係式から$\displaystyle \sigma(a) = \frac{1}{2}\left\{ \tanh \left(\frac{a}{2}\right) + 1\right\}$となるので、

$$
\begin{aligned}
    w_0+\sum_{j=1}^M w_j \sigma\left( \frac{x-\mu_j}{s} \right) &= w_0 + \sum_{j=1}^M \left\{ \frac{w_j}{2} \tanh \left( \frac{x-\mu_j}{2s} \right) + \frac{w_j}{2} \right\} \\
    &= w_0 + \sum_{j=1}^M \frac{w_j}{2} + \sum_{j=1}^M \frac{w_j}{2} \tanh \left( \frac{x-\mu_j}{2s} \right)
\end{aligned}
$$
これと$(3.102)$式の形を比較すれば
$$
u_0 = w_0 + \sum_{j=1}^M \frac{w_j}{2},\quad u_j = \frac{w_j}{2}
$$
と関係付けることができる。

## 演習 3.2
<div class="panel-primary">

行列

$$
\mathbf{\Phi}\left(\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm{T}} \tag{3.103}
$$

は任意のベクトル$\mathbf{v}$を$\mathbf{\Phi}$の列ベクトルで張られる空間の上に正射影することを示せ．そしてこの結果を使って，最小二乗解

$$
\mathbf{w}_{\mathrm{ML}}=\left(\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm{T}} \textsf{t} \tag{3.15}
$$

は図3.2で示した多様体$\mathcal{S}$の上にベクトル$\mathsf{t}$を正射影することに対応していることを示せ．

![PRML_Fig3.2.png](/attachment/5f4281e56b27b0172dd66336)<br>
Figure 3.2

</div>

【前半】

この問題で線形空間（多様体）$\mathcal{S}$は行列$\mathbf{\Phi}$の$j$番目の列ベクトル$\boldsymbol{\varphi}_j$を基底としている。つまり、任意のスカラー$x_j$を用いて$\displaystyle \sum_{j=1}^M x_j\boldsymbol{\varphi}_j$の形で書けるベクトルは線形空間$\mathcal{S}$に含まれる。

あるベクトル$\mathbf{v}$の線形空間$\mathcal{S}$への正射影とは、次の2つを満たすベクトル$\mathbf{v}^{\prime}$のことである。

1. ベクトル$\mathbf{v}^{\prime}$が$\mathcal{S}$上に存在する。すなわち$\displaystyle \mathbf{v}^{\prime} = \sum_{j=1}^M x_j\boldsymbol{\varphi}_j=\mathbf{\Phi}\mathbf{x}$と書ける。

2. ベクトル$\mathbf{v}-\mathbf{v}^{\prime}$が線形空間$\mathcal{S}$と直交する。すなわち任意の$j$について$\boldsymbol{\varphi}_j^{\mathrm{T}}(\mathbf{v}-\mathbf{v}^{\prime})=0$つまり$\displaystyle \mathbf{\Phi}^{\mathrm{T}}(\mathbf{v}-\mathbf{v}^{\prime})=0$が成立する。

以上を踏まえて、まずベクトル$\mathbf{v}$に$\mathbf{\Phi}\left(\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm{T}}$を作用させた$\mathbf{v}^{\prime}$を考える。すなわち
$$
\begin{aligned}
\mathbf{v}^{\prime} &= \mathbf{\Phi}\left(\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm{T}}\mathbf{v} = \mathbf{\Phi}\tilde{{\mathbf{v}}}=\sum_{j=1}^M \tilde{v_j} \boldsymbol{\varphi}_j
\end{aligned}
$$
とする。ここで$\boldsymbol{\varphi}_j$は$\mathbf{\Phi}$の$j$番目の列ベクトルで$\tilde{\mathbf{v}} \equiv \left(\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm{T}}\mathbf{v}$とする。$\tilde{v_j}$はベクトル$\tilde{\mathbf{v}}$の$j$番目の要素である（スカラー）。これは$\boldsymbol{\varphi}_j$についての線形結合となっているので、上の正射影の条件1を満たしている。

ちなみに

$$
\Phi=\begin{pmatrix}\mathbf{\phi}_{0}\left(\mathbf{x}_{1}\right) & \phi_{1}\left(\mathbf{x}_{1}\right) & \cdots & \phi_{M-1}\left(\mathbf{x}_{1}\right) \\ \phi_{0}\left(\mathbf{x}_{2}\right) & \phi_{1}\left(\mathbf{x}_{2}\right) & \cdots & \phi_{M-1}\left(\mathbf{x}_{2}\right) \\ \vdots & \vdots & \ddots & \vdots \\ \phi_{0}\left(\mathbf{x}_{N}\right) & \phi_{1}\left(\mathbf{x}_{N}\right) & \cdots & \phi_{M-1}\left(\mathbf{x}_{N}\right)\end{pmatrix} = (\boldsymbol{\varphi}_1\ \boldsymbol{\varphi}_2\ \cdots \ \boldsymbol{\varphi}_{M})  \tag{3.16}
$$

としたので番号は1つずつずれているけれど気にしないでOK。

次に正射影の条件2のために$\mathbf{v}-\mathbf{v}^{\prime}$と$\mathcal{S}$の直交性を調べる。

$$
\begin{aligned}
\mathbf{\Phi}^{\mathrm{T}}(\mathbf{v}-\mathbf{v}^{\prime}) &= \mathbf{\Phi}^{\mathrm{T}}(\mathbf{I} - \mathbf{\Phi}(\mathbf{\Phi}^{\mathrm{T}}\mathbf{\Phi})^{-1}\mathbf{\Phi}^{\mathrm{T}})\mathbf{v} \\
&=\mathbf{\Phi}^{\mathrm{T}}\mathbf{v} - \mathbf{\Phi}^{\mathrm{T}}\mathbf{v} \\
&= \mathbf{0}
\end{aligned}
$$
であるから、$\mathbf{v}-\mathbf{v}^{\prime}$と$\mathcal{S}$は直交していることが示された。以上条件1と2が成立しているので、$\mathbf{v}^{\prime}$は$\mathbf{v}$の$\mathcal{S}$への正射影であることが示された。

（参考：https://python.atelierkobato.com/projection/）

【後半】

続いて、3.1.2節の流れから$n$番目の要素が$y(\mathbf{x}_n,\mathbf{w})$で与えられる$N$次元ベクトル$\mathsf{y}$を定義すると、$\mathsf{y}$の構成は

$$
\mathsf{y} = \begin{pmatrix}y(\mathbf{x}_1,\mathbf{w}) \\ \vdots \\ y(\mathbf{x}_n,\mathbf{w})\end{pmatrix} = \begin{pmatrix}\mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}(\mathbf{x}_1) \\ \vdots \\ \mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}(\mathbf{x}_n)\end{pmatrix} = \begin{pmatrix}\boldsymbol{\phi}(\mathbf{x}_1)^{\mathrm{T}}\mathbf{w} \\ \vdots \\ \boldsymbol{\phi}(\mathbf{x}_n)^{\mathrm{T}}\mathbf{w}\end{pmatrix} = \mathbf{\Phi}\mathbf{w}
$$

となっている。これと$(3.15)$から$\mathbf{w}_{\mathrm{ML}} = (\mathbf{\Phi}^{\mathrm{T}}\mathbf{\Phi})^{-1}\mathbf{\Phi}^{\mathrm{T}} \mathsf{t}$を代入してみると

$$
\mathsf{y} = \mathbf{\Phi}\mathbf{w}_{\mathrm{ML}} = \mathbf{\Phi}(\mathbf{\Phi}^{\mathrm{T}}\mathbf{\Phi})^{-1}\mathbf{\Phi}^{\mathrm{T}} \mathsf{t}
$$

となる。ここで、この数式と【前半】の議論より、$\mathsf{y}$がベクトル$\mathsf{t}$の線形空間$\mathcal{S}$への正射影であることは明らかになっている。したがって題意（最小二乗解$\mathsf{y}$はデータベクトル$\mathsf{t}$の部分空間$\mathcal{S}$上への正射影に対応する）は示された。

> 統計のための行列代数（上）の第12章 射影と射影行列に詳しい議論が載っている。

## 演習 3.3
<div class="panel-primary">

それぞれのデータ点$t_n$に重み要素$r_n>0$が割り当てられており，二乗和誤差関数が

$$
E_{D}(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N} r_{n}\left\{t_{n}-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right\}^{2} \tag{3.104}
$$

となるデータ集合を考える．このとき，この誤差関数を最小にする解$\mathbf{w}^{*}$についての式を求めよ．また，(i)ノイズの分散がデータに依存する場合，(ii)データ点に重複がある場合に照らして，それぞれ重み付き二乗和誤差関数の解釈を与えよ．

</div>

誤差関数を最小にする解を求めたいので、$(3.104)$式を$\mathbf{w}$で微分する。その前に$(3.104)$式を行列形式で書き直したい。

$\displaystyle \mathbf{R} = \operatorname{diag}(r_1,r_2,\ldots,r_n)$（つまり対角成分が$r_1,r_2,\ldots,r_n$で残りが$0$の行列）とすると、$(3.104)$式は次のように書ける。

$$
\begin{aligned}
E_{D}(\mathbf{w}) &=\frac{1}{2} \sum_{n=1}^{N} r_{n}\left\{t_{n}-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right\}^{2} \\
&=\frac{1}{2}\left\{\left(\begin{array}{c}
t_{1} \\
\vdots \\
t_{n}
\end{array}\right)-\left(\boldsymbol{\phi}\left(\mathbf{x}_{1}\right), \cdots, \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right)\left(\begin{array}{c}
w_{1} \\
\vdots \\
w_{n}
\end{array}\right)\right\}^{\mathrm{T}} \mathbf{R} \left\{\left(\begin{array}{c}
t_{1} \\
\vdots \\
t_{n}
\end{array}\right)-\left(\boldsymbol{\phi}\left(\mathbf{x}_{1}\right), \cdots, \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right)\left(\begin{array}{c}
w_{1} \\
\vdots \\
w_{n}
\end{array}\right)\right\} \\
&=\frac{1}{2}(\mathsf{t}-\mathbf{\Phi} \mathbf{w})^{\mathrm{T}} \mathbf{R}(\mathsf{t}-\mathbf{\Phi} \mathbf{w})
\end{aligned}
$$
$\mathbf{w}$で微分すると
$$
\begin{aligned}
\frac{\partial E_D}{\partial \mathbf{w}} &=\frac{1}{2}\left\{-2 \mathbf{\Phi}^{\mathrm{T}} \mathbf{R}(\mathsf{t}-\mathbf{\Phi} \mathbf{w})\right\} \quad \left(\because \frac{\partial}{\partial \mathbf{s}}(\mathbf{x}-\mathbf{A} \mathbf{s})^{\mathrm{T}} \mathbf{W}(\mathbf{x}-\mathbf{A} \mathbf{s})=-2 \mathbf{A}^{\mathrm{T}} \mathbf{W}(\mathbf{x}-\mathbf{A} \mathbf{s}),\ \textrm{if}\ \mathbf{W} \textrm{is symmetric.}\right)\\
&=-\mathbf{\Phi}^{\mathrm{T}} \mathbf{R}(\mathsf{t}-\mathbf{\Phi} \mathbf{w})=0 \\
& \mathbf{\Phi}^{\mathrm{T}} \mathbf{R} \mathsf{t}=\mathbf{\Phi}^{\mathrm{T}} \mathbf{R} \mathbf{\Phi} \mathbf{w}^{*} \\
\therefore \quad \mathbf{w}^{*}&=\left(\mathbf{\Phi}^{\mathrm{T}} \mathbf{R} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm{T}} \mathbf{R} \mathsf{t}
\end{aligned}
$$
なお行列の微分には https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf の$(84)$の公式を使った。また、こうして得られた$\mathbf{w}^{*}$は単位行列$\mathbf{R} = \mathbf{I}$とすれば$(3.15)$の正規方程式と一致する。

1. ノイズの分散がデータに依存する場合というのは$(3.8)$において全データの分散の逆数（精度）が$\beta$で一定ではなく、データごとに$y_n$と変化することを意味する。このとき$(3.8)-(3.12)$の導出に沿って対数尤度関数をとると
$$
\begin{aligned}
\ln p(\mathsf{t} \mid \mathbf{x}, \mathbf{w}, \mathbf{y}) &=\sum_{n=1}^{N} \ln \mathcal{N}\left(t_n \mid \mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right), y_{n}^{-1}\right) \\
&=\sum_{n=1}^{N} \ln \left\{\frac{y_{n}^{1 / 2}}{(2 \pi)^{1 / 2}} \exp \left(-\frac{y_{n}\left(t_{n}-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right)^{2}}{2}\right)\right\} \\
&=\frac{1}{2} \sum_{n=1}^{N} \ln y_{n}-\frac{N}{2} \ln (2 \pi)-\frac{1}{2} \sum_{n=1}^{N} y_{n}\left\{t_{n}-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right\}^{2}
\end{aligned}
$$
となる。ここで$y_n = r_n$とすればまさに重み付き二乗和誤差関数として$(3.104)$が現れていることがわかる。

2. データ点に重複がある場合、そのデータ点についての**実効的な**数として見なすことができる。尤度関数$\ln p(\mathsf{t}\mid \mathbf{w},\beta)$の最大化は二乗和誤差関数$E_D(\mathbf{w})$の最小化と等価であることを考えれば、例えば$N$個の点のうち$(\mathbf{x}_1, t_1), (\mathbf{x}_2, t_2)$のみが同じだった場合に$r_1=1, r_2=0$とおけば実質1つカウントとして見なせるし、そうしないこともできる……ってことかな？

## 演習 3.4
<div class="panel-primary">

次の形の線形モデル

$$
y(\mathbf{x}, \mathbf{w})=w_{0}+\sum_{i=1}^{D} w_{i} x_{i} \tag{3.105}
$$

と二乗和誤差関数

$$
E_{D}(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N}\left\{y\left(\mathbf{x}_{n}, \mathbf{w}\right)-t_{n}\right\}^{2} \tag{3.106}
$$

を考える．平均$0$，分散$\sigma^2$のガウスノイズ$\epsilon_i$が独立にそれぞれの入力変数$x_i$に加わるものとする．$\mathbb{E}[\epsilon_i] = 0$と$\mathbb{E}\left[\epsilon_{i} \epsilon_{j}\right]=\delta_{i j} \sigma^{2}$の2つの性質を用いて，$E_D$のノイズ分布に関する平均を最小にすることは，ノイズのない入力変数に対する二乗和誤差と荷重減衰の正則化項の和を最小にすることと等価であることを示せ．ただし，正則化項にバイアスパラメータ$w_0$は含めない．

</div>

※ 線形モデルの入力ベクトル$\mathbf{x}$の各次元$x_i$にノイズ$\epsilon_i$が加わったときの二乗和誤差について、その二乗和誤差のノイズ$\epsilon_i$についての期待値を取ったものを最小化することが、$\mathbf{w}$の正則化項を考慮した最小二乗法と同じ形式になることを示します。

ガウスノイズ$\epsilon_{i}$が入力変数$x_i$に加えられるので
$$
\begin{aligned}
    \tilde{y}(\mathbf{x}_n,\mathbf{w}) &= w_0 + \sum_{i=1}^D w_{ni}(x_{ni}+\epsilon_{ni}) \\
    &= y(\mathbf{x}_n,\mathbf{w})+\sum_{i=1}^D w_i \epsilon_{ni}
\end{aligned}
$$
となる。これの二乗和誤差関数は
$$
\begin{aligned}
    \tilde{E}_D(\mathbf{w}) &= \frac{1}{2}\sum_{n=1}^N\left\{
        \tilde{y}(\mathbf{x}_n,\mathbf{w}) - t_n
        \right\}^2 \\
    &=\frac{1}{2}\sum_{n=1}^N\left\{
        \tilde{y}_n^2-2\tilde{y}_nt_n+t_n
        \right\}^2 \\
    &=\frac{1}{2}\sum_{n=1}^N\left\{
        y_n^2+2y_n\sum_{i=1}^D{w_i\epsilon_{ni}}+\left(\sum_{i=1}^D w_i \epsilon_{ni} \right)^2 -2 y_n t_n-2\sum_{i=1}^D w_i\epsilon_{ni} + t_n^2
    \right\} \quad \cdots (*)
\end{aligned}
$$
と展開できる。ここで$\tilde{E}_D(\mathbf{w})$についての期待値$\mathbb{E}\left[\tilde{E}_D(\mathbf{w})\right]$をとると$\mathbb{E}[\epsilon_i]=0$より$(*)$の第2項と第5項は0になる。また、第3項については$\mathbb{E}[\epsilon_i \epsilon_j]=\delta_{ij}\sigma^2$から
$$
\mathbb{E}\left[\left( \sum_{i=1}^D w_i \epsilon_{ni}\right)^2\right] = \sum_{i=1}^D w_i^2\sigma^2
$$
となるので
$$
\mathbb{E}\left[\tilde{E}_D(\mathbf{w})\right] = E_D(\mathbf{w}) + \frac{1}{2}\sum_{i=1}^D w_i^2\sigma^2
$$
と表せる。さらに$(3.25)$のような重みベクトルの二乗和$E_W(\mathbf{w})=\frac{1}{2}\mathbf{w}^{\mathrm{T}}\mathbf{w}$を用いると
$$
\mathbb{E}\left[\tilde{E}_D(\mathbf{w})\right] = E_D(\mathbf{w}) + \frac{\sigma^2}{2}\mathbf{w}^{\mathrm{T}}\mathbf{w}
$$
となる。これは、ノイズのない入力変数に対する二乗和誤差$E_D(\mathbf{w})$と$\lambda=\sigma^2$にしたときの荷重減衰の正則化項の和$E_W(\mathbf{w})$の和の形になっている。したがって題意が示された。

> この問題の意味を考えてみましょう。簡単のため、入力が一次元$x_1$だけだったとします。
> 入力$x_1$にノイズ$\epsilon_1$が加わると、そのノイズもパラメータ$w_1$倍されて、誤差に含まれることになります。二乗誤差なので$w_1^2$倍になります。ノイズの分布に関して二乗誤差の期待値を取ると、ノイズの平均はゼロなので、平均の影響は消えてしまうのですが、分散の影響は$\sigma^2 \epsilon_1^2$として残ります。つまり、分散が大きいほど、二乗誤差が大きくなります。そのため、この二乗誤差を最小化しようとすると、$w_1^2$を小さくする方向に力が働く(正則化)ことになります。
>
> 入力が多次元だったとしても、基本的な考えは同じです。ノイズの平均と共分散がゼロなので、ほとんどの項がうまく消えてしまいます。入力がノイズで撹乱されるほど(分散$\sigma^2$が大きいほど)、二乗和誤差は増加します。

## 演習 3.5
<div class="panel-primary">

付録Eに示したラグランジュ未定乗数法を用いて，正則化誤差関数

$$
\frac{1}{2} \sum_{n=1}^{N}\left\{t_{n}-\mathbf{w}^{\mathrm{T}} \phi\left(\mathrm{x}_{n}\right)\right\}^{2}+\frac{\lambda}{2} \sum_{j=1}^{M}\left|w_{j}\right|^{q} \tag{3.29}
$$

の最小化と，正則化されていない二乗和誤差

$$
E_{D}(\mathbf{w})=\frac{1}{2} \sum_{n=1}^{N}\left\{t_{n}-\mathbf{w}^{\mathrm{T}} \phi\left(\mathbf{x}_{n}\right)\right\}^{2} \tag{3.12}
$$

の制約条件

$$
\sum_{j=1}^{M}|w_j|^q \le \eta \tag{3.30}
$$

下での最小化が等価であることを示せ．そして，パラメータ$\eta$と$\lambda$の関係を議論せよ．

</div>

※ ヒントとしてラグランジュの未定乗数法を使うと書いてあるのですぐに導けるが、このヒントがないとなぜ等価なのかわかりにくいかもしれない、という問題。

制約条件$(3.30)$が不等式なので付録Eのラグランジュ未定乗数法の不等式制約の場合を参考にする。

$(3.30)$を変形すると$\displaystyle \frac{1}{2}\left( \sum_{j=1}^M|w_j|^q - \eta \right) \leq 0$である。（後で$(3.29)$に合わせるために$\frac{1}{2}$をわざとつけている）

ラグランジュの未定乗数法を用いると

$$
\begin{aligned}
L(\mathbf{w},\lambda) &= \frac{1}{2}\sum_{j=1}^{M}\left\{ t_n - \mathbf{w}^{\mathrm{T}}\phi(\mathbf{x}_n)\right\}^2 + \frac{\lambda}{2}\left( \sum_{j=1}^M|w_j|^q - \eta \right) \\
&= \frac{1}{2}\sum_{j=1}^{M}\left\{ t_n - \mathbf{w}^{\mathrm{T}}\phi(\mathbf{x}_n)\right\}^2 + \frac{\lambda}{2}\sum_{j=1}^M|w_j|^q - \frac{\lambda\eta}{2}
\end{aligned}
$$

これについて$\displaystyle \frac{\partial}{\partial\mathbf{w}}\left( - \frac{\lambda\eta}{2} \right) = 0$なので、$L(\mathbf{w},\lambda)$を$\mathbf{w}$について最小化させることと、$(3.29)$式の$\mathbf{w}$についての最小化は等価であることが示された。

またラグランジュの未定乗数法の不等式制約におけるKarush-Kuhn-Tucker条件は
$$
\left\{
  \begin{array}{l}
    \frac{1}{2}\left( \sum_{j=1}^M|w_j|^q - \eta \right) \leq 0 \\
    \lambda \geq 0 \\
    \frac{\lambda}{2}\left( \sum_{j=1}^M|w_j|^q - \eta \right) = 0
  \end{array}
\right.
$$
となる。最後の等式が$L(\mathbf{w},\lambda)$を$\mathbf{w}$について最小化させた$\mathbf{w}^*$で成立する必要がある。
すなわち、$\mathbf{w}^*$で$\displaystyle \eta = \sum_{j=1}^M|w_j|^q$となる。これより、$q$は正則化項の形状（lasso, ridge...）を示している。$\lambda$は正則化項の大きさを表している。

> とても重要な回帰におけるパラメータ正則化(もしくは重み減衰)の問題。パラメータ正則化は非常に重要なテクニックなのですが、よくある下の図を見てなんとなく理解したつもりになっている人が多い気がします。<br>
<img src="/attachment/611f4b7fe07f664fed1981a3" width="400px"><br>
$q=2$のときの正則化項を加えた$(3.24)$の誤差関数を最小化することが上の図の通りになることを考察してみます。制約条件$(3.30)$はパラメータ$\mathbf{w}$が図の領域のオレンジ部分$\sum_{j=1}^{M}|w_j|^q \leq \eta$内に存在しなければならないことを表しています。青線は正則化されていない二乗和誤差関数$(3.26)$式の$E_D(\mathbf{w})$の等高線表示です。ラグランジュの未定乗数法を使うことで、領域$(3.30)$の制限下での誤差関数$E_D(\mathbf{w})$の最小化ができるようになり、これが$(3.27)$（$q=2$の場合）、$(3.29)$(一般形)と等価になります。

## 演習 3.6
<div class="panel-primary">

ガウス分布に従う複数の目標変数$\mathbf{t}$を持つ次の形の線形基底関数モデルを考える．

$$
p(\mathbf{t} | \mathbf{W}, \mathbf{\Sigma})=\mathcal{N}(\mathbf{t} | \mathbf{y}(\mathbf{x}, \mathbf{W}), \mathbf{\Sigma}) \tag{3.107}
$$

ただし，

$$
\mathbf{y}(\mathbf{x}, \mathbf{W}) = \mathbf{W}^{\mathrm{T}}\boldsymbol{\phi}(\mathbf{x}) \tag{3.108}
$$

である．入力基底ベクトルの$\boldsymbol{\phi}(\mathbf{x}_n)\ (n=1,\ldots,N)$とそれに対応する目標ベクトル$\mathbf{t}_n$が訓練データ集合として与えられるとき，パラメータ行列$\mathbf{W}$の最尤推定解$\mathbf{W}_{\mathrm{ML}}$のそれぞれの列が，等方性のノイズ分布に対する解の

$$
\mathbf{w}_{\mathrm{ML}}=\left(\Phi^{\mathrm{T}} \Phi\right)^{-1} \Phi^{\mathrm{T}} \mathbf{t} \tag{3.15}
$$

の形の式で与えられることを示せ．これは共分散行列$\mathbf{\Sigma}$にはよらないことに注意せよ．さらに，$\mathbf{\Sigma}$の最尤推定解が

$$
\mathbf{\Sigma}=\frac{1}{N} \sum_{n=1}^{N}\left(\mathbf{t}_{n}-\mathbf{W}_{\mathrm{ML}}^{\mathrm{T}} \phi\left(\mathbf{x}_{n}\right)\right)\left(\mathbf{t}_{n}-\mathbf{W}_{\mathrm{ML}}^{\mathrm{T}} \phi\left(\mathbf{x}_{n}\right)\right)^{\mathrm{T}} \tag{3.109}
$$

で与えられることを示せ．

</div>

※ **3.1.5 出力変数が多次元の場合**　を参考にしてすすめる。また、途中の行列の微分については付録だけでは足りないので https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf を参考にする。

$\mathbf{y}(\mathbf{X},\mathbf{W})=\mathbf{W}^{\mathrm T}\boldsymbol{\phi}(\mathbf{x})$について、$\mathbf{y}$は$K$次元列ベクトル、$\mathbf{W}$は$M\times K$のパラメータ行列、$\boldsymbol{\phi}(\mathbf{x})$は$\boldsymbol{\phi}_j(\mathbf{x})$を$j$番目の要素に持つ$M$次元の列ベクトルである。また、$n$番目の観測値$\mathbf{t}_n$は$K$次元の列ベクトル、観測値の集合$\mathbf{t}_1, \mathbf{t}_2, \cdots \mathbf{t}_N$をまとめて$n$番目の行が$\mathbf{t}_n^{\mathrm T}$となる$N\times K$行列$\mathbf{T}$とする。

$(3.107)$について最尤推定解を得るために対数尤度関数を考えると

$$
\begin{aligned}
\ln p(\mathbf{t} \mid \mathbf{W}, \mathbf{\Sigma}) &=\ln \prod_{n=1}^{N} \mathcal{N}\left(\mathbf{t}_{n} \mid \mathbf{W}^{\mathrm T} \boldsymbol{\phi}(\mathbf{x}), \mathbf{\Sigma}\right) \\
&=\sum_{n=1}^{N} \ln \mathcal{N}\left(\mathbf{t}_{n} \mid \mathbf{W}^{\mathrm T} \boldsymbol{\phi}(\mathbf{x}), \mathbf{\Sigma}\right) \\
&=\sum_{n=1}^{N} \ln \left\{\frac{1}{(2 \pi)^{\frac{K}{2}}} \frac{1}{\left|\mathbf{\Sigma}\right|^{\frac{1}{2}}} \exp \left(-\frac{1}{2}\left(\mathbf{t}_{n}-\mathbf{W}^{\mathrm T} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right)^{\mathrm T} \mathbf{\Sigma}^{-1} \left(\mathbf{t}_{n}-\mathbf{W}^{\mathrm T} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right)\right)\right\}\\
&=-\frac{N K}{2} \ln (2 \pi)-\frac{N}{2} \ln |\mathbf{\Sigma}|-\frac{1}{2} \sum_{n=1}^{N}\left(\mathbf{t}_{n}-\mathbf{W}^{\mathrm T} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right)^{\mathrm T} \mathbf{\Sigma}^{-1} \left(\mathbf{t}_{n}-\mathbf{W}^{\mathrm T} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right)
\end{aligned}
$$

$\mathbf{W}_{\mathrm{ML}}$を求めるために$\mathbf{W}$について偏微分すると

$$
\begin{aligned}
\frac{\partial}{\partial \mathbf{W}} \ln p(t \mid \mathbf{W}, \Sigma) &=-\frac{1}{2} \sum_{n=1}^{N} \frac{\partial}{\partial \mathbf{W}}\left\{\left(\mathbf{t}_{n}-\mathbf{W}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right)^{\mathrm{T}} \mathbf{\Sigma}^{-1} \left(\mathbf{t}_{n}-\mathbf{W}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right)\right\} \\
&= -\frac{1}{2}\sum_{n=1}^N \left( \mathbf{\Sigma}^{-1} + (\mathbf{\Sigma}^{-1})^{\mathrm T}\right) \left(\mathbf{t}_{n}-\mathbf{W}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right) \left(- \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)^{\mathrm T}\right) \quad \left(\because \frac{\partial}{\partial \mathbf{X}}(\mathbf{X} \mathbf{b}+\mathbf{c})^{\mathrm T} \mathbf{D}(\mathbf{X} \mathbf{b}+\mathbf{c})=\left(\mathbf{D}+\mathbf{D}^{\mathrm T}\right)(\mathbf{X} \mathbf{b}+\mathbf{c}) \mathbf{b}^{\mathrm T} \right)\\
&= \sum_{n=1}^N \mathbf{\Sigma}^{-1}\left(\mathbf{t}_{n}-\mathbf{W}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right) \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)^{\mathrm T} \quad \left( \because \mathbf{\Sigma}^{-1} = (\mathbf{\Sigma}^{-1})^{\mathrm T} \right)
\end{aligned}
$$

これを$0$とすると
$$
\sum_{n=1}^N \mathbf{\Sigma}^{-1}\mathbf{t}_n\boldsymbol{\phi}(\mathbf{x}_n)^{\mathrm T} = \sum_{n=1}^N \mathbf{\Sigma}^{-1}\left( \mathbf{W}_{\mathrm{ML}}^{\mathrm T} \boldsymbol{\phi}(\mathbf{x}_n)\right) \boldsymbol{\phi}(\mathbf{x}_n)^{\mathrm T}
$$
両辺に$\mathbf{\Sigma}$をかけると
$$
\sum_{n=1}^N \mathbf{t}_n\boldsymbol{\phi}(\mathbf{x}_n)^{\mathrm T} =
\mathbf{W}_{\mathrm{ML}}^{\mathrm T} \sum_{n=1}^N \boldsymbol{\phi}(\mathbf{x}_n)\boldsymbol{\phi}(\mathbf{x}_n)^{\mathrm T}
$$
これは行列形式で書くと
$$
\left( \mathbf{t}_1, \mathbf{t}_2 \cdots, \mathbf{t}_n\right)\begin{pmatrix}
\boldsymbol{\phi}\left(\mathbf{x}_{1}\right)^{\mathrm{T}} \\
\boldsymbol{\phi}\left(\mathbf{x}_{2}\right)^{\mathrm{T}} \\
\vdots \\
\boldsymbol{\phi}\left(\mathbf{x}_{N}\right)^{\mathrm{T}}
\end{pmatrix} = \mathbf{W}_{\mathrm{ML}}^{\mathrm T} \left(\boldsymbol{\phi}\left(\mathbf{x}_{1}\right), \boldsymbol{\phi}\left(\mathbf{x}_{2}\right),  \ldots, \boldsymbol{\phi}\left(\mathbf{x}_{N}\right)\right)\begin{pmatrix}
\boldsymbol{\phi}\left(\mathbf{x}_{1}\right)^{\mathrm{T}} \\
\boldsymbol{\phi}\left(\mathbf{x}_{2}\right)^{\mathrm{T}} \\
\vdots \\
\boldsymbol{\phi}\left(\mathbf{x}_{N}\right)^{\mathrm{T}}
\end{pmatrix}
$$
と書き直せる。ここで、計画行列が
$$
\mathbf{\Phi}=\begin{pmatrix}
\boldsymbol{\phi}\left(\mathbf{x}_{1}\right)^{\mathrm{T}} \\
\boldsymbol{\phi}\left(\mathbf{x}_{2}\right)^{\mathrm{T}} \\
\vdots \\
\boldsymbol{\phi}\left(\mathbf{x}_{N}\right)^{\mathrm{T}}
\end{pmatrix}, \quad \mathbf{\Phi}^{\mathrm{T}}=\left(\boldsymbol{\phi}\left(\mathbf{x}_{1}\right), \boldsymbol{\phi}\left(\mathbf{x}_{2}\right),  \ldots , \boldsymbol{\phi}\left(\mathbf{x}_{N}\right)\right)
$$
のように書けることを考えると、
$$
\mathbf{T}^{\mathrm T}\mathbf{\Phi} = \mathbf{W}_{\mathrm{ML}}^{\mathrm T}\mathbf{\Phi}^{\mathrm T}\mathbf{\Phi}
$$
と書き表せる。これより$\mathbf{W}_{\mathrm{ML}}^{\mathrm T}=\mathbf{T}^{\mathrm T}\mathbf{\Phi}\left(\mathbf{\Phi}^{\mathrm T}\mathbf{\Phi}\right)^{-1}$なので、$\mathbf{W}_{\mathrm{ML}}$を求めるよう変換していくと
$$
\begin{aligned}
\mathbf{W}_{\mathrm{ML}} &=\left(\mathbf{T}^{\mathrm T}\mathbf{\Phi}\left(\mathbf{\Phi}^{\mathrm T}\mathbf{\Phi}\right)^{-1}\right)^{\mathrm T} \\
&=\left(\left(\mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right)^{-1}\right)^{\mathrm T}\left(\mathbf{T}^{\mathrm T} \mathbf{\Phi}\right)^{\mathrm T} \quad \left(\because (\mathbf{AB})^{\mathrm T} = \mathbf{B}^{\mathrm T}\mathbf{A}^{\mathrm T}\right)\\
&=\left(\mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm T} \mathbf{T}
\end{aligned}
$$
これは$(3.15)$式のような形で求まっている。また$\mathbf{\Sigma}$によらず決まることが分かる。

$\mathbf{\Sigma}$についての最尤推定解は[演習問題2.34と同じ手続き](../PRML%E3%81%AE%E6%BC%94%E7%BF%92%E5%95%8F%E9%A1%8C%E8%A7%A3%E7%AD%94%E9%9B%86%20%E7%AC%AC2%E7%AB%A0%202.61%E3%81%BE%E3%81%A7#%E6%BC%94%E7%BF%92%202.34)で求めることができるので省略。

## 演習 3.7
<div class="panel-primary">

$\mathbf{m}_N$と$\mathbf{S}_N$がそれぞれ

$$
\mathbf{m}_{N} =\mathbf{S}_{N}\left(\mathbf{S}_{0}^{-1} \mathbf{m}_{0}+\beta \mathbf{\Phi}^{\mathrm{T}} \textsf{t}\right) \tag{3.50}
$$
$$
 \mathbf{S}_{N}^{-1} =\mathbf{S}_{0}^{-1}+\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \tag{3.51}
$$

で定義される線形基底関数モデルを考える．平方完成を用いて，このモデルのパラメータ$\mathbf{w}$の事後分布が
$$
p(\mathbf{w}| \textsf{t})=\mathcal{N}\left(\mathbf{w} \mid \mathbf{m}_{N}, \mathbf{S}_{N}\right) \tag{3.49}
$$
で与えられることを確かめよ．

</div>

（事後分布）$\propto$（尤度関数）×（事前分布）の関係式、すなわち$p(\mathbf{w}|\textsf{t}) \propto p(\textsf{t}|\mathbf{w}) p(\mathbf{w})$を使って計算する。正規化係数部分は吸収されるので、指数部分だけに着目する。

$$
\begin{aligned}
    p(\mathbf{w}|\textsf{t}) &= \prod_{n=1}^{N}\mathcal{N}(t_n|\mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}(\mathbf{x}_n))\mathcal{N}(\mathbf{w}|\mathbf{m}_0,\mathbf{S}_0) \\
    &\propto \left( \prod_{n=1}^N \exp\left[ -\frac{1}{2} \left\{ t_n - \mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x}_n) \right\}^2 \beta \right] \right) \exp \left[ -\frac{1}{2} (\mathbf{w} - \mathbf{m}_0)^{\mathrm{T}} \mathbf{S}_0^{-1}(\mathbf{w} - \mathbf{m}_0) \right] \\
    &= \exp\left[  -\frac{1}{2} \left\{ \beta \sum_{n=1}^N \left( t_n - \mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x}_n)) \right)^2 + (\mathbf{w} - \mathbf{m}_0)^{\mathrm{T}} \mathbf{S}_0^{-1}(\mathbf{w} - \mathbf{m}_0) \right\} \right] \\
\end{aligned}
$$

ここで、指数部分のみ着目すると

$$
\begin{aligned}
    & \beta \sum_{n=1}^N \left( t_n - \mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}(\mathbf{x}_n) \right)^2 + (\mathbf{w} - \mathbf{m}_0)^{\mathrm{T}}\mathbf{S}_0^{-1}(\mathbf{w} - \mathbf{m}_0)\\
    = &
        \beta \begin{pmatrix}
        t_1 - \mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}(\mathbf{x}_1) \\
        \vdots\\
        t_N - \mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}(\mathbf{x}_N)
        \end{pmatrix}^{\mathrm{T}} \begin{pmatrix}
        t_1 - \mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}(\mathbf{x}_1) \\
        \vdots\\
        t_N - \mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}(\mathbf{x}_N)
        \end{pmatrix}
        + (\mathbf{w} - \mathbf{m}_0)^{\mathrm{T}}\mathbf{S}_0^{-1}(\mathbf{w} - \mathbf{m}_0) \\
    = &
        \beta \begin{pmatrix}
        t_1 - \boldsymbol{\phi}(\mathbf{x}_1)^{\mathrm{T}}\mathbf{w} \\
        \vdots\\
        t_N - \boldsymbol{\phi}(\mathbf{x}_N)^{\mathrm{T}}\mathbf{w}
        \end{pmatrix}^{\mathrm{T}} \begin{pmatrix}
        t_1 - \boldsymbol{\phi}(\mathbf{x}_1)^{\mathrm{T}}\mathbf{w} \\
        \vdots\\
        t_N - \boldsymbol{\phi}(\mathbf{x}_N)^{\mathrm{T}}\mathbf{w}
        \end{pmatrix}
        + (\mathbf{w} - \mathbf{m}_0)^{\mathrm{T}}\mathbf{S}_0^{-1}(\mathbf{w} - \mathbf{m}_0) \hspace{1em} (\because \mathbf{a}^{\mathrm{T}}\mathbf{b} = \mathbf{b}^{\mathrm{T}}\mathbf{a})\\
\end{aligned}
$$

ここで、$(3.16)$式の計画行列が$\displaystyle \mathbf{\Phi} = \begin{pmatrix} \boldsymbol{\phi}(\mathbf{x}_1)^{\mathrm{T}} \\ \boldsymbol{\phi}(\mathbf{x}_2)^{\mathrm{T}} \\ \vdots \\ \boldsymbol{\phi}(\mathbf{x}_N)^{\mathrm{T}} \end{pmatrix}$と書けることを利用すると、上の式は以下のように変形できる。

$$
\begin{aligned}
    &= \beta (\textsf{t} - \mathbf{\Phi} \mathbf{w})^{\mathrm{T}}(\textsf{t} - \mathbf{\Phi} \mathbf{w}) + (\mathbf{w} - \mathbf{m}_0)^{\mathrm{T}}\mathbf{S}_0^{-1}(\mathbf{w} - \mathbf{m}_0)\\
    &= \beta (\mathbf{w}^{\mathrm{T}}\mathbf{\Phi}^{\mathrm{T}}\mathbf{\Phi} \mathbf{w} - \mathbf{w}^{\mathrm{T}}\mathbf{\Phi}^{\mathrm{T}}\textsf{t} - \textsf{t}^{\mathrm{T}}\mathbf{\Phi} \mathbf{w} ) + \mathbf{w}^{\mathrm{T}}\mathbf{S}_0^{-1}\mathbf{w} - \mathbf{w}^{\mathrm{T}}\mathbf{S}_0^{-1}\mathbf{m}_0 - \mathbf{m}_0^{\mathrm{T}}\mathbf{S}_0^{-1}\mathbf{w} + \text{const.} \\
    &= \mathbf{w}^{\mathrm{T}}(\mathbf{S}_0^{-1} + \beta \mathbf{\Phi}^{\mathrm{T}}\mathbf{\Phi})\mathbf{w} - \mathbf{w}^{\mathrm{T}}(\mathbf{S}_0^{-1}\mathbf{m}_0 + \beta \mathbf{\Phi}^{\mathrm{T}} \textsf{t}) - (\mathbf{S}_0^{-1}\mathbf{m}_0 + \beta \mathbf{\Phi}^{\mathrm{T}} \textsf{t})^{\mathrm{T}}\mathbf{w} + \text{const.}
\end{aligned}
$$

$\mathbf{S}_0$は共分散なので対称行列である。この逆行列も対称行列である。よって$(\mathbf{S}_0^{-1})^{\mathrm{T}} = \mathbf{S}_0^{-1}$となることに注意する。

一方で、事後分布の形$\mathcal{N}(\mathbf{w}|\mathbf{m}_N, \mathbf{S}_N)$の指数部分を同様に展開すると

$$
\mathbf{w}^{\mathrm{T}}\mathbf{S}_N^{-1}\mathbf{w} - \mathbf{w}^{\mathrm{T}}(\mathbf{S}_N^{-1}\mathbf{m}_N) - (\mathbf{S}_N^{-1}\mathbf{m}_N)^{\mathrm{T}}\mathbf{w} + \text{const.}
$$

となる。これらの係数を比較すれば

$$
\begin{aligned}
    \mathbf{S}_N^{-1} &= \mathbf{S}_0^{-1} + \beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \\
    \mathbf{m}_N &= \mathbf{S}_N(\mathbf{S}_0^{-1} \mathbf{m}_0 + \beta \mathbf{\Phi}^{\mathrm{T}}\textsf{t})
\end{aligned}
$$

すなわち$(3.50)$, $(3.51)$式が導けた。

## 演習 3.8
<div class="panel-primary">

3.1節の線形基底関数モデルを考える．そして，すでに$N$個のデータ点が観測され，$\mathbf{w}$の事後分布が
$$
p(\mathbf{w}|\textsf{t})=\mathcal{N}\left(\mathbf{w} \mid \mathbf{m}_{N}, \mathbf{S}_{N}\right) \tag{3.49}
$$
で与えられるとする．この事後分布は次に観測されるデータの事前確率とみなすことができる．追加のデータ点$(\mathbf{x}_{N+1}, t_{N+1})$を考え，指数関数の中で平方完成することにより，事後確率が再び$(3.49)$の形式で与えられ，$\mathbf{S}_N$を$\mathbf{S}_{N+1}$に，$\mathbf{m}_N$を$\mathbf{m}_{N+1}$にそれぞれ置き換えたものになることを示せ．

</div>

事後分布は尤度関数と事前分布の掛け算に比例するので

$$
\begin{aligned}
p(\mathbf{w|t})&\propto p(\mathbf{t|w})\cdot p(\mathbf{w})
\end{aligned}
$$

が満たされることがわかる．また事前分布は$p(\mathbf{w}) = \mathcal{N}(\mathbf{w} \mid \mathbf{m}_{N},\mathbf{S}_{N})$で、尤度関数が$p(t_{N+1}\mathbf{|w}) = \prod_{n=1}^{N}\mathcal{N}({t_{N+1}\mid \mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}_{N+1}}, \beta^{-1})$なので、
$$
\begin{aligned}
p(\mathbf{w|t})&\propto p(t_{N+1}|\mathbf{w}) \cdot p(\mathbf{w})\\
p(t_{N+1}|\mathbf{w}) \cdot p(\mathbf{w}) &\propto \text{exp}\left(-\frac{1}{2}(\mathbf{w-m_{N}})^{\mathbf{T}}\mathbf{S}_{N}^{-1}(\mathbf{w-m_{N}}) - \frac{\beta}{2}(t_{N+1} - \mathbf{w^{T}}\boldsymbol{\phi}_{N+1})^{2}\right)\\
\end{aligned}
$$
となる（ここで、$\mathbf{\phi}_{N+1} = \mathbf{\phi}(\mathbf{x}_{N+1})$と表記している）．指数部分だけを見ていくと

$$
\begin{aligned}
&\quad(\mathbf{w-m_{N}})^{\mathrm{T}}\mathbf{S}_{N}^{-1}(\mathbf{w-m_{N}}) + \beta(t_{N+1} - \mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}_{N+1})^{2}\\
&= \mathbf{w}^{\mathrm{T}}\mathbf{S}_{N}^{-1}\mathbf{w} -2\mathbf{w}^{\mathrm{T}}\mathbf{S}_{N}^{-1}\mathbf{m_{N}} +\beta\mathbf{w}^{\mathrm{T}}\boldsymbol{\phi}_{N+1}^{\mathrm{T}}\boldsymbol{\phi}_{N+1}\mathbf{w} - 2\beta\mathbf{w^{T}}\boldsymbol{\phi}_{N+1}t_{N+1} + \text{const.}\\
&= \mathbf{w}^{\mathrm{T}}(\mathbf{S}_{N}^{-1}+\beta\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^{\mathrm{T}})\mathbf{w} -2\mathbf{w}^{\mathrm{T}}(\mathbf{S}_{N}^{-1}\mathbf{m_{N}} + \beta\boldsymbol{\phi}_{N+1}t_{N+1}) + \text{const.}\\
\end{aligned}
$$

となるので、演習問題3.7の最後の部分と比較すれば

$$
\begin{aligned}
\therefore \quad \mathbf{S}_{N+1}^{-1} &= \mathbf{S}_{N}^{-1}+\beta\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^{\mathrm{T}}\\
\mathbf{m}_{N+1} &= \mathbf{S}_{N+1}(\mathbf{S}_{N}^{-1}\mathbf{m}_{N} + \beta\boldsymbol{\phi}_{N+1}t_{N+1})
\end{aligned}
$$

となって、題意は示された．

## 演習 3.9
<div class="panel-primary">

上記の問題を平方完成ではなく，線形ガウスモデルの一般的な結果

$$
p(\mathbf{x} \mid \mathbf{y})=\mathcal{N}\left(\mathbf{x} \mid \mathbf{\Sigma}\left\{\mathbf{A}^{\mathrm{T}} \mathbf{L}(\mathbf{y}-\mathbf{b})+\mathbf{\Lambda} \boldsymbol{\mu}\right\}, \mathbf{\Sigma}\right) \tag{2.116}
$$

を用いて示せ．

</div>

$$
\begin{aligned}
\\
(2.112) \quad p(\mathbf{x}) &= \mathcal{N}(\mathbf{x|}\boldsymbol{\mu,\Lambda^{-1}})\\
(2.114) \quad p(\mathbf{y|x}) &= \mathcal{N}(\mathbf{y|Ax+b,L}^{-1})\\
(2.116) \quad p(\mathbf{x|y}) &= \mathcal{N}(\mathbf{x|}\boldsymbol{\Sigma}\{\mathbf{A^{T}L(y-b)+\boldsymbol{\Lambda\mu}}\},\boldsymbol{\Sigma})\quad(\boldsymbol{\Sigma=(\Lambda+\mathbf{A^{T}LA})^{-1}})
\\
\end{aligned}
$$

を使って、演習3.8をもう一度解ける、観察すれば$\mathbf{w}\leftrightarrow\mathbf{x}\quad,t_{N+1}\leftrightarrow\mathbf{y}$で関係付けることができるので

$$
\begin{aligned}
p(\mathbf{w}) &= \mathcal{N}(\mathbf{w|m_{N},S_{N}})\\
p(t_{N+1}\mathbf{|w}) &= \mathcal{N}({t_{N+1}\mathbf{|w^{T}}\boldsymbol{\phi}_{N+1}}, \beta^{-1})\\
\end{aligned}
$$

と比較すれば、さらに各パラメータを次のように決めることができる．

$$
\begin{aligned}
\boldsymbol{\mu}&=\mathbf{m_{N}}\\
\boldsymbol{\Lambda}&=\mathbf{S}_{N}^{-1}\\
\mathbf{A}&=\boldsymbol{\phi}_{N+1}^{T}\\
\mathbf{b}&=0\\
\mathbf{L}&=\beta\\
\end{aligned}
$$

$(2.116)$に代入すれば

$$
\begin{aligned}
p(\mathbf{w}|t_{N+1}) &= \mathcal{N}(\mathbf{w|m}_{N+1}, \mathbf{S}_{N+1})\\
\mathbf{S}_{N+1}^{-1} &= \boldsymbol{\Sigma}^{-1} \\
&=\boldsymbol{\Lambda}+\mathbf{A^{T}LA}\\
&= \mathbf{S}_{N}^{-1}+\beta\boldsymbol{\phi}_{N+1}^{T}\boldsymbol{\phi}_{N+1}\\
\mathbf{m}_{N+1} &= \boldsymbol{\Sigma}\{\textbf{A}^\textbf{T}\textbf{L(y}-\textbf{b)} + \boldsymbol{\Lambda\mu}\}\\
&=\textbf{S}_{N+1}(\textbf{S}_{N}^{-1}\textbf{m}_{N} + \beta\boldsymbol{\phi}_{N+1}t_{N+1})\\
\end{aligned}
$$

となって、題意は示された．

## 演習 3.10
<div class="panel-primary">

$$
p(\mathbf{y})=\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \boldsymbol{\mu}+\mathbf{b}, \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathbf{T}}\right) \tag{2.115}
$$
の結果を用いて

$$
p(t \mid \mathbf{t}, \alpha, \beta)=\int p(t \mid \mathbf{w}, \beta) p(\mathbf{w} \mid \mathbf{t}, \alpha, \beta) \mathrm{d} \mathbf{w} \tag{3.57}
$$

の積分を評価し，ベイズ線形回帰モデルの予測分布が
$$
p(t \mid \mathbf{x}, \mathbf{t}, \alpha, \beta)=\mathcal{N}\left(t \mid \mathbf{m}_{N}^{\mathrm{T}} \boldsymbol{\phi}(\mathbf{x}), \sigma_{N}^{2}(\mathbf{x})\right) \tag{3.58}
$$
で与えられることを確かめよ．ただし，入力に依存する分散は
$$
\sigma_{N}^{2}(\mathbf{x})=\frac{1}{\beta}+\boldsymbol{\phi}(\mathbf{x})^{\mathrm{T}} \mathbf{S}_{N} \boldsymbol{\phi}(\mathbf{x}) \tag{3.59}
$$
で与えられる．

</div>

$(3.57)$の条件付き分布と事後分布は, それぞれ以下の式で表される。

$$
p \left (t \mid \mathbf w,\beta \right ) = \mathcal { N } \left (t \mid \mathbf w^\textrm T \boldsymbol \phi (\mathbf x),\beta^{-1} \right ) \tag{3.3, 3.8}
$$

$$
p \left (\mathbf w \mid \mathbf t,\alpha,\beta \right ) = \mathcal { N } \left (\mathbf w \mid \mathbf m _ N,\mathbf S_N \right )
\tag{3.49}
$$

ここで, $(2.115)$の式は,

$$
p \left (\mathbf x \right ) = \mathcal{N} \left( \boldsymbol{\mu},\mathbf{\Lambda}^{-1} \right) \tag{2.113}
$$

$$
p \left (\mathbf y \mid \mathbf x \right ) = \mathcal{N} \left (\mathbf{y} \mid \mathbf{A} \mathbf x + \mathbf b,\mathbf L^{-1} \right ) \tag{2.114}
$$

が与えられた際の周辺分布だったことに注意して, $(2.113)$から$(2.115)$について,

$$
\mathbf y \rightarrow \textit t, \quad \mathbf x \rightarrow \mathbf w, \quad \boldsymbol \mu \rightarrow \mathbf m_N, \quad \mathbf{\Lambda}^{-1} \rightarrow \mathbf S_N,\quad  \mathbf A \rightarrow \boldsymbol \phi (\mathbf x)^\textrm T,\quad \mathbf L^{-1}→\beta^{-1}
$$

と置き換えると, $(3.57)$を評価できる。

したがって, $(2.115)$にそれぞれを代入すると,

$$
p \left (t \mid \mathbf x,\mathbf t,\alpha,\beta \right ) = \mathcal { N } \left (t \mid \mathbf m^\textrm T_N \boldsymbol \phi (\mathbf x),\sigma ^2_N (\mathbf x) \right ) \tag{3.58}
$$
と求まる。ここで, 入力に依存する分散は

$$
\sigma ^ 2 _ N (\mathbf x) = \frac{1}{\beta}+\boldsymbol \phi (\mathbf x)^\textrm T \mathbf S_N \boldsymbol \phi (\mathbf x) \tag{3.59}
$$
である。

## 演習 3.11
<div class="panel-primary">

データ集合のサイズが増えるにつれて，モデルパラメータの事後分布に関する不確かさが減少することについて説明した．次の行列の公式（付録C参照）

$$
\left(\mathbf{M}+\mathbf{vv}^{\mathrm{T}}\right)^{-1}=\mathbf{M}^{-1}-\frac{\left(\mathbf{M}^{-1} \mathbf{v}\right)\left(\mathbf{v}^{\mathrm{T}} \mathbf{M}^{-1}\right)}{1+\mathbf{v}^{\mathrm{T}} \mathbf{M}^{-1} \mathbf{v}} \tag{3.110}
$$

を用いて，
$$
\sigma_{N}^{2}(\mathbf{x})=\frac{1}{\beta}+\boldsymbol{\phi}(\mathbf{x})^{\mathrm{T}} \mathbf{S}_{N} \boldsymbol{\phi}(\mathbf{x}) \tag{3.59}
$$
の線形回帰モデルに関する不確かさ$\sigma_{N}^{2}(\mathbf{x})$が

$$
\sigma_{N+1}^{2}(\mathbf{x}) \leq \sigma_{N}^{2}(\mathbf{x}) \tag{3.111}
$$

を満たすことを示せ．

</div>

演習問題3.8で示したように、新しい点$(\mathbf{x}_{N+1}, t_{N+1})$が与えられたとき、$\mathcal{N}(\mathbf{w}\mid \mathbf{m}_N, \mathbf{S}_N)$に対する事後分布は$\mathcal{N}(\mathbf{w}\mid \mathbf{m}_{N+1}, \mathbf{S}_{N+1})$と書け、

$$
\begin{aligned}
    \mathbf{S}_{N+1}&=\left(\mathbf{S}_{N}^{-1}+\beta \boldsymbol{\phi}_{N+1} \boldsymbol{\phi}_{N+1}^{\mathrm{T}}\right)^{-1} \\
    \mathbf{m}_{N+1}&=\mathbf{S}_{N+1}\left(\mathbf{S}_{N}^{-1} \mathbf{m}_{N}+\beta \boldsymbol{\phi}_{N+1} t_{N+1}\right)
\end{aligned}
$$

で与えられる（$\boldsymbol{\phi}_{N+1} = \boldsymbol{\phi}(\mathbf{x}_{N+1})$である）。

問いとしては$\sigma_N^{2}(\mathbf{x})-\sigma_{N+1}^{2}(\mathbf{x}) \ge 0$であることを証明すれば良い。これはまず

$$
\begin{aligned}
\sigma_N^{2}(\mathbf{x})-\sigma_{N+1}^{2}(\mathbf{x}) &= \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\mathbf{S}_N\boldsymbol{\phi}(\mathbf{x}) - \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\mathbf{S}_{N+1}\boldsymbol{\phi}(\mathbf{x}) \\
&= \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}(\mathbf{S}_{N} - \mathbf{S}_{N+1})\boldsymbol{\phi}(\mathbf{x})
\end{aligned}
$$

であるから、$\mathbf{S}_{N} - \mathbf{S}_{N+1}$を計算すると

$$
\begin{aligned}
    \mathbf{S}_{N} - \mathbf{S}_{N+1} &= \mathbf{S}_{N} - \left(\mathbf{S}_{N}^{-1} + \beta \boldsymbol{\phi}_{N+1} \boldsymbol{\phi}_{N+1}^{\mathrm{T}}\right)^{-1} \\
    &= \mathbf{S}_{N} - \left(\mathbf{S}_{N}^{-1} + \boldsymbol{\psi}_{N+1} \boldsymbol{\psi}_{N+1}^{\mathrm{T}} \right) \quad (\boldsymbol{\psi}_{N+1} = \beta^{1/2} \boldsymbol{\phi}_{N+1})\\
    &=\frac{\left(\mathbf{S}_{N} \boldsymbol{\psi}_{N+1}\right)\left(\boldsymbol{\psi}_{N+1}^{\mathrm{T}} \mathbf{S}_{N}\right)}{1+ \boldsymbol{\psi}_{N+1}^{\mathrm{T}} \mathbf{S}_{N} \boldsymbol{\psi}_{N+1}} \\
    &=\frac{\beta \mathbf{S}_{N} \boldsymbol{\phi}_{N+1} \boldsymbol{\phi}_{N+1}^{\mathrm{T}} \mathbf{S}_{N}}{1+\beta \boldsymbol{\phi}_{N+1}^{\mathrm{T}} \mathbf{S}_{N} \boldsymbol{\phi}_{N+1}}
\end{aligned}
$$

これより
$$
\begin{aligned}
\sigma_N^{2}(\mathbf{x})-\sigma_{N+1}^{2}(\mathbf{x}) &= \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\mathbf{S}_N\boldsymbol{\phi}(\mathbf{x}) - \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\mathbf{S}_{N+1}\boldsymbol{\phi}(\mathbf{x}) \\
&= \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}(\mathbf{S}_{N} - \mathbf{S}_{N+1})\boldsymbol{\phi}(\mathbf{x}) \\
&= \frac{\beta \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\mathbf{S}_{N} \boldsymbol{\phi}_{N+1} \boldsymbol{\phi}_{N+1}^{\mathrm{T}} \mathbf{S}_{N} \boldsymbol{\phi}(\mathbf{x})}{1+\beta \boldsymbol{\phi}_{N+1}^{\mathrm{T}} \mathbf{S}_{N} \boldsymbol{\phi}_{N+1}} \\
&= \frac{\beta \left\| \boldsymbol{\phi}(\mathbf{x})^{\mathrm T}\mathbf{S}_{N} \boldsymbol{\phi}_{N+1} \right\|^2}{1+\beta \boldsymbol{\phi}_{N+1}^{\mathrm{T}} \mathbf{S}_{N} \boldsymbol{\phi}_{N+1}}
\end{aligned}
$$

ここで、$\mathbf{S}_{N}$は実対称行列であり（$(3.54)$式から $\mathbf{S}_{N} = \left( \alpha \mathbf{I} + \beta\mathbf{\Phi}^{\mathrm T}\mathbf{\Phi} \right)^{-1}$）、このため正定値行列である（後述）。すなわち任意のベクトル$\mathbf{x}$において$\mathbf{x}^{\mathrm T}\mathbf{S}_{N}\mathbf{x} \gt 0$が成立するため、$\boldsymbol{\phi}_{N+1}^{\mathrm{T}} \mathbf{S}_{N} \boldsymbol{\phi}_{N+1} \gt 0$となる（また当然ながら$\beta \gt 0$）。これより、

$$
\sigma_{N+1}^{2}(\mathbf{x}) \leq \sigma_{N}^{2}(\mathbf{x}) \tag{3.111}
$$

が証明された。

※ 最後の正定値行列であることについて、定義として、任意の零ベクトルでないベクトル$\mathbf{x}$について行列$\mathbf{P}$が

$$
\mathbf{x}^{\mathrm T}\mathbf{Px} \gt 0
$$

を満たすならば、$\mathbf{P}$は正定値行列である。また、[その逆行列$\mathbf{P}^{-1}$も正定値行列](https://www.iwanttobeacat.com/entry/2019/12/31/152102)である。

そこで$\mathbf{S}_{N} = \left( \alpha \mathbf{I} + \beta\mathbf{\Phi}^{\mathrm T}\mathbf{\Phi} \right)^{-1}$について、逆行列$\mathbf{S}_{N}^{-1} = \alpha \mathbf{I} + \beta\mathbf{\Phi}^{\mathrm T}\mathbf{\Phi}$が正定値行列であることを示す。

まず$\alpha \mathbf{I}$は正定値行列である（$\because \mathbf{x}^{\mathrm T}(\alpha \mathbf{I})\mathbf{x} = \alpha \mathbf{x}^{\mathrm T}\mathbf{x} = \alpha \|\mathbf{x}\|^{2} \gt 0$）。また、$\mathbf{x}^{\mathrm T}(\beta\mathbf{\Phi}^{\mathrm T}\mathbf{\Phi})\mathbf{x} = \beta \|\mathbf{\Phi x}\|^2 \ge 0$が成り立つ（これは半正定値）。したがって線形結合である$\mathbf{S}_N^{-1}$も正定値行列であるので、$\mathbf{S}_N$が正定値行列であることが示された。

> むしろ等号成立条件がわからないけど

## 演習 3.12
<div class="panel-primary">

2.3.6節で，平均および精度（分散の逆数）がともに未知のガウス分布に対応する共役事前分布は正規ガンマ分布であることを述べた．この性質は，線形回帰モデルの条件付きガウス分布$p(t|\mathbf{x}, \mathbf{w}, \beta)$の場合にも成り立つ．尤度関数が
$$
p(\mathsf{t} \mid \mathbf{X}, \mathbf{w}, \beta)=\prod_{n=1}^{N} \mathcal{N}\left(t_{n} \mid \mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right), \beta^{-1}\right) \tag{3.10}
$$
で与えられるとき，$\mathbf{w}$と$\beta$の共役事前分布が

$$
p(\mathbf{w}, \beta)=\mathcal{N}\left(\mathbf{w} | \mathbf{m}_{0}, \beta^{-1} \mathbf{S}_{0}\right) \operatorname{Gam}\left(\beta | a_{0}, b_{0}\right) \tag{3.112}
$$

で与えられることを示せ．さらに，対応する事後分布が同様に

$$
p(\mathbf{w}, \beta | \mathbf{t})=\mathcal{N}\left(\mathbf{w} | \mathbf{m}_{N}, \beta^{-1} \mathbf{S}_{N}\right) \operatorname{Gam}\left(\beta | a_{N}, b_{N}\right) \tag{3.113}
$$

の関数形で与えられることを示し，パラメータ$\mathbf{m}_{N}, \mathbf{S}_{N}, a_N, b_N$についての式を求めよ．

</div>

（事後確率）$\propto$（尤度関数）$\times$（共役事前分布）なので、$p(\mathbf{w},\beta\mid \mathsf{t},\mathbf{x}) \propto p(\mathbf{t} \mid \mathbf{x}, \mathbf{w}, \beta)p(\mathbf{w},\beta)$である。
> 実際には入力変数$\mathbf{x}$をモデル化しようとしていないので、$\mathbf{x}$は条件としてしか現れないので$p(\mathbf{w},\beta\mid t)\propto p(\mathsf{t}\mid \mathbf{w},\beta)p(\mathbf{w},\beta)$である。

P.98を読むことで、この共役事前分布も正規-ガンマ関数として書けることがわかる。よって、$p(\mathbf{w}, \beta)=\mathcal{N}\left(\mathbf{w} | \mathbf{m}_{0}, \beta^{-1} \mathbf{S}_{0}\right) \operatorname{Gam}\left(\beta | a_{0}, b_{0}\right)$の形で書くことができる。

> 本当にこれで示せたことになるのか？感はある

これが与えられているとき、対数で事後分布を考えると
$$
\ln p(\mathbf{w},\beta\mid \mathsf{t}) = \ln p(\mathsf{t}\mid \mathbf{x},\mathbf{w},\beta) + \ln p(\mathbf{w},\beta) + \textrm{const.}
$$
と書くことができる。これを展開すると

$$
\begin{aligned}
\ln p(\mathbf{w}, \beta \mid \mathbf{t})=& \sum_{n=1}^{N} \ln p\left(t_{n} \mid \mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right), \beta^{-1}\right) + \ln p(\mathbf{w}, \beta)\\
=& \frac{M}{2} \ln \beta-\frac{1}{2} \ln \left|\mathbf{S}_{0}\right|-\frac{\beta}{2}\left(\mathbf{w}-\mathbf{m}_{0}\right)^{\mathrm{T}} \mathbf{S}_{0}^{-1}\left(\mathbf{w}-\mathbf{m}_{0}\right) - b_{0} \beta+\left(a_{0}-1\right) \ln \beta \\
+& \frac{N}{2} \ln \beta-\frac{\beta}{2} \sum_{n=1}^{N}\left\{\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)-t_{n}\right\}^{2}+\text{const.}
\end{aligned} \tag{*}
$$

この事後分布も正規-ガンマ分布の形で分解できることが示されている。すなわち$p(\mathbf{w},\beta\mid \mathsf{t}) = p(\mathbf{w} \mid \beta, \mathsf{t})p(\beta\mid \mathsf{t})$と分解でき、$\mathbf{w}$に依存する$p(\mathbf{w} \mid \beta, \mathsf{t})$がガウス分布の形$\mathcal{N}(\mathbf{w}\mid \mathbf{m}_N,\beta^{-1}\mathbf{S}_N)$になるはずである。

(*)式を$\mathbf{w}$についての関数としてまとめ直すと、$\displaystyle \sum_{n=1}^{N}\left\{\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)-t_{n}\right\}^{2} = (\mathbf{\Phi}\mathbf{w} - \mathsf{t})^{\mathrm T}(\mathbf{\Phi}\mathbf{w} - \mathsf{t})$を利用して

$$
\ln p(\mathbf{w} \mid \beta, \mathbf{t})=-\frac{\beta}{2} \mathbf{w}^{\mathrm{T}}\left[\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}+\mathbf{S}_{0}^{-1}\right] \mathbf{w}+\mathbf{w}^{\mathrm{T}}\left[\beta \mathbf{S}_{0}^{-1} \mathbf{m}_{0}+\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{t}\right]+\mathrm{const.}
$$

となる。よって、$\mathcal{N}(\mathbf{w}\mid \mathbf{m}_N, \mathbf{S}_N)$と比較すると（P.84のように$\mathbf{S}_N$から求める）、

$$
\begin{aligned}
\mathbf{S}_{N}^{-1} &=\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} + \mathbf{S}_{0}^{-1} \\
\mathbf{m}_{N} &=\mathbf{S}_{N}\left[\mathbf{S}_{0}^{-1} \mathbf{m}_{0}+\mathbf{\Phi}^{\mathrm{T}} \mathsf{t}\right]
\end{aligned}
$$

となる（$(3.50), (3.51)$と似ているようで異なる）。

次に、$p(\beta\mid \mathsf{t})$がガンマ分布の形になるはずなので(*)の$\beta$に依存する残りの項をまとめる。ただし、$\frac{M}{2}\ln \beta$は上記のガウス分布の正規化定数に相当するので$\ln p(\beta\mid \mathsf{t})$には現れなくなることに注意する。

$$
\begin{aligned}
\ln p(\beta \mid \mathbf{t}) &= \ln p(\mathbf{w},\beta\mid \mathsf{t}) - \ln p(\mathbf{w}\mid \beta, \mathsf{t})
\\ &= -\frac{\beta}{2} \mathbf{m}_{0}^{\mathrm{T}} \mathbf{S}_{0}^{-1} \mathbf{m}_{0}+\frac{\beta}{2} \mathbf{m}_{N}^{\mathrm{T}} \mathbf{S}_{N}^{-1} \mathbf{m}_{N} + \frac{N}{2} \ln \beta-b_{0} \beta+\left(a_{0}-1\right) \ln \beta-\frac{\beta}{2} \sum_{n=1}^{N} t_{n}^{2}+\mathrm{const.} \\
&=\left(\frac{N}{2}+a_0 -1\right)\ln \beta - \frac{1}{2} \left( \mathbf{m}_{0}^{\mathrm{T}} \mathbf{S}_{0}^{-1} \mathbf{m}_{0} - \mathbf{m}_{N}^{\mathrm{T}} \mathbf{S}_{N}^{-1} \mathbf{m}_{N} + \sum_{n=1}^{N} t_{n}^{2} \right)\beta - b_0\beta + \mathrm{const.}
\end{aligned}
$$

これがガンマ分布の対数形$\ln \operatorname{Gam}(\beta\mid a_N, b_N) = (a_N -1)\ln \beta - b_N \beta + \mathrm{const.}$の式と同形になるべきなので、両式を比較することで

$$
\begin{aligned}
a_{N}&=a_{0}+\frac{N}{2} \\
b_{N}&=b_{0}+\frac{1}{2}\left(\mathbf{m}_{0}^{\mathrm{T}} \mathbf{S}_{0}^{-1} \mathbf{m}_{0}-\mathbf{m}_{N}^{\mathrm{T}} \mathbf{S}_{N}^{-1} \mathbf{m}_{N}+\sum_{n=1}^{N} t_{n}^{2}\right)
\end{aligned}
$$

が得られる。

## 演習 3.13
<div class="panel-primary">

演習問題3.12で議論したモデルに対する予測分布$p(t|\mathbf{x},\mathsf{t})$が次の形のスチューデントのt分布

$$
p(t | \mathbf{x}, \mathsf{t})=\operatorname{St}(t | \mu, \lambda, \nu) \tag{3.114}
$$

で与えられることを示し，$\mu, \lambda, \nu$についての式を求めよ．

</div>

$$ \tag{3.8}
p(t \mid \mathbf{x}, \mathbf{w}, \beta)=\mathcal{N}\left(t \mid y(\mathbf{x}, \mathbf{w}), \beta^{-1}\right)
$$

$$ \tag{3.113}
p(\mathbf{w}, \beta \mid \mathsf{t}, \mathbf{X}) = \mathcal{N}\left(\mathbf{w} \mid \mathbf{m}_{N}, \beta^{-1} \mathbf{S}_{N}\right) \operatorname{Gam}\left(\beta \mid a_{N}, b_{N}\right)
$$

と$3.3.2$節の議論より，予測分布は

$$
\begin{aligned}
p(t \mid \mathbf{x}, \mathbf{X}, \mathsf{t})
&= \iint p(t \mid \mathbf{x}, \mathbf{w}, \beta) p(\mathbf{w}, \beta \mid \mathsf{t}, \mathbf{X}) \mathrm{d}\mathbf{w} \mathrm{d}\beta \\
&= \iint \mathcal{N}\left(t \mid \phi(\mathbf{x})^{\mathrm{T}} \mathbf{w}, \beta^{-1}\right) \mathcal{N}\left(\mathbf{w} \mid \mathbf{m}_{N}, \beta^{-1} \mathbf{S}_{N}\right) \mathrm{d} \mathbf{w} \operatorname{Gam}\left(\beta \mid a_{N}, b_{N}\right) \mathrm{d} \beta
\end{aligned}
$$

となる．

ここで，$\mathbf{w}$についての積分は線形ガウスモデルなので，公式

$$ \tag{2.113}
p(\mathbf{x})=\mathcal{N}\left(\mathbf{x} \mid \boldsymbol{\mu}, \mathbf{\Lambda}^{-1}\right)
$$

$$ \tag{2.114}
p(\mathbf{y} \mid \mathbf{x})=\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \mathbf{x}+\mathbf{b}, \mathbf{L}^{-1}\right)
$$

$$ \tag{2.115}
p(\mathbf{y})=\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \boldsymbol{\mu}+\mathbf{b}, \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathbf{T}}\right)
$$

を用いる．

$$ \tag{3.49}
p(\mathbf{w} \mid \mathbf{t})=\mathcal{N}\left(\mathbf{w} \mid \mathbf{m}_{N}, \mathbf{S}_{N}\right)
$$

を$(2.113)$に，

$$ \tag{3.8}
p(t \mid \mathbf{x}, \mathbf{w}, \beta)=\mathcal{N}\left(t \mid y(\mathbf{x}, \mathbf{w}), \beta^{-1}\right)
$$

$$ \tag{3.3}
y(\mathbf{x}, \mathbf{w})=\sum_{j=0}^{M-1} w_{j} \phi_{j}(\mathbf{x})=\mathbf{w}^{\mathrm{T}} \phi(\mathbf{x})
$$

を$(2.114)$に適応するために



$$
\begin{aligned}
\mathbf{x} \Rightarrow \mathbf{w} \quad \boldsymbol{\mu} \Rightarrow \mathbf{m}_{N} \quad \mathbf{\Lambda}^{-1} \Rightarrow \mathbf{S}_{N} \quad \mathbf{y} \Rightarrow t \quad \mathbf{A} \Rightarrow \phi(\mathbf{x})^{\mathrm{T}}=\phi^{\mathrm{T}} \quad \mathbf{b} \Rightarrow \mathbf{0} \quad \mathbf{L}^{-1} \Rightarrow \beta^{-1}
\end{aligned}
$$

と置き換えると，$(2.115)$より

$$
\begin{aligned}
p(t \mid \beta) &=\mathcal{N}\left(t \mid \phi^{\mathrm{T}} \mathbf{m}_{N}, \beta^{-1}+\phi^{\mathrm{T}} \mathbf{S}_{N} \phi\right) \\
&=\mathcal{N}\left(t \mid \phi^{\mathrm{T}} \mathbf{m}_{N}, \beta^{-1}\left(1+\phi^{\mathrm{T}}\left(\mathbf{S}_{0}+\phi^{\mathrm{T}} \phi\right)^{-1} \phi\right)\right)
\end{aligned}
$$

となる．ただし，演習問題$3.12$より，$\beta \mathbf{S}_{N}^{-1} = \beta\left[ \mathbf{S}_0^{-1} + \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \right]$を用いた．

したがって予測分布は

$$
p(t \mid \mathbf{x}, \mathbf{X}, \mathbf{t}) = \int \mathcal{N}\left(t \mid \phi^{\mathrm{T}} \mathbf{m}_{N}, \beta^{-1} s\right) \operatorname{Gam}\left(\beta \mid a_{N}, b_{N}\right) \mathrm{d} \beta
$$

と変形できる．ただし

$$
s=1+\boldsymbol{\phi}^{\mathrm{T}}\left(\mathbf{S}_{0}+\boldsymbol{\phi}^{\mathrm{T}} \boldsymbol{\phi}\right)^{-1} \boldsymbol{\phi}
$$

とおいた．ここで，スチューデントのt分布の式

$$ \tag{2.158}
\begin{aligned}
p(x \mid \mu, a, b) &=\int_{0}^{\infty} \mathcal{N}\left(x \mid \mu, \tau^{-1}\right) \operatorname{Gam}(\tau \mid a, b) \mathrm{d} \tau \\
&=\int_{0}^{\infty} \frac{b^{a} e^{(-b \tau)} \tau^{a-1}}{\Gamma(a)}\left(\frac{\tau}{2 \pi}\right)^{1 / 2} \exp \left\{-\frac{\tau}{2}(x-\mu)^{2}\right\} \mathrm{d} \tau \\
&=\frac{b^{a}}{\Gamma(a)}\left(\frac{1}{2 \pi}\right)^{1 / 2}\left[b+\frac{(x-\mu)^{2}}{2}\right]^{-a-1 / 2} \Gamma(a+1 / 2)
\end{aligned}
$$

$$ \tag{2.159}
\operatorname{St}(x \mid \mu, \lambda, \nu)=\frac{\Gamma(\nu / 2+1 / 2)}{\Gamma(\nu / 2)}\left(\frac{\lambda}{\pi \nu}\right)^{1 / 2}\left[1+\frac{\lambda(x-\mu)^{2}}{\nu}\right]^{-\nu / 2-1 / 2}
$$

を参考にすると

$$
\begin{aligned}
p(t \mid \mathbf{x}, \mathbf{X}, \mathbf{t})
&= \int_{0}^{\infty} \frac{b_N^{a_N} e^{(-b_N \beta)} \beta^{a_N-1}}{\Gamma(a_N)}\left(\frac{\beta s^{-1}}{2 \pi}\right)^{1 / 2} \exp \left\{-\frac{\beta s^{-1}}{2}(t-\phi^{\mathrm{T}} \mathbf{m}_{N})^{2}\right\} \mathrm{d} \beta  \\
&=\frac{b_N^{a_N}}{\Gamma(a_N)}\left(\frac{s^{-1}}{2 \pi}\right)^{1 / 2} \int_{0}^{\infty} \beta^{(a_N + \frac{1}{2})-1} \exp \left\{-\left(b_N + \frac{s^{-1}}{2}(t-\phi^{\mathrm{T}} \mathbf{m}_{N})^{2} \right)\beta \right\} \mathrm{d} \beta  \\
\end{aligned}
$$

ここで$\displaystyle u = \left(b_N + \frac{s^{-1}}{2}(t-\phi^{\mathrm{T}} \mathbf{m}_{N})^{2} \right)\beta$と変数変換すると，$\displaystyle \mathrm{d}u = \left(b_N + \frac{s^{-1}}{2}(t-\phi^{\mathrm{T}} \mathbf{m}_{N})^{2} \right) \mathrm{d}\beta$と積分範囲に注意すると

$$
\begin{aligned}
p(t \mid \mathbf{x}, \mathbf{X}, \mathbf{t})
&= \frac{b_N^{a_N}}{\Gamma(a_N)}\left(\frac{s^{-1}}{2 \pi}\right)^{1 / 2} \int_{0}^{\infty} \beta^{(a_N + \frac{1}{2})-1} \exp \left\{-\left(b_N + \frac{s^{-1}}{2}(t-\phi^{\mathrm{T}} \mathbf{m}_{N})^{2} \right)\beta \right\} \mathrm{d} \beta  \\
&= \frac{b_N^{a_N}}{\Gamma(a_N)}\left(\frac{s^{-1}}{2 \pi}\right)^{1 / 2} \int_{0}^{\infty} \left(b_N + \frac{s^{-1}}{2}(t-\phi^{\mathrm{T}} \mathbf{m}_{N})^{2} \right)^{-\left\{(a_N + \frac{1}{2})-1\right\}-1} u^{(a_N + \frac{1}{2})-1} e^{-u} \mathrm{d}u  \\
&= \frac{b_N^{a_N}}{\Gamma(a_N)}\left(\frac{s^{-1}}{2 \pi}\right)^{1 / 2} \left[b_N + \frac{s^{-1}}{2}(t-\phi^{\mathrm{T}} \mathbf{m}_{N})^{2} \right]^{-a_N - \frac{1}{2}} \Gamma\left(a_N + \frac{1}{2}\right)  \\
&= \frac{\Gamma\left(a_N + 1/2\right)}{\Gamma(a_N)}\left(\frac{s^{-1}}{2 \pi}\right)^{1 / 2} b_N^{a_N} \left[b_N + \frac{s^{-1}}{2}(t-\phi^{\mathrm{T}} \mathbf{m}_{N})^{2} \right]^{-a_N - 1/2}  \\
&= \frac{\Gamma\left(a_N + 1/2\right)}{\Gamma(a_N)}\left(\frac{s^{-1}}{2 \pi}\right)^{1 / 2} b_N^{a_N} b_N^{-a_N - 1/2} \left[1 + \frac{a_N}{b_N} \frac{s^{-1}}{2a_N}(t-\phi^{\mathrm{T}} \mathbf{m}_{N})^{2} \right]^{-a_N - 1/2}  \\
&= \frac{\Gamma\left(a_N + 1/2\right)}{\Gamma(a_N)}\left(\frac{a_N}{b_N}\frac{s^{-1}}{2 a_N\pi}\right)^{1 / 2} \left[1 + \frac{a_N}{b_N} \frac{s^{-1}}{2a_N}(t-\phi^{\mathrm{T}} \mathbf{m}_{N})^{2} \right]^{-a_N - 1/2}  \\
&= \operatorname{St}(t \mid \mu, \lambda, \nu)
\end{aligned}
$$

となることがわかる．ただし

$$
\begin{aligned}
\mu &= \phi^{\mathrm{T}} \mathbf{m}_{N} \\
\lambda &= \frac{a_{N}}{b_{N}} s^{-1} \\
\nu &= 2 a_{N}
\end{aligned}
$$

である．

## 演習 3.14
<div class="panel-primary">

この演習問題では，
$$
k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\beta \boldsymbol{\phi}(\mathbf{x})^{\mathrm{T}} \mathbf{S}_{N} \boldsymbol{\phi}\left(\mathbf{x}^{\prime}\right) \tag{3.62}
$$
で定義される等価カーネルのより深い性質を調べよう．ただし，$\mathbf{S}_N$は
$$
\mathbf{S}_{N}^{-1}=\alpha \mathbf{I}+\beta \Phi^{\mathrm{T}} \boldsymbol{\Phi} \tag{3.54}
$$
で定義される．基底関数$\phi_j(\mathbf{x})$は線形独立であると仮定し，データ点の数$N$は基底関数の数$M$よりも大きいものとする．さらに，基底関数の1つは定数，すなわち$\phi_0(\mathbf{x})=1$とするこれらの基底関数の適当な線形結合を取り，同じ空間を張る新しい基底関数集合$\psi_j(\mathbf{x})$を生成することができる．ただし，新しい基底関数は正規直交である．

$$
\sum_{n=1}^{N} \psi_{j}\left(\mathbf{x}_{n}\right) \psi_{k}\left(\mathbf{x}_{n}\right)=I_{j k} \tag{3.115}
$$

$I_{jk}$は$j=k$のとき$1$を取り，それ以外は$0$を取る．また，$\psi_0(\mathbf{x})=1$と定義する．このとき$\alpha=0$に対して，等価カーネルが$k\left(\mathbf{x}, \mathbf{x}^{\prime}\right)=\boldsymbol{\psi}(\mathbf{x})^{\mathrm{T}} \boldsymbol{\psi}\left(\mathbf{x}^{\prime}\right)$と書けることを示せ．ただし，$\boldsymbol{\psi}=(\psi_0,\ldots,\psi_M)^{\mathrm{T}}$である．そしてこの結果を用いて，上記のカーネルが

$$
\sum_{n=1}^{N} k\left(\mathbf{x}, \mathbf{x}_{n}\right)=1 \tag{3.116}
$$

を満たすことを示せ．

</div>


$$
\alpha = 0より
\mathbf{S}_{N}^{-1}=\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}
$$
また、基底関数の適当な線型結合をとり互いに正規直交な新しい基底関数集合$\psi_j(\mathbf{x})$を

$$
\boldsymbol{\psi(\mathbf{x})}=\mathbf{A}\boldsymbol{\phi(\mathbf{x})}
$$
とおく。また、これを用いて$\mathbf{\Psi}$を

$$
\mathbf{\Psi}=\mathbf{\Phi}\mathbf{A}^{\mathrm{T}} \\
\mathbf{\Psi}(\mathbf{A}^{\mathrm{T}})^{-1} =\mathbf{\Phi}
$$

と定義する。ここで$(3.115)$を用いると$\mathbf{\Phi}^\mathrm{T}\mathbf{\Phi}=\mathbf{I}なので$

$$
\begin{aligned}
\mathbf{S}_{N}^{-1}&=\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\\
&= \beta\mathbf{A}^{-1}\mathbf{\Psi}^{\mathrm{T}}\mathbf{\Psi}(\mathbf{A}^\mathrm{T})^{-1}\\
&= \beta(\mathbf{A}^\mathrm{T}\mathbf{A})^{-1}
\end{aligned}
$$
これを$(3.62)$に代入すると

$$
\begin{aligned}
k\left(\mathbf{x}, \mathbf{x}'\right)&=\boldsymbol{\phi}^\mathrm{T}(\mathbf{x})\mathbf{A}^\mathrm{T}\mathbf{A}\boldsymbol{\phi}(\mathbf{x}') \\
&= \boldsymbol{\psi}^\mathrm{T}(\mathbf{x})\boldsymbol{\psi}(\mathbf{x}')
\end{aligned}
$$
が得られる。

また、これを$(3.116)$に代入すると

$$
\begin{aligned}
(3.116) &= \sum_{n=1}^{N} k\left(\mathbf{x}, \mathbf{x}_n\right) \\
&= \sum_{n=1}^{N}\boldsymbol{\psi}^\mathrm{T}(\mathbf{x})\boldsymbol{\psi}(\mathbf{x}_n) \\
&= \sum_{n=1}^{N} \sum_{i=0}^{M-1} \psi_i(\mathbf{x})\psi_i(\mathbf{x}_n) \\
&= \sum_{i=0}^{M-1}\psi_i(\mathbf{x})\sum_{n=1}^{N}\psi_i(\mathbf{x}_n)
\end{aligned}
$$
ここで、$(3.115)$の$k=0$のときを考えると$\psi_0(\mathbf{x})=1$なので

$$
\begin{aligned}
\sum_{n=1}^{N}\psi_j(\mathbf{x}_n)\psi_0(\mathbf{x}_n)&=\sum_{n=1}^{N}\psi_j(\mathbf{x}_n) = \mathbf{I}_{j0}
\end{aligned}
$$
よって

$$
\begin{aligned}
\sum_{i=0}^{M-1}\psi_i(\mathbf{x})\sum_{n=1}^{N}\psi_i(\mathbf{x}_n)=\sum_{i=0}^{M-1}\psi_i(\mathbf{x})\mathbf{I}_{i0}=\psi_0(\mathbf{x}) = 1
\end{aligned}
$$
以上より

$$
\begin{aligned}
\sum_{n=1}^{N}k(\mathbf{x},\mathbf{x}_n) = 1
\end{aligned}
$$
を満たすことを示した。


## 演習 3.15
<div class="panel-primary">

線形基底関数からなる回帰モデルの超パラメータ$\alpha,\ \beta$をエビデンスの枠組みを用いて決定する場合を考える．
$$
E\left(\mathbf{m}_{N}\right)=\frac{\beta}{2}\left\|\mathbf{t}-\Phi \mathbf{m}_{N}\right\|^{2}+\frac{\alpha}{2} \mathbf{m}_{N}^{\mathrm{T}} \mathbf{m}_{N} \tag{3.82}
$$
で定義される関数$E(\mathbf{m}_N)$が関係式$2E(\mathbf{m}_N)=N$を満たすことを示せ．

</div>


$(3.92)$式と$(3.95)$式を代入するだけで答えが出る


$$
\alpha = \frac{ \gamma }{ \mathbf{m}_{N}^\mathrm{T} \mathbf{m}_{N} } \tag{3.92}
$$


$$
\beta = (N-\gamma) \left\{\sum_{n=1}^{N}\left\{t_n-\mathbf{m}_{N}^\mathrm{T}\boldsymbol{\phi}(\mathbf{x}_n)\right\}^2\right\}^{-1} \tag{3.95}
$$

これらを$(3.82)$式に代入すると

$$
\begin{aligned}
    E(\mathbf{m}_N) &= \frac{(N-\gamma)\left\|\mathbf{t}-\Phi \mathbf{m}_{N}\right\|^{2}}{2 \sum_{n=1}^{N}\left\{t_n-\mathbf{m}_{N}^\mathrm{T}\boldsymbol{\phi}(\mathbf{x}_n)\right\}^2}+\frac{\gamma \mathbf{m}_{N}^\mathrm{T}\mathbf{m}_{N}}{2\mathbf{m}_{N}^\mathrm{T}\mathbf{m}_{N}} \\
    &=\frac{N-\gamma}{2}+\frac{\gamma}{2} \\
    &= \frac{N}{2}
\end{aligned}
$$
よって$2E(\mathbf{m}_N)=N$が示せた。

## 演習 3.16
<div class="panel-primary">

$$
p(\mathbf{t} \mid \alpha, \beta)=\int p(\mathbf{t} \mid \mathbf{w}, \beta) p(\mathbf{w} \mid \alpha) \mathrm{d} \mathbf{w} \tag{3.77}
$$
の積分の評価に
$$
p(\mathbf{y})=\mathcal{N}\left(\mathbf{y} \mid \mathbf{A} \boldsymbol{\mu}+\mathbf{b}, \mathbf{L}^{-1}+\mathbf{A} \mathbf{\Lambda}^{-1} \mathbf{A}^{\mathbf{T}}\right) \tag{2.115}
$$
を直接用いて，
$$
\ln p(\mathbf{t} \mid \alpha, \beta)=\frac{M}{2} \ln \alpha+\frac{N}{2} \ln \beta-E\left(\mathbf{m}_{N}\right)-\frac{1}{2} \ln |\mathbf{A}|-\frac{N}{2} \ln (2 \pi) \tag{3.86}
$$
で与えられる線形回帰モデルの対数エビデンス関数$p(\mathbf{t}|\alpha, \beta)$の結果を導け．

</div>

※ $(2.115)$式を適用するところまではそこまで難しくないが、$-  \frac {1}{2} \ln  \left | \beta^{-1} \mathbf I_N+\alpha ^{-1} \mathbf{\Phi}\mathbf{\Phi}^{\mathrm T} \right |$と$-\frac{1}{2} \mathsf{t}^{\mathrm T} \left( \beta^{-1} \mathbf{I}_N+\alpha ^{-1} \mathbf{\Phi}\mathbf{\Phi}^{\mathrm T} \right)^{-1} \mathsf{t}$を$(3.86)$式のように変形するところが非常にテクニカル。付録CのWoodburyの公式(C.7)や[行列式についての変形の定理](MASP/%E8%A1%8C%E5%88%97%E5%BC%8F#%5Cmathbf%7BR%7D%2B%5Cmathbf%7BSTU%7D%E3%81%AE%E5%BD%A2%E3%81%AE%E8%A1%8C%E5%88%97%E3%81%AE%E8%A1%8C%E5%88%97%E5%BC%8F)(C.14)を利用する必要がある。

$(3.77)$式の積分を計算する。$p(\mathsf{t}\mid \mathbf{w}, \beta)$は$(3.10)$式から、$p(\mathbf{w}|\alpha)$は$(3.52)$式から与えられる。

$(3.52)$式より$p(\mathbf{w}\mid \alpha) = \mathcal{N}(\mathbf{w}\mid \mathbf{0},\alpha^{-1}\mathbf{I}_M)$である。また、$(3.10)$式から

$$
\begin{aligned}
p(\mathsf{t} \mid \mathbf{w}, \beta) &=\prod_{n=1}^{N} \mathcal{N}\left(t_n \mid \mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right), \beta^{-1}\right) \\
&=\prod_{n=1}^{N}\left(\frac{\beta}{2 \pi}\right)^{\frac{1}{2}} \exp \left\{-\frac{\beta}{2}\left(t_n-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right)^{2}\right\} \\
&=\left(\frac{\beta}{2 \pi}\right)^{\frac{N}{2}} \exp \left\{-\frac{\beta}{2} \sum_{n=1}^{N}\left(t_n-\mathbf{w}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right)^{2}\right\} \\
&=\left(\frac{\beta}{2 \pi}\right)^{\frac{N}{2}} \exp \left\{-\frac{\beta}{2}\left(\mathsf{t}-\mathbf{\Phi}\mathbf{w}\right)^{\mathrm{T}}\left(\mathsf{t}-\mathbf{\Phi}\mathbf{w}\right)\right\} \\
&=\mathcal{N}(\mathsf{t}\mid \mathbf{\Phi}\mathbf{w},\beta^{-1}\mathbf{I}_N)
\end{aligned}
$$
となる。注意点として、$\mathbf{\Phi}$は$N\times M$行列、$\mathbf{w}$は$M$次元の列ベクトルである。

ここで、問題文のヒントにしたがって

$$
\begin{aligned}
\mathbf y \rightarrow \mathbf t, \quad \mathbf x \rightarrow\mathbf w, \quad \boldsymbol \mu\rightarrow\mathbf 0, \quad \mathbf{\Lambda}^{-1}\rightarrow\alpha ^{-1}\mathbf I_M,\quad  \mathbf A\rightarrow\mathbf{\Phi} ,\quad \mathbf L^{-1}\rightarrow\beta^{-1} \mathbf I_N
\end{aligned}
$$

と置き換えると、$(2.115)$式を使って$p(\mathsf{t}\mid \alpha, \beta)$を求めることができる。これより

$$
p\left(\mathsf{t} \mid \alpha,\beta \right) = \mathcal{N}\left(\mathsf{t}\mid \mathbf{0}, ~ \beta^{-1} \mathbf{I}_N+\alpha^{-1} \mathbf{\Phi\Phi}^{\mathrm T} \right)
$$

と求まる。これについて対数をとって展開していくと（$(2.43)$の対数表現にあてはめて）

$$
\ln p\left( \mathsf{t} \mid \alpha,\beta \right) = -\frac {N}{2} \ln \left ( 2 \pi \right )  -  \frac {1}{2} \ln  \left | \beta^{-1} \mathbf I_N+\alpha ^{-1} \mathbf{\Phi}\mathbf{\Phi}^{\mathrm T} \right | -\frac{1}{2} \mathsf{t}^{\mathrm T} \left( \beta^{-1} \mathbf{I}_N+\alpha ^{-1} \mathbf{\Phi}\mathbf{\Phi}^{\mathrm T} \right)^{-1} \mathsf{t}
$$
となる。この第2項と第3項について計算していく。

まず第2項について

$$
\begin{aligned}
\left|\beta^{-1} \mathbf{I}_{N}+\alpha^{-1} \mathbf{\Phi\Phi}^{\mathrm T}\right| &=\beta^{-N} \cdot \beta^{N}\left|\beta^{-1} \mathbf{I}_{N}+\alpha^{-1} \mathbf{\Phi\Phi}^{\mathrm T}\right| \\
&=\beta^{-N}\left|\mathbf{I}_{N}+\beta \alpha^{-1} \mathbf{\Phi\Phi}^{\mathrm T}\right| \quad\left(\because k^{N}|\mathbf{N}|=|k \mathbf{N}|\right) \\
&=\beta^{-N}\left|\mathbf{I}_{M}+\beta \alpha^{-1} \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right| \quad(\because \text {Appendix}\ (\text{C}.14)) \\
&=\beta^{-N} \alpha^{-M}\left|\alpha \mathbf{I}_{M}+\beta \mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}\right|\quad\left(\because k^M|\mathbf{M}|=|k \mathbf{M}|\right) \\
&=\beta^{-N} \alpha^{-M}|\mathbf{A}|\quad(\because \mathbf{A} = \alpha \mathbf{I}_M+\beta\mathbf{\Phi}^{\mathrm T}\mathbf{\Phi}\quad (3.81))
\end{aligned}
$$
が得られる。ここで、以下の定理を用いた。
> 任意の$n\times n$行列$\mathbf{A}$と任意のスカラー値$k$に対して
> $|k\mathbf{A}| = k^n|\mathbf{A}|$
> が成り立つ（統計のための行列代数P.217, 系13.2.4）

また対数を取った時の第3項についてはまず
$$
-\frac{1}{2} \mathsf{t}^{\mathrm T} \left( \beta^{-1} \mathbf{I}_N+\alpha ^{-1} \mathbf{\Phi}\mathbf{\Phi}^{\mathrm T} \right)^{-1} \mathsf{t} = -\frac{1}{2} \mathsf{t}^{\mathrm T} \left( \beta^{-1} \mathbf{I}_N+\mathbf{\Phi}(\alpha ^{-1} \mathbf{I}_M)\mathbf{\Phi}^{\mathrm T} \right)^{-1} \mathsf{t}
$$
としてからWoodburyの公式

$$
\left ( \mathbf A + \mathbf {BD}^{-1} \mathbf C \right ) ^{-1} = \mathbf A^{-1}-\mathbf A^{-1} \mathbf{B} \left ( \mathbf D + \mathbf{CA}^{-1}\mathbf B \right ) ^{-1}\mathbf{CA}^{-1} \tag {C.7}
$$

に当てはめると

$$
\begin{aligned}
-\frac{1}{2} \mathsf{t}^{\mathrm{T}}\left(\beta^{-1} \mathbf{I}_{N}+\alpha^{-1} \mathbf{\Phi} \Phi^{\mathrm{T}}\right)^{-1} \mathsf{t} &=-\frac{1}{2} \mathsf{t}^{\mathrm{T}}\left[\beta \mathbf{I}_{N}-\beta\mathbf{I}_{N} \mathbf{\Phi}\left(\alpha \mathbf{I}_{M}+\mathbf{\Phi}^{\mathrm{T}}(\beta\mathbf{I}_N)\mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm{T}}(\beta\mathbf{I}_N)\right] \mathsf{t} \\
&=-\frac{1}{2} \mathsf{t}^{\mathrm{T}}\left[\beta \mathbf{I}_{N}-\beta \mathbf{\Phi}\left(\alpha \mathbf{I}_{M}+\beta \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}\right)^{-1} \mathbf{\Phi}^{\mathrm{T}} \beta\right] \mathsf{t} \\
&=-\frac{\beta}{2} \mathsf{t}^{\mathrm{T}} \mathsf{t}+\frac{\beta^{2}}{2} \mathsf{t}^{\mathrm{T}} \mathbf{\Phi} \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}} \mathsf{t} \\
&=-\frac{\beta}{2} \mathsf{t}^{\mathrm{T}} \mathsf{t}+\frac{1}{2} \mathbf{m}_{N}^{\mathrm{T}} \mathbf{A} \mathbf{m}_{N} \quad (\because \mathbf{m}_{N}=\beta \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}} \mathsf{t}, (\mathbf{A}^{-1})^{\mathrm{T}} = (\mathbf{A}^{\mathrm{T}})^{-1} = \mathbf{A}^{-1}) \\
&=-\frac{1}{2}\left(\beta \mathsf{t}^{\mathrm{T}} \mathsf{t}-2 \mathbf{m}_{N}^{\mathrm{T}} \mathbf{A} \mathbf{m}_{N}+\mathbf{m}_{N}^{\mathrm{T}} \mathbf{A} \mathbf{m}_{N}\right) \\
&=-\frac{1}{2}\left(\beta \mathsf{t}^{\mathrm{T}} \mathsf{t}-2 \mathbf{m}_{N}^{\mathrm{T}} \mathbf{A}\left(\beta \mathbf{A}^{-1} \mathbf{\Phi}^{\mathrm{T}} \mathsf{t}\right)+\mathbf{m}_{N}^{\mathrm{T}}\left(\alpha \mathbf{I}_{M}+\beta \mathbf{\Phi}^{\mathrm{T}} \boldsymbol{\Phi}\right) \mathbf{m}_{N}\right) \\
&=-\frac{1}{2}\left(\beta \mathsf{t}^{\mathrm{T}} \mathsf{t}-2 \mathbf{m}_{N}^{\mathrm{T}} \mathbf{\Phi}^{\mathrm{T}} \mathsf{t} \beta+\beta \mathbf{m}_{N}^{\mathrm{T}} \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \mathbf{m}_{N}+\alpha \mathbf{m}_{N}^{\mathrm{T}} \mathbf{m}_{N}\right) \\
&=-\frac{1}{2}\left(\beta\left(\mathsf{t}-\mathbf{\Phi} \mathbf{m}_{N}\right)^{\mathrm{T}}\left(\mathsf{t}-\mathbf{\Phi} \mathbf{m}_{N}\right)+\alpha \mathbf{m}_{N}^{\mathrm{T}} \mathbf{m}_{N}\right) \\
&=-\frac{\beta}{2}\left\|\mathsf{t}-\mathbf{\Phi} \mathbf{m}_{N}\right\|^{2}-\frac{1}{2} \alpha \mathbf{m}_{N}^{\mathrm{T}} \mathbf{m}_{N} \\
&=-E(\mathbf{m}_N)\quad(\because\ (3.82))
\end{aligned}
$$

以上から

$$
\ln p \left ( \mathsf{t} \mid \alpha,\beta \right ) =\frac {M}{2} \ln \alpha + \frac {N}{2} \ln \beta - E \left ( \mathbf m_N \right ) -\frac{1}{2} \ln \left | \mathbf A \right | -\frac {N}{2} \ln \left ( 2 \pi \right )  \tag{3.86}
$$

を導出することができた。

## 演習 3.17
<div class="panel-primary">

ベイズ線形回帰モデルに対するエビデンス関数が
$$
p(\mathbf{t} \mid \alpha, \beta)=\left(\frac{\beta}{2 \pi}\right)^{N / 2}\left(\frac{\alpha}{2 \pi}\right)^{M / 2} \int \exp \{-E(\mathbf{w})\} \mathrm{d} \mathbf{w} \tag{3.78}
$$
の形式で書けることを示せ．ただし，
$$
\begin{aligned}
E(\mathbf{w}) &=\beta E_{D}(\mathbf{w})+\alpha E_{W}(\mathbf{w}) \\
&=\frac{\beta}{2}\|\mathbf{t}-\mathbf{\Phi} \mathbf{w}\|^{2}+\frac{\alpha}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w}
\end{aligned} \tag{3.79}
$$
で定義される．

</div>

※演習問題3.16よりも簡単。

$p(\mathsf{t}\mid \alpha, \beta) = \int p(\mathsf{t}\mid \mathbf{w},\beta)p(\mathbf{w}\mid\alpha)d\mathbf{w}\quad (3.77)$を求める。**演習3.16**で示した通り

$$
p(\mathsf{t}\mid \mathbf{w},\beta) = \left(\frac{\beta}{2 \pi}\right)^{\frac{N}{2}} \exp \left\{-\frac{\beta}{2}(\mathsf{t}-\boldsymbol{\Phi} \mathbf{w})^{\mathrm{T}}(\mathsf{t}-\boldsymbol{\Phi} \mathbf{w})\right\}\\
$$
$$
\begin{aligned}
    p(\mathbf{w}\mid\alpha) &= \mathcal{N}(\mathbf{w}\mid \mathbf{0}, \alpha^{-1}\mathbf{I}_M) \\
    &=\left( \frac{\alpha}{2\pi} \right)^{\frac{M}{2}}\exp \left\{ -\frac{1}{2}\mathbf{w}^{\mathrm{T}}(\alpha^{-1}\mathbf{I}_M)^{-1}\mathbf{w}\right\} \\
    &=\left( \frac{\alpha}{2\pi} \right)^{\frac{M}{2}}\exp \left\{ -\frac{\alpha}{2}\mathbf{w}^{\mathrm{T}}\mathbf{w}\right\}
\end{aligned}
$$
なのでこれらを代入すると
$$
\begin{aligned}
p(\mathbf{t} \mid \alpha, \beta)&=\int\left(\frac{\beta}{2 \pi}\right)^{N / 2}\left(\frac{\alpha}{2 \pi}\right)^{M / 2} \exp \left\{-\frac{\beta}{2}(\mathsf{t}-\mathbf{\Phi}\mathbf{w})^{\mathrm{T}}(\mathsf{t}-\mathbf{\Phi}\mathbf{w}) - \frac{\alpha}{2}\mathbf{w}^{\mathrm{T}}\mathbf{w}\right\} \mathrm{d} \mathbf{w} \\
&=\left(\frac{\beta}{2 \pi}\right)^{N / 2}\left(\frac{\alpha}{2 \pi}\right)^{M / 2}\int \exp\left\{-\frac{\beta}{2}||\mathsf{t}-\mathbf{\Phi}\mathbf{w}||^{2} - \frac{\alpha}{2}\mathbf{w}^{\mathrm{T}}\mathbf{w}\right\} \mathrm{d} \mathbf{w}
\end{aligned}
$$
となる。これは$(3.78)$,$(3.79)$の形になっている。

## 演習 3.18
<div class="panel-primary">

$\mathbf{w}$に関して平方完成することにより，

$$
\begin{aligned}
E(\mathbf{w}) &=\beta E_{D}(\mathbf{w})+\alpha E_{W}(\mathbf{w}) \\
&=\frac{\beta}{2}\|\mathbf{t}-\mathbf{\Phi} \mathbf{w}\|^{2}+\frac{\alpha}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w}
\end{aligned} \tag{3.79}
$$

で定義されるベイズ線形回帰の誤差関数が

$$
E(\mathbf{w})=E\left(\mathbf{m}_{N}\right)+\frac{1}{2}\left(\mathbf{w}-\mathbf{m}_{N}\right)^{\mathrm{T}} \mathbf{A}\left(\mathbf{w}-\mathbf{m}_{N}\right) \tag{3.80}
$$

の形で書けることを示せ．

</div>

※誘導に従って平方完成して式変形していくだけ。$\mathbf{A} = \alpha \mathbf{I}_M+\beta\mathbf{\Phi}^{\mathrm T}\mathbf{\Phi}=(\alpha \mathbf{I}_M+\mathbf{\Phi}^{\mathrm T}(\beta\mathbf{I}_M)\mathbf{\Phi})$と$(3.84)$式の定義$\mathbf{m}_N=\beta \mathbf{A}^{-1}\mathbf{\Phi}^{\mathrm T}\mathsf{t}$を途中で導入する。

$$
\begin{aligned}
E(\mathbf{w}) &= \frac{\beta}{2} ||\mathbf{t}-\mathbf{\Phi} \mathbf{w}\|^{2}+\frac{\alpha}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w} \\
&=\frac{\beta}{2}\left(\mathbf{t}^{\mathrm{T}} \mathbf{t}-2 \mathbf{t}^{\mathrm{T}} \mathbf{\Phi} \mathbf{w}+\mathbf{w}^{\mathrm{T}} \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \mathbf{w}\right)+\frac{\alpha}{2} \mathbf{w}^{\mathrm{T}} \mathbf{w} \\
&=\frac{1}{2}\left(\mathbf{w}^{\mathrm{T}}\mathbf{\Phi}^{\mathrm{T}}(\beta\mathbf{I}_M)\mathbf{\Phi} \mathbf{w}+\mathbf{w}^{\mathrm{T}}(\alpha \mathbf{I}_M)\mathbf{w}-2\beta\mathbf{t}^{\mathrm{T}} \mathbf{\Phi} \mathbf{w}+\beta \mathbf{t}^{\mathrm{T}} \mathbf{t} \right) \\
&=\frac{1}{2}\left( \mathbf{w}^{\mathrm{T}}\mathbf{A}\mathbf{w}-2\beta\mathbf{t}^{\mathrm{T}} \mathbf{\Phi} \mathbf{w}+\beta \mathbf{t}^{\mathrm{T}} \mathbf{t}\right) \\
&=\frac{1}{2}\left( \mathbf{w}^{\mathrm{T}}\mathbf{A}\mathbf{w}-2\mathbf{m}_N^{\mathrm T}\mathbf{A}^{\mathrm T}\mathbf{\Phi}^{-1}\mathbf{\Phi} \mathbf{w}+\beta \mathbf{t}^{\mathrm{T}} \mathbf{t}\right) \\
&=\frac{1}{2}\left( \mathbf{w}^{\mathrm{T}}\mathbf{A}\mathbf{w}-2\mathbf{m}_N^{\mathrm T}\mathbf{A}^{\mathrm T}\mathbf{w}+\mathbf{m}_N^{\mathrm T}\mathbf{A}\mathbf{m}_N\right) - \frac{1}{2}\mathbf{m}_N^{\mathrm T} \mathbf{A} \mathbf{m}_N + \frac{\beta}{2}\mathbf{t}^{\mathrm{T}} \mathbf{t} \\
&=\frac{1}{2}(\mathbf{w} - \mathbf{m}_N)^{\mathrm T}\mathbf{A}(\mathbf{w} - \mathbf{m}_N)- \frac{1}{2}\mathbf{m}_N^{\mathrm T} \mathbf{A} \mathbf{m}_N + \frac{\beta}{2}\mathbf{t}^{\mathrm{T}} \mathbf{t}
\end{aligned}
$$

ここで、$\displaystyle -\frac{1}{2}\mathbf{m}_N^{\mathrm T} \mathbf{A} \mathbf{m}_N + \frac{\beta}{2}\mathbf{t}^{\mathrm{T}} \mathbf{t}$については**演習問題3.16**の後半の式変形と同じなので
$$
\begin{aligned}
-\frac{1}{2}\mathbf{m}_N^{\mathrm T} \mathbf{A} \mathbf{m}_N + \frac{\beta}{2}\mathbf{t}^{\mathrm{T}} \mathbf{t} &= \frac{\alpha}{2}\mathbf{m}_N^{\mathrm T}\mathbf{m}_N + \frac{\beta}{2}\left\|\mathsf{t}-\mathbf{\Phi} \mathbf{m}_{N}\right\|^{2}\\
&=E(\mathbf{m}_N)
\end{aligned}
$$
となるので、結果として$(3.80)$式
$$
E(\mathbf{w})=E\left(\mathbf{m}_{N}\right)+\frac{1}{2}\left(\mathbf{w}-\mathbf{m}_{N}\right)^{\mathrm{T}} \mathbf{A}\left(\mathbf{w}-\mathbf{m}_{N}\right) \tag{3.80}
$$
が成立する。

## 演習 3.19
<div class="panel-primary">

ベイズ線形回帰モデルの$\mathbf{w}$に関する積分が

$$
\int \exp \{-E(\mathbf{w})\} \mathrm{d} \mathbf{w} =\exp \left\{-E\left(\mathbf{m}_{N}\right)\right\}(2 \pi)^{M / 2}|\mathbf{A}|^{-1 / 2}
\tag{3.85}
$$

で与えられることを示せ．したがって，対数周辺尤度が
$$
\ln p(\mathbf{t} \mid \alpha, \beta)=\frac{M}{2} \ln \alpha+\frac{N}{2} \ln \beta-E\left(\mathbf{m}_{N}\right)-\frac{1}{2} \ln |\mathbf{A}|-\frac{N}{2} \ln (2 \pi) \tag{3.86}
$$
で与えられることを示せ．

</div>

$(3.85)$の積分が成立することを示す。

$$
E(\mathbf{w})=E\left(\mathbf{m}_{N}\right)+\frac{1}{2}\left(\mathbf{w}-\mathbf{m}_{N}\right)^{\mathrm{T}} \mathbf{A}\left(\mathbf{w}-\mathbf{m}_{N}\right)
\tag{3.80}
$$

$$
E\left(\mathbf{m}_{N}\right)=\frac{\beta}{2}\left\|\mathbf{t}-\mathbf{\Phi} \mathbf{m}_{N}\right\|^{2}+\frac{\alpha}{2} \mathbf{m}_{N}^{\mathrm{T}} \mathbf{m}_{N}
\tag{3.82}
$$

$(3.80)$式から$(3.85)$が成り立つことを示す。$(3.82)$より$E\left(\mathbf{m}_{N}\right)$は$\mathbf{w}$の関数ではないため積分の外に出すことができる。

$$
\begin{aligned}
\int \exp \{-E(\mathbf{w})\} \mathrm{d} \mathbf{w} =\exp \left\{-E\left(\mathbf{m}_{N}\right)\right\} \int \exp \left\{-\frac{1}{2}\left(\mathbf{w}-\mathbf{m}_{N}\right)^{\mathrm{T}} \mathbf{A}\left(\mathbf{w}-\mathbf{m}_{N}\right)\right\} \mathrm{d} \mathbf{w}
\end{aligned}
$$
今$\mathbf{w}$の次元は$M$であるので、正規化された多次元ガウス分布の形
$$
\frac{1}{(2 \pi)^{M / 2}} \frac{1}{|\mathbf{A}|^{1 / 2}}\int \exp \left\{-\frac{1}{2}\left(\mathbf{w}-\mathbf{m}_{N}\right)^{\mathrm{T}} \mathbf{A}\left(\mathbf{w}-\mathbf{m}_{N}\right)\right\} \mathrm{d} \mathbf{w} = 1
$$
から正規化係数部分を取り出せば

$$
\int \exp \{-E(\mathbf{w})\} \mathrm{d} \mathbf{w} =\exp \left\{-E\left(\mathbf{m}_{N}\right)\right\}(2 \pi)^{M / 2}|\mathbf{A}|^{-1 / 2}
\tag{3.85}
$$
$(3.85)$を示すことができる。

対数周辺尤度は

$$
p(\mathsf{t} | \alpha, \beta)=\left(\frac{\beta}{2 \pi}\right)^{N / 2}\left(\frac{\alpha}{2 \pi}\right)^{M / 2} \int \exp \{-E(\mathbf{w})\} \mathrm{d} \mathbf{w}
\tag{3.78}
$$

で表すことができ、$(3.85)$の結果と合わせると、

$$
p(\mathsf{t} | \alpha, \beta)=\left(\frac{\beta}{2 \pi}\right)^{N / 2}\left(\frac{\alpha}{2 \pi}\right)^{M / 2} \exp \left\{-E\left(\mathbf{m}_{N}\right)\right\}(2 \pi)^{M / 2}|\mathbf{A}|^{-1 / 2}
$$
この等式に対数を取ってやると、

$$
\ln p(\mathsf{t} | \alpha, \beta)=\frac{M}{2} \ln \alpha+\frac{N}{2} \ln \beta-E\left(\mathbf{m}_{N}\right)-\frac{1}{2} \ln |\mathbf{A}|-\frac{N}{2} \ln (2 \pi)
\tag{3.86}
$$
よって対数周辺尤度が$(3.86)$で与えられることが示された。

## 演習 3.20
<div class="panel-primary">

対数周辺尤度関数

$$
\ln p(\mathbf{t} \mid \alpha, \beta)=\frac{M}{2} \ln \alpha+\frac{N}{2} \ln \beta-E\left(\mathbf{m}_{N}\right)-\frac{1}{2} \ln |\mathbf{A}|-\frac{N}{2} \ln (2 \pi) \tag{3.86}
$$

の$\alpha$に関する最大化が再推定方程式

$$
\alpha = \frac{\gamma}{\mathbf{m}_N^{\mathrm{T}}\mathbf{m}_N},\quad \gamma = \sum_{i}\frac{\lambda_i}{\alpha+\lambda_i} \tag{3.92}
$$
に帰着されることを示すのに必要なすべての段階を$(3.86)$から始めて確かめよ．

</div>

※**3.5.2 エビデンス関数の最大化**をなぞるだけ。

$(3.86)$式を$\alpha$で偏微分する。そのために，まず次の固有ベクトル方程式を考える。
$$
\left(\beta \mathbf{\Phi}^{\mathrm T}\mathbf{\Phi} \right)\mathbf{u}_i = \lambda_i \mathbf{u}_i \tag{3.87}
$$
また、$\alpha \mathbf{I}_M$についての固有値は当然$\alpha$であり、$(\alpha \mathbf{I}_M)\mathbf{u}_i = \alpha \mathbf{u}_i$のように書けるので、この2式を足せば
$$
\left(\alpha \mathbf{I}_M + \beta \mathbf{\Phi}^{\mathrm T}\mathbf{\Phi} \right)\mathbf{u}_i = (\alpha + \lambda_i)\mathbf{u}_i
$$
となるので、$\mathbf{A}$は固有値$\alpha + \lambda_i$を持つことがわかる。ここで，$(3.86)$に含まれる$\ln |\mathbf{A}|$の項の$\alpha$に関する導関数を考えると
$$
\frac{d}{d \alpha} \ln |\mathbf{A}|=\frac{d}{d \alpha} \ln \prod_{i=1}^M \left(\lambda_{i}+\alpha\right)=\frac{d}{d \alpha} \sum_{i=1}^M \ln \left(\lambda_{i}+\alpha\right)=\sum_{i=1}^M \frac{1}{\lambda_{i}+\alpha} \tag{3.88}
$$
が得られる。これより，$(3.86)$の$\alpha$に関する停留点は
$$
0 = \frac{M}{2\alpha}-\frac{1}{2}\mathbf{m}_N^{\mathrm T}\mathbf{m}_N-\frac{1}{2}\sum_{i=1}^M \frac{1}{\lambda_i+\alpha} \tag{3.89}
$$
を満たす。$2\alpha$を掛け，式を整理すれば
$$
\alpha\mathbf{m}_N^{\mathrm T}\mathbf{m}_N = M - \alpha \sum_{i=1}^M \frac{1}{\lambda_i + \alpha} = \sum_{i=1}^M \left( 1- \frac{1}{\lambda_i + \alpha} \right) = \sum_{i=1}^M \frac{\lambda_i}{\lambda_i + \alpha} \equiv \gamma
$$
が得られる。よって
$$
\alpha = \frac{\gamma}{\mathbf{m}_N^{\mathrm T}\mathbf{m}_N} \tag{3.92}
$$
となる。


## 演習 3.21
<div class="panel-primary">

$(3.92)$はエビデンスの枠組みにおける最適な$\alpha$の値である．この結果は，次の等式を使って導出することもできる．

$$
\frac{d}{d \alpha} \ln |\mathbf{A}|=\operatorname{Tr}\left(\mathbf{A}^{-1} \frac{d}{d \alpha} \mathbf{A}\right) \tag{3.117}
$$

実対称行列$\mathbf{A}$の固有値展開，および$\mathbf{A}$の行列式とトレースの固有値表現の標準的結果（付録C参照）を用いて，この等式を証明せよ．そして，$(3.117)$を用いて，$(3.86)$から$(3.92)$を導け．

</div>

※$(3.117)$を証明する。付録Cも参照。
まず$\mathbf{A} = \alpha \mathbf{I}_M + \beta\mathbf{\Phi}^{\mathrm T}\mathbf{\Phi}$であり、$\mathbf{A}\mathbf{u}_i = \lambda_i \mathbf{u}_i$となるような固有値$\lambda_i$と固有ベクトル$\mathbf{u}_i$が存在する。この2つはそれぞれ$\alpha$に依存する。

$\mathbf{U} = (\mathbf{u}_1,\mathbf{u}_2,\ldots,\mathbf{u}_M)$とすると$\mathbf{AU} = \mathbf{U\Lambda}$と書くことができる。ここで$\mathbf{\Lambda}$は$\lambda_i$を対角成分とする$M\times M$対角行列である（付録Cの(C.38)）。

実対称行列$\mathbf{A}$についての$\mathbf{U}$は正規直交行列となるようにとることができるので(C.29)〜(C.36)、$\mathbf{U}^{\mathrm T}\mathbf{U}=\mathbf{I}$, よって$\mathbf{U}^{\mathrm T} = \mathbf{U}^{-1}$となる。これより$\mathbf{A} = \mathbf{U\Lambda U}^{-1}$が得られ、
$$
|\mathbf{A}| = |\mathbf{U}||\mathbf{\Lambda}||\mathbf{U}^{-1}|=|\mathbf{\Lambda}|=\prod_{i=1}^M \lambda_i
$$
となることが分かる。

一方で
$$
\operatorname{Tr}(\mathbf{A}) = \operatorname{Tr}(\mathbf{U\Lambda U}^{-1}) = \operatorname{Tr}(\mathbf{U}^{-1}\mathbf{U\Lambda}) = \operatorname{Tr}(\mathbf{\Lambda}) = \sum_{i=1}^M \lambda_i
$$
である。

以上から$(3.117)$の左辺について変形すると
$$
\frac{d}{d \alpha} \ln |\mathbf{A}|=\frac{d}{d \alpha} \ln \prod_{i=1}^{M} \lambda_{i}=\frac{d}{d \alpha} \sum_{i=1}^{M} \ln \lambda_{i}=\sum_{i=1}^{M} \frac{1}{\lambda_{i}} \frac{d}{d \alpha} \lambda_{i}
$$
となる。続いて右辺について

$$
\begin{aligned}
\operatorname{Tr}\left(\mathbf{A}^{-1} \frac{d}{d \alpha} \mathbf{A}\right) &=\operatorname{Tr}\left(\sum_{i=1}^{M} \frac{1}{\lambda_{i}} \mathbf{u}_{i} \mathbf{u}_{i}^{\mathrm T} \frac{d}{d \alpha} \sum_{j=1}^{M} \lambda_{j} \mathbf{u}_{j} \mathbf{u}_{j}^{\mathrm T}\right) \\
&=\operatorname{Tr}\left(\sum_{i=1}^{M} \frac{1}{\lambda_{i}} \mathbf{u}_{i} \mathbf{u}_{i}^{\mathrm T}\left\{\sum_{j=1}^{M}\left(\frac{d \lambda_{j}}{d \alpha} \mathbf{u}_{j} \mathbf{u}_{j}^{\mathrm T}+\lambda_{j} \frac{d \mathbf{u}_{j}}{d \alpha} \mathbf{u}_{j}^{\mathrm T}+\lambda_j \mathbf{u}_{j} \frac{d \mathbf{u}_{j}^{\mathrm T}}{d \alpha}\right)\right\}\right) \\
&=\operatorname{Tr}\left(\sum_{i=1}^{M} \frac{1}{\lambda_{i}} \mathbf{u}_{i} \mathbf{u}_{i}^{\mathrm T} \sum_{j=1}^{M} \frac{d \lambda_{j}}{d \alpha} \mathbf{u}_{j} \mathbf{u}_{j}^{\mathrm T}\right)+\operatorname{Tr}\left(\sum_{i=1}^{M} \frac{1}{\lambda_{i}} \mathbf{u}_{i} \mathbf{u}_{i}^{\mathrm T} \left( \sum_{j=1}^{M} \lambda_{j} \frac{d \mathbf{u}_{j}}{d \alpha} \mathbf{u}_{j}^{\mathrm T}+\lambda_j \mathbf{u}_{j} \frac{d \mathbf{u}_{j}^{\mathrm T}}{d \alpha}\right) \right)\\
&=\operatorname{Tr}\left(\sum_{i=1}^{M} \sum_{j=1}^{M} \frac{1}{\lambda_{i}} \frac{d \lambda_{j}}{d \alpha} \mathbf{u}_{i} \mathbf{u}_{i}^{\mathrm T} \mathbf{u}_{j} \mathbf{u}_{j}^{\mathrm T}\right)+\operatorname{Tr}\left(\sum_{i=1}^{M} \sum_{j=1}^{M} \frac{2\lambda_{j}}{\lambda_{i}} \mathbf{u}_{i} \mathbf{u}_{i}^{\mathrm T} \mathbf{u}_{j} \frac{d \mathbf{u}_{j}^{\mathrm T}}{d \alpha} \right) \quad \left(\because \sum_i \alpha_i\sum_j \beta_j = \sum_i \sum_j \alpha_i \beta_j \right)\\
&=\operatorname{Tr}\left(\sum_{i=1}^{M} \frac{1}{\lambda_{i}} \frac{d \lambda_{i}}{d \alpha} \mathbf{u}_{i} \mathbf{u}_{i}^{\mathrm T}\right)+\operatorname{Tr}\left(\sum_{i=1}^{M} 2 \mathbf{u}_{i} \frac{d \mathbf{u}_{i}^{\mathrm T}}{d \alpha}\right) \quad \left( \because \mathbf{u}_i^{\mathrm T}\mathbf{u}_j = \delta_{ij}より, i=jの項だけが残る \right)\\
&=\sum_{i=1}^{M} \frac{1}{\lambda_{i}} \frac{d \lambda_{i}}{d \alpha}+\operatorname{Tr}\left(\frac{d}{d \alpha}\left(\frac{d \mathbf{u}_{i}}{d \alpha} \mathbf{u}_{i}^{\mathrm T}+\mathbf{u}_{i} \frac{d \mathbf{u}_{i}^{\mathrm T}}{d \alpha}\right)\right) \\
&=\sum_{i=1}^{M} \frac{1}{\lambda_{i}} \frac{d \lambda_{i}}{d \alpha}+\operatorname{Tr}\left(\frac{d}{d \alpha} \sum_{i=1}^{M} \mathbf{u}_{i} \mathbf{u}_{i}^{\mathrm T}\right) \\
&=\sum_{i=1}^{M} \frac{1}{\lambda_{i}} \frac{d \lambda_{i}}{d \alpha}+\operatorname{Tr}\left(\frac{d}{d \alpha} \mathbf{I}_{M}\right) \\
&=\sum_{i=1}^{M} \frac{1}{\lambda_{i}} \frac{d \lambda_{i}}{d \alpha}
\end{aligned}
$$

以上の式変形から
$$
\frac{d}{d \alpha} \ln |\mathbf{A}| = \operatorname{Tr}\left(\mathbf{A}^{-1} \frac{d}{d \alpha} \mathbf{A}\right) \tag{3.117}
$$
が示された。

> 統計のための行列代数第15章 15.8 **行列式と逆行列と随伴行列の一次偏導関数**（P.365）の話によれば、余因子行列を使って$(3.117)$式を証明することもできるらしい。

後半の$(3.92)$の導出は演習問題3.20とほぼ同じなので省略。

## 演習 3.22
<div class="panel-primary">

対数周辺尤度関数

$$
\ln p(\mathbf{t} \mid \alpha, \beta)=\frac{M}{2} \ln \alpha+\frac{N}{2} \ln \beta-E\left(\mathbf{m}_{N}\right)-\frac{1}{2} \ln |\mathbf{A}|-\frac{N}{2} \ln (2 \pi) \tag{3.86}
$$

の$\beta$に関する最大化が再推定方程式

$$
\frac{1}{\beta}=\frac{1}{N-\gamma} \sum_{n=1}^{N}\left\{t_{n}-\mathbf{m}_{N}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right\}^{2} \tag{3.95}
$$

に帰着されることを示すのにすべての段階を，$(3.86)$から始めて確かめよ．

</div>

※P.168をなぞるだけ

$(3.86)$の$\ln p(\mathsf{t}\mid \alpha, \beta)$を$\beta$で偏微分する。準備として、$\displaystyle \frac{\partial}{\partial \beta}\ln |\mathbf{A}|$について、$\beta$と$\lambda_i$は比例するので$\displaystyle \frac{\partial \lambda_i}{\partial \beta} = \frac{\lambda_i}{\beta}$より

$$
\frac{d}{d \beta} \ln |\mathbf{A}|=\frac{d}{d \beta} \sum_{i} \ln \left(\lambda_{i}+\alpha\right)=\frac{1}{\beta} \sum_{i} \frac{\lambda_{i}}{\lambda_{i}+\alpha}=\frac{\gamma}{\beta}
$$
が得られる。したがって, 周辺尤度の停留点は
$$
0=\frac{N}{2 \beta}-\frac{1}{2} \sum_{n=1}^{N}\left\{t_{n}-\mathbf{m}_{N}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right\}^{2}-\frac{\gamma}{2 \beta} \tag{3.94}
$$
これを整理すれば
$$
\frac{1}{\beta}=\frac{1}{N-\gamma} \sum_{n=1}^{N}\left\{t_{n}-\mathbf{m}_{N}^{\mathrm{T}} \boldsymbol{\phi}\left(\mathbf{x}_{n}\right)\right\}^{2} \tag{3.95}
$$
が得られる。

> $\mathbf{m}_N$は$\alpha, \beta$に依存しているので本当は$E(\mathbf{m}_N)$も$\beta$で偏微分するともっと複雑な式になるが（$\partial \mathbf{m}_N/\partial \beta$の項を考える必要が出てくる）、P.168の$\alpha$のときのように繰り返し法で解くことを想定しているので$\mathbf{m}_N$の$\beta$依存性は考慮しなくてよいことになっている。

## 演習 3.23
<div class="panel-primary">

演習問題3.12で説明したモデルに対するデータの周辺確率（言い換えるとモデルエビデンス）が

$$
p(\mathsf{t})=\frac{1}{(2 \pi)^{N / 2}} \frac{b_{0}^{a_{0}}}{b_{N}^{a_{N}}} \frac{\Gamma\left(a_{N}\right)}{\Gamma\left(a_{0}\right)} \frac{\left|\mathbf{S}_{N}\right|^{1 / 2}}{\left|\mathbf{S}_{0}\right|^{1 / 2}} \tag{3.118}
$$

で与えられることを示せ．まず最初に$\mathbf{w}$に関して周辺化し，そして次に$\beta$に関して周辺化するとよい．

</div>

ベイズの定理と周辺確率から
$$
p(t)=\iint p(\mathsf{t}, \mathbf{w}, \beta) d\mathbf{w} d \beta=\iint p(\mathsf{t} \mid \mathbf{w}, \beta) p(\mathbf{w}, \beta) d\mathbf{w} d\beta
$$
と書くことができる。
演習問題3.12でやったように、$p(\mathsf{t} \mid \mathbf{w}, \beta)$は尤度関数、$p(\mathbf{w},\beta)$は共役事前分布となる（正規-ガンマ分布）。

$p(\mathsf{t} \mid \mathbf{w}, \beta)$は演習問題3.16でやったように$\mathcal{N}(\mathsf{t}\mid \mathbf{\Phi}\mathbf{w},\beta^{-1}\mathbf{I}_N)$と表すことができる。よってこれらの式を使うと、
$$
\begin{aligned}
p(\mathsf{t}) &=\iint \mathcal{N}\left(\mathsf{t} \mid \mathbf{\Phi} \mathbf{w}, \beta^{-1} \mathbf{I}_{N}\right) \mathcal{N}\left(\mathbf{w} \mid \mathbf{m}_{0}, \beta^{-1}  \mathbf{S}_{0}\right) {\operatorname{Gam}}\left(\beta \mid a_{0}, b_{0}\right) d \mathbf{w} d \beta \\
&=\iint\left(\frac{\beta}{2 \pi}\right)^{\frac{N}{2}} \exp \left\{-\frac{\beta}{2}(\mathsf{t}-\mathbf{\Phi} \mathbf{w})^{\mathrm{T}}(\mathsf{t}-\mathbf{\Phi} \mathbf{w})\right\}\left(\frac{\beta}{2 \pi}\right)^{\frac{M}{2}} \frac{1}{\left| \mathbf{S}_{0}\right|^{\frac{1}{2}}} \exp \left\{-\frac{\beta}{2}\left(\mathbf{w}-\mathbf{m}_{0}\right)^{\mathrm{T}}  \mathbf{S}_{0}^{-1}\left(\mathbf{w}-\mathbf{m}_{0}\right)\right\}\Gamma\left(a_{0}\right)^{-1} b_{0}^{a_{0}} \beta^{a_{0}-1} \exp \left(-b_{0} \beta\right) d \mathbf{w} d \beta \\
&=\frac{b_{0}^{a_{0}}}{\Gamma\left(a_{0}\right)(2 \pi)^{\frac{M+N}{2}}\left| \mathbf{S}_{0}\right|^{\frac{1}{2}}} \iint \beta^{\frac{M}{2}+\frac{N}{2}+a_{0}-1} \exp \left[-\frac{\beta}{2}\left\{\mathbf{w}^{\mathrm{T}}\left(\mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi}+ \mathbf{S}_{0}^{-1}\right) \mathbf{w}-2 \mathbf{w}^{\mathrm{T}}\left(\mathbf{\Phi}^{\mathrm{T}} \mathsf{t}+ \mathbf{S}_{0}^{-1} \mathbf{m}_{0}\right)\right\}\right] \exp \left[-\frac{\beta}{2}\left(\mathsf{t}^{\mathrm{T}} \mathsf{t}+\mathbf{m}_{0}^{\mathrm{T}}  \mathbf{S}_{0}^{-1} \mathbf{m}_{0}\right)\right] \exp \left(-b_{0} \beta\right) d \mathbf{w} d \beta
\end{aligned}
$$
演習問題3.12で求めた$\mathbf{S}_{N}^{-1}=\mathbf{\Phi}^{\mathrm T} \mathbf{\Phi}+\mathbf{S}_{0}^{-1}, \quad \mathbf{m}_{N}=\mathbf{S}_{N}\left(\mathbf{\Phi}^{\mathrm T} \mathsf{t}+\mathbf{S}_{0}^{-1} \mathbf{m}_{0}\right)$（※教科書$(3.50), (3.51)$のものとは異なるので注意）を使ってこれを書き換えると

$$
\begin{aligned}
p(\mathsf{t})&=\frac{b_{0}^{a_{0}}}{\Gamma\left(a_{0}\right)(2 \pi)^{\frac{M+N}{2}}\left| \mathbf{S}_{0}\right|^{\frac{1}{2}}} \iint \beta^{\frac{M}{2}+\frac{N}{2}+a_{0}-1} \exp \left[-\frac{\beta}{2}\left\{\mathbf{w}^{\mathrm{T}}  \mathbf{S}_{N}^{-1} \mathbf{w}-2 \mathbf{w}^{\mathrm{T}}  \mathbf{S}_{N}^{-1} \mathbf{m}_{N}\right\}\right] \exp \left[-\frac{\beta}{2}\left(\mathsf{t}^{\mathrm{T}} \mathsf{t}+\mathbf{m}_{0}^{\mathrm{T}}  \mathbf{S}_{0}^{-1} \mathbf{m}_{0}\right)\right] \exp \left(-b_{0} \beta\right) d \mathbf{w} d \beta \\
&=\frac{b_{0}^{a_{0}}}{\Gamma\left(a_{0}\right)(2 \pi)^{\frac{M+N}{2}}\left| \mathbf{S}_{0}\right|^{\frac{1}{2}}} \iint \beta^{\frac{M}{2}+\frac{N}{2}+a_{0}-1} \exp \left[-\frac{\beta}{2}\left(\mathbf{w}-\mathbf{m}_{N}\right)^{\mathrm{T}}  \mathbf{S}_{N}^{-1}\left(\mathbf{w}-\mathbf{m}_{N}\right)\right] \exp \left[-\frac{\beta}{2}\left(\mathsf{t}^{\mathrm{T}} \mathsf{t}+\mathbf{m}_{0}^{\mathrm{T}}  \mathbf{S}_{0}^{-1} \mathbf{m}_{0}-\mathbf{m}_{N}^{\mathrm{T}}  \mathbf{S}_{N}^{-1} \mathbf{m}_{N}\right)\right] \exp \left(-b_{0} \beta\right) d \mathbf{w} d \beta \\
\end{aligned}
$$
そしてさらに演習問題3.12で求めた$\displaystyle a_{N}=a_{0}+\frac{N}{2},\quad b_{N}=b_{0}+\frac{1}{2}\left(\mathbf{m}_{0}^{\mathrm{T}} \mathbf{S}_{0}^{-1} \mathbf{m}_{0}-\mathbf{m}_{N}^{\mathrm{T}} \mathbf{S}_{N}^{-1} \mathbf{m}_{N}+\mathsf{t}^{\mathrm{T}} \mathsf{t}\right)$を使うと

$$
\begin{aligned}
p(\mathsf{t}) &= \underbrace{\frac{\beta^{\frac{M}{2}}}{(2 \pi)^{\frac{M}{2}}|\mathbf{S}_N|^{\frac{1}{2}}} \int \exp \left[-\frac{\beta}{2}\left(\mathbf{w}-\mathbf{m}_{N}\right)^{\mathrm{T}} \mathbf{S}_{N}^{-1}\left(\mathbf{w}-\mathbf{m}_{N}\right)\right] d \mathbf{w}}_{\text{Normal distribution, equal to 1}} \cdot \frac{\left|\mathbf{S}_{N}\right|^{\frac{1}{2}} b_{0}^{a_{0}}}{\Gamma\left(a_{0}\right)(2 \pi)^{\frac{N}{2}}\left|\mathbf{S}_{0}\right|^{\frac{1}{2}}} \underbrace{\int \beta^{a_{N}-1} \exp \left(-b_{N} \beta\right) d \beta}_{\text{Gamma distribution (not normalized)}} \\
&= \frac{b_{0}^{a_{0}}\left|\mathbf{S}_{N}\right|^{\frac{1}{2}}}{\Gamma\left(a_{0}\right)(2 \pi)^{\frac{N}{2}}\left|\mathbf{S}_{0}\right|^{\frac{1}{2}}} \frac{\Gamma\left(a_{N}\right)}{b_{N}^{a_{N}}} \\
&=\frac{1}{(2 \pi)^{N / 2}} \frac{b_{0}^{a_{0}}}{b_{N}^{a_{N}}} \frac{\Gamma\left(a_{N}\right)}{\Gamma\left(a_{0}\right)} \frac{\left|\mathbf{S}_{N}\right|^{1 / 2}}{\left|\mathbf{S}_{0}\right|^{1 / 2}}
\end{aligned}
$$

以上から$(3.118)$式となることが示された。

## 演習 3.24
<div class="panel-primary">

次の形のベイズの定理に事前，事後分布と尤度関数を代入して上記の$(3.118)$が成立することを示せ．

$$
p(\mathsf{t})=\frac{p(\mathsf{t} | \mathbf{w}, \beta) p(\mathbf{w}, \beta)}{p(\mathbf{w}, \beta | \mathsf{t})} \tag{3.119}
$$

</div>

$(3.119)$の分母は演習問題3.12の$(3.113)$の$p(\mathbf{w}, \beta | \mathbf{t})=\mathcal{N}\left(\mathbf{w} | \mathbf{m}_{N}, \beta^{-1} \mathbf{S}_{N}\right) \operatorname{Gam}\left(\beta | a_{N}, b_{N}\right)$で、分子の$p(\mathsf{t} | \mathbf{w}, \beta)$は演習問題3.16の$\mathcal{N}(\mathsf{t}\mid \mathbf{\Phi}\mathbf{w},\beta^{-1}\mathbf{I}_N)$で、$p(\mathbf{w}, \beta)$は$\mathcal{N}\left(\mathbf{w} \mid \mathbf{m}_{0}, \beta^{-1}  \mathbf{S}_{0}\right) {\operatorname{Gam}}\left(\beta \mid a_{0}, b_{0}\right)$で、それぞれ与えられる。これらを代入して展開する。まず分母について計算すると

$$
\begin{aligned}
p(\mathbf{w}, \beta | \mathbf{t})&=\mathcal{N}\left(\mathbf{w} | \mathbf{m}_{N}, \beta^{-1} \mathbf{S}_{N}\right) \operatorname{Gam}\left(\beta | a_{N}, b_{N}\right) \\
&=\left(\frac{\beta}{2 \pi}\right)^{M / 2}\left|\mathbf{S}_{N}\right|^{-1 / 2} \exp \left(-\frac{\beta}{2}\left(\mathbf{w}^{\mathrm{T}} \mathbf{S}_{N}^{-1} \mathbf{w}-\mathbf{w}^{\mathrm{T}} \mathbf{S}_{N}^{-1} \mathbf{m}_{N}-\mathbf{m}_{N}^{\mathrm{T}} \mathbf{S}_{N}^{-1} \mathbf{w} +\mathbf{m}_{N}^{\mathrm{T}} \mathbf{S}_{N}^{-1} \mathbf{m}_{N}\right)\right) \Gamma\left(a_{N}\right)^{-1} b_{N}^{a_{N}} \beta^{a_{N}-1} \exp \left(-b_{N} \beta\right) \\
&= \left(\frac{\beta}{2 \pi}\right)^{M / 2}\left|\mathbf{S}_{N}\right|^{-1 / 2} \exp \left(-\frac{\beta}{2}\left(\mathbf{w}^{\mathrm{T}} \mathbf{S}_{0}^{-1} \mathbf{w}+\mathbf{w}^{\mathrm{T}} \mathbf{\Phi}^{\mathrm{T}} \mathbf{\Phi} \mathbf{w}-\mathbf{w}^{\mathrm{T}} \mathbf{S}_{0}^{-1} \mathbf{m}_{0} - \mathbf{w}^{\mathrm{T}} \mathbf{\Phi}^{\mathrm{T}} \mathbf{t}-\mathbf{m}_{0}^{\mathrm{T}} \mathbf{S}_{N}^{-1} \mathbf{w}-\mathbf{t}^{\mathrm{T}} \mathbf{\Phi} \mathbf{w}+\mathbf{m}_{N}^{\mathrm{T}} \mathbf{S}_{N}^{-1} \mathbf{m}_{N}\right)\right) \\ &\quad \ \Gamma\left(a_{N}\right)^{-1} b_{N}^{a_{N}} \beta^{a_{0}+N / 2-1} \exp \left(-\left(b_{0}+\frac{1}{2}\left(\mathbf{m}_{0}^{\mathrm{T}} \mathbf{S}_{0}^{-1} \mathbf{m}_{0}-\mathbf{m}_{N}^{\mathrm{T}} \mathbf{S}_{N}^{-1} \mathbf{m}_{N}+\mathbf{t}^{\mathrm{T}} \mathbf{t}\right)\right) \beta\right) \\
&=\left(\frac{\beta}{2 \pi}\right)^{M / 2}\left|\mathbf{S}_{N}\right|^{-1 / 2} \exp \left(-\frac{\beta}{2}\left(\left(\mathbf{w}-\mathbf{m}_{0}\right)^{\mathrm{T}} \mathbf{S}_{0}^{-1}\left(\mathbf{w}-\mathbf{m}_{0}\right)+\|\mathbf{t}-\Phi \mathbf{w}\|^{2}\right)\right) \Gamma\left(a_{N}\right)^{-1} b_{N}^{a_{N}} \beta^{a_{0}+N / 2-1} \exp \left(-b_{0} \beta\right)
\end{aligned}
$$

一方で分子は
$$
\begin{aligned}
p(\mathsf{t} | \mathbf{w}, \beta) p(\mathbf{w}, \beta) &= \left(\frac{\beta}{2 \pi}\right)^{N / 2} \exp \left(-\frac{\beta}{2}\|\mathbf{t}-\Phi \mathbf{w}\|^{2}\right) \left(\frac{\beta}{2 \pi}\right)^{M / 2}\left|\mathbf{S}_{0}\right|^{-1 / 2} \exp \left(-\frac{\beta}{2}\left(\mathbf{w}-\mathbf{m}_{0}\right)^{\mathrm{T}} \mathbf{S}_{0}^{-1}\left(\mathbf{w}-\mathbf{m}_{0}\right)\right) \\
&\quad\ \Gamma\left(a_{0}\right)^{-1} b_{0}^{a_{0}} \beta^{a_{0}-1} \exp \left(-b_{0} \beta\right)
\end{aligned}
$$

よってこれらを用いて約分すると

$$
\begin{aligned}
p(\mathsf{t}) &= \frac{p(\mathsf{t} | \mathbf{w}, \beta) p(\mathbf{w}, \beta)}{p(\mathbf{w}, \beta | \mathsf{t})} \\
&= \frac{\left(\frac{\beta}{2 \pi}\right)^{N / 2}\left|\mathbf{S}_{0}\right|^{-1 / 2}\Gamma\left(a_{0}\right)^{-1} b_{0}^{a_{0}} \beta^{a_{0}-1}}{\left|\mathbf{S}_{N}\right|^{-1 / 2}\Gamma\left(a_{N}\right)^{-1} b_{N}^{a_{N}} \beta^{a_{0}+N / 2-1}} \\
&= \frac{1}{(2\pi)^{N/2}}\frac{\left|\mathbf{S}_{0}\right|^{-1 / 2}\Gamma\left(a_{0}\right)^{-1} b_{0}^{a_{0}}}{\left|\mathbf{S}_{N}\right|^{-1 / 2}\Gamma\left(a_{N}\right)^{-1} b_{N}^{a_{N}}} \\
&=\frac{1}{(2 \pi)^{N / 2}} \frac{b_{0}^{a_{0}}}{b_{N}^{a_{N}}} \frac{\Gamma\left(a_{N}\right)}{\Gamma\left(a_{0}\right)} \frac{\left|\mathbf{S}_{N}\right|^{1 / 2}}{\left|\mathbf{S}_{0}\right|^{1 / 2}}
\end{aligned}
$$

となり、$(3.118)$式が得られることが確認された。
